
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>6.3.1 Nadaraya Watson model</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>00_6.3.1_Nadaraya-Watson_model</h1>
<pre><code>Lecture: 6_Kernel_Methods/6.3_Radial_Basis_Function_Networks
Content: 00_6.3.1_Nadaraya-Watson_model
</code></pre>
<h2>详细分析第6.3.1节：Nadaraya-Watson模型</h2>
<h3>引言</h3>
<p>Nadaraya-Watson模型是非参数回归的一种经典方法，通过核密度估计对输入数据进行平滑处理，预测新的输入值。该方法在处理具有高噪声和非线性关系的数据时表现出色。第6.3.1节详细介绍了这一模型的原理和应用。</p>
<h3>基本概念</h3>
<p>Nadaraya-Watson模型基于核密度估计，通过对输入点的加权平均来预测输出值。具体来说，对于一个给定的输入点 $ x $，其预测值 $ \hat{y}(x) $ 可以表示为：
$$ \hat{y}(x) = \frac{\sum_{i=1}^{N} K(x, x_i) y_i}{\sum_{i=1}^{N} K(x, x_i)} $$
其中，$ K(x, x_i) $ 是核函数，表示输入点 $ x $ 和训练数据点 $ x_i $ 之间的相似度，常用的核函数包括高斯核、拉普拉斯核等。</p>
<h3>核函数的选择</h3>
<p>核函数的选择对模型的性能有显著影响。常见的核函数有：</p>
<ol>
<li>
<p><strong>高斯核</strong>：
$$ K(x, x_i) = \exp \left( -\frac{|x - x_i|^2}{2\sigma^2} \right) $$
高斯核对数据点之间的距离进行了指数衰减，使得距离越近的点对预测的贡献越大。</p>
</li>
<li>
<p><strong>拉普拉斯核</strong>：
$$ K(x, x_i) = \exp \left( -\frac{|x - x_i|}{\sigma} \right) $$
拉普拉斯核也是一种常用的核函数，与高斯核类似，但其对距离的衰减速度不同。</p>
</li>
</ol>
<h3>模型训练与预测</h3>
<p>Nadaraya-Watson模型的训练过程非常简单，只需保存训练数据即可。在预测时，通过计算核函数来确定输入点与每个训练数据点的相似度，然后进行加权平均得到预测值。</p>
<h4>步骤</h4>
<ol>
<li><strong>训练阶段</strong>：保存训练数据 ${(x_i, y_i)}_{i=1}^{N}$。</li>
<li><strong>预测阶段</strong>：对于新的输入点 $ x $，计算每个训练点的核函数值 $ K(x, x_i) $，然后通过加权平均计算预测值 $ \hat{y}(x) $。</li>
</ol>
<h3>优势与局限</h3>
<h4>优势</h4>
<ul>
<li><strong>非参数性</strong>：Nadaraya-Watson模型不假设数据的分布形式，适用于多种复杂的分布情况。</li>
<li><strong>平滑性</strong>：通过核函数的平滑效果，可以有效处理数据中的噪声。</li>
</ul>
<h4>局限</h4>
<ul>
<li><strong>计算成本</strong>：对每个预测点，都需要计算与所有训练数据点的核函数值，计算量较大。</li>
<li><strong>边界效应</strong>：在输入空间的边界处，由于缺乏足够的邻近点，模型预测可能不准确。</li>
</ul>
<h3>实例分析</h3>
<p>假设我们有一个一维回归问题，输入变量为 $ x $，目标变量为 $ y $。我们生成一个包含噪声的非线性数据集，并使用Nadaraya-Watson模型进行预测。</p>
<h4>数据生成</h4>
<p>生成的数据如下：
$$ x_i \sim \mathcal{U}(0, 10) $$
$$ y_i = \sin(x_i) + \epsilon_i $$
其中，$ \epsilon_i \sim \mathcal{N}(0, 0.1) $ 是高斯噪声。</p>
<h4>模型训练与预测</h4>
<ol>
<li><strong>训练</strong>：保存生成的数据 ${(x_i, y_i)}_{i=1}^{N}$。</li>
<li><strong>预测</strong>：对于新的输入点 $ x $，计算高斯核函数 $ K(x, x_i) $，并进行加权平均得到预测值 $ \hat{y}(x) $。</li>
</ol>
<h4>结果分析</h4>
<p>绘制输入 $ x $ 和目标 $ y $ 的散点图，并叠加Nadaraya-Watson模型的预测结果。通过观察，可以发现模型能够很好地捕捉数据的非线性关系，同时对噪声有较好的平滑效果。</p>
<h3>小结</h3>
<p>Nadaraya-Watson模型通过核密度估计，实现了对非线性和高噪声数据的有效建模。其简单直观的计算过程，使得该模型在非参数回归中具有重要应用。然而，计算成本和边界效应仍需在实际应用中加以注意和优化。</p>

    <h3>Python 文件</h3>
    <pre><code># 00_6.3.1_Nadaraya-Watson_model

"""
Lecture: 6_Kernel_Methods/6.3_Radial_Basis_Function_Networks
Content: 00_6.3.1_Nadaraya-Watson_model
"""

</code></pre>
  </div>
</body>
</html>
  