
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>4.3.2 Logistic regression</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>01_4.3.2_Logistic_regression</h1>
<pre><code>Lecture: 4_Linear_Models_for_Classification/4.3_Probabilistic_Discriminative_Models
Content: 01_4.3.2_Logistic_regression
</code></pre>
<h2>详解PRML中的4.3.2节：逻辑回归</h2>
<p>《模式识别与机器学习》（Pattern Recognition and Machine Learning, PRML）是由Christopher M. Bishop所著的一本经典教材，其中第4章涵盖了线性分类模型的内容。在第4.3节，作者介绍了概率判别模型（Probabilistic Discriminative Models）。具体来说，第4.3.2节探讨了逻辑回归（Logistic Regression）。以下是对这一节内容的详细分析。</p>
<h3>逻辑回归的背景</h3>
<p>逻辑回归是一种广泛应用于二分类问题的判别模型。与生成模型不同，判别模型直接建模条件概率 $p(C_k|x)$，并不关心数据的生成机制。逻辑回归通过将输入变量的线性组合映射到 $[0, 1]$ 区间，用于估计类别的后验概率。</p>
<h3>逻辑回归模型</h3>
<p>逻辑回归模型假设类别 $C_1$ 的后验概率 $p(C_1|\phi)$ 可以表示为：</p>
<p>$$ p(C_1|\phi) = y(\phi) = \sigma(w^T\phi) $$</p>
<p>其中，$\sigma(z)$ 是逻辑Sigmoid函数，定义为：</p>
<p>$$ \sigma(z) = \frac{1}{1 + \exp(-z)} $$</p>
<p>对于类别 $C_2$，我们有：</p>
<p>$$ p(C_2|\phi) = 1 - p(C_1|\phi) $$</p>
<h4>线性函数的表示</h4>
<p>对于一个M维的特征空间 $\phi$，该模型有M个可调参数。线性函数表示为：</p>
<p>$$ a = w^T\phi $$</p>
<p>其中，$w$ 是权重向量，$\phi$ 是特征向量。</p>
<h3>最大似然估计</h3>
<p>为了确定逻辑回归模型的参数，我们使用最大似然估计。首先，定义数据集 ${(\phi_n, t_n)}$，其中 $t_n \in {0, 1}$ 表示类别标签，$\phi_n = \phi(x_n)$ 表示特征向量。似然函数可以写为：</p>
<p>$$ p(t|w) = \prod_{n=1}^{N} y_n^{t_n} (1 - y_n)^{1 - t_n} $$</p>
<p>其中，$ y_n = p(C_1|\phi_n) $。取对数后得到对数似然函数：</p>
<p>$$ \ln p(t|w) = \sum_{n=1}^{N} {t_n \ln y_n + (1 - t_n) \ln (1 - y_n)} $$</p>
<h3>交叉熵损失函数</h3>
<p>通过取负对数似然函数，我们得到交叉熵损失函数：</p>
<p>$$ E(w) = -\ln p(t|w) = -\sum_{n=1}^{N} {t_n \ln y_n + (1 - t_n) \ln (1 - y_n)} $$</p>
<h3>梯度下降法</h3>
<p>为了最小化损失函数，我们可以使用梯度下降法。损失函数关于 $w$ 的梯度为：</p>
<p>$$ \nabla E(w) = \sum_{n=1}^{N} (y_n - t_n) \phi_n $$</p>
<p>其中，$ y_n = \sigma(w^T\phi_n) $。我们可以通过以下迭代更新权重：</p>
<p>$$ w \leftarrow w - \eta \nabla E(w) $$</p>
<p>其中，$\eta$ 是学习率。</p>
<h3>逻辑回归的优势</h3>
<ol>
<li><strong>计算效率</strong>：逻辑回归的参数优化通常可以通过梯度下降等数值方法高效地实现。</li>
<li><strong>可解释性强</strong>：模型参数具有明确的物理意义，便于解释和分析。</li>
<li><strong>线性模型的推广</strong>：逻辑回归可以看作是线性回归的推广，通过Sigmoid函数将输出限制在 $[0, 1]$ 区间。</li>
</ol>
<h3>逻辑回归的局限性</h3>
<ol>
<li><strong>线性可分性假设</strong>：逻辑回归假设数据在特征空间中是线性可分的，对于非线性可分的数据，模型表现较差。</li>
<li><strong>易受异常值影响</strong>：由于损失函数的性质，逻辑回归对异常值较为敏感。</li>
</ol>
<h3>扩展到多分类</h3>
<p>对于多分类问题，我们可以使用Softmax回归。Softmax回归将输入映射到多个类别，通过Softmax函数将输出归一化，使其表示类别的后验概率。</p>
<p>通过对PRML第4.3.2节的深入解析，我们可以更好地理解逻辑回归在机器学习中的理论基础和实际应用。掌握这些方法有助于我们在实际问题中选择合适的模型和算法，提高分类和预测的准确性。</p>

    <h3>Python 文件</h3>
    <pre><code># 01_4.3.2_Logistic_regression

"""
Lecture: 4_Linear_Models_for_Classification/4.3_Probabilistic_Discriminative_Models
Content: 01_4.3.2_Logistic_regression
"""

import numpy as np
from typing import Tuple

class LogisticRegression:
    """
    逻辑回归分类器

    Parameters:
    -----------
    learning_rate : float
        学习率 (默认值为 0.01)
    n_iter : int
        训练数据迭代次数 (默认值为 1000)
        
    Attributes:
    -----------
    w_ : np.ndarray
        权重向量
    cost_ : list
        每次迭代中的损失值
    """
    def __init__(self, learning_rate: float = 0.01, n_iter: int = 1000) -> None:
        self.learning_rate = learning_rate
        self.n_iter = n_iter
        self.w_ = None
        self.cost_ = []

    def fit(self, X: np.ndarray, y: np.ndarray) -> None:
        """
        训练逻辑回归分类器
        
        Parameters:
        -----------
        X : np.ndarray, shape = [n_samples, n_features]
            训练向量
        y : np.ndarray, shape = [n_samples]
            目标值
        """
        # 初始化权重向量
        self.w_ = np.zeros(X.shape[1] + 1)
        self.cost_ = []

        X_bias = np.insert(X, 0, 1, axis=1)  # 添加偏置项

        for _ in range(self.n_iter):
            z = np.dot(X_bias, self.w_)
            y_hat = self._sigmoid(z)
            errors = y_hat - y
            gradient = np.dot(X_bias.T, errors)
            self.w_ -= self.learning_rate * gradient
            cost = self._cost_function(y, y_hat)
            self.cost_.append(cost)

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        返回类标预测值
        
        Parameters:
        -----------
        X : np.ndarray
            输入向量
        
        Returns:
        --------
        np.ndarray
            类标预测值
        """
        X_bias = np.insert(X, 0, 1, axis=1)  # 添加偏置项
        z = np.dot(X_bias, self.w_)
        return np.where(self._sigmoid(z) >= 0.5, 1, 0)

    def _sigmoid(self, z: np.ndarray) -> np.ndarray:
        """
        计算Sigmoid函数
        
        Parameters:
        -----------
        z : np.ndarray
            输入值
        
        Returns:
        --------
        np.ndarray
            Sigmoid函数值
        """
        return 1 / (1 + np.exp(-z))

    def _cost_function(self, y: np.ndarray, y_hat: np.ndarray) -> float:
        """
        计算交叉熵损失函数
        
        Parameters:
        -----------
        y : np.ndarray
            真实值
        y_hat : np.ndarray
            预测值
        
        Returns:
        --------
        float
            交叉熵损失值
        """
        return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))

def generate_data(n_samples: int = 100) -> Tuple[np.ndarray, np.ndarray]:
    """
    生成二分类数据集
    
    Parameters:
    -----------
    n_samples : int
        样本数量 (默认值为 100)
        
    Returns:
    --------
    Tuple[np.ndarray, np.ndarray]
        特征矩阵和目标值向量
    """
    np.random.seed(0)
    X1 = np.random.randn(n_samples // 2, 2) + np.array([2, 2])
    X2 = np.random.randn(n_samples // 2, 2) + np.array([-2, -2])
    X = np.vstack((X1, X2))
    y = np.hstack((np.ones(n_samples // 2), np.zeros(n_samples // 2)))
    return X, y

def main() -> None:
    """
    主函数，运行逻辑回归并打印结果
    """
    X, y = generate_data()
    lr = LogisticRegression(learning_rate=0.01, n_iter=1000)
    lr.fit(X, y)
    predictions = lr.predict(X)
    
    print("权重向量 w:")
    print(lr.w_)
    print("每次迭代的损失值:")
    print(lr.cost_)

if __name__ == "__main__":
    main()
</code></pre>
  </div>
</body>
</html>
  