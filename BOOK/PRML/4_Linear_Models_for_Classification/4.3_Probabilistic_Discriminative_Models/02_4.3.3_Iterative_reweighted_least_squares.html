
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>4.3.3 Iterative reweighted least squares</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>02_4.3.3_Iterative_reweighted_least_squares</h1>
<pre><code>Lecture: 4_Linear_Models_for_Classification/4.3_Probabilistic_Discriminative_Models
Content: 02_4.3.3_Iterative_reweighted_least_squares
</code></pre>
<h3>详解PRML中的4.3.3节：迭代重加权最小二乘法</h3>
<p>《模式识别与机器学习》（Pattern Recognition and Machine Learning, PRML）是由Christopher M. Bishop所著的一本经典教材，其中第4章涵盖了线性分类模型的内容。在第4.3节，作者介绍了概率判别模型（Probabilistic Discriminative Models）。具体来说，第4.3.3节探讨了迭代重加权最小二乘法（Iterative Reweighted Least Squares, IRLS）。以下是对这一节内容的详细分析。</p>
<h3>迭代重加权最小二乘法的背景</h3>
<p>在线性回归模型中，假设高斯噪声模型的最大似然解具有闭式解，这是由于对数似然函数相对于参数向量 $w$ 的二次依赖性。对于逻辑回归，由于逻辑Sigmoid函数的非线性，已不存在闭式解。然而，对数似然函数的偏离二次形式并不显著，误差函数是凹的，因此具有唯一最小值。误差函数可以通过一种基于牛顿-拉弗森迭代优化方案的高效迭代技术来最小化，该方案使用对数似然函数的局部二次近似。</p>
<h3>牛顿-拉弗森方法</h3>
<p>牛顿-拉弗森更新公式用于最小化函数 $E(w)$，其形式为：</p>
<p>$$ w_{\text{new}} = w_{\text{old}} - H^{-1} \nabla E(w) $$</p>
<p>其中，$H$ 是Hessian矩阵，其元素包括 $E(w)$ 关于 $w$ 分量的二阶导数。</p>
<h4>应用于线性回归模型</h4>
<p>对于线性回归模型，误差函数的梯度和Hessian为：</p>
<p>$$ \nabla E(w) = \sum_{n=1}^{N} (w^T \phi_n - t_n) \phi_n = \Phi^T \Phi w - \Phi^T t $$</p>
<p>$$ H = \nabla \nabla E(w) = \sum_{n=1}^{N} \phi_n \phi_n^T = \Phi^T \Phi $$</p>
<p>牛顿-拉弗森更新公式为：</p>
<p>$$ w_{\text{new}} = w_{\text{old}} - (\Phi^T \Phi)^{-1} (\Phi^T \Phi w_{\text{old}} - \Phi^T t) = (\Phi^T \Phi)^{-1} \Phi^T t $$</p>
<p>这与标准的最小二乘解相同，因为误差函数在这种情况下是二次的，因此牛顿-拉弗森公式在一步中给出了确切解。</p>
<h4>应用于逻辑回归模型</h4>
<p>对于逻辑回归模型，交叉熵误差函数的梯度和Hessian为：</p>
<p>$$ \nabla E(w) = \sum_{n=1}^{N} (y_n - t_n) \phi_n = \Phi^T (y - t) $$</p>
<p>$$ H = \nabla \nabla E(w) = \sum_{n=1}^{N} y_n (1 - y_n) \phi_n \phi_n^T = \Phi^T R \Phi $$</p>
<p>其中，$y_n$ 是逻辑回归模型的预测，$R$ 是对角矩阵，其元素为：</p>
<p>$$ R_{nn} = y_n (1 - y_n) $$</p>
<p>牛顿-拉弗森更新公式为：</p>
<p>$$ w_{\text{new}} = w_{\text{old}} - (\Phi^T R \Phi)^{-1} \Phi^T (y - t) = (\Phi^T R \Phi)^{-1} \Phi^T R z $$</p>
<p>其中，$z$ 是一个N维向量，其元素为：</p>
<p>$$ z = \Phi w_{\text{old}} - R^{-1} (y - t) $$</p>
<h3>迭代重加权最小二乘法（IRLS）</h3>
<p>IRLS算法用于逻辑回归中，通过迭代更新权重向量 $w$，每次使用新的权重向量 $w$ 计算修正后的加权矩阵 $R$。因此，IRLS算法被称为迭代重加权最小二乘法。</p>
<h3>IRLS的优势</h3>
<ol>
<li><strong>收敛速度快</strong>：由于IRLS使用了Hessian矩阵的二阶信息，通常比简单的梯度下降法收敛更快。</li>
<li><strong>唯一最小值</strong>：误差函数是凹的，因此IRLS方法能找到全局最小值。</li>
<li><strong>适用于广泛的问题</strong>：除了逻辑回归，IRLS还可以应用于其他广泛的广义线性模型。</li>
</ol>
<h3>结论</h3>
<p>通过以上分析可以看出，迭代重加权最小二乘法是一种高效的优化方法，尤其适用于逻辑回归等广义线性模型。它利用牛顿-拉弗森方法，通过迭代更新参数，使得模型能快速收敛到全局最小值。掌握IRLS方法的理论和应用，有助于我们在实际问题中选择合适的模型和算法，提高分类和预测的准确性。</p>

    <h3>Python 文件</h3>
    <pre><code># 02_4.3.3_Iterative_reweighted_least_squares

"""
Lecture: 4_Linear_Models_for_Classification/4.3_Probabilistic_Discriminative_Models
Content: 02_4.3.3_Iterative_reweighted_least_squares
"""

import numpy as np
from scipy.linalg import solve
from typing import Tuple

class IRLSLogisticRegression:
    """
    迭代重加权最小二乘法 (IRLS) 用于逻辑回归分类器

    Parameters:
    -----------
    n_iter : int
        训练数据迭代次数 (默认值为 100)
    tol : float
        收敛阈值 (默认值为 1e-6)

    Attributes:
    -----------
    w_ : np.ndarray
        权重向量
    cost_ : list
        每次迭代中的损失值
    """
    def __init__(self, n_iter: int = 100, tol: float = 1e-6) -> None:
        self.n_iter = n_iter
        self.tol = tol
        self.w_ = None
        self.cost_ = []

    def fit(self, X: np.ndarray, y: np.ndarray) -> None:
        """
        训练逻辑回归分类器
        
        Parameters:
        -----------
        X : np.ndarray, shape = [n_samples, n_features]
            训练向量
        y : np.ndarray, shape = [n_samples]
            目标值
        """
        # 初始化权重向量
        self.w_ = np.zeros(X.shape[1] + 1)
        self.cost_ = []

        X_bias = np.insert(X, 0, 1, axis=1)  # 添加偏置项

        for _ in range(self.n_iter):
            z = np.dot(X_bias, self.w_)
            y_hat = self._sigmoid(z)
            R = np.diag(y_hat * (1 - y_hat))
            gradient = np.dot(X_bias.T, (y_hat - y))
            H = np.dot(np.dot(X_bias.T, R), X_bias)
            delta_w = solve(H, gradient)
            self.w_ -= delta_w
            cost = self._cost_function(y, y_hat)
            self.cost_.append(cost)

            # 检查收敛性
            if np.linalg.norm(delta_w) < self.tol:
                break

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        返回类标预测值
        
        Parameters:
        -----------
        X : np.ndarray
            输入向量
        
        Returns:
        --------
        np.ndarray
            类标预测值
        """
        X_bias = np.insert(X, 0, 1, axis=1)  # 添加偏置项
        z = np.dot(X_bias, self.w_)
        return np.where(self._sigmoid(z) >= 0.5, 1, 0)

    def _sigmoid(self, z: np.ndarray) -> np.ndarray:
        """
        计算Sigmoid函数
        
        Parameters:
        -----------
        z : np.ndarray
            输入值
        
        Returns:
        --------
        np.ndarray
            Sigmoid函数值
        """
        return 1 / (1 + np.exp(-z))

    def _cost_function(self, y: np.ndarray, y_hat: np.ndarray) -> float:
        """
        计算交叉熵损失函数
        
        Parameters:
        -----------
        y : np.ndarray
            真实值
        y_hat : np.ndarray
            预测值
        
        Returns:
        --------
        float
            交叉熵损失值
        """
        return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))

def generate_data(n_samples: int = 100) -> Tuple[np.ndarray, np.ndarray]:
    """
    生成二分类数据集
    
    Parameters:
    -----------
    n_samples : int
        样本数量 (默认值为 100)
        
    Returns:
    --------
    Tuple[np.ndarray, np.ndarray]
        特征矩阵和目标值向量
    """
    np.random.seed(0)
    X1 = np.random.randn(n_samples // 2, 2) + np.array([2, 2])
    X2 = np.random.randn(n_samples // 2, 2) + np.array([-2, -2])
    X = np.vstack((X1, X2))
    y = np.hstack((np.ones(n_samples // 2), np.zeros(n_samples // 2)))
    return X, y

def main() -> None:
    """
    主函数，运行逻辑回归并打印结果
    """
    X, y = generate_data()
    irls = IRLSLogisticRegression(n_iter=100, tol=1e-6)
    irls.fit(X, y)
    predictions = irls.predict(X)
    
    print("权重向量 w:")
    print(irls.w_)
    print("每次迭代的损失值:")
    print(irls.cost_)

if __name__ == "__main__":
    main()
</code></pre>
  </div>
</body>
</html>
  