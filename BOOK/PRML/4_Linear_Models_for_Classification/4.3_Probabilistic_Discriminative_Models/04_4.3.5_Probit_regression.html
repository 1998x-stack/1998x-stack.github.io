
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>4.3.5 Probit regression</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>04_4.3.5_Probit_regression</h1>
<pre><code>Lecture: 4_Linear_Models_for_Classification/4.3_Probabilistic_Discriminative_Models
Content: 04_4.3.5_Probit_regression
</code></pre>
<h3>详解PRML中的4.3.5节：Probit回归</h3>
<p>《模式识别与机器学习》（Pattern Recognition and Machine Learning, PRML）是由Christopher M. Bishop所著的一本经典教材，其中第4章涵盖了线性分类模型的内容。在第4.3节，作者介绍了概率判别模型（Probabilistic Discriminative Models）。具体来说，第4.3.5节探讨了Probit回归（Probit Regression）。以下是对这一节内容的详细分析。</p>
<h3>Probit回归的背景</h3>
<p>在前面的内容中，我们已经了解了逻辑回归（Logistic Regression）如何通过逻辑Sigmoid函数将线性函数映射到类别的后验概率。尽管逻辑回归在很多情况下表现良好，但并不是所有的条件分布都能通过逻辑函数来转换。在这种背景下，引入了Probit回归作为另一种判别概率模型。</p>
<h3>Probit函数</h3>
<p>Probit回归基于累积正态分布函数（Cumulative Normal Distribution Function），即Probit函数。具体形式为：</p>
<p>$$ \Phi(a) = \int_{-\infty}^{a} N(\theta|0, 1) d\theta $$</p>
<p>其中，$ N(\theta|0, 1) $ 是均值为0、方差为1的标准正态分布。</p>
<h3>模型形式</h3>
<p>Probit回归的模型形式为：</p>
<p>$$ p(t=1|a) = \Phi(a) $$</p>
<p>其中，$ a = w^T \phi $，$ \phi $ 是特征向量，$ w $ 是权重向量。</p>
<h3>与逻辑回归的比较</h3>
<p>Probit函数与逻辑Sigmoid函数在形状上非常相似，但Probit函数的尾部衰减速度比逻辑Sigmoid函数快。这意味着Probit模型对离群点（outliers）更加敏感，因为在远离决策边界的地方，Probit函数的值变化更快。</p>
<h4>逻辑回归与Probit回归的比较图</h4>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/ee/Logistic_vs_probit.svg/330px-Logistic_vs_probit.svg.png" alt="Comparison"></p>
<p>图中显示了逻辑Sigmoid函数（红色）和Probit函数（蓝色）的比较，可以看到两者在形状上非常相似，但Probit函数的曲线更加陡峭。</p>
<h3>Probit回归的推导</h3>
<p>Probit回归可以通过最大似然估计（Maximum Likelihood Estimation, MLE）来确定模型参数。具体步骤如下：</p>
<ol>
<li>
<p><strong>定义似然函数</strong></p>
<p>给定数据集 ${(\phi_n, t_n)}$，其中 $t_n \in {0, 1}$ 表示类别标签，$\phi_n$ 表示特征向量。似然函数为：</p>
<p>$$ p(t|w) = \prod_{n=1}^{N} \Phi(a_n)^{t_n} [1 - \Phi(a_n)]^{1 - t_n} $$</p>
</li>
<li>
<p><strong>对数似然函数</strong></p>
<p>取对数后得到对数似然函数：</p>
<p>$$ \ln p(t|w) = \sum_{n=1}^{N} {t_n \ln \Phi(a_n) + (1 - t_n) \ln (1 - \Phi(a_n))} $$</p>
</li>
<li>
<p><strong>梯度下降法</strong></p>
<p>通过梯度下降法最小化负对数似然函数，得到模型参数 $ w $。梯度为：</p>
<p>$$ \nabla \ln p(t|w) = \sum_{n=1}^{N} (t_n - \Phi(a_n)) \phi_n $$</p>
</li>
</ol>
<h3>实际应用</h3>
<ol>
<li>
<p><strong>分类问题</strong></p>
<p>Probit回归主要用于二分类问题，与逻辑回归类似。它在金融风险评估、医学诊断等领域有广泛应用。</p>
</li>
<li>
<p><strong>离群点检测</strong></p>
<p>由于Probit回归对离群点更加敏感，因此在一些需要检测离群点的应用中表现优异。</p>
</li>
</ol>
<h3>优势与局限性</h3>
<p><strong>优势</strong>：</p>
<ol>
<li><strong>数学上的优雅性</strong>：Probit函数基于正态分布，有良好的数学性质。</li>
<li><strong>对小概率事件的处理</strong>：在处理极端概率事件时，Probit模型表现更好。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>计算复杂性</strong>：Probit函数的计算相对复杂，尤其是在高维数据中。</li>
<li><strong>对离群点敏感</strong>：虽然这一点在某些情况下是优势，但在数据噪声较大的情况下可能导致模型不稳定。</li>
</ol>
<h3>结论</h3>
<p>通过以上分析可以看出，Probit回归是一种强大的概率判别模型，特别适用于一些特殊的分类问题。它与逻辑回归在很多方面相似，但在处理极端概率事件和离群点检测方面表现优异。掌握Probit回归的理论和应用，有助于我们在实际问题中选择合适的模型和算法，提高分类和预测的准确性。</p>

    <h3>Python 文件</h3>
    <pre><code># 04_4.3.5_Probit_regression

"""
Lecture: 4_Linear_Models_for_Classification/4.3_Probabilistic_Discriminative_Models
Content: 04_4.3.5_Probit_regression
"""

import numpy as np
from scipy.stats import norm
from scipy.optimize import minimize
from typing import Tuple

class ProbitRegression:
    """
    Probit 回归分类器

    Parameters:
    -----------
    max_iter : int
        训练数据迭代次数 (默认值为 100)
    tol : float
        收敛阈值 (默认值为 1e-6)

    Attributes:
    -----------
    w_ : np.ndarray
        权重向量
    """
    def __init__(self, max_iter: int = 100, tol: float = 1e-6) -> None:
        self.max_iter = max_iter
        self.tol = tol
        self.w_ = None

    def fit(self, X: np.ndarray, y: np.ndarray) -> None:
        """
        训练 Probit 回归分类器
        
        Parameters:
        -----------
        X : np.ndarray, shape = [n_samples, n_features]
            训练向量
        y : np.ndarray, shape = [n_samples]
            目标值
        """
        # 初始化权重向量
        self.w_ = np.zeros(X.shape[1] + 1)

        X_bias = np.insert(X, 0, 1, axis=1)  # 添加偏置项

        def neg_log_likelihood(w):
            z = np.dot(X_bias, w)
            likelihood = y * np.log(norm.cdf(z)) + (1 - y) * np.log(1 - norm.cdf(z))
            return -np.sum(likelihood)

        def grad_neg_log_likelihood(w):
            z = np.dot(X_bias, w)
            pdf = norm.pdf(z)
            cdf = norm.cdf(z)
            gradient = np.dot(X_bias.T, (y - cdf) * pdf / (cdf * (1 - cdf)))
            return -np.sum(gradient, axis=1)

        result = minimize(neg_log_likelihood, self.w_, jac=grad_neg_log_likelihood, 
                          options={'maxiter': self.max_iter, 'disp': True, 'gtol': self.tol})
        self.w_ = result.x

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        返回类标预测值
        
        Parameters:
        -----------
        X : np.ndarray
            输入向量
        
        Returns:
        --------
        np.ndarray
            类标预测值
        """
        X_bias = np.insert(X, 0, 1, axis=1)  # 添加偏置项
        z = np.dot(X_bias, self.w_)
        return np.where(norm.cdf(z) >= 0.5, 1, 0)

def generate_data(n_samples: int = 100) -> Tuple[np.ndarray, np.ndarray]:
    """
    生成二分类数据集
    
    Parameters:
    -----------
    n_samples : int
        样本数量 (默认值为 100)
        
    Returns:
    --------
    Tuple[np.ndarray, np.ndarray]
        特征矩阵和目标值向量
    """
    np.random.seed(0)
    X1 = np.random.randn(n_samples // 2, 2) + np.array([2, 2])
    X2 = np.random.randn(n_samples // 2, 2) + np.array([-2, -2])
    X = np.vstack((X1, X2))
    y = np.hstack((np.ones(n_samples // 2), np.zeros(n_samples // 2)))
    return X, y

def main() -> None:
    """
    主函数，运行 Probit 回归并打印结果
    """
    X, y = generate_data()
    probit = ProbitRegression(max_iter=100, tol=1e-6)
    probit.fit(X, y)
    predictions = probit.predict(X)
    
    print("权重向量 w:")
    print(probit.w_)

if __name__ == "__main__":
    main()
</code></pre>
  </div>
</body>
</html>
  