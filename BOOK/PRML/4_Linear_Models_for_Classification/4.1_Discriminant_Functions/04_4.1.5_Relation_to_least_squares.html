
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>4.1.5 Relation to least squares</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>04_4.1.5_Relation_to_least_squares</h1>
<pre><code>Lecture: 4_Linear_Models_for_Classification/4.1_Discriminant_Functions
Content: 04_4.1.5_Relation_to_least_squares
</code></pre>
<h2>探索PRML.pdf中的知识</h2>
<p>PRML (Pattern Recognition and Machine Learning) 是Christopher M. Bishop所著的一本经典教材，涵盖了模式识别和机器学习领域的核心知识。在本书中，第4章介绍了线性分类模型，包括判别函数、最小二乘法分类、Fisher线性判别、感知器算法等。在第4.1.5节中，作者详细讨论了最小二乘法与Fisher线性判别之间的关系。</p>
<h3>第4.1.5节：最小二乘法与Fisher线性判别的关系</h3>
<p>在这一节中，作者探讨了最小二乘法求解线性判别的思路与Fisher线性判别方法的关系，特别是针对两类分类问题。通过对不同目标编码方案的讨论，作者展示了如何通过最小二乘法得到与Fisher线性判别相同的解。</p>
<h2>深入分析</h2>
<h3>背景介绍</h3>
<p>最小二乘法（Least Squares, LS）是一种经典的回归分析方法，通常用于最小化模型预测值与实际观察值之间的平方误差。在分类问题中，最小二乘法可以用来拟合分类模型，尽管这种方法在分类精度和鲁棒性上可能存在一定的缺陷。</p>
<p>Fisher线性判别（Fisher's Linear Discriminant, FLD）是一种用于寻找能够最大化类间方差和最小化类内方差的投影方向的技术。这种方法通过最大化类间散布矩阵与类内散布矩阵之比来找到最佳投影方向。</p>
<h3>最小二乘法分类与Fisher线性判别</h3>
<p>在分类问题中，我们通常使用目标值的1-of-K编码方案，其中K是类别数。然而，作者提出了一种不同的目标编码方案，通过这种方案可以使得最小二乘法的解与Fisher线性判别的解一致。</p>
<ol>
<li>
<p><strong>目标编码方案</strong>：</p>
<ul>
<li>对于类C1，目标值设为 $ \frac{N}{N1} $，其中N1是类C1中的样本数，N是总样本数。</li>
<li>对于类C2，目标值设为 $ -\frac{N}{N2} $，其中N2是类C2中的样本数。</li>
</ul>
</li>
<li>
<p><strong>最小二乘法求解</strong>：</p>
<ul>
<li>最小二乘法的目标是最小化以下平方误差函数：
$$
E = \frac{1}{2} \sum_{n=1}^{N} (w^T x_n + w_0 - t_n)^2
$$</li>
<li>对误差函数E分别对 $ w_0 $ 和 $ w $ 求导，并令其为零，得到两个方程：
$$
\sum_{n=1}^{N} (w^T x_n + w_0 - t_n) = 0
$$
$$
\sum_{n=1}^{N} (w^T x_n + w_0 - t_n) x_n = 0
$$</li>
<li>通过代入目标编码方案，可以得到偏置 $ w_0 $ 的表达式：
$$
w_0 = -w^T m
$$
其中，$ m $ 是总数据集的均值：
$$
m = \frac{1}{N} \sum_{n=1} x_n = \frac{N1 m1 + N2 m2}{N}
$$</li>
</ul>
</li>
<li>
<p><strong>求解权重向量</strong>：</p>
<ul>
<li>将上述偏置 $ w_0 $ 代入第二个方程，并通过一些代数运算，最终得到以下形式的方程：
$$
(SW + \frac{N1 N2}{N} SB) w = N (m1 - m2)
$$
其中，$ SW $ 是类内散布矩阵，$ SB $ 是类间散布矩阵。</li>
<li>通过简化，可以得到权重向量 $ w $ 的解：
$$
w \propto S^{-1}_W (m2 - m1)
$$
这与Fisher判别的解相同。</li>
</ul>
</li>
</ol>
<h3>结论</h3>
<p>通过以上分析可以看出，在特定的目标编码方案下，最小二乘法求解的线性判别函数与Fisher线性判别法的解是一致的。这表明，尽管两种方法的出发点和优化目标不同，但在某些情况下，它们可以导出相同的分类决策边界。</p>
<p>这种分析为理解不同机器学习方法之间的关系提供了有价值的视角，有助于我们在实际应用中选择合适的模型和算法。</p>

    <h3>Python 文件</h3>
    <pre><code># 04_4.1.5_Relation_to_least_squares

"""
Lecture: 4_Linear_Models_for_Classification/4.1_Discriminant_Functions
Content: 04_4.1.5_Relation_to_least_squares
"""

import numpy as np
from typing import Tuple

class LeastSquaresFisherLDA:
    """
    最小二乘法与Fisher线性判别分析 (LDA) 分类器
    
    Parameters:
    -----------
    None
    
    Attributes:
    -----------
    w_ : np.ndarray
        权重向量
    w0_ : float
        偏置
    """
    def __init__(self) -> None:
        self.w_ = None
        self.w0_ = None

    def fit(self, X: np.ndarray, y: np.ndarray) -> None:
        """
        训练分类器
        
        Parameters:
        -----------
        X : np.ndarray, shape = [n_samples, n_features]
            训练向量
        y : np.ndarray, shape = [n_samples]
            目标值
        """
        # 计算总均值
        mean_overall = np.mean(X, axis=0)
        
        # 计算类均值
        mean_vectors = []
        class_labels = np.unique(y)
        for label in class_labels:
            mean_vectors.append(np.mean(X[y == label], axis=0))
        
        # 计算类内散布矩阵
        S_W = np.zeros((X.shape[1], X.shape[1]))
        for label, mv in zip(class_labels, mean_vectors):
            class_scatter = np.zeros((X.shape[1], X.shape[1]))
            for row in X[y == label]:
                row, mv = row.reshape(X.shape[1], 1), mv.reshape(X.shape[1], 1)
                class_scatter += (row - mv) @ (row - mv).T
            S_W += class_scatter

        # 计算类间散布矩阵
        S_B = np.zeros((X.shape[1], X.shape[1]))
        for i, mean_vec in enumerate(mean_vectors):
            n = X[y == class_labels[i], :].shape[0]
            mean_vec = mean_vec.reshape(X.shape[1], 1)
            overall_mean = mean_overall.reshape(X.shape[1], 1)
            S_B += n * (mean_vec - overall_mean) @ (mean_vec - overall_mean).T

        # 计算权重向量
        A = np.linalg.inv(S_W) @ (mean_vectors[1] - mean_vectors[0])
        self.w_ = A

        # 计算偏置
        N1 = X[y == class_labels[0]].shape[0]
        N2 = X[y == class_labels[1]].shape[0]
        self.w0_ = -self.w_.T @ mean_overall

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        预测输入数据的类别
        
        Parameters:
        -----------
        X : np.ndarray, shape = [n_samples, n_features]
            输入向量
        
        Returns:
        --------
        np.ndarray
            类别预测
        """
        return np.where((X @ self.w_ + self.w0_) >= 0, 1, 0)

def generate_data(n_samples: int = 100) -> Tuple[np.ndarray, np.ndarray]:
    """
    生成二分类数据集
    
    Parameters:
    -----------
    n_samples : int
        样本数量 (默认值为 100)
        
    Returns:
    --------
    Tuple[np.ndarray, np.ndarray]
        特征矩阵和目标值向量
    """
    np.random.seed(0)
    X1 = np.random.randn(n_samples // 2, 2) + np.array([2, 2])
    X2 = np.random.randn(n_samples // 2, 2) + np.array([-2, -2])
    X = np.vstack((X1, X2))
    y = np.hstack((np.zeros(n_samples // 2), np.ones(n_samples // 2)))
    return X, y

def main() -> None:
    """
    主函数，运行分类器并打印结果
    """
    X, y = generate_data()
    classifier = LeastSquaresFisherLDA()
    classifier.fit(X, y)
    predictions = classifier.predict(X)
    
    print("权重向量 w:")
    print(classifier.w_)
    print("偏置 w0:")
    print(classifier.w0_)
    print("预测值:")
    print(predictions)

if __name__ == "__main__":
    main()
</code></pre>
  </div>
</body>
</html>
  