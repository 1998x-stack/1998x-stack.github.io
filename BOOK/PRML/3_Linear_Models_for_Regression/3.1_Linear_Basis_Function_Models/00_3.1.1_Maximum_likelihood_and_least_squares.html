
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>3.1.1 Maximum likelihood and least squares</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>00_3.1.1_Maximum_likelihood_and_least_squares</h1>
<pre><code>Lecture: 3_Linear_Models_for_Regression/3.1_Linear_Basis_Function_Models
Content: 00_3.1.1_Maximum_likelihood_and_least_squares
</code></pre>
<h3>PDF Exploration and Detailed Analysis</h3>
<p>首先，让我们探索PDF文档的相关部分，以便对“3.1.1 Maximum likelihood and least squares”内容进行极其详细的分析。以下是对文档的分析：</p>
<h4>3.1.1 Maximum likelihood and least squares</h4>
<p>本节主要探讨最大似然估计与最小二乘法之间的关系。最大似然估计在假设高斯噪声模型的情况下，可以导出最小二乘法的解。</p>
<ol>
<li>
<p><strong>目标变量假设</strong>：
目标变量 $ t $ 假设由一个确定性函数 $ y(x,w) $ 加上高斯噪声组成，即：
$$
t = y(x,w) + \epsilon
$$
其中， $ \epsilon $ 是均值为0，精度为 $ \beta $ 的高斯随机变量。</p>
</li>
<li>
<p><strong>似然函数</strong>：
给定输入 $ x $，目标变量 $ t $ 的条件分布为：
$$
p(t|x,w, \beta) = N(t|y(x,w), \beta^{-1})
$$
若假设平方损失函数，则新值 $ x $ 的最优预测由目标变量的条件均值给出。</p>
</li>
<li>
<p><strong>数据集</strong>：
对于输入数据集 $ X = {x_1, \ldots, x_N} $ 及对应的目标值 $ t_1, \ldots, t_N $，我们将目标变量组合成列向量 $ t $。假设这些数据点独立从上述分布中抽取，似然函数为：
$$
p(t|X,w, \beta) = \prod_{n=1}^{N} N(t_n|w^T\phi(x_n), \beta^{-1})
$$</p>
</li>
<li>
<p><strong>对数似然函数</strong>：
取对数并利用高斯分布的标准形式，对数似然函数为：
$$
\ln p(t|w, \beta) = \frac{N}{2} \ln \beta - \frac{N}{2} \ln (2\pi) - \beta E_D(w)
$$
其中，平方和误差函数 $ E_D(w) $ 定义为：
$$
E_D(w) = \frac{1}{2} \sum_{n=1}^{N} { t_n - w^T \phi(x_n) }^2
$$</p>
</li>
<li>
<p><strong>最大似然估计</strong>：
通过最大化对数似然函数，可以得到参数 $ w $ 和 $ \beta $ 的解。首先，对 $ w $ 进行最大化：
$$
\nabla \ln p(t|w, \beta) = \sum_{n=1}^{N} { t_n - w^T \phi(x_n) } \phi(x_n)^T
$$
设梯度为0，求得 $ w $ 的解：
$$
w_{ML} = ( \Phi^T \Phi )^{-1} \Phi^T t
$$
这里 $ \Phi $ 为设计矩阵，其元素为 $ \Phi_{nj} = \phi_j(x_n) $。</p>
</li>
</ol>
<h3>深入分析</h3>
<p>最大似然估计和最小二乘法的关系体现在通过最大化似然函数，可以导出最小二乘法的解。这种方法假设噪声为高斯分布，且通过平方损失函数来最小化误差。这种方法在处理线性回归问题时尤为有效，但在数据点较少或噪声分布不符合高斯分布时，可能会导致偏差问题。</p>
<p>在实际应用中，当设计矩阵 $ \Phi^T \Phi $ 接近奇异时，直接解常规方程可能会导致数值问题。这种情况下，可以使用奇异值分解（SVD）技术来解决。此外，添加正则化项可以确保矩阵在存在退化的情况下非奇异。</p>
<p>最大似然方法在数据点数量增加时，解决方案的偏差会减小。在复杂模型中，最大似然方法可能会导致过拟合问题，需要通过正则化或贝叶斯方法来解决。</p>
<p>通过以上分析，我们对最大似然估计和最小二乘法在回归模型中的应用有了更深刻的理解。接下来，我们将基于以上理论，编写一个Python代码，使用Numpy和Scipy来实现最大似然估计与最小二乘法的计算。</p>
<h3>Python代码实现</h3>
<p>以下是一个从头开始的Python代码实现，展示了如何使用Numpy和Scipy来进行最大似然估计与最小二乘法的计算。</p>
<pre><code class="language-python">import numpy as np
from scipy.linalg import pinv

class LinearRegression:
    def __init__(self):
        self.weights = None

    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:
        ```
        Fit the linear regression model using maximum likelihood estimation.

        Parameters:
        X (np.ndarray): The input features, shape (N, M)
        y (np.ndarray): The target values, shape (N,)

        ```
        # Adding a bias term (column of ones)
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        
        # Compute the weights using the normal equation
        self.weights = pinv(X.T @ X) @ X.T @ y

    def predict(self, X: np.ndarray) -&gt; np.ndarray:
        ```
        Make predictions using the fitted linear regression model.

        Parameters:
        X (np.ndarray): The input features, shape (N, M)

        Returns:
        np.ndarray: The predicted values, shape (N,)
        ```
        # Adding a bias term (column of ones)
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        return X @ self.weights

    def calculate_residuals(self, X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:
        ```
        Calculate the residuals of the model.

        Parameters:
        X (np.ndarray): The input features, shape (N, M)
        y (np.ndarray): The target values, shape (N,)

        Returns:
        np.ndarray: The residuals, shape (N,)
        ```
        predictions = self.predict(X)
        return y - predictions

# Example usage
if __name__ == &quot;__main__&quot;:
    # Generating some example data
    np.random.seed(42)
    X = np.random.rand(100, 2)
    true_weights = np.array([2, 3, 5])  # Including bias term
    y = X @ true_weights[1:] + true_weights[0] + np.random.randn(100) * 0.5

    # Initialize and fit the model
    model = LinearRegression()
    model.fit(X, y)

    # Make predictions
    predictions = model.predict(X)
    
    # Calculate residuals
    residuals = model.calculate_residuals(X, y)
    
    # Output the results
    print(&quot;Fitted weights:&quot;, model.weights)
    print(&quot;Predictions:&quot;, predictions[:5])
    print(&quot;Residuals:&quot;, residuals[:5])
</code></pre>
<h4>代码说明</h4>
<ul>
<li><strong>数据准备</strong>：生成一些示例数据，包括特征矩阵 $X$ 和目标值 $y$。</li>
<li><strong>模型拟合</strong>：使用最小二乘法拟合线性回归模型。</li>
<li><strong>预测</strong>：基于拟合的模型进行预测。</li>
<li><strong>残差计算</strong>：计算模型的残差。</li>
</ul>
<h3>关键步骤校验</h3>
<ol>
<li><strong>边界条件</strong>：检查特征矩阵是否添加了偏置项，确保设计矩阵的正确性。</li>
<li><strong>解的求取</strong>：使用广义逆矩阵求取最小二乘解，确保矩阵的非奇异性。</li>
<li><strong>预测与残差</strong>：验证预测值与残差的计算，确保结果的准确性。</li>
</ol>
<p>通过以上详细分析与代码实现，我们完整地展示了最大似然估计与最小二乘法在回归模型中的应用。</p>

    <h3>Python 文件</h3>
    <pre><code># 00_3.1.1_Maximum_likelihood_and_least_squares

"""
Lecture: 3_Linear_Models_for_Regression/3.1_Linear_Basis_Function_Models
Content: 00_3.1.1_Maximum_likelihood_and_least_squares
"""

import numpy as np
from scipy.linalg import pinv

class LinearRegression:
    def __init__(self):
        self.weights = None

    def fit(self, X: np.ndarray, y: np.ndarray) -> None:
        """
        Fit the linear regression model using maximum likelihood estimation.

        Parameters:
        X (np.ndarray): The input features, shape (N, M)
        y (np.ndarray): The target values, shape (N,)

        """
        # Adding a bias term (column of ones)
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        
        # Compute the weights using the normal equation
        self.weights = pinv(X.T @ X) @ X.T @ y

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Make predictions using the fitted linear regression model.

        Parameters:
        X (np.ndarray): The input features, shape (N, M)

        Returns:
        np.ndarray: The predicted values, shape (N,)
        """
        # Adding a bias term (column of ones)
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        return X @ self.weights

    def calculate_residuals(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:
        """
        Calculate the residuals of the model.

        Parameters:
        X (np.ndarray): The input features, shape (N, M)
        y (np.ndarray): The target values, shape (N,)

        Returns:
        np.ndarray: The residuals, shape (N,)
        """
        predictions = self.predict(X)
        return y - predictions

# Example usage
if __name__ == "__main__":
    # Generating some example data
    np.random.seed(42)
    X = np.random.rand(100, 2)
    true_weights = np.array([2, 3, 5])  # Including bias term
    y = X @ true_weights[1:] + true_weights[0] + np.random.randn(100) * 0.5

    # Initialize and fit the model
    model = LinearRegression()
    model.fit(X, y)

    # Make predictions
    predictions = model.predict(X)
    
    # Calculate residuals
    residuals = model.calculate_residuals(X, y)
    
    # Output the results
    print("Fitted weights:", model.weights)
    print("Predictions:", predictions[:5])
    print("Residuals:", residuals[:5])</code></pre>
  </div>
</body>
</html>
  