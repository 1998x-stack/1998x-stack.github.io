
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>3.1.2 Geometry of least squares</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>01_3.1.2_Geometry_of_least_squares</h1>
<pre><code>Lecture: 3_Linear_Models_for_Regression/3.1_Linear_Basis_Function_Models
Content: 01_3.1.2_Geometry_of_least_squares
</code></pre>
<h3>详细分析 3.1.2 最小二乘法的几何解释</h3>
<p>在本节中，我们详细讨论了最小二乘法（Least Squares Method）的几何解释。最小二乘法是回归分析中的一种常见方法，它通过最小化预测值与实际值之间的平方误差来找到模型参数。</p>
<h4>几何解释</h4>
<p>首先，我们考虑一个N维空间，其坐标轴由目标变量$t_n$组成，因此$t = (t_1, t_2, ..., t_N)^T$是这个空间中的一个向量。每一个基函数$\phi_j(x_n)$在N个数据点处的值也可以表示为这个空间中的一个向量，记为$\phi_j$。这些向量与矩阵$\Phi$的列相对应。</p>
<p>当基函数的数量M小于数据点的数量N时，M个向量$\phi_j(x_n)$将会在N维空间中张成一个维度为M的线性子空间S。我们定义向量$y$为N维向量，其第n个元素为$y(x_n, w)$，其中$n = 1, ..., N$。由于$y$是向量$\phi_j$的线性组合，因此它可以在M维子空间S中的任意位置。</p>
<p>平方误差（Sum-of-Squares Error，$E(w)$）等于$y$与$t$之间的欧氏距离的平方。因此，最小二乘法解对应于位于子空间S中最接近$t$的$y$值。这实际上是$t$在子空间S上的正交投影，如图3.2所示。这可以通过以下方式验证：最小二乘解$y$由$\Phi w_{ML}$给出，这正是正交投影的形式。</p>
<h4>具体步骤</h4>
<ol>
<li>
<p><strong>定义目标向量和基函数向量</strong>：</p>
<ul>
<li>$t = (t_1, t_2, ..., t_N)^T$</li>
<li>$\phi_j = (\phi_j(x_1), \phi_j(x_2), ..., \phi_j(x_N))^T$</li>
</ul>
</li>
<li>
<p><strong>构建子空间S</strong>：由M个基函数向量$\phi_j$张成一个M维的线性子空间S。</p>
</li>
<li>
<p><strong>定义预测向量</strong>：</p>
<ul>
<li>$y = (y(x_1, w), y(x_2, w), ..., y(x_N, w))^T$</li>
</ul>
</li>
<li>
<p><strong>最小化平方误差</strong>：寻找位于子空间S中最接近目标向量$t$的预测向量$y$，即找到$t$在子空间S上的正交投影。</p>
</li>
</ol>
<h4>例子</h4>
<p>假设有一个简单的线性回归模型，其目标是通过最小二乘法找到模型参数，使得预测值与实际值之间的平方误差最小。通过将数据点表示为向量，并在高维空间中进行几何分析，我们可以清晰地理解最小二乘法的作用以及其几何意义。</p>
<p>通过这种几何解释，我们可以更直观地理解最小二乘法的本质，这对于深入理解回归分析中的各种方法具有重要意义。</p>
<h3>代码实现</h3>
<pre><code class="language-python">import numpy as np
from scipy.linalg import svd

class LinearRegression:
    ```
    使用最小二乘法进行线性回归的实现
    
    Attributes:
        weights (np.ndarray): 线性回归模型的权重
    ```
    
    def __init__(self):
        self.weights = None
    
    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:
        ```
        拟合线性回归模型
        
        Args:
            X (np.ndarray): 训练数据集的特征矩阵
            y (np.ndarray): 训练数据集的目标变量向量
        ```
        X_bias = np.hstack([np.ones((X.shape[0], 1)), X])  # 添加偏置项
        self.weights = np.linalg.pinv(X_bias.T @ X_bias) @ X_bias.T @ y  # 使用伪逆计算权重
    
    def predict(self, X: np.ndarray) -&gt; np.ndarray:
        ```
        使用拟合好的模型进行预测
        
        Args:
            X (np.ndarray): 测试数据集的特征矩阵
            
        Returns:
            np.ndarray: 预测的结果
        ```
        X_bias = np.hstack([np.ones((X.shape[0], 1)), X])  # 添加偏置项
        return X_bias @ self.weights

    def score(self, X: np.ndarray, y: np.ndarray) -&gt; float:
        ```
        计算模型的决定系数 R^2
        
        Args:
            X (np.ndarray): 测试数据集的特征矩阵
            y (np.ndarray): 测试数据集的目标变量向量
            
        Returns:
            float: 模型的 R^2 值
        ```
        y_pred = self.predict(X)
        ss_total = np.sum((y - np.mean(y)) ** 2)
        ss_residual = np.sum((y - y_pred) ** 2)
        return 1 - (ss_residual / ss_total)

# 示例使用
if __name__ == &quot;__main__&quot;:
    # 生成示例数据
    np.random.seed(0)
    X = 2 * np.random.rand(100, 1)
    y = 4 + 3 * X + np.random.randn(100, 1)
    
    # 拟合模型
    model = LinearRegression()
    model.fit(X, y)
    
    # 打印权重
    print(&quot;模型权重:&quot;, model.weights)
    
    # 预测
    X_new = np.array([[0], [2]])
    y_pred = model.predict(X_new)
    print(&quot;预测结果:&quot;, y_pred)
    
    # 计算 R^2 值
    r2_score = model.score(X, y)
    print(&quot;模型的 R^2 值:&quot;, r2_score)
</code></pre>
<h4>代码解释</h4>
<ul>
<li><code>LinearRegression</code>类实现了一个简单的线性回归模型，包含<code>fit</code>、<code>predict</code>和<code>score</code>方法。</li>
<li><code>fit</code>方法使用最小二乘法拟合模型，通过求解$w = (X^TX)^{-1}X^Ty$来计算权重。</li>
<li><code>predict</code>方法根据输入特征矩阵进行预测。</li>
<li><code>score</code>方法计算模型的决定系数$R^2$，用于评估模型的拟合优度。</li>
</ul>
<p>这种实现方式确保了代码的可读性和可维护性，同时符合工业界的高可扩展性要求。通过添加中文注释，确保了代码的可读性和理解性。</p>
<p><strong>检查步骤</strong></p>
<ul>
<li>确认输入数据的维度和格式。</li>
<li>验证权重的计算是否正确。</li>
<li>测试预测结果的准确性。</li>
<li>计算决定系数$R^2$验证模型的拟合优度。</li>
</ul>

    <h3>Python 文件</h3>
    <pre><code># 01_3.1.2_Geometry_of_least_squares

"""
Lecture: 3_Linear_Models_for_Regression/3.1_Linear_Basis_Function_Models
Content: 01_3.1.2_Geometry_of_least_squares
"""

import numpy as np
from scipy.linalg import svd

class LinearRegression:
    """
    使用最小二乘法进行线性回归的实现
    
    Attributes:
        weights (np.ndarray): 线性回归模型的权重
    """
    
    def __init__(self):
        self.weights = None
    
    def fit(self, X: np.ndarray, y: np.ndarray) -> None:
        """
        拟合线性回归模型
        
        Args:
            X (np.ndarray): 训练数据集的特征矩阵
            y (np.ndarray): 训练数据集的目标变量向量
        """
        X_bias = np.hstack([np.ones((X.shape[0], 1)), X])  # 添加偏置项
        self.weights = np.linalg.pinv(X_bias.T @ X_bias) @ X_bias.T @ y  # 使用伪逆计算权重
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        使用拟合好的模型进行预测
        
        Args:
            X (np.ndarray): 测试数据集的特征矩阵
            
        Returns:
            np.ndarray: 预测的结果
        """
        X_bias = np.hstack([np.ones((X.shape[0], 1)), X])  # 添加偏置项
        return X_bias @ self.weights

    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        """
        计算模型的决定系数 R^2
        
        Args:
            X (np.ndarray): 测试数据集的特征矩阵
            y (np.ndarray): 测试数据集的目标变量向量
            
        Returns:
            float: 模型的 R^2 值
        """
        y_pred = self.predict(X)
        ss_total = np.sum((y - np.mean(y)) ** 2)
        ss_residual = np.sum((y - y_pred) ** 2)
        return 1 - (ss_residual / ss_total)

# 示例使用
if __name__ == "__main__":
    # 生成示例数据
    np.random.seed(0)
    X = 2 * np.random.rand(100, 1)
    y = 4 + 3 * X + np.random.randn(100, 1)
    
    # 拟合模型
    model = LinearRegression()
    model.fit(X, y)
    
    # 打印权重
    print("模型权重:", model.weights)
    
    # 预测
    X_new = np.array([[0], [2]])
    y_pred = model.predict(X_new)
    print("预测结果:", y_pred)
    
    # 计算 R^2 值
    r2_score = model.score(X, y)
    print("模型的 R^2 值:", r2_score)
</code></pre>
  </div>
</body>
</html>
  