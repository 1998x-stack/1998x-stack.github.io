
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>5.7.3 Bayesian neural networks for classification</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>02_5.7.3_Bayesian_neural_networks_for_classification</h1>
<pre><code>Lecture: 5_Neural_Networks/5.7_Bayesian_Neural_Networks
Content: 02_5.7.3_Bayesian_neural_networks_for_classification
</code></pre>
<h2>详细分析第5.7.3节：用于分类的贝叶斯神经网络</h2>
<h3>引言</h3>
<p>贝叶斯神经网络（BNN）在分类任务中表现出色，主要因为其能够量化预测中的不确定性，并通过对模型参数的后验分布进行推断，提供更加稳健的预测。第5.7.3节讨论了如何将贝叶斯方法应用于神经网络分类任务中。</p>
<h3>贝叶斯分类模型</h3>
<p>贝叶斯分类器的核心思想是利用贝叶斯公式来计算类别后验概率。对于一个输入向量$x$，模型输出属于类别$C_k$的后验概率为：
$$ p(C_k|x, D) = \frac{p(x|C_k)p(C_k)}{p(x)} $$
其中，$ p(x|C_k) $是类别$C_k$的似然，$ p(C_k) $是类别先验，$ p(x) $是证据。</p>
<h4>神经网络输出</h4>
<p>在神经网络分类器中，输出层通常采用Softmax函数，将网络的输出$a_k$转化为类别的概率分布：
$$ y_k = \frac{\exp(a_k)}{\sum_{j}\exp(a_j)} $$
这样，$ y_k $ 表示输入 $ x $ 属于类别 $ k $ 的概率。</p>
<h3>贝叶斯神经网络的训练</h3>
<p>训练贝叶斯神经网络涉及到对参数的后验分布进行估计。由于精确的后验分布难以计算，通常使用近似方法，如拉普拉斯近似或变分推断。下面介绍两种常见的方法：</p>
<h4>拉普拉斯近似</h4>
<p>拉普拉斯近似通过将后验分布近似为高斯分布，简化了计算过程。具体步骤如下：</p>
<ol>
<li><strong>找到后验分布的最大值</strong> $ w_{MAP} $，通过最大化后验对数来实现：
$$ \ln p(w|D) = -E(w) + const $$
其中 $ E(w) $ 是负对数似然。</li>
<li><strong>计算Hessian矩阵</strong> $ H $，这是误差函数对参数 $ w $ 的二阶导数矩阵：
$$ H = \nabla\nabla E(w) $$</li>
<li><strong>高斯近似</strong> $ q(w|D) = N(w|w_{MAP}, H^{-1}) $</li>
</ol>
<h4>变分推断</h4>
<p>变分推断通过优化一个可变分布 $ q(w) $ 来近似后验分布 $ p(w|D) $，使得 $ q(w) $ 和 $ p(w|D) $ 之间的KL散度最小。变分推断的步骤如下：</p>
<ol>
<li><strong>选择变分分布</strong> $ q(w) $，通常选择一个简单的分布如高斯分布。</li>
<li><strong>优化变分参数</strong>，通过最大化变分下界（ELBO）来进行：
$$ \text{ELBO} = \mathbb{E}_{q(w)}[\ln p(D|w)] - \text{KL}(q(w) || p(w)) $$</li>
</ol>
<h3>预测分布</h3>
<p>对于新的输入 $ x^* $，贝叶斯神经网络的预测分布通过对参数进行边缘化处理得到：
$$ p(C_k|x^<em>, D) = \int p(C_k|x^</em>, w) p(w|D) , dw $$
在拉普拉斯近似下，这个积分可以通过采样或者变分方法来近似计算。</p>
<h3>不确定性量化</h3>
<p>贝叶斯神经网络的一个显著优势是其能够量化预测的不确定性，这在很多实际应用中非常重要。模型输出的概率分布不仅提供了最可能的预测，还反映了模型对预测的置信程度。</p>
<h3>小结</h3>
<p>贝叶斯神经网络通过对参数的后验分布进行建模和推断，能够提供更加稳健的预测，并有效量化预测中的不确定性。拉普拉斯近似和变分推断是两种常用的近似方法，它们使得贝叶斯方法在复杂模型中的应用成为可能。</p>

    <h3>Python 文件</h3>
    <pre><code># 02_5.7.3_Bayesian_neural_networks_for_classification

"""
Lecture: 5_Neural_Networks/5.7_Bayesian_Neural_Networks
Content: 02_5.7.3_Bayesian_neural_networks_for_classification
"""

</code></pre>
  </div>
</body>
</html>
  