
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>5.3.2 A simple example</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>01_5.3.2_A_simple_example</h1>
<pre><code>Lecture: 5_Neural_Networks/5.3_Error_Backpropagation
Content: 01_5.3.2_A_simple_example
</code></pre>
<h3>5.3.2 一个简单示例——极其详细和深入分析</h3>
<h4>前言</h4>
<p>在上一节中，我们讨论了反向传播算法的理论基础和数学推导。本节将通过一个具体的示例，来详细说明反向传播算法的应用过程。这个示例被选中不仅因为其简单性，还因为其在实际应用中的重要性。许多文献中报道的神经网络应用都使用了这种类型的网络。</p>
<h4>网络结构</h4>
<p>我们考虑一个两层的前馈神经网络，如图5.1所示。该网络的输出单元使用线性激活函数，而隐藏单元使用逻辑sigmoid激活函数。具体来说，输出单元的激活函数为：
$$ y_k = a_k $$
而隐藏单元的激活函数为：
$$ h(a) \equiv \tanh(a) $$
其中：
$$ \tanh(a) = \frac{e^a - e^{-a}}{e^a + e^{-a}} $$
这种激活函数的一个有用特性是其导数形式非常简单：
$$ h'(a) = 1 - h(a)^2 $$</p>
<p>我们考虑一个标准的平方和误差函数，对于模式 $ n $ ，其误差定义为：
$$ E_n = \frac{1}{2} \sum_{k=1}^{K} (y_{k} - t_{k})^2 $$
其中， $ y_k $ 是输出单元 $ k $ 的激活， $ t_k $ 是对应的目标值，针对特定的输入模式 $ x_n $。</p>
<h4>前向传播</h4>
<p>对于训练集中的每个模式，我们首先进行前向传播计算输出：
$$ a_j = \sum_{i=0}^{D} w_{ji}^{(1)} x_i $$
$$ z_j = \tanh(a_j) $$
$$ y_k = \sum_{j=0}^{M} w_{kj}^{(2)} z_j $$</p>
<h4>反向传播</h4>
<p>接下来，我们计算每个输出单元的 $\delta$ 值：
$$ \delta_k = y_k - t_k $$
然后，我们将这些 $\delta$ 值反向传播到隐藏单元：
$$ \delta_j = (1 - z_j^2) \sum_{k=1}^{K} w_{kj} \delta_k $$</p>
<p>最后，计算关于第一层和第二层权重的导数：
$$ \frac{\partial E_n}{\partial w_{ji}^{(1)}} = \delta_j x_i $$
$$ \frac{\partial E_n}{\partial w_{kj}^{(2)}} = \delta_k z_j $$</p>
<h4>实际应用</h4>
<p>在实际应用中，反向传播算法被用于计算所有样本的梯度，并通过梯度下降法更新权重。对于批处理方法，通过对训练集中的所有模式重复上述步骤，然后对所有模式的结果进行求和，可以得到总误差 $ E $ 的导数：
$$ \frac{\partial E}{\partial w_{ji}} = \sum_{n} \frac{\partial E_n}{\partial w_{ji}} $$</p>
<h4>数值验证</h4>
<p>为了确保反向传播实现的正确性，通常会将反向传播计算的导数与数值微分方法计算的导数进行比较。数值微分方法通过扰动每个权重并计算误差变化，来近似导数：
$$ \frac{\partial E_n}{\partial w_{ji}} \approx \frac{E_n(w_{ji} + \epsilon) - E_n(w_{ji})}{\epsilon} $$
这种方法尽管计算量大，但对于验证导数计算的正确性非常有用。</p>
<h4>总结</h4>
<p>通过一个简单的两层网络示例，我们详细说明了反向传播算法的前向传播和反向传播过程。反向传播算法因其高效性和准确性，在神经网络训练中得到了广泛应用。理解反向传播算法的实际应用，对于设计和实现高效的神经网络训练方法至关重要。通过结合数值验证方法，可以确保反向传播算法的实现是正确的，从而提高模型的性能和稳定性。</p>

    <h3>Python 文件</h3>
    <pre><code># 01_5.3.2_A_simple_example

"""
Lecture: 5_Neural_Networks/5.3_Error_Backpropagation
Content: 01_5.3.2_A_simple_example
"""

</code></pre>
  </div>
</body>
</html>
  