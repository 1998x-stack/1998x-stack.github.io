
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>5.2.2 Local quadratic approximation</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>01_5.2.2_Local_quadratic_approximation</h1>
<pre><code>Lecture: 5_Neural_Networks/5.2_Network_Training
Content: 01_5.2.2_Local_quadratic_approximation
</code></pre>
<h3>5.2.2 局部二次近似——极其详细和深入分析</h3>
<h4>前言</h4>
<p>在神经网络训练过程中，为了更好地理解优化问题及其解决方法，可以考虑误差函数的局部二次近似。这一方法通过泰勒展开，将误差函数在权重空间的某一点附近进行近似，从而简化优化过程。接下来，我们将对局部二次近似进行极其详细和深入的分析。</p>
<h4>局部二次近似的理论基础</h4>
<p>为了简化误差函数 $ E(\mathbf{w}) $ 的优化过程，我们可以在权重空间的某个点 $ \hat{\mathbf{w}} $ 处对其进行泰勒展开。假设 $ E(\mathbf{w}) $ 是在 $ \hat{\mathbf{w}} $ 处的泰勒展开形式如下：
$$ E(\mathbf{w}) \approx E(\hat{\mathbf{w}}) + (\mathbf{w} - \hat{\mathbf{w}})^T \mathbf{b} + \frac{1}{2} (\mathbf{w} - \hat{\mathbf{w}})^T \mathbf{H} (\mathbf{w} - \hat{\mathbf{w}}) $$
其中，梯度 $ \mathbf{b} $ 和 Hessian 矩阵 $ \mathbf{H} $ 分别表示为：
$$ \mathbf{b} = \nabla E \bigg|<em>{\mathbf{w}=\hat{\mathbf{w}}} $$
$$ \mathbf{H} = \nabla \nabla E \bigg|</em>{\mathbf{w}=\hat{\mathbf{w}}} $$</p>
<p>在这个展开式中，我们忽略了三次及更高次的项，从而简化计算。对于 $ \mathbf{w} $ 足够接近 $ \hat{\mathbf{w}} $ 的情况，这个近似可以较好地描述误差函数及其梯度。</p>
<h4>最小点的二次近似</h4>
<p>考虑在误差函数的某个最小点 $ \mathbf{w}^* $ 附近的局部二次近似。此时，由于在最小点处梯度为零，即 $ \nabla E(\mathbf{w}^<em>) = 0 $，因此展开式中的线性项消失，误差函数可以简化为：
$$ E(\mathbf{w}) \approx E(\mathbf{w}^</em>) + \frac{1}{2} (\mathbf{w} - \mathbf{w}^<em>)^T \mathbf{H} (\mathbf{w} - \mathbf{w}^</em>) $$</p>
<p>为了更好地理解这种近似，可以考虑 Hessian 矩阵的特征值分解。设 Hessian 矩阵 $ \mathbf{H} $ 的特征值分解为：
$$ \mathbf{H} \mathbf{u}_i = \lambda_i \mathbf{u}_i $$
其中， $ \mathbf{u}_i $ 是特征向量， $ \lambda_i $ 是对应的特征值。特征向量 $ \mathbf{u}_i $ 构成了一个完备的正交基（参见附录C），即：
$$ \mathbf{u}_i^T \mathbf{u}<em ij="">j = \delta</em> $$</p>
<p>我们可以将 $ \mathbf{w} - \mathbf{w}^* $ 展开为这些特征向量的线性组合：
$$ \mathbf{w} - \mathbf{w}^* = \sum_i \alpha_i \mathbf{u}_i $$</p>
<p>将其代入误差函数的近似式，并利用特征值分解，可以得到：
$$ E(\mathbf{w}) \approx E(\mathbf{w}^*) + \frac{1}{2} \sum_i \lambda_i \alpha_i^2 $$</p>
<h4>Hessian 矩阵的几何解释</h4>
<p>在新的坐标系中，误差函数的等高线将是以最小点为中心的椭圆，其轴与 Hessian 矩阵的特征向量对齐，长度与特征值的平方根成反比。这意味着在特征值较小的方向上，误差函数变化较缓，而在特征值较大的方向上，误差函数变化较快。</p>
<h4>Hessian 矩阵的正定性</h4>
<p>一个矩阵 $ \mathbf{H} $ 如果对任意非零向量 $ \mathbf{v} $ 满足 $ \mathbf{v}^T \mathbf{H} \mathbf{v} &gt; 0 $，则称其为正定矩阵。在局部二次近似中，如果 Hessian 矩阵是正定的，则误差函数在该点附近有一个局部最小值。</p>
<h4>实践中的应用</h4>
<p>局部二次近似在实际应用中的一个重要用途是优化算法的设计。例如，在 Newton-Raphson 方法中，通过使用 Hessian 矩阵的逆，可以显著加快收敛速度。对于复杂的误差函数，通过局部二次近似，可以在每次迭代中有效地找到前进方向，从而提高优化效率。</p>
<p>此外，局部二次近似还可以用于分析优化问题的困难程度。通过特征值分解，可以判断误差函数在不同方向上的曲率，从而选择合适的优化策略。例如，对于高维度的神经网络，可以采用共轭梯度法等优化算法，以避免在特征值较小的方向上收敛缓慢的问题。</p>
<h4>总结</h4>
<p>局部二次近似是一种强大的工具，可以帮助我们更好地理解和解决神经网络中的优化问题。通过对误差函数进行泰勒展开，我们可以简化计算，并利用 Hessian 矩阵的特征值分解，深入分析误差函数的几何性质。理解和应用局部二次近似，可以显著提高优化算法的效率和稳定性，从而提升神经网络的训练效果。</p>

    <h3>Python 文件</h3>
    <pre><code># 01_5.2.2_Local_quadratic_approximation

"""
Lecture: 5_Neural_Networks/5.2_Network_Training
Content: 01_5.2.2_Local_quadratic_approximation
"""

</code></pre>
  </div>
</body>
</html>
  