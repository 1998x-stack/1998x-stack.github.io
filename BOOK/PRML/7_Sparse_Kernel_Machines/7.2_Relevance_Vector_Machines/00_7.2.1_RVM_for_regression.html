
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>7.2.1 RVM for regression</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>00_7.2.1_RVM_for_regression</h1>
<pre><code>Lecture: 7_Sparse_Kernel_Machines/7.2_Relevance_Vector_Machines
Content: 00_7.2.1_RVM_for_regression
</code></pre>
<h3>相关向量机用于回归 (RVM for Regression) 详细分析</h3>
<h4>1. 引言</h4>
<p>相关向量机（Relevance Vector Machine，RVM）是一种贝叶斯稀疏核技术，用于回归和分类任务。与支持向量机（SVM）类似，RVM 旨在构建稀疏模型，但它采用贝叶斯框架，使得模型具有概率解释。相比于SVM，RVM模型通常更加稀疏，因此在测试数据上的性能更快，同时保持了可比的泛化误差。</p>
<h4>2. 模型定义</h4>
<p>RVM是一种线性模型，其形式与第3章中研究的模型相似，但其先验经过修改以产生稀疏解。模型定义了一个给定输入向量 $x$ 的实值目标变量 $t$ 的条件分布，其形式为：
$$ p(t|x,w, \beta) = N (t|y(x), \beta^{-1}) $$
其中，$\beta = \sigma^{-2}$ 是噪声精度（噪声方差的倒数），均值由以下线性模型给出：
$$ y(x) = \sum_{i=1}^{M} w_i \phi_i(x) = w^T \phi(x) $$
其中 $\phi_i(x)$ 是固定的非线性基函数，通常包括一个常数项，以便相应的权重参数表示偏置。</p>
<h4>3. 核函数与稀疏性</h4>
<p>RVM模型的基础函数由核函数给出，每个核函数与训练集中的一个数据点相关联。通用表达式可以写成类似SVM的形式：
$$ y(x) = \sum_{n=1}^{N} w_n k(x,x_n) + b $$
其中，$b$ 是偏置参数。在这种情况下，参数的数量为 $M = N + 1$，$y(x)$ 的形式与SVM的预测模型相同，只是系数 $a_n$ 在这里表示为 $w_n$。</p>
<h4>4. 先验分布与后验分布</h4>
<p>RVM引入了一个对参数向量 $w$ 的先验分布。与第3章中类似，我们考虑一个零均值高斯先验，但RVM的关键区别在于我们为每个权重参数 $w_i$ 引入了一个独立的超参数 $\alpha_i$，而不是单一的共享超参数。因此，权重的先验分布形式为：
$$ p(w|\alpha) = \prod_{i=1}^{M} N (w_i|0, \alpha_i^{-1}) $$
其中，$\alpha_i$ 表示相应参数 $w_i$ 的精度，$\alpha$ 表示所有超参数的向量。</p>
<p>通过最大化证据的方法（也称为类型2最大似然法），我们可以确定 $\alpha$ 和 $\beta$ 的值，从而得到稀疏模型。</p>
<h4>5. 证据最大化与优化</h4>
<p>证据最大化的边缘似然函数通过对权重参数进行积分得到：
$$ p(t|X, \alpha, \beta) = \int p(t|X,w, \beta)p(w|\alpha) dw $$
后验分布依然是高斯分布，形式为：
$$ p(w|t,X, \alpha, \beta) = N (w|m, \Sigma) $$
其中，均值和协方差为：
$$ m = \beta \Sigma \Phi^T t $$
$$ \Sigma = (A + \beta \Phi^T \Phi)^{-1} $$
其中，$\Phi$ 是设计矩阵，元素 $\Phi_{ni} = \phi_i(x_n)$，$A = \text{diag}(\alpha_i)$。</p>
<h4>6. 稀疏性与相关向量</h4>
<p>在最大化证据的过程中，一部分超参数 $\alpha_i$ 会趋于无穷大，相应的权重参数 $w_i$ 的后验分布会集中在零。这样，与这些参数相关的基函数在模型中不起作用，从而实现稀疏性。剩余的非零权重对应的输入点被称为相关向量（Relevance Vectors）。</p>
<h4>7. 预测分布</h4>
<p>给定新的输入 $x$，可以评估目标变量 $t$ 的预测分布：
$$ p(t|x,X, t, \alpha, \beta) = N (t|m^T \phi(x), \sigma^2(x)) $$
其中，预测均值为 $m^T \phi(x)$，预测方差为：
$$ \sigma^2(x) = \beta^{-1} + \phi(x)^T \Sigma \phi(x) $$</p>
<h4>8. 计算成本与优势</h4>
<p>RVM的主要优势在于其稀疏性，通常可以生成比SVM更加紧凑的模型，从而在测试数据上的处理速度更快。然而，RVM的训练涉及优化一个非凸函数，训练时间可能比SVM更长。对于具有 $M$ 个基函数的模型，RVM需要求解一个大小为 $M \times M$ 的矩阵的逆，计算成本为 $O(M^3)$。</p>
<h4>9. 实际应用</h4>
<p>在实际应用中，RVM被发现能够在广泛的回归和分类任务中生成比SVM更加稀疏的模型，从而显著提高了测试数据上的处理速度。尽管RVM的训练时间较长，但其在测试阶段的高效性和稀疏性使其在许多应用中具有吸引力。</p>
<h3>总结</h3>
<p>相关向量机通过引入独立的超参数和贝叶斯框架，实现了对回归问题的稀疏解。与支持向量机相比，RVM在保持可比泛化误差的同时，通常能生成更加稀疏的模型，从而在测试数据上表现出更高的处理速度。尽管其训练时间较长，但其稀疏性和高效性使其在许多实际应用中具有重要意义。</p>
<h3>参考文献</h3>
<ul>
<li>Tipping, M. E. (2001). Sparse Bayesian Learning and the Relevance Vector Machine. Journal of Machine Learning Research, 1, 211-244.</li>
<li>Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.</li>
</ul>

    <h3>Python 文件</h3>
    <pre><code># 00_7.2.1_RVM_for_regression

"""
Lecture: 7_Sparse_Kernel_Machines/7.2_Relevance_Vector_Machines
Content: 00_7.2.1_RVM_for_regression
"""

import numpy as np
from scipy.linalg import cholesky, cho_solve, solve_triangular
from scipy.optimize import minimize
from typing import Tuple, List, Callable

class RelevanceVectorMachine:
    """
    相关向量机（Relevance Vector Machine, RVM）用于回归任务

    Attributes:
        kernel (Callable[[np.ndarray, np.ndarray], float]): 核函数
        alpha (np.ndarray): 超参数向量
        beta (float): 噪声精度
        weights (np.ndarray): 模型权重
        relevance_vectors (np.ndarray): 相关向量
        relevance_targets (np.ndarray): 相关向量对应的目标值
    """

    def __init__(self, kernel: Callable[[np.ndarray, np.ndarray], float] = None):
        self.kernel = kernel if kernel else self.linear_kernel
        self.alpha = None
        self.beta = None
        self.weights = None
        self.relevance_vectors = None
        self.relevance_targets = None

    def fit(self, X: np.ndarray, y: np.ndarray) -> None:
        """
        拟合模型，训练RVM

        Args:
            X (np.ndarray): 训练数据特征，形状为 (n_samples, n_features)
            y (np.ndarray): 训练数据标签，形状为 (n_samples,)
        """
        N = X.shape[0]
        self.alpha = np.ones(N)
        self.beta = 1.0

        # 计算核矩阵
        Phi = self.kernel_matrix(X, X)

        def objective(params: np.ndarray) -> float:
            """
            优化目标函数，负对数边际似然
            """
            alpha, beta = np.exp(params[:N]), np.exp(params[N])
            S_inv = np.diag(alpha) + beta * Phi
            L = cholesky(S_inv, lower=True)
            m = cho_solve((L, True), beta * y)

            log_likelihood = (
                0.5 * (N * np.log(2 * np.pi) - np.sum(np.log(alpha)) + N * np.log(beta))
                + 0.5 * y.T @ (beta * np.eye(N) - beta ** 2 * Phi @ cho_solve((L, True), Phi.T)) @ y
            )
            return log_likelihood

        params0 = np.log(np.hstack((self.alpha, self.beta)))
        result = minimize(objective, params0, method='L-BFGS-B')
        self.alpha, self.beta = np.exp(result.x[:N]), np.exp(result.x[N])

        S_inv = np.diag(self.alpha) + self.beta * Phi
        L = cholesky(S_inv, lower=True)
        self.weights = cho_solve((L, True), self.beta * y)
        self.relevance_vectors = X
        self.relevance_targets = y

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        预测新数据的目标值

        Args:
            X (np.ndarray): 测试数据特征，形状为 (n_samples, n_features)

        Returns:
            np.ndarray: 预测值，形状为 (n_samples,)
        """
        K = self.kernel_matrix(X, self.relevance_vectors)
        return K @ self.weights

    def kernel_matrix(self, X1: np.ndarray, X2: np.ndarray) -> np.ndarray:
        """
        计算核矩阵

        Args:
            X1 (np.ndarray): 输入数据1
            X2 (np.ndarray): 输入数据2

        Returns:
            np.ndarray: 核矩阵
        """
        n1, n2 = X1.shape[0], X2.shape[0]
        K = np.zeros((n1, n2))
        for i in range(n1):
            for j in range(n2):
                K[i, j] = self.kernel(X1[i], X2[j])
        return K

    @staticmethod
    def linear_kernel(x1: np.ndarray, x2: np.ndarray) -> float:
        """
        线性核函数

        Args:
            x1 (np.ndarray): 输入向量1
            x2 (np.ndarray): 输入向量2

        Returns:
            float: 线性核的计算结果
        """
        return np.dot(x1, x2)

def main():
    # 示例数据
    X = np.array([
        [1, 2],
        [2, 3],
        [3, 4],
        [4, 5],
        [5, 6],
        [6, 7],
        [7, 8],
        [8, 9]
    ])
    y = np.array([2.3, 2.9, 3.8, 4.2, 5.1, 5.8, 6.9, 7.2])

    # 初始化和训练模型
    rvm = RelevanceVectorMachine()
    rvm.fit(X, y)

    # 测试数据
    X_test = np.array([
        [2, 3],
        [3, 4],
        [5, 6],
        [9, 10]
    ])

    # 预测
    predictions = rvm.predict(X_test)
    print("Predicted values:", predictions)

if __name__ == "__main__":
    main()</code></pre>
  </div>
</body>
</html>
  