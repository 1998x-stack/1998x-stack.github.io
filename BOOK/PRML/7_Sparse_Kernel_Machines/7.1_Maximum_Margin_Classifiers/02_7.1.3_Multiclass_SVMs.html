
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>7.1.3 Multiclass SVMs</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>02_7.1.3_Multiclass_SVMs</h1>
<pre><code>Lecture: 7_Sparse_Kernel_Machines/7.1_Maximum_Margin_Classifiers
Content: 02_7.1.3_Multiclass_SVMs
</code></pre>
<h3>多分类支持向量机 (Multiclass SVMs) 详细分析</h3>
<h4>1. 引言</h4>
<p>多分类支持向量机（Multiclass SVMs）是从二分类支持向量机（SVMs）扩展而来的，用于处理多于两类的分类问题。SVM 本质上是一个二分类器，通过在特征空间中找到一个最佳的分离超平面来最大化两个类别之间的间隔。然而，在实际应用中，常常需要处理多于两个类别的分类问题，这就需要将 SVM 扩展到多分类场景。</p>
<h4>2. 一对多方法（One-Versus-Rest）</h4>
<p><strong>原理：</strong></p>
<ul>
<li>构建 $K$ 个独立的 SVM，其中第 $k$ 个模型 $y_k(x)$ 使用类别 $C_k$ 的数据作为正例，剩余 $K-1$ 个类别的数据作为负例进行训练。</li>
</ul>
<p><strong>优点：</strong></p>
<ul>
<li>实现简单，只需训练 $K$ 个 SVM 模型。</li>
<li>对于少量类别（小 $K$ 值）效果较好。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>不一致性：不同分类器之间的决策可能会导致输入被同时分配到多个类别，或者没有分配到任何类别。</li>
<li>数据不平衡：当 $K$ 增加时，每个分类器的负例数据远多于正例数据，可能导致分类器性能下降。</li>
</ul>
<p><strong>改进：</strong></p>
<ul>
<li>Lee 等人（2001）提出了一种变体方法，通过修改目标值，使得正类的目标值为 $+1$，而负类的目标值为 $-1/(K-1)$。这样可以在一定程度上平衡数据，但仍存在一些缺陷。</li>
</ul>
<h4>3. Weston 和 Watkins 的方法</h4>
<p><strong>原理：</strong></p>
<ul>
<li>定义一个训练所有 $K$ 个 SVM 的单一目标函数，基于最大化每个类别到其余类别的间隔。</li>
<li>通过优化这个单一目标函数来得到所有 $K$ 个分类器。</li>
</ul>
<p><strong>优点：</strong></p>
<ul>
<li>理论上更优，因为直接优化了一个全局目标函数。</li>
<li>避免了数据不平衡的问题。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>训练速度较慢，因为需要解决一个规模为 $(K-1)N$ 的单一优化问题，导致整体计算成本为 $O(K^2N^2)$。</li>
</ul>
<h4>4. 一对一方法（One-Versus-One）</h4>
<p><strong>原理：</strong></p>
<ul>
<li>训练 $K(K-1)/2$ 个不同的二分类 SVM，处理所有可能的类别对。</li>
<li>测试时，根据获得最多“票数”的类别对测试点进行分类。</li>
</ul>
<p><strong>优点：</strong></p>
<ul>
<li>每个分类器只需处理两个类别，避免了数据不平衡的问题。</li>
<li>分类结果相对稳定，较少出现不一致性。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>训练时间和计算量大大增加，对于较大的 $K$ 值尤其明显。</li>
</ul>
<h4>5. DAG-SVM 方法</h4>
<p><strong>原理：</strong></p>
<ul>
<li>将成对分类器组织成有向无环图（Directed Acyclic Graph）。</li>
<li>对于 $K$ 个类别，总共有 $K(K-1)/2$ 个分类器，而对新的测试点进行分类时，只需要评估 $K-1$ 个成对分类器。</li>
</ul>
<p><strong>优点：</strong></p>
<ul>
<li>分类速度较快，因为只需要评估 $K-1$ 个分类器。</li>
<li>理论上可以避免一些不一致性的问题。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>实现较复杂，需要构建和维护有向无环图。</li>
</ul>
<h4>6. 错误纠正输出码方法（Error-Correcting Output Codes）</h4>
<p><strong>原理：</strong></p>
<ul>
<li>通过设计一个编码矩阵，将多分类问题转化为多个二分类问题。</li>
<li>训练多个二分类器，并将它们的输出组合起来进行最终分类。</li>
</ul>
<p><strong>优点：</strong></p>
<ul>
<li>对错误和输出不明确性具有鲁棒性。</li>
<li>可以有效地处理多分类问题，并且具有良好的理论基础。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>设计合适的编码矩阵较复杂。</li>
<li>需要训练较多的二分类器，计算成本较高。</li>
</ul>
<h4>7. 实际应用</h4>
<p>在实际应用中，多分类 SVM 的选择往往取决于具体问题的需求和限制。以下是几种常见方法的应用场景：</p>
<ul>
<li><strong>一对多方法</strong>：实现简单，适用于少量类别的分类问题。</li>
<li><strong>一对一方法</strong>：适用于类别较多，但训练数据量不大的问题。</li>
<li><strong>DAG-SVM 方法</strong>：适用于需要快速分类的实时应用场景。</li>
<li><strong>错误纠正输出码方法</strong>：适用于对分类精度要求较高，且能够接受较高计算成本的应用。</li>
</ul>
<h3>总结</h3>
<p>多分类 SVM 是一种从二分类 SVM 扩展而来的方法，通过组合多个二分类器来处理多分类问题。主要的方法包括一对多、一对一、DAG-SVM 和错误纠正输出码方法等。每种方法都有其优缺点和适用场景，在实际应用中，根据具体问题选择合适的方法是关键。</p>
<h3>参考文献</h3>
<ul>
<li>Vapnik, V. (1998). Statistical learning theory. Wiley.</li>
<li>Lee, Y., Lin, Y., &amp; Wahba, G. (2001). Multicategory support vector machines, theory, and application to the classification of microarray data and satellite radiance data. Journal of the American Statistical Association, 99(465), 67-81.</li>
<li>Weston, J., &amp; Watkins, C. (1999). Support vector machines for multi-class pattern recognition. In ESANN (Vol. 99, pp. 219-224).</li>
<li>Platt, J. C., Cristianini, N., &amp; Shawe-Taylor, J. (2000). Large margin DAGs for multiclass classification. In Advances in neural information processing systems (pp. 547-553).</li>
<li>Dietterich, T. G., &amp; Bakiri, G. (1995). Solving multiclass learning problems via error-correcting output codes. Journal of artificial intelligence research, 2, 263-286.</li>
<li>Allwein, E. L., Schapire, R. E., &amp; Singer, Y. (2000). Reducing multiclass to binary: A unifying approach for margin classifiers. Journal of machine learning research, 1(Dec), 113-141.</li>
</ul>

    <h3>Python 文件</h3>
    <pre><code># 02_7.1.3_Multiclass_SVMs

"""
Lecture: 7_Sparse_Kernel_Machines/7.1_Maximum_Margin_Classifiers
Content: 02_7.1.3_Multiclass_SVMs
"""

import numpy as np
from scipy.optimize import minimize
from typing import Tuple, List, Callable


class MulticlassSVM:
    """
    多分类支持向量机（Multiclass SVM）实现类

    Attributes:
        C (float): 正则化参数
        kernel (Callable[[np.ndarray, np.ndarray], float]): 核函数
        classifiers (List[Tuple[int, int, np.ndarray]]): 一对一分类器的列表，包含类别对和权重向量
    """

    def __init__(self, C: float = 1.0, kernel: Callable[[np.ndarray, np.ndarray], float] = None):
        self.C = C
        self.kernel = kernel if kernel else self.linear_kernel
        self.classifiers = []

    def fit(self, X: np.ndarray, y: np.ndarray) -> None:
        """
        拟合模型，训练多分类支持向量机

        Args:
            X (np.ndarray): 训练数据特征，形状为 (n_samples, n_features)
            y (np.ndarray): 训练数据标签，形状为 (n_samples,)
        """
        self.classifiers = []
        classes = np.unique(y)
        for i, class_i in enumerate(classes):
            for j, class_j in enumerate(classes):
                if i < j:
                    # 提取类别 i 和类别 j 的数据
                    idx = np.where((y == class_i) | (y == class_j))
                    X_ij = X[idx]
                    y_ij = y[idx]
                    y_ij = np.where(y_ij == class_i, 1, -1)

                    # 训练二分类SVM
                    weights = self._fit_binary_svm(X_ij, y_ij)
                    self.classifiers.append((class_i, class_j, weights))

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        预测新数据的类别

        Args:
            X (np.ndarray): 测试数据特征，形状为 (n_samples, n_features)

        Returns:
            np.ndarray: 预测标签，形状为 (n_samples,)
        """
        votes = np.zeros((X.shape[0], len(self.classifiers)))

        for k, (class_i, class_j, weights) in enumerate(self.classifiers):
            predictions = np.sign(X.dot(weights[:-1]) + weights[-1])
            votes[:, k] = np.where(predictions == 1, class_i, class_j)

        # 投票确定最终类别
        y_pred = np.apply_along_axis(lambda x: np.bincount(x.astype(int)).argmax(), axis=1, arr=votes)
        return y_pred

    def _fit_binary_svm(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:
        """
        训练二分类SVM

        Args:
            X (np.ndarray): 二分类数据特征
            y (np.ndarray): 二分类数据标签

        Returns:
            np.ndarray: 学习到的权重向量
        """
        n_samples, n_features = X.shape
        K = self.kernel(X, X)

        # 定义优化问题
        P = np.outer(y, y) * K
        q = -np.ones(n_samples)
        G = np.vstack((-np.eye(n_samples), np.eye(n_samples)))
        h = np.hstack((np.zeros(n_samples), self.C * np.ones(n_samples)))
        A = y.reshape(1, -1)
        b = np.zeros(1)

        def objective(alpha: np.ndarray) -> float:
            return 0.5 * alpha.dot(P).dot(alpha) - alpha.sum()

        def zerofun(alpha: np.ndarray) -> float:
            return alpha.dot(y)

        # 求解拉格朗日乘数
        constraints = {'type': 'eq', 'fun': zerofun}
        bounds = [(0, self.C) for _ in range(n_samples)]
        result = minimize(objective, np.zeros(n_samples), bounds=bounds, constraints=constraints)
        alpha = result.x

        # 计算权重向量
        support_vectors = alpha > 1e-5
        alpha = alpha[support_vectors]
        support_vectors_X = X[support_vectors]
        support_vectors_y = y[support_vectors]

        weights = np.sum(alpha * support_vectors_y[:, np.newaxis] * support_vectors_X, axis=0)
        bias = np.mean(support_vectors_y - support_vectors_X.dot(weights))

        return np.append(weights, bias)

    @staticmethod
    def linear_kernel(x1: np.ndarray, x2: np.ndarray) -> float:
        """
        线性核函数

        Args:
            x1 (np.ndarray): 输入向量1
            x2 (np.ndarray): 输入向量2

        Returns:
            float: 线性核的计算结果
        """
        return np.dot(x1, x2.T)


def main():
    # 示例数据
    X = np.array([
        [2, 3],
        [3, 3],
        [3, 4],
        [5, 6],
        [6, 6],
        [6, 5],
        [10, 10],
        [10, 11],
        [11, 11]
    ])
    y = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])

    # 初始化和训练模型
    svm = MulticlassSVM(C=1.0)
    svm.fit(X, y)

    # 测试数据
    X_test = np.array([
        [4, 5],
        [8, 8],
        [10, 12]
    ])

    # 预测
    predictions = svm.predict(X_test)
    print("Predicted labels:", predictions)


if __name__ == "__main__":
    main()</code></pre>
  </div>
</body>
</html>
  