
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>7.1.4 SVMs for regression</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>03_7.1.4_SVMs_for_regression</h1>
<pre><code>Lecture: 7_Sparse_Kernel_Machines/7.1_Maximum_Margin_Classifiers
Content: 03_7.1.4_SVMs_for_regression
</code></pre>
<h3>支持向量机用于回归 (SVMs for Regression)</h3>
<h4>1. 引言</h4>
<p>支持向量机（SVM）不仅可以用于分类问题，还可以扩展到回归问题。在回归问题中，我们的目标是找到一个函数 $y(x)$，使得它能够尽可能准确地预测目标值 $t$。为了实现这一目标，我们可以引入一种称为 $\epsilon$-不敏感误差函数的工具。</p>
<h4>2. 简单线性回归</h4>
<p>在简单线性回归中，我们通过最小化正则化误差函数来进行拟合：
$$ \frac{1}{2} \sum_{n=1}^{N} (y_n - t_n)^2 + \frac{\lambda}{2} |w|^2 $$
其中，$y_n$ 是预测值，$t_n$ 是目标值，$\lambda$ 是正则化参数，$w$ 是权重向量 。</p>
<h4>3. $\epsilon$-不敏感误差函数</h4>
<p>为了获得稀疏解，我们将上述的二次误差函数替换为 $\epsilon$-不敏感误差函数（Vapnik, 1995），其在预测值 $y(x)$ 和目标值 $t$ 之间的绝对差值小于 $\epsilon$ 时给出零误差：
$$ E_{\epsilon}(y(x) - t) =
\begin{cases}
0 &amp; \text{if } |y(x) - t| &lt; \epsilon \
|y(x) - t| - \epsilon &amp; \text{otherwise}
\end{cases} $$
如图 7.6 所示 。</p>
<h4>4. 正则化误差函数</h4>
<p>我们最小化一个正则化误差函数：
$$ C \sum_{n=1}^{N} E_{\epsilon}(y(x_n) - t_n) + \frac{1}{2} |w|^2 $$
其中，$C$ 是正则化参数，控制了误差项的权重 。</p>
<h4>5. 松弛变量</h4>
<p>引入松弛变量后，可以重新表达优化问题。对于每个数据点 $x_n$，我们需要两个松弛变量 $\xi_n \geq 0$ 和 $\hat{\xi}_n \geq 0$，其中 $\xi_n &gt; 0$ 表示 $t_n &gt; y(x_n) + \epsilon$，而 $\hat{\xi}_n &gt; 0$ 表示 $t_n &lt; y(x_n) - \epsilon$。如图 7.7 所示 。</p>
<p>目标是最小化误差函数：
$$ C \sum_{n=1}^{N} (\xi_n + \hat{\xi}_n) + \frac{1}{2} |w|^2 $$
并且需要满足以下约束条件：
$$
t_n \leq y(x_n) + \epsilon + \xi_n \
t_n \geq y(x_n) - \epsilon - \hat{\xi}_n
$$</p>
<h4>6. 拉格朗日乘子法</h4>
<p>为了求解这个优化问题，我们引入拉格朗日乘子 $\alpha_n \geq 0$、$\hat{\alpha}_n \geq 0$、$\mu_n \geq 0$ 和 $\hat{\mu}<em n="1">n \geq 0$ 来优化拉格朗日函数：
$$
L = C \sum</em>^{N} (\xi_n + \hat{\xi}<em n="1">n) + \frac{1}{2} |w|^2 - \sum</em>^{N} (\mu_n \xi_n + \hat{\mu}<em n="1">n \hat{\xi}<em n="1">n) - \sum</em>^{N} \alpha_n (\epsilon + \xi_n + y_n - t_n) - \sum</em>^{N} \hat{\alpha}_n (\epsilon + \hat{\xi}_n - y_n + t_n)
$$
通过对拉格朗日函数求导并设置导数为零，可以得到优化条件 。</p>
<h4>7. 支持向量和决策函数</h4>
<p>支持向量是指那些位于 $\epsilon$-不敏感管之外的数据点，它们对应的拉格朗日乘子 $\alpha_n$ 或 $\hat{\alpha}<em n="1">n$ 不为零。这些支持向量决定了最终的回归函数。决策函数可以表示为：
$$ y(x) = \sum</em>^{N} (\alpha_n - \hat{\alpha}_n) k(x_n, x) + b $$
其中，$k(x_n, x)$ 是核函数，$b$ 是偏置项 。</p>
<h4>8. 参数选择</h4>
<p>在实际应用中，参数 $\epsilon$ 和 $C$ 通常通过交叉验证来选择，以获得最佳的模型性能。</p>
<h3>总结</h3>
<p>支持向量机用于回归（SVMs for Regression）通过引入 $\epsilon$-不敏感误差函数和松弛变量，实现了对回归问题的处理。该方法不仅保留了支持向量机在分类问题中的稀疏性和最大间隔特性，还能有效地处理回归任务。</p>
<h3>参考文献</h3>
<ul>
<li>Vapnik, V. (1995). The Nature of Statistical Learning Theory. Springer.</li>
<li>Platt, J. C., Cristianini, N., &amp; Shawe-Taylor, J. (2000). Large margin DAGS for multiclass classification. In Advances in neural information processing systems (pp. 547-553).</li>
</ul>

    <h3>Python 文件</h3>
    <pre><code># 03_7.1.4_SVMs_for_regression

"""
Lecture: 7_Sparse_Kernel_Machines/7.1_Maximum_Margin_Classifiers
Content: 03_7.1.4_SVMs_for_regression
"""

</code></pre>
  </div>
</body>
</html>
  