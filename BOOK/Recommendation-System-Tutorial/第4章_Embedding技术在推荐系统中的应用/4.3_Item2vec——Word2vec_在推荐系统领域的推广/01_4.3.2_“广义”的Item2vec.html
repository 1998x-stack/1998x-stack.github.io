
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>4.3.2 “广义”的Item2vec</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>01_4.3.2 “广义”的Item2vec</h1>
<pre><code>Lecture: 第4章 Embedding技术在推荐系统中的应用/4.3 Item2vec——Word2vec 在推荐系统领域的推广
Content: 01_4.3.2 “广义”的Item2vec
</code></pre>
<h2>4.3.2 “广义”的Item2vec</h2>
<h3>背景与概述</h3>
<p>在推荐系统中，Item2vec方法基于Word2vec模型，通过将用户的历史行为数据（如点击、购买等）视为“句子”，将物品视为“词”，利用词嵌入的方法生成物品的向量表示。这些向量表示能够捕捉物品之间的潜在关系和相似性，从而提升推荐系统的效果。然而，“广义”的Item2vec不仅限于Word2vec的方法，而是泛指所有能够生成物品嵌入向量的技术和方法，如双塔模型、图神经网络（Graph Neural Networks, GNNs）等。</p>
<h3>双塔模型（Twin Towers Model）</h3>
<h4>模型结构</h4>
<p>双塔模型是一种常用于推荐系统和广告系统中的嵌入模型，主要包括用户塔（User Tower）和物品塔（Item Tower）两个部分。其基本思想是通过两个独立的神经网络分别生成用户和物品的嵌入向量，然后通过计算用户向量和物品向量的内积或其他相似度度量来进行推荐。</p>
<ul>
<li><strong>用户塔</strong>：输入用户的各种特征（如人口统计特征、历史行为数据等），通过嵌入层和多层神经网络生成用户的嵌入向量。</li>
<li><strong>物品塔</strong>：输入物品的各种特征（如物品类别、描述、历史点击率等），通过嵌入层和多层神经网络生成物品的嵌入向量。</li>
</ul>
<h4>优点</h4>
<ol>
<li><strong>并行计算</strong>：用户塔和物品塔可以并行计算，提高了模型的训练效率和推断速度。</li>
<li><strong>灵活性</strong>：可以根据实际需求调整输入特征和网络结构，适应不同的推荐场景。</li>
<li><strong>扩展性</strong>：容易扩展到多模态数据（如文本、图像、视频等），通过融合多种特征提升推荐效果。</li>
</ol>
<h4>实际应用</h4>
<p>双塔模型广泛应用于电商、广告、视频推荐等领域。例如，在广告推荐中，用户塔输入用户的历史点击行为、人口统计特征等，物品塔输入广告的文本描述、类别等，通过计算用户和广告嵌入向量的相似度进行广告推荐。</p>
<h3>图神经网络（Graph Neural Networks, GNNs）</h3>
<h4>模型结构</h4>
<p>GNNs是处理图结构数据的一类神经网络模型，适用于捕捉物品之间复杂的关系和交互。在推荐系统中，用户和物品可以自然地表示为图中的节点，用户-物品交互可以表示为图中的边。通过GNNs，可以生成更加精细的物品嵌入向量。</p>
<ul>
<li><strong>图构建</strong>：将用户和物品表示为图中的节点，用户与物品的交互行为表示为边，构建用户-物品图。</li>
<li><strong>消息传递</strong>：通过图卷积等操作在图中进行消息传递，更新节点（用户和物品）的特征表示。</li>
<li><strong>嵌入生成</strong>：最终生成每个物品的嵌入向量，捕捉物品之间的高阶关系和语义信息。</li>
</ul>
<h4>优点</h4>
<ol>
<li><strong>高阶关系</strong>：能够捕捉用户和物品之间的高阶关系，提高推荐的准确性。</li>
<li><strong>灵活性</strong>：可以处理多种类型的图结构数据，适应复杂的推荐场景。</li>
<li><strong>信息融合</strong>：能够自然地融合用户和物品的多种特征信息，提高模型的表达能力。</li>
</ol>
<h4>实际应用</h4>
<p>GNNs在社交网络、知识图谱、电商推荐等领域有广泛应用。例如，在社交网络中，GNNs可以利用用户的社交关系和互动行为进行推荐；在电商推荐中，可以利用用户的购买历史和商品的关联关系进行推荐。</p>
<h3>总结</h3>
<p>广义的Item2vec模型不仅限于传统的Word2vec方法，而是包括了各种生成物品嵌入向量的方法，如双塔模型和图神经网络。通过学习用户的历史行为数据和物品之间的复杂关系，这些方法能够生成高质量的物品嵌入向量，从而提升推荐系统的准确性和性能。广义的Item2vec模型具有灵活性、高效性和扩展性，能够适应不同的推荐场景，为构建智能化、个性化的推荐系统提供强有力的支持。</p>

    <h3>Python 文件</h3>
    <pre><code># 01_4.3.2 “广义”的Item2vec

"""
Lecture: 第4章 Embedding技术在推荐系统中的应用/4.3 Item2vec——Word2vec 在推荐系统领域的推广
Content: 01_4.3.2 “广义”的Item2vec
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from typing import List, Dict, Tuple

class TwinTowersDataset(Dataset):
    """
    双塔模型数据集类，用于存储和提供训练数据。

    Attributes:
        user_sequences: 用户交互序列列表。
        item_sequences: 物品交互序列列表。
        user_to_idx: 用户到索引的映射字典。
        item_to_idx: 物品到索引的映射字典。
    """
    def __init__(self, user_sequences: List[List[int]], item_sequences: List[List[int]]):
        self.user_sequences = user_sequences
        self.item_sequences = item_sequences
        self.user_to_idx, self.idx_to_user = self._create_vocab(user_sequences)
        self.item_to_idx, self.idx_to_item = self._create_vocab(item_sequences)

    def _create_vocab(self, sequences: List[List[int]]) -> Tuple[Dict[int, int], Dict[int, int]]:
        """
        创建物品和索引之间的映射。
        
        Args:
            sequences: 用户或物品的交互序列列表。
        
        Returns:
            token_to_idx: 物品或用户到索引的映射字典。
            idx_to_token: 索引到物品或用户的映射字典。
        """
        token_to_idx = {}
        idx_to_token = {}
        idx = 0
        for seq in sequences:
            for token in seq:
                if token not in token_to_idx:
                    token_to_idx[token] = idx
                    idx_to_token[idx] = token
                    idx += 1
        return token_to_idx, idx_to_token

    def __len__(self) -> int:
        return len(self.user_sequences)

    def __getitem__(self, index: int) -> Tuple[List[int], List[int]]:
        user_seq = [self.user_to_idx[user] for user in self.user_sequences[index]]
        item_seq = [self.item_to_idx[item] for item in self.item_sequences[index]]
        return user_seq, item_seq

class TwinTowersModel(nn.Module):
    """
    双塔模型类，通过用户塔和物品塔生成嵌入向量。
    
    Attributes:
        user_embedding_dim: 用户嵌入向量的维度。
        item_embedding_dim: 物品嵌入向量的维度。
        user_vocab_size: 用户词汇表大小。
        item_vocab_size: 物品词汇表大小。
        user_embeddings: 用户嵌入层。
        item_embeddings: 物品嵌入层。
    """
    def __init__(self, user_vocab_size: int, item_vocab_size: int, embedding_dim: int):
        super(TwinTowersModel, self).__init__()
        self.user_embedding_dim = embedding_dim
        self.item_embedding_dim = embedding_dim
        self.user_embeddings = nn.Embedding(user_vocab_size, embedding_dim)
        self.item_embeddings = nn.Embedding(item_vocab_size, embedding_dim)

    def forward(self, user_inputs: torch.Tensor, item_inputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        user_embedding = self.user_embeddings(user_inputs)
        item_embedding = self.item_embeddings(item_inputs)
        return user_embedding, item_embedding

class TwinTowersTrainer:
    """
    双塔模型训练器类，负责数据预处理、模型训练和评估。
    
    Attributes:
        user_sequences: 用户的交互序列。
        item_sequences: 物品的交互序列。
        embedding_dim: 嵌入向量的维度。
        learning_rate: 学习率。
        epochs: 训练的轮数。
    """
    def __init__(self, user_sequences: List[List[int]], item_sequences: List[List[int]], embedding_dim: int, learning_rate: float, epochs: int):
        self.user_sequences = user_sequences
        self.item_sequences = item_sequences
        self.embedding_dim = embedding_dim
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.dataset = TwinTowersDataset(user_sequences, item_sequences)
        self.user_vocab_size = len(self.dataset.user_to_idx)
        self.item_vocab_size = len(self.dataset.item_to_idx)
        self.model = TwinTowersModel(self.user_vocab_size, self.item_vocab_size, embedding_dim)
        self.criterion = nn.MSELoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)

    def _generate_training_data(self) -> List[Tuple[int, int]]:
        """
        生成训练数据。
        
        Returns:
            training_data: 训练数据对。
        """
        training_data = []
        for user_seq, item_seq in zip(self.user_sequences, self.item_sequences):
            for user, item in zip(user_seq, item_seq):
                user_idx = self.dataset.user_to_idx[user]
                item_idx = self.dataset.item_to_idx[item]
                training_data.append((user_idx, item_idx))
        return training_data

    def train(self):
        """
        训练双塔模型。
        """
        training_data = self._generate_training_data()
        data_loader = DataLoader(training_data, batch_size=128, shuffle=True)
        
        for epoch in range(self.epochs):
            total_loss = 0
            for user_batch, item_batch in data_loader:
                self.optimizer.zero_grad()
                user_batch = user_batch.to(torch.int64)
                item_batch = item_batch.to(torch.int64)
                user_embedding, item_embedding = self.model(user_batch, item_batch)
                loss = self.criterion(user_embedding, item_embedding)
                loss.backward()
                self.optimizer.step()
                total_loss += loss.item()
            print(f"Epoch: {epoch + 1}, Loss: {total_loss:.4f}")

    def get_user_embedding(self, user: int) -> np.ndarray:
        """
        获取指定用户的嵌入向量。
        
        Args:
            user: 用户ID。
        
        Returns:
            嵌入向量。
        """
        user_idx = self.dataset.user_to_idx[user]
        embedding_vector = self.model.user_embeddings.weight[user_idx].detach().numpy()
        return embedding_vector

    def get_item_embedding(self, item: int) -> np.ndarray:
        """
        获取指定物品的嵌入向量。
        
        Args:
            item: 物品ID。
        
        Returns:
            嵌入向量。
        """
        item_idx = self.dataset.item_to_idx[item]
        embedding_vector = self.model.item_embeddings.weight[item_idx].detach().numpy()
        return embedding_vector

# 数据准备
user_sequences = [
    [1, 2, 3, 4, 2],
    [2, 3, 5, 6],
    [1, 4, 2, 5],
    # 更多用户序列...
]

item_sequences = [
    [101, 102, 103, 104, 102],
    [102, 103, 105, 106],
    [101, 104, 102, 105],
    # 更多物品序列...
]

# 训练双塔模型
trainer = TwinTowersTrainer(user_sequences=user_sequences, item_sequences=item_sequences, embedding_dim=50, learning_rate=0.001, epochs=10)
trainer.train()

# 获取用户和物品的嵌入向量
user_id = 1
item_id = 101
user_embedding_vector = trainer.get_user_embedding(user_id)
item_embedding_vector = trainer.get_item_embedding(item_id)
print(f"User {user_id} embedding vector: {user_embedding_vector}")
print(f"Item {item_id} embedding vector: {item_embedding_vector}")
</code></pre>
  </div>
</body>
</html>
  