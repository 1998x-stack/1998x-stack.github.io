
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>3.7.1 FNN——用FM的隐向量完成Embedding层初始化</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>00_3.7.1 FNN——用FM的隐向量完成Embedding层初始化</h1>
<pre><code>Lecture: 第3章 浪潮之巅——深度学习在推荐系统中的应用/3.7 FM与深度学习模型的结合
Content: 00_3.7.1 FNN——用FM的隐向量完成Embedding层初始化
</code></pre>
<h3>3.7.1 FNN——用FM的隐向量完成Embedding层初始化</h3>
<h4>1. 背景和动机</h4>
<p>在深度学习的推荐系统中，Embedding层的初始化对模型的训练速度和效果有着重要影响。传统的随机初始化方法虽然简单，但往往不包含任何先验信息，导致Embedding层的收敛速度较慢，影响整个神经网络的训练效率。为了提升Embedding层的初始化质量，FNN（Factorization Machine supported Neural Network）模型应运而生。</p>
<h4>2. FNN模型概述</h4>
<p>FNN模型由伦敦大学学院的研究人员于2016年提出，模型结构如图3-17所示。初看之下，FNN模型类似于经典的深度神经网络模型，例如Deep Crossing，从稀疏输入向量到稠密向量的转换过程也是经典的Embedding层结构。那么，FNN模型与FM模型是如何结合的呢？关键在于Embedding层的初始化方式。</p>
<h4>3. Embedding层的初始化改进</h4>
<p>在FNN模型中，Embedding层的初始化不再采用传统的随机方法，而是使用FM（Factorization Machine）模型训练得到的隐向量。FM模型通过对特征进行隐向量分解，能够捕捉到特征之间的交互信息，这些隐向量包含了有价值的先验知识。将这些隐向量作为Embedding层的初始权重，相当于在神经网络训练的起点上已经接近目标最优点，从而加速整个模型的收敛过程。</p>
<h4>4. 为什么Embedding层的收敛速度慢</h4>
<p>在深度学习网络中，Embedding层的作用是将稀疏输入向量转换成稠密向量。然而，由于以下原因，Embedding层的收敛速度往往较慢：</p>
<ol>
<li><strong>参数数量巨大</strong>：假设输入层的维度是100,000，Embedding层的输出维度是32，上层再加5层32维的全连接层，最终输出层维度是10，那么Embedding层的参数数量是32×100,000=3,200,000，而其余所有层的参数总数是4416。Embedding层的参数占比达到99.86%。</li>
<li><strong>稀疏输入的影响</strong>：在随机梯度下降的过程中，只有与非零特征相连的Embedding层权重会被更新，进一步降低了Embedding层的收敛速度。</li>
</ol>
<h4>5. FNN模型的具体实现</h4>
<p>在FNN模型中，FM模型的隐向量用于初始化Embedding层的参数。具体过程如下：</p>
<ol>
<li><strong>FM模型训练</strong>：首先使用FM模型对特征进行训练，得到各特征的隐向量。</li>
<li><strong>初始化Embedding层</strong>：将这些隐向量作为Embedding层的初始权重。假设FM隐向量的维度为m，第i个特征域的第k维特征的隐向量为$$v_{ik}$$，那么隐向量的第l维$$v_{il}$$就会成为连接输入神经元k和Embedding神经元l之间连接权重的初始值。</li>
<li><strong>多特征域的处理</strong>：在FNN模型中，不同特征被分成不同特征域，每个特征域具有对应的Embedding层，且每个特征域的Embedding维度应与FM隐向量维度保持一致。</li>
</ol>
<h4>6. FNN模型的优势</h4>
<p>使用FM模型的隐向量初始化Embedding层权重，FNN模型在以下方面具有显著优势：</p>
<ol>
<li><strong>收敛速度加快</strong>：由于初始化时已经引入了有价值的先验信息，整个神经网络的训练过程更加高效。</li>
<li><strong>参数优化</strong>：Embedding层的初始权重更接近最终的最优值，减少了训练过程中参数调整的幅度。</li>
</ol>
<h4>7. 总结</h4>
<p>FNN模型通过将FM模型的隐向量用于Embedding层的初始化，克服了传统随机初始化方法的不足，显著提升了Embedding层的收敛速度和模型的训练效率。这种创新性的结合方式，不仅在理论上提供了新的思路，也在实际应用中展现了强大的性能优势。</p>

    <h3>Python 文件</h3>
    <pre><code># 00_3.7.1 FNN——用FM的隐向量完成Embedding层初始化

"""
Lecture: 第3章 浪潮之巅——深度学习在推荐系统中的应用/3.7 FM与深度学习模型的结合
Content: 00_3.7.1 FNN——用FM的隐向量完成Embedding层初始化
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np

# 定义FM模型
class FM(nn.Module):
    def __init__(self, num_features, k):
        super(FM, self).__init__()
        self.num_features = num_features
        self.k = k
        self.linear = nn.Linear(num_features, 1)
        self.v = nn.Parameter(torch.randn(num_features, k))

    def forward(self, x):
        linear_part = self.linear(x)
        interactions_part_1 = torch.pow(torch.matmul(x, self.v), 2)
        interactions_part_2 = torch.matmul(torch.pow(x, 2), torch.pow(self.v, 2))
        interactions_part = 0.5 * torch.sum(interactions_part_1 - interactions_part_2, dim=1, keepdim=True)
        output = linear_part + interactions_part
        return output

# 定义FNN模型
class FNN(nn.Module):
    def __init__(self, num_features, k, hidden_dims, fm_model):
        super(FNN, self).__init__()
        self.num_features = num_features
        self.k = k

        # 使用FM模型的隐向量初始化Embedding层
        self.embeddings = nn.Parameter(fm_model.v.clone().detach())
        
        # 定义全连接层
        layers = []
        input_dim = num_features * k
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(input_dim, hidden_dim))
            layers.append(nn.ReLU())
            input_dim = hidden_dim
        layers.append(nn.Linear(input_dim, 1))
        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        x_embed = torch.matmul(x, self.embeddings)
        x_embed = x_embed.view(x.size(0), -1)
        output = self.layers(x_embed)
        return output

# 数据准备
num_features = 100000
k = 32
hidden_dims = [64, 32]
batch_size = 64
num_epochs = 10

# 生成示例数据
X = torch.randn(batch_size, num_features)
y = torch.randn(batch_size, 1)

# 训练FM模型
fm_model = FM(num_features, k)
optimizer_fm = optim.Adam(fm_model.parameters(), lr=0.01)
criterion = nn.MSELoss()

for epoch in range(num_epochs):
    fm_model.train()
    optimizer_fm.zero_grad()
    outputs = fm_model(X)
    loss = criterion(outputs, y)
    loss.backward()
    optimizer_fm.step()
    print(f"FM Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

# 训练FNN模型
fnn_model = FNN(num_features, k, hidden_dims, fm_model)
optimizer_fnn = optim.Adam(fnn_model.parameters(), lr=0.01)

for epoch in range(num_epochs):
    fnn_model.train()
    optimizer_fnn.zero_grad()
    outputs = fnn_model(X)
    loss = criterion(outputs, y)
    loss.backward()
    optimizer_fnn.step()
    print(f"FNN Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")</code></pre>
  </div>
</body>
</html>
  