
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>3.6.2 Wide&Deep模型的结构</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>01_3.6.2 Wide&amp;Deep模型的结构</h1>
<pre><code>Lecture: 第3章 浪潮之巅——深度学习在推荐系统中的应用/3.6 Wide&amp;Deep 模型——记忆能力和泛化能力的综合
Content: 01_3.6.2 Wide&amp;Deep模型的结构
</code></pre>
<h3>Wide &amp; Deep模型的结构</h3>
<h4>一、引言</h4>
<p>Wide &amp; Deep模型是Google于2016年提出的一种混合模型，旨在结合简单模型的记忆能力和深度神经网络的泛化能力，从而提升推荐系统的性能。以下将详细分析Wide &amp; Deep模型的结构。</p>
<h4>二、Wide &amp; Deep模型的基本结构</h4>
<p>Wide &amp; Deep模型由两个部分组成：Wide部分和Deep部分。Wide部分处理大规模稀疏特征，Deep部分则通过多层神经网络处理密集特征。这两个部分的输出在最终的输出层进行组合，形成一个统一的模型  。</p>
<h5>1. Wide部分</h5>
<p>Wide部分是一个线性模型，主要处理稀疏的类别型特征（如用户历史行为、物品属性等）。通过交叉积变换（Cross Product Transformation）函数，Wide部分能够有效地记住和利用历史数据中的共现频率。例如，对于用户安装的应用和曝光的应用，这两类特征的组合可以直接影响推荐结果。</p>
<h6>1.1 交叉积变换</h6>
<p>交叉积变换函数用于将稀疏特征组合起来，具体的形式化定义如下：
$$ c_{ki} = \begin{cases}
1, &amp; \text{如果第} i \text{个特征属于第} k \text{个组合特征} \
0, &amp; \text{否则}
\end{cases} $$
其中，$ x_i $ 是第 $ i $ 个特征的值。例如，对于组合特征“AND（user_installed_app=netflix，impression_app=pandora）”，只有当“user_installed_app=netflix”和“impression_app=pandora”这两个特征同时为1时，其对应的交叉积变换层的结果才为1，否则为0  。</p>
<h5>2. Deep部分</h5>
<p>Deep部分通过多层神经网络进行特征交叉和处理，主要包括以下几个层次：</p>
<h6>2.1 Embedding层</h6>
<p>Embedding层的作用是将稀疏的类别型特征转换成稠密的Embedding向量。每个类别型特征（如用户安装的应用、设备类型等）经过Embedding层后，会转换成对应的Embedding向量。这些Embedding向量将拼接成一个高维的特征向量  。</p>
<h6>2.2 多层全连接层（Dense Layers）</h6>
<p>在Embedding层之后，特征向量会依次经过多层全连接层（Dense Layers）。这些全连接层通常使用ReLU（Rectified Linear Unit）激活函数，以增强模型的非线性表达能力。这些层的主要作用是进行深层特征交叉，挖掘特征背后的数据模式  。</p>
<h6>2.3 输出层</h6>
<p>最后，Deep部分的输出会与Wide部分的输出一起输入到一个逻辑回归层中，进行最终的目标拟合。通过这种方式，Wide &amp; Deep模型能够同时利用记忆能力和泛化能力，既能快速响应用户的历史行为，又能准确推荐新物品  。</p>
<h4>三、Wide &amp; Deep模型的详细结构</h4>
<p>具体的Wide &amp; Deep模型结构如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/jiachen0212/images/main/wide_and_deep.png" alt="Wide &amp; Deep模型结构图"></p>
<p>图中展示了Wide部分和Deep部分的详细结构。Wide部分处理用户的历史行为特征，Deep部分处理用户的基本属性、设备信息等全量特征  。</p>
<h5>3.1 Wide部分输入</h5>
<ul>
<li><strong>已安装应用（User Installed App）</strong>：表示用户已经安装的应用，代表用户的历史行为。</li>
<li><strong>曝光应用（Impression App）</strong>：表示用户在当前会话中看到的待推荐应用。</li>
</ul>
<h5>3.2 Deep部分输入</h5>
<ul>
<li><strong>用户年龄（Age）</strong>：表示用户的年龄。</li>
<li><strong>已安装应用数量（#App Installs）</strong>：表示用户已安装的应用数量。</li>
<li><strong>设备类型（Device Class）</strong>：表示用户使用的设备类型。</li>
<li><strong>已安装应用（User Installed App）</strong>：需要经过Embedding层处理。</li>
<li><strong>曝光应用（Impression App）</strong>：需要经过Embedding层处理。</li>
</ul>
<p>这些类别型特征通过Embedding层处理后，会拼接成一个1200维的Embedding向量，再依次经过3层ReLU全连接层，最终输入到LogLoss输出层  。</p>
<h4>四、总结</h4>
<p>Wide &amp; Deep模型通过结合Wide部分的记忆能力和Deep部分的泛化能力，形成了一个强大的混合模型。这种设计使模型能够快速处理并记忆大量历史行为特征，同时具有强大的表达能力，能够深度发掘数据中的潜在模式。因此，Wide &amp; Deep模型在各种推荐系统中表现出色，成为业界主流模型之一   。</p>

    <h3>Python 文件</h3>
    <pre><code># 01_3.6.2 Wide&Deep模型的结构

"""
Lecture: 第3章 浪潮之巅——深度学习在推荐系统中的应用/3.6 Wide&Deep 模型——记忆能力和泛化能力的综合
Content: 01_3.6.2 Wide&Deep模型的结构
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import pandas as pd
from typing import List, Tuple

class WideAndDeepDataset(Dataset):
    """自定义数据集类，用于加载用户、物品和评分数据。
    
    Args:
        wide_features (np.ndarray): Wide部分的输入特征。
        deep_features (np.ndarray): Deep部分的输入特征。
        labels (np.ndarray): 标签（目标值）。
    """
    
    def __init__(self, wide_features: np.ndarray, deep_features: np.ndarray, labels: np.ndarray):
        self.wide_features = torch.FloatTensor(wide_features)
        self.deep_features = torch.LongTensor(deep_features)
        self.labels = torch.FloatTensor(labels)
    
    def __len__(self) -> int:
        return len(self.labels)
    
    def __getitem__(self, idx: int) -> Tuple[torch.FloatTensor, torch.LongTensor, torch.FloatTensor]:
        return self.wide_features[idx], self.deep_features[idx], self.labels[idx]

class WideAndDeepModel(nn.Module):
    """Wide & Deep模型定义。
    
    Args:
        input_dim_wide (int): Wide部分的输入特征维度。
        num_embeddings (int): Embedding层输入的类别数量。
        embedding_dim (int): Embedding层输出的维度。
        hidden_layers (List[int]): Deep部分隐藏层的神经元数量。
    """
    
    def __init__(self, input_dim_wide: int, num_embeddings: int, embedding_dim: int, hidden_layers: List[int]):
        super(WideAndDeepModel, self).__init__()
        
        # Wide部分
        self.linear = nn.Linear(input_dim_wide, 1)
        
        # Deep部分
        self.embedding = nn.Embedding(num_embeddings, embedding_dim)
        self.deep_layers = nn.Sequential()
        input_dim_deep = embedding_dim * input_dim_wide
        
        for i, hidden_dim in enumerate(hidden_layers):
            self.deep_layers.add_module(f'fc{i}', nn.Linear(input_dim_deep, hidden_dim))
            self.deep_layers.add_module(f'relu{i}', nn.ReLU())
            input_dim_deep = hidden_dim
            
        self.deep_layers.add_module('output', nn.Linear(input_dim_deep, 1))
        
    def forward(self, wide_input: torch.FloatTensor, deep_input: torch.LongTensor) -> torch.FloatTensor:
        # Wide部分
        wide_output = self.linear(wide_input)
        
        # Deep部分
        deep_input_emb = self.embedding(deep_input).view(deep_input.size(0), -1)
        deep_output = self.deep_layers(deep_input_emb)
        
        # Wide & Deep结合
        output = torch.sigmoid(wide_output + deep_output)
        return output

def train_model(model: nn.Module, dataloader: DataLoader, criterion: nn.BCELoss, optimizer: optim.Adam, epochs: int) -> None:
    """训练Wide & Deep模型。
    
    Args:
        model (nn.Module): Wide & Deep模型。
        dataloader (DataLoader): 训练数据的DataLoader。
        criterion (nn.BCELoss): 损失函数。
        optimizer (optim.Adam): 优化器。
        epochs (int): 训练轮数。
    """
    for epoch in range(epochs):
        model.train()
        epoch_loss = 0
        for wide_input, deep_input, label in dataloader:
            optimizer.zero_grad()
            outputs = model(wide_input, deep_input)
            loss = criterion(outputs.squeeze(), label)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        print(f'Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(dataloader):.4f}')

def evaluate_model(model: nn.Module, dataloader: DataLoader, criterion: nn.BCELoss) -> None:
    """评估Wide & Deep模型。
    
    Args:
        model (nn.Module): Wide & Deep模型。
        dataloader (DataLoader): 验证数据的DataLoader。
        criterion (nn.BCELoss): 损失函数。
    """
    model.eval()
    eval_loss = 0
    with torch.no_grad():
        for wide_input, deep_input, label in dataloader:
            outputs = model(wide_input, deep_input)
            loss = criterion(outputs.squeeze(), label)
            eval_loss += loss.item()
    print(f'Evaluation Loss: {eval_loss / len(dataloader):.4f}')

def load_data(file_path: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """加载和预处理数据。
    
    Args:
        file_path (str): 数据文件路径。

    Returns:
        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]: 训练集和验证集的特征和标签。
    """
    data = pd.read_csv(file_path)
    wide_features = data.iloc[:, :5].values  # 假设前5列是Wide特征
    deep_features = data.iloc[:, 5:-1].values  # 假设5列之后到倒数第二列是Deep特征
    labels = data.iloc[:, -1].values  # 假设最后一列是标签
    split_idx = int(len(data) * 0.8)
    return (wide_features[:split_idx], deep_features[:split_idx], labels[:split_idx],
            wide_features[split_idx:], deep_features[split_idx:], labels[split_idx:])

def main() -> None:
    """主函数，执行Wide & Deep模型的训练和评估。"""
    # 加载数据
    wide_features_train, deep_features_train, labels_train, wide_features_val, deep_features_val, labels_val = load_data('data.csv')
    
    # 创建Dataset和DataLoader
    train_dataset = WideAndDeepDataset(wide_features_train, deep_features_train, labels_train)
    val_dataset = WideAndDeepDataset(wide_features_val, deep_features_val, labels_val)
    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)

    # 模型初始化
    num_embeddings = int(deep_features_train.max()) + 1  # 假设Deep特征是类别型特征
    model = WideAndDeepModel(input_dim_wide=wide_features_train.shape[1], num_embeddings=num_embeddings, embedding_dim=8, hidden_layers=[64, 32, 16])
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # 训练模型
    train_model(model, train_dataloader, criterion, optimizer, epochs=20)

    # 评估模型
    evaluate_model(model, val_dataloader, criterion)

if __name__ == '__main__':
    main()
</code></pre>
  </div>
</body>
</html>
  