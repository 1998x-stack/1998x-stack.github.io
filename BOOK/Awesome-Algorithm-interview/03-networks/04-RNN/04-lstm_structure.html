
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>04-lstm structure</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <blockquote>
<p>详细给出lstm的结构，写出公式</p>
</blockquote>
<h3>LSTM的结构和公式</h3>
<p>长短期记忆网络（LSTM）是一种改进的循环神经网络（RNN），通过引入门控机制来解决传统RNN中的梯度消失和梯度爆炸问题。LSTM单元主要包括三个门：遗忘门、输入门和输出门，以及一个记忆单元（Cell State）。</p>
<h4>LSTM的结构</h4>
<ol>
<li>
<p><strong>遗忘门（Forget Gate）</strong>：</p>
<ul>
<li>决定需要丢弃的信息。</li>
<li>输入上一个时刻的隐藏状态 $ h_{t-1} $ 和当前输入 $ x_t $，通过一个Sigmoid激活函数计算。</li>
<li>公式：
$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$
其中，$ \sigma $ 是Sigmoid函数，$ W_f $ 是权重矩阵，$ b_f $ 是偏置项。</li>
</ul>
</li>
<li>
<p><strong>输入门（Input Gate）</strong>：</p>
<ul>
<li>决定需要更新的信息。</li>
<li>包括两个部分：更新的候选值 $ \tilde{C}_t $ 和决定更新的比例 $ i_t $。</li>
<li>公式：
$$
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
$$
$$
\tilde{C}<em t-1="">t = \tanh(W_C \cdot [h</em>, x_t] + b_C)
$$
其中，$ \tanh $ 是Tanh函数，$ W_i $ 和 $ W_C $ 是权重矩阵，$ b_i $ 和 $ b_C $ 是偏置项。</li>
</ul>
</li>
<li>
<p><strong>记忆单元（Cell State）</strong>：</p>
<ul>
<li>通过遗忘门和输入门的作用更新记忆单元。</li>
<li>公式：
$$
C_t = f_t * C_{t-1} + i_t * \tilde{C}<em t-1="">t
$$
其中，$ C</em> $ 是上一个时刻的记忆单元状态。</li>
</ul>
</li>
<li>
<p><strong>输出门（Output Gate）</strong>：</p>
<ul>
<li>决定当前时刻的隐藏状态 $ h_t $。</li>
<li>公式：
$$
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
$$
$$
h_t = o_t * \tanh(C_t)
$$
其中，$ W_o $ 是权重矩阵，$ b_o $ 是偏置项。</li>
</ul>
</li>
</ol>
<h3>完整的LSTM公式总结</h3>
<ol>
<li>
<p><strong>遗忘门</strong>：
$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$</p>
</li>
<li>
<p><strong>输入门</strong>：
$$
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
$$
$$
\tilde{C}<em t-1="">t = \tanh(W_C \cdot [h</em>, x_t] + b_C)
$$</p>
</li>
<li>
<p><strong>记忆单元更新</strong>：
$$
C_t = f_t * C_{t-1} + i_t * \tilde{C}_t
$$</p>
</li>
<li>
<p><strong>输出门</strong>：
$$
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
$$
$$
h_t = o_t * \tanh(C_t)
$$</p>
</li>
</ol>
<h3>参考资料</h3>
<ol>
<li>
<p><strong>&quot;Long Short-Term Memory&quot; by Hochreiter and Schmidhuber</strong>：</p>
<ul>
<li>这篇论文详细描述了LSTM的结构和解决长时间依赖问题的方法。</li>
<li><a href="https://www.bioinf.jku.at/publications/older/2604.pdf">论文链接</a></li>
</ul>
</li>
<li>
<p><strong>&quot;Understanding LSTM Networks&quot; by Christopher Olah</strong>：</p>
<ul>
<li>这篇博客文章通俗易懂地解释了LSTM的工作原理和各个门的作用。</li>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">博客链接</a></li>
</ul>
</li>
<li>
<p><strong>&quot;Neural Networks and Deep Learning&quot; by Michael Nielsen</strong>：</p>
<ul>
<li>本书介绍了深度学习的基础知识，包括LSTM在内的多种神经网络结构。</li>
<li><a href="http://neuralnetworksanddeeplearning.com/">在线书籍链接</a></li>
</ul>
</li>
</ol>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  