
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>04-degradation</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <blockquote>
<p>resnet解决的问题是什么，和过拟合有什么区别？</p>
</blockquote>
<h3>ResNet解决的问题</h3>
<h4>1. 深度神经网络训练中的退化问题</h4>
<p><strong>退化问题（Degradation Problem）</strong>：在深度神经网络中，随着网络层数的增加，模型的训练误差不仅没有减小，反而出现了增大的现象。这种现象称为退化问题。即使模型没有出现过拟合问题，深层网络仍然难以在训练集上达到良好的性能。</p>
<p><strong>ResNet的解决方法</strong>：</p>
<ul>
<li><strong>残差模块（Residual Block）</strong>：ResNet通过引入残差模块来解决退化问题。残差模块通过引入“快捷连接”（Skip Connection），让输入直接跳过某些层并加到输出上，从而简化了梯度的传递。</li>
<li><strong>公式</strong>：一个典型的残差模块表达式如下：
$$
\mathbf{y} = \mathcal{F}(\mathbf{x}, {W_i}) + \mathbf{x}
$$
其中，$\mathbf{x}$ 是输入，$\mathcal{F}(\mathbf{x}, {W_i})$ 是要学习的残差函数。</li>
</ul>
<p><strong>效果</strong>：</p>
<ul>
<li><strong>有效缓解梯度消失和梯度爆炸问题</strong>：残差模块使得梯度可以更直接地传递，缓解了梯度消失和梯度爆炸问题。</li>
<li><strong>简化模型优化</strong>：残差模块使得优化深度网络变得更加容易，从而可以训练更深的网络（如ResNet-50、ResNet-101等）。</li>
</ul>
<h3>ResNet与过拟合的区别</h3>
<h4>1. 过拟合（Overfitting）</h4>
<p><strong>定义</strong>：
过拟合是指模型在训练数据上表现很好，但在测试数据或新数据上表现较差。模型对训练数据中的噪声或随机波动进行了过度学习，导致其泛化能力差。</p>
<p><strong>特点</strong>：</p>
<ul>
<li><strong>训练误差低，测试误差高</strong>：模型在训练数据上的误差非常低，但在测试数据上的误差较高。</li>
<li><strong>模型复杂度过高</strong>：过拟合通常发生在模型过于复杂（如参数过多）的时候。</li>
</ul>
<p><strong>解决方法</strong>：</p>
<ul>
<li><strong>正则化</strong>：如L1和L2正则化。</li>
<li><strong>数据增强</strong>：增加训练数据量。</li>
<li><strong>Dropout</strong>：在训练过程中随机忽略一部分神经元。</li>
<li><strong>提前停止</strong>：在验证误差不再下降时停止训练。</li>
</ul>
<h4>2. ResNet与过拟合的区别</h4>
<p><strong>不同问题的解决</strong>：</p>
<ul>
<li><strong>退化问题 vs. 过拟合问题</strong>：
<ul>
<li><strong>退化问题</strong>：指的是随着网络层数增加，训练误差反而增大的现象。ResNet通过残差模块解决退化问题，使得更深的网络能够有效训练。</li>
<li><strong>过拟合问题</strong>：指的是模型在训练数据上表现很好，但在测试数据上表现较差。解决过拟合问题的方法包括正则化、数据增强、Dropout等。</li>
</ul>
</li>
</ul>
<p><strong>不同的侧重点</strong>：</p>
<ul>
<li><strong>ResNet</strong>：主要侧重于解决深度神经网络中的退化问题，通过引入残差模块来使得训练更深的网络成为可能。</li>
<li><strong>过拟合</strong>：主要侧重于提高模型的泛化能力，确保模型在未见过的数据上有良好的表现。</li>
</ul>
<h3>参考资料</h3>
<ol>
<li>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). &quot;Deep Residual Learning for Image Recognition.&quot; <a href="https://arxiv.org/abs/1512.03385">Paper link</a></li>
<li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). &quot;Deep Learning.&quot; MIT Press. <a href="http://www.deeplearningbook.org/">Book link</a></li>
<li>&quot;Understanding Overfitting in Machine Learning Models.&quot; Towards Data Science. <a href="https://towardsdatascience.com/understanding-overfitting-in-machine-learning-models-586188e0176e">Article link</a></li>
<li>&quot;A Comprehensive Survey on Deep Residual Networks.&quot; arXiv. <a href="https://arxiv.org/abs/2004.02967">Survey link</a></li>
</ol>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  