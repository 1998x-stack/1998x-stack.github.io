
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>12-dropout bn</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <blockquote>
<p>Dropout和Batch Normalization在前向传播和反向传播阶段的区别</p>
</blockquote>
<h4>Dropout</h4>
<p><strong>前向传播阶段</strong>：</p>
<ol>
<li>
<p><strong>随机忽略神经元</strong>：</p>
<ul>
<li>在训练过程中，Dropout会以一定的概率 $ p $ 随机忽略（置为零）一部分神经元的输出。</li>
<li>这种随机忽略使得每次前向传播的网络结构都不同。</li>
<li>在测试阶段，所有神经元都参与计算，但其输出会按 $ p $ 进行缩放，以补偿训练时的Dropout效果。</li>
</ul>
<p><strong>公式</strong>：</p>
<ul>
<li>训练阶段：$ \mathbf{y} = \mathbf{x} \odot \mathbf{m} $，其中 $\mathbf{m} \sim \text{Bernoulli}(p)$</li>
<li>测试阶段：$ \mathbf{y} = p \mathbf{x} $</li>
</ul>
</li>
</ol>
<p><strong>反向传播阶段</strong>：</p>
<ol>
<li><strong>梯度传递</strong>：
<ul>
<li>仅保留未被忽略的神经元传递梯度。被忽略的神经元的梯度直接设为零，不参与反向传播。</li>
<li>这意味着在训练过程中，每次更新时的有效梯度流动路径都是不同的。</li>
</ul>
</li>
</ol>
<h4>Batch Normalization (BN)</h4>
<p><strong>前向传播阶段</strong>：</p>
<ol>
<li>
<p><strong>归一化</strong>：</p>
<ul>
<li>对每一批输入数据的激活值进行归一化，使其均值为0，方差为1。</li>
<li>归一化公式：
$$
\hat{\mathbf{x}} = \frac{\mathbf{x} - \mathbb{E}[\mathbf{x}]}{\sqrt{\text{Var}[\mathbf{x}] + \epsilon}}
$$</li>
<li>其中，$\mathbb{E}[\mathbf{x}]$ 和 $\text{Var}[\mathbf{x}]$ 是当前批次的均值和方差，$\epsilon$ 是一个小常数，防止除零。</li>
</ul>
</li>
<li>
<p><strong>缩放和平移</strong>：</p>
<ul>
<li>归一化后的激活值经过可学习的缩放（$\gamma$）和平移（$\beta$）操作：
$$
\mathbf{y} = \gamma \hat{\mathbf{x}} + \beta
$$</li>
</ul>
</li>
</ol>
<p><strong>反向传播阶段</strong>：</p>
<ol>
<li>
<p><strong>梯度传递</strong>：</p>
<ul>
<li>在反向传播阶段，梯度不仅需要对网络权重进行更新，还需要对BN层的缩放参数 $\gamma$ 和平移参数 $\beta$ 进行更新。</li>
<li>反向传播过程中，需要计算归一化步骤和缩放步骤对输入 $\mathbf{x}$ 的梯度，并传递给前一层。</li>
</ul>
</li>
<li>
<p><strong>计算复杂度增加</strong>：</p>
<ul>
<li>相较于Dropout，Batch Normalization在反向传播时计算复杂度更高，因为需要计算并传递与归一化相关的梯度。</li>
</ul>
</li>
</ol>
<h3>小结</h3>
<ul>
<li>
<p><strong>Dropout</strong>：</p>
<ul>
<li><strong>前向传播</strong>：训练时随机忽略部分神经元，测试时按 $ p $ 缩放输出。</li>
<li><strong>反向传播</strong>：忽略的神经元不传递梯度，其余部分正常传递梯度。</li>
</ul>
</li>
<li>
<p><strong>Batch Normalization</strong>：</p>
<ul>
<li><strong>前向传播</strong>：对每一批输入数据进行归一化，再进行缩放和平移。</li>
<li><strong>反向传播</strong>：传递归一化步骤和缩放步骤的梯度，计算复杂度较高。</li>
</ul>
</li>
</ul>
<h3>参考文献</h3>
<ol>
<li>
<p><strong>&quot;Dropout: A Simple Way to Prevent Neural Networks from Overfitting&quot; by Srivastava et al.</strong>:</p>
<ul>
<li>详细介绍了Dropout的机制和效果。</li>
<li><a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">论文链接</a></li>
</ul>
</li>
<li>
<p><strong>&quot;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&quot; by Ioffe and Szegedy</strong>:</p>
<ul>
<li>详细介绍了Batch Normalization的机制和效果。</li>
<li><a href="https://arxiv.org/abs/1502.03167">论文链接</a></li>
</ul>
</li>
<li>
<p><strong>Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville</strong>:</p>
<ul>
<li>本书详细讨论了Dropout和Batch Normalization等正则化技术。</li>
<li><a href="http://www.deeplearningbook.org/">书籍链接</a></li>
</ul>
</li>
</ol>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  