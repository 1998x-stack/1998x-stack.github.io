
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>08-sigmoid tanh vanishing gradients</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <blockquote>
<p>sigmoid和tanh为什么会导致梯度消失？</p>
</blockquote>
<h3>Sigmoid和Tanh导致梯度消失的原因</h3>
<h4>1. Sigmoid函数</h4>
<p><strong>定义</strong>：
$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$</p>
<p><strong>导数</strong>：
$$ \sigma'(x) = \sigma(x)(1 - \sigma(x)) $$</p>
<p><strong>原因</strong>：</p>
<ul>
<li><strong>输出范围</strong>：Sigmoid函数的输出范围在 (0, 1) 之间。当输入值较大或较小时，输出值会趋近于0或1。</li>
<li><strong>梯度</strong>：在这些区域，Sigmoid函数的导数会变得非常小。例如，当输入值 $x$ 很大时， $\sigma(x) \approx 1$，则 $\sigma'(x) \approx 0$。同样地，当输入值 $x$ 很小时， $\sigma(x) \approx 0$，则 $\sigma'(x) \approx 0$。</li>
<li><strong>梯度传递</strong>：在深度网络中，这些小的梯度值通过链式法则传递到前层时，会被多次乘小数，导致梯度在传递过程中迅速减小，甚至接近于零，从而使得前层的权重几乎不更新。</li>
</ul>
<h4>2. Tanh函数</h4>
<p><strong>定义</strong>：
$$ \text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$</p>
<p><strong>导数</strong>：
$$ \text{tanh}'(x) = 1 - \text{tanh}^2(x) $$</p>
<p><strong>原因</strong>：</p>
<ul>
<li><strong>输出范围</strong>：Tanh函数的输出范围在 (-1, 1) 之间。当输入值较大或较小时，输出值会趋近于1或-1。</li>
<li><strong>梯度</strong>：在这些区域，Tanh函数的导数会变得非常小。例如，当输入值 $x$ 很大时， $\text{tanh}(x) \approx 1$，则 $\text{tanh}'(x) \approx 0$。同样地，当输入值 $x$ 很小时， $\text{tanh}(x) \approx -1$，则 $\text{tanh}'(x) \approx 0$。</li>
<li><strong>梯度传递</strong>：在深度网络中，这些小的梯度值通过链式法则传递到前层时，会被多次乘小数，导致梯度在传递过程中迅速减小，甚至接近于零，从而使得前层的权重几乎不更新。</li>
</ul>
<h3>解决方法</h3>
<ol>
<li>
<p><strong>使用ReLU激活函数</strong>：</p>
<ul>
<li>ReLU（Rectified Linear Unit）函数定义为：
$$
\text{ReLU}(x) = \max(0, x)
$$</li>
<li>ReLU的导数为1或0，避免了梯度消失问题。</li>
</ul>
</li>
<li>
<p><strong>批归一化（Batch Normalization）</strong>：</p>
<ul>
<li>对每一层的输出进行归一化，减小内部协变量偏移，保持激活值在合理范围内。</li>
</ul>
</li>
<li>
<p><strong>适当的权重初始化</strong>：</p>
<ul>
<li>使用如Xavier初始化或He初始化等方法，根据网络层数和激活函数特点合理初始化权重，避免初始阶段的梯度消失问题。</li>
</ul>
</li>
</ol>
<h3>参考资料</h3>
<ol>
<li><a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">Understanding the vanishing gradient problem</a></li>
<li><a href="https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484">Why does the sigmoid and tanh activation function lead to vanishing gradients?</a></li>
<li><a href="http://www.deeplearningbook.org/">Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville</a></li>
</ol>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  