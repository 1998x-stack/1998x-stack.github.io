
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>02-bias var for ensemble</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <blockquote>
<p>如何从偏差和方差的角度解释bagging和boosting的原理？</p>
</blockquote>
<h3>从偏差和方差的角度解释Bagging和Boosting的原理</h3>
<p>在机器学习中，模型的泛化误差可以分为三个部分：偏差（Bias）、方差（Variance）和噪声（Noise）。偏差反映了模型预测值与真实值的偏离程度，方差反映了模型对不同训练集的敏感程度。Bagging和Boosting是两种常见的集成学习方法，通过不同的机制来减小偏差和方差，从而提高模型的泛化能力。</p>
<h4>Bagging（Bootstrap Aggregating）</h4>
<p><strong>原理</strong>：</p>
<ul>
<li>Bagging通过生成多个不同的子训练集（通常是通过有放回抽样），训练多个基模型，并将这些基模型的预测结果进行平均（回归任务）或投票（分类任务），以减少方差。</li>
</ul>
<p><strong>偏差和方差分析</strong>：</p>
<ul>
<li><strong>减少方差</strong>：通过训练多个基模型并将它们的结果平均或投票，Bagging降低了单个模型对数据变化的敏感性，从而减少了方差。这是因为不同的基模型可能会对数据中的噪声作出不同的响应，平均这些响应可以平滑掉一些噪声带来的误差。</li>
<li><strong>偏差不变</strong>：Bagging方法中，每个基模型的偏差与单个模型相同，因此整体偏差不会显著改变。如果基模型存在高偏差（欠拟合），Bagging不能显著改善偏差问题。</li>
</ul>
<p><strong>代表性模型</strong>：</p>
<ul>
<li>随机森林（Random Forest）是Bagging的典型应用，通过对多个决策树进行Bagging，显著减少方差，提升模型稳定性。</li>
</ul>
<h4>Boosting</h4>
<p><strong>原理</strong>：</p>
<ul>
<li>Boosting通过逐步训练多个基模型，每个模型关注前一轮模型中错误分类的样本。每轮训练时调整样本权重，使得被错误分类的样本在下一轮中得到更多关注。最终将所有基模型的结果加权组合，生成最终的预测。</li>
</ul>
<p><strong>偏差和方差分析</strong>：</p>
<ul>
<li><strong>减少偏差</strong>：Boosting通过逐步纠正前一轮模型的错误，显著减少了偏差。每个新模型都是在前一个模型的残差上进行训练，逐步逼近真实值，从而降低偏差。</li>
<li><strong>增加方差</strong>：由于每个基模型都在纠正前一轮的错误，Boosting模型对数据中的噪声较为敏感，可能会增加方差。不过，通过适当的正则化（如限制每个基模型的复杂度），可以控制方差的增加。</li>
</ul>
<p><strong>代表性模型</strong>：</p>
<ul>
<li>AdaBoost和梯度提升机（Gradient Boosting Machine, GBM）是Boosting的典型应用。XGBoost、LightGBM和CatBoost是GBM的高效实现，广泛用于各类竞赛和实际应用中。</li>
</ul>
<h3>参考文献</h3>
<ol>
<li>
<p><strong>&quot;Pattern Recognition and Machine Learning&quot; by Christopher M. Bishop</strong>：</p>
<ul>
<li>本书详细介绍了偏差、方差以及集成学习方法的理论基础。</li>
<li><a href="https://www.springer.com/gp/book/9780387310732">书籍链接</a></li>
</ul>
</li>
<li>
<p><strong>&quot;The Elements of Statistical Learning&quot; by Trevor Hastie, Robert Tibshirani, and Jerome Friedman</strong>：</p>
<ul>
<li>这本书对Bagging和Boosting的原理及其在偏差和方差上的影响进行了深入分析。</li>
<li><a href="https://web.stanford.edu/~hastie/ElemStatLearn//">书籍链接</a></li>
</ul>
</li>
<li>
<p><strong>&quot;Random Forests&quot; by Leo Breiman</strong>：</p>
<ul>
<li>详细讨论了随机森林及其通过Bagging减少方差的机制。</li>
<li><a href="https://link.springer.com/article/10.1023/A:1010933404324">论文链接</a></li>
</ul>
</li>
<li>
<p><strong>&quot;A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting&quot; by Yoav Freund and Robert E. Schapire</strong>：</p>
<ul>
<li>介绍了AdaBoost算法及其逐步减少偏差的机制。</li>
<li><a href="https://cseweb.ucsd.edu/~yfreund/papers/adaboost.pdf">论文链接</a></li>
</ul>
</li>
</ol>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  