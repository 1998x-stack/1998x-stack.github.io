
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>01-ensemble methods</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <blockquote>
<p>集成学习的分类？有什么代表性的模型和方法？</p>
</blockquote>
<h3>集成学习的分类及代表性模型和方法</h3>
<p>集成学习（Ensemble Learning）是通过结合多个基模型（Base Models）来提高模型性能和泛化能力的一种机器学习技术。集成学习方法可以分为三大类：袋装（Bagging）、提升（Boosting）和堆叠（Stacking）。</p>
<h4>1. 袋装（Bagging）</h4>
<p><strong>原理</strong>：</p>
<ul>
<li>袋装方法通过在训练集上生成多个随机子集，训练多个基模型，并将它们的预测结果进行平均或投票。Bagging可以有效减少模型的方差，提高稳定性。</li>
</ul>
<p><strong>代表性模型和方法</strong>：</p>
<ul>
<li>
<p><strong>随机森林（Random Forest）</strong>：</p>
<ul>
<li>通过对多个决策树进行训练并将其结果进行平均或投票，来提升分类和回归任务的性能。</li>
<li><a href="https://link.springer.com/article/10.1023/A:1010933404324">Random Forest原论文</a></li>
</ul>
</li>
<li>
<p><strong>Bagged Trees</strong>：</p>
<ul>
<li>通过对决策树进行Bagging，生成多个决策树的集合，并将它们的预测结果进行平均或投票。</li>
</ul>
</li>
</ul>
<h4>2. 提升（Boosting）</h4>
<p><strong>原理</strong>：</p>
<ul>
<li>提升方法通过逐步训练基模型，每次训练时关注前一轮训练中被错误分类的样本，逐步提高模型的准确性。Boosting可以有效减少模型的偏差。</li>
</ul>
<p><strong>代表性模型和方法</strong>：</p>
<ul>
<li>
<p><strong>AdaBoost（Adaptive Boosting）</strong>：</p>
<ul>
<li>每一轮训练时，调整样本的权重，使得被错误分类的样本在下一轮中得到更多关注。最终的预测结果是所有基模型加权投票的结果。</li>
<li><a href="https://cseweb.ucsd.edu/~yfreund/papers/adaboost.pdf">AdaBoost原论文</a></li>
</ul>
</li>
<li>
<p><strong>梯度提升机（Gradient Boosting Machine, GBM）</strong>：</p>
<ul>
<li>通过逐步训练新的基模型来纠正前一轮基模型的错误。每个新的基模型是在前一个模型的残差上进行训练。</li>
<li><strong>XGBoost</strong>：
<ul>
<li>一种高效的梯度提升树实现，具有高性能和灵活性。</li>
<li><a href="https://arxiv.org/abs/1603.02754">XGBoost论文</a></li>
</ul>
</li>
<li><strong>LightGBM</strong>：
<ul>
<li>另一种高效的梯度提升树实现，特别适用于大规模数据。</li>
<li><a href="https://www.microsoft.com/en-us/research/publication/lightgbm-a-highly-efficient-gradient-boosting-decision-tree/">LightGBM论文</a></li>
</ul>
</li>
<li><strong>CatBoost</strong>：
<ul>
<li>一种适用于类别特征处理的梯度提升树实现。</li>
<li><a href="https://arxiv.org/abs/1706.09516">CatBoost论文</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4>3. 堆叠（Stacking）</h4>
<p><strong>原理</strong>：</p>
<ul>
<li>堆叠方法通过训练多个基模型，将它们的预测结果作为新的特征，输入到一个更高层次的模型中进行最终预测。堆叠可以有效地结合不同模型的优势。</li>
</ul>
<p><strong>代表性模型和方法</strong>：</p>
<ul>
<li>
<p><strong>Stacked Generalization</strong>：</p>
<ul>
<li>通过将多个基模型的预测结果作为输入特征，训练一个元模型（Meta Model），以提高预测性能。</li>
<li><a href="https://machinelearning.wustl.edu/mlpapers/paper_files/Breiman96.pdf">Stacked Generalization原论文</a></li>
</ul>
</li>
<li>
<p><strong>Blending</strong>：</p>
<ul>
<li>与Stacking类似，但训练集和验证集的划分方式有所不同。Blending通常使用一个较小的验证集，并在其上进行元模型的训练。</li>
</ul>
</li>
</ul>
<h3>参考文献</h3>
<ol>
<li><strong>Random Forests</strong> by Leo Breiman:
<ul>
<li><a href="https://link.springer.com/article/10.1023/A:1010933404324">原论文链接</a></li>
</ul>
</li>
<li><strong>A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting</strong> by Yoav Freund and Robert E. Schapire:
<ul>
<li><a href="https://cseweb.ucsd.edu/~yfreund/papers/adaboost.pdf">原论文链接</a></li>
</ul>
</li>
<li><strong>XGBoost: A Scalable Tree Boosting System</strong> by Tianqi Chen and Carlos Guestrin:
<ul>
<li><a href="https://arxiv.org/abs/1603.02754">原论文链接</a></li>
</ul>
</li>
<li><strong>LightGBM: A Highly Efficient Gradient Boosting Decision Tree</strong> by Guolin Ke et al.:
<ul>
<li><a href="https://www.microsoft.com/en-us/research/publication/lightgbm-a-highly-efficient-gradient-boosting-decision-tree/">原论文链接</a></li>
</ul>
</li>
<li><strong>CatBoost: unbiased boosting with categorical features</strong> by Liudmila Prokhorenkova et al.:
<ul>
<li><a href="https://arxiv.org/abs/1706.09516">原论文链接</a></li>
</ul>
</li>
<li><strong>Stacked Generalization</strong> by David H. Wolpert:
<ul>
<li><a href="https://machinelearning.wustl.edu/mlpapers/paper_files/Breiman96.pdf">原论文链接</a></li>
</ul>
</li>
</ol>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  