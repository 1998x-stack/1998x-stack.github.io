
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>03-SVM</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <blockquote>
<p>详细手推SVM</p>
</blockquote>
<h3>手推支持向量机（SVM）</h3>
<p>支持向量机（Support Vector Machine, SVM）是一种用于分类任务的监督学习算法，其核心思想是找到一个最优超平面，使得超平面两侧的样本间隔最大，从而实现分类。SVM可以用于线性可分和非线性可分的数据集。</p>
<h4>1. 线性可分SVM</h4>
<p><strong>目标</strong>：
找到一个超平面将数据集正确分类，并使得分类间隔最大化。超平面的方程为：
$$ \mathbf{w}^T \mathbf{x} + b = 0 $$</p>
<p><strong>约束条件</strong>：
对于每个训练样本 $(\mathbf{x}_i, y_i)$，其中 $y_i \in {-1, 1}$，满足以下约束：
$$ y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 $$</p>
<p><strong>优化目标</strong>：
最大化间隔等价于最小化 $ |\mathbf{w}|^2 $：
$$ \min_{\mathbf{w}, b} \frac{1}{2} |\mathbf{w}|^2 $$</p>
<p><strong>拉格朗日函数</strong>：
引入拉格朗日乘子 $\alpha_i $，构建拉格朗日函数：
$$ L(\mathbf{w}, b, \alpha) = \frac{1}{2} |\mathbf{w}|^2 - \sum_{i=1}^{m} \alpha_i [y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1] $$</p>
<p><strong>KKT条件</strong>：
根据KKT条件，求导并设置导数为零：
$$ \frac{\partial L}{\partial \mathbf{w}} = \mathbf{w} - \sum_{i=1}^{m} \alpha_i y_i \mathbf{x}<em i="1">i = 0 $$
$$ \mathbf{w} = \sum</em>^{m} \alpha_i y_i \mathbf{x}_i $$</p>
<p>$$ \frac{\partial L}{\partial b} = \sum_{i=1}^{m} \alpha_i y_i = 0 $$</p>
<p><strong>对偶问题</strong>：
将以上结果代入拉格朗日函数，得到对偶问题：
$$ \max_{\alpha} \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}<em i="1">j $$
$$ \text{subject to } \sum</em>^{m} \alpha_i y_i = 0 \text{ and } \alpha_i \geq 0 $$</p>
<p><strong>决策函数</strong>：
通过求解对偶问题得到拉格朗日乘子 $\alpha_i$，进而得到模型参数 $\mathbf{w}$ 和 $b$。最终的分类决策函数为：
$$ f(\mathbf{x}) = \text{sign}(\sum_{i=1}^{m} \alpha_i y_i \mathbf{x}_i^T \mathbf{x} + b) $$</p>
<h4>2. 非线性可分SVM</h4>
<p>对于非线性可分数据，SVM通过核技巧（Kernel Trick）将输入数据映射到高维空间，使得在高维空间中线性可分。</p>
<p><strong>核函数</strong>：
常用的核函数包括：</p>
<ul>
<li>多项式核（Polynomial Kernel）：$$ K(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i^T \mathbf{x}_j + c)^d $$</li>
<li>高斯核（RBF Kernel）：$$ K(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\gamma |\mathbf{x}_i - \mathbf{x}_j|^2) $$</li>
</ul>
<p>通过使用核函数，优化问题变为：
$$ \max_{\alpha} \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_i \alpha_j y_i y_j K(\mathbf{x}_i, \mathbf{x}_j) $$</p>
<p><strong>决策函数</strong>：
非线性SVM的决策函数为：
$$ f(\mathbf{x}) = \text{sign}(\sum_{i=1}^{m} \alpha_i y_i K(\mathbf{x}_i, \mathbf{x}) + b) $$</p>
<h3>参考文献</h3>
<ol>
<li>
<p><strong>Pattern Recognition and Machine Learning by Christopher M. Bishop</strong>：</p>
<ul>
<li>提供了支持向量机的详细理论和推导。</li>
<li><a href="https://www.springer.com/gp/book/9780387310732">书籍链接</a></li>
</ul>
</li>
<li>
<p><strong>The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman</strong>：</p>
<ul>
<li>介绍了SVM及其他机器学习算法。</li>
<li><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">书籍链接</a></li>
</ul>
</li>
<li>
<p><strong>An Introduction to Support Vector Machines and Other Kernel-based Learning Methods by Nello Cristianini and John Shawe-Taylor</strong>：</p>
<ul>
<li>详细讨论了支持向量机和核方法。</li>
<li><a href="https://www.cambridge.org/core/books/an-introduction-to-support-vector-machines-and-other-kernel-based-learning-methods/B5CE6D8A8D9B9474AB899CF0AE8987BC">书籍链接</a></li>
</ul>
</li>
</ol>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  