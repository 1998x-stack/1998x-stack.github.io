
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>02-text feature</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <blockquote>
<p>文本如何构造特征？</p>
</blockquote>
<p>文本特征构造是自然语言处理（NLP）中的一个关键步骤，通过提取和表示文本中的有用信息，可以更好地进行分类、聚类和其他任务。以下是几种常用的文本特征构造方法：</p>
<h3>1. 基于词频的方法</h3>
<p><strong>词袋模型（Bag of Words, BoW）</strong>：</p>
<ul>
<li>将文本表示为词频向量。每个维度对应一个词，值为该词在文本中出现的频率。</li>
<li>优点：简单直观，适用于大多数文本分类任务。</li>
<li>缺点：忽略了词序和语义信息，高维度导致稀疏矩阵。</li>
</ul>
<p><strong>词频-逆文档频率（TF-IDF）</strong>：</p>
<ul>
<li>对词频进行加权，以减少常见词对文本特征的影响。TF-IDF值越高，词对文本的区分度越大。</li>
<li>公式：
$$
\text{TF-IDF}(t,d) = \text{TF}(t,d) \times \text{IDF}(t)
$$
其中，$\text{TF}(t,d)$ 是词 $t$ 在文档 $d$ 中的词频，$\text{IDF}(t)$ 是词 $t$ 的逆文档频率。</li>
<li>优点：强调重要词汇，减小常见词的影响。</li>
<li>缺点：仍然忽略了词序和上下文信息。</li>
</ul>
<h3>2. 基于词向量的方法</h3>
<p><strong>词嵌入（Word Embedding）</strong>：</p>
<ul>
<li>使用预训练的词嵌入模型（如Word2Vec、GloVe）将词表示为低维向量。词向量捕捉了词的语义和上下文信息。</li>
<li>优点：能够捕捉词语的语义关系，提高模型效果。</li>
<li>缺点：需要大量的预训练数据和计算资源。</li>
</ul>
<p><strong>上下文嵌入（Contextual Embedding）</strong>：</p>
<ul>
<li>使用上下文敏感的词嵌入模型（如BERT、GPT-3），根据词在上下文中的语境生成词向量。</li>
<li>优点：能够捕捉词的多义性和上下文信息，性能更强。</li>
<li>缺点：计算复杂度高，需要大量计算资源。</li>
</ul>
<h3>3. 基于文本结构和统计特征的方法</h3>
<p><strong>N-gram模型</strong>：</p>
<ul>
<li>将文本分割为连续的 $n$ 个词的序列（如二元组、三元组）。通过这些N-gram来表示文本特征。</li>
<li>优点：部分捕捉了词序信息，提高模型性能。</li>
<li>缺点：维度较高，计算复杂。</li>
</ul>
<p><strong>文本统计特征</strong>：</p>
<ul>
<li>计算文本的各种统计特征，如文本长度、平均词长、特殊字符数等。</li>
<li>优点：简单有效，适用于特定任务。</li>
<li>缺点：特征较为简单，无法捕捉复杂的语义信息。</li>
</ul>
<h3>4. 基于主题模型的方法</h3>
<p><strong>潜在狄利克雷分配（Latent Dirichlet Allocation, LDA）</strong>：</p>
<ul>
<li>通过主题模型将文档表示为多个主题的概率分布，每个主题由若干词组成。</li>
<li>优点：能够发现文档的潜在主题，提高文本分类效果。</li>
<li>缺点：模型训练复杂，参数调整困难。</li>
</ul>
<h3>实现示例（使用Python的scikit-learn和gensim库）</h3>
<p><strong>词袋模型和TF-IDF</strong>：</p>
<pre><code class="language-python">from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# 示例文本
corpus = [
    &quot;I love programming.&quot;,
    &quot;Programming is fun.&quot;,
    &quot;I love machine learning.&quot;
]

# 词袋模型
vectorizer = CountVectorizer()
X_bow = vectorizer.fit_transform(corpus)
print(&quot;词袋模型特征：\n&quot;, X_bow.toarray())

# TF-IDF
tfidf_vectorizer = TfidfVectorizer()
X_tfidf = tfidf_vectorizer.fit_transform(corpus)
print(&quot;TF-IDF特征：\n&quot;, X_tfidf.toarray())
</code></pre>
<p><strong>词嵌入（使用gensim库的Word2Vec）</strong>：</p>
<pre><code class="language-python">from gensim.models import Word2Vec

# 示例文本
sentences = [
    [&quot;I&quot;, &quot;love&quot;, &quot;programming&quot;],
    [&quot;Programming&quot;, &quot;is&quot;, &quot;fun&quot;],
    [&quot;I&quot;, &quot;love&quot;, &quot;machine&quot;, &quot;learning&quot;]
]

# 训练Word2Vec模型
model = Word2Vec(sentences, vector_size=10, window=5, min_count=1, workers=4)
word_vectors = model.wv

# 获取词向量
vector = word_vectors[&quot;programming&quot;]
print(&quot;Word2Vec 词向量：\n&quot;, vector)
</code></pre>
<h3>参考文献</h3>
<ol>
<li>
<p><strong>Pattern Recognition and Machine Learning by Christopher M. Bishop</strong>：</p>
<ul>
<li>提供了文本特征构造的详细理论和方法。</li>
<li><a href="https://www.springer.com/gp/book/9780387310732">书籍链接</a></li>
</ul>
</li>
<li>
<p><strong>The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman</strong>：</p>
<ul>
<li>介绍了文本特征构造及其他机器学习算法。</li>
<li><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">书籍链接</a></li>
</ul>
</li>
<li>
<p><strong>Natural Language Processing with Python by Steven Bird, Ewan Klein, and Edward Loper</strong>：</p>
<ul>
<li>提供了使用Python进行NLP的实用指南。</li>
<li><a href="https://www.oreilly.com/library/view/natural-language-processing/9780596516499/">书籍链接</a></li>
</ul>
</li>
</ol>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  