
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>03-category encoding</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <blockquote>
<p>类别变量如何构造特征？</p>
</blockquote>
<p>类别变量是指具有有限个离散值的变量，比如性别、颜色、职业等。在机器学习模型中，类别变量不能直接用于计算，因此需要进行特征构造。以下是几种常用的方法：</p>
<h3>1. 标签编码（Label Encoding）</h3>
<p><strong>原理</strong>：
将类别变量的每个类别映射到一个唯一的整数。</p>
<p><strong>实现</strong>：</p>
<pre><code class="language-python">from sklearn.preprocessing import LabelEncoder

# 示例数据
data = ['red', 'green', 'blue', 'green', 'red', 'blue']

# 标签编码
label_encoder = LabelEncoder()
encoded_data = label_encoder.fit_transform(data)

print(&quot;标签编码后的数据：&quot;, encoded_data)
</code></pre>
<p><strong>优点</strong>：</p>
<ul>
<li>简单易行，适用于有序类别。</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>对无序类别可能引入大小关系，影响模型性能。</li>
</ul>
<h3>2. 独热编码（One-Hot Encoding）</h3>
<p><strong>原理</strong>：
将每个类别变量转换为一个二进制向量，向量中只有一个位置为1，其余为0。</p>
<p><strong>实现</strong>：</p>
<pre><code class="language-python">from sklearn.preprocessing import OneHotEncoder
import numpy as np

# 示例数据
data = np.array(['red', 'green', 'blue', 'green', 'red', 'blue']).reshape(-1, 1)

# 独热编码
onehot_encoder = OneHotEncoder(sparse=False)
encoded_data = onehot_encoder.fit_transform(data)

print(&quot;独热编码后的数据：\n&quot;, encoded_data)
</code></pre>
<p><strong>优点</strong>：</p>
<ul>
<li>消除类别之间的大小关系，适用于无序类别。</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>高维度问题：类别数较多时会导致维度爆炸。</li>
</ul>
<h3>3. 二值化编码（Binary Encoding）</h3>
<p><strong>原理</strong>：
将类别变量先标签编码，然后将标签编码转换为二进制，再将二进制数的每一位作为一个新的特征。</p>
<p><strong>实现</strong>：</p>
<pre><code class="language-python">import category_encoders as ce

# 示例数据
data = ['red', 'green', 'blue', 'green', 'red', 'blue']

# 二值化编码
binary_encoder = ce.BinaryEncoder()
encoded_data = binary_encoder.fit_transform(data)

print(&quot;二值化编码后的数据：\n&quot;, encoded_data)
</code></pre>
<p><strong>优点</strong>：</p>
<ul>
<li>维度较低，适用于类别数较多的情况。</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>需要第三方库支持。</li>
</ul>
<h3>4. 频率编码（Frequency Encoding）</h3>
<p><strong>原理</strong>：
将每个类别的出现频率作为该类别的编码值。</p>
<p><strong>实现</strong>：</p>
<pre><code class="language-python">import pandas as pd

# 示例数据
data = pd.Series(['red', 'green', 'blue', 'green', 'red', 'blue'])

# 频率编码
frequency_encoding = data.value_counts() / len(data)
encoded_data = data.map(frequency_encoding)

print(&quot;频率编码后的数据：\n&quot;, encoded_data)
</code></pre>
<p><strong>优点</strong>：</p>
<ul>
<li>简单易行，保留类别出现的信息。</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>对类别频率差异较大的数据效果较差。</li>
</ul>
<h3>5. 目标编码（Target Encoding）</h3>
<p><strong>原理</strong>：
将类别变量的每个类别用目标变量的平均值进行编码。</p>
<p><strong>实现</strong>：</p>
<pre><code class="language-python">import category_encoders as ce

# 示例数据
data = pd.DataFrame({
    'color': ['red', 'green', 'blue', 'green', 'red', 'blue'],
    'target': [1, 0, 1, 0, 1, 1]
})

# 目标编码
target_encoder = ce.TargetEncoder()
encoded_data = target_encoder.fit_transform(data['color'], data['target'])

print(&quot;目标编码后的数据：\n&quot;, encoded_data)
</code></pre>
<p><strong>优点</strong>：</p>
<ul>
<li>适用于类别变量与目标变量相关联的情况。</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>可能导致数据泄露，需在交叉验证中谨慎使用。</li>
</ul>
<h3>参考文献</h3>
<ol>
<li>
<p><strong>Pattern Recognition and Machine Learning by Christopher M. Bishop</strong>：</p>
<ul>
<li>提供了特征构造的详细理论和方法。</li>
<li><a href="https://www.springer.com/gp/book/9780387310732">书籍链接</a></li>
</ul>
</li>
<li>
<p><strong>The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman</strong>：</p>
<ul>
<li>介绍了特征构造及其他机器学习算法。</li>
<li><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">书籍链接</a></li>
</ul>
</li>
<li>
<p><strong>scikit-learn文档</strong>：</p>
<ul>
<li>提供了特征构造算法的实际实现和案例。</li>
<li><a href="https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features">scikit-learn文档</a></li>
</ul>
</li>
</ol>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  