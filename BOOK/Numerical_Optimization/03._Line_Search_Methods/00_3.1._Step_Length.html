
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>3.1. Step Length</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>00_3.1._Step_Length</h1>
<pre><code>Lecture: /03._Line_Search_Methods
Content: 00_3.1._Step_Length
</code></pre>
<h3>第3.1节 步长</h3>
<p>在数值优化中的线搜索方法章节中，第3.1节探讨了步长选择的问题。步长选择在优化算法中至关重要，因为它直接影响算法的收敛性和效率。本节内容极为详细，包括步长的定义、常用的步长条件以及具体的步长选择算法。</p>
<h4>步长的定义</h4>
<p>步长（Step Length）是指在优化过程中，从当前点沿某个方向前进的距离。其主要目的是在尽可能少的迭代次数内，找到使目标函数值降低的最优点。步长的选择需要权衡计算成本和步长的有效性。</p>
<h4>Wolfe条件</h4>
<p>Wolfe条件包括两个主要部分：Armijo条件（或称为充分减小条件）和曲率条件。</p>
<ol>
<li>
<p><strong>Armijo条件</strong>
Armijo条件确保所选步长足够大，使得目标函数值有显著的减少。具体表达式为：
$$
f(x_k + \alpha p_k) \leq f(x_k) + c_1 \alpha \nabla f(x_k)^T p_k
$$
其中，$0 &lt; c_1 &lt; 1$ 是一个小常数，通常取值为 $10^{-4}$ 或 $10^{-3}$。</p>
</li>
<li>
<p><strong>曲率条件</strong>
曲率条件保证步长不会太小，确保在前进方向上有足够的下降速率。具体表达式为：
$$
\nabla f(x_k + \alpha p_k)^T p_k \geq c_2 \nabla f(x_k)^T p_k
$$
其中，$c_1 &lt; c_2 &lt; 1$。</p>
</li>
</ol>
<p>满足这两个条件的步长即为Wolfe条件步长。Wolfe条件既保证了步长的有效性，又避免了过小的步长导致的收敛速度过慢。</p>
<h4>Goldstein条件</h4>
<p>Goldstein条件与Wolfe条件类似，但在步长的选择上更为严格。Goldstein条件的两个不等式如下：
$$
f(x_k + \alpha p_k) \leq f(x_k) + c \alpha \nabla f(x_k)^T p_k
$$
$$
f(x_k + \alpha p_k) \geq f(x_k) + (1 - c) \alpha \nabla f(x_k)^T p_k
$$
其中，$0 &lt; c &lt; 0.5$。</p>
<p>这两个不等式确保步长既不会太大也不会太小，从而在每一步中既能有效减少目标函数值，又能避免过小步长带来的效率问题。</p>
<h4>足够减小与回溯</h4>
<p>足够减小（Sufficient Decrease）条件是步长选择中最基本的条件。通过回溯法（Backtracking Line Search）来满足足够减小条件，是常见的步长选择方法。步骤如下：</p>
<ol>
<li>选择初始步长 $\alpha = \alpha_0$，通常 $\alpha_0 = 1$。</li>
<li>检查Armijo条件是否满足。</li>
<li>如果不满足，则将步长乘以一个常数因子 $\rho$（例如 $\rho = 0.5$），重复步骤2，直到条件满足为止。</li>
</ol>
<p>这种方法简单且易于实现，广泛应用于各种优化算法中。</p>
<h4>步长选择算法</h4>
<p>本节介绍了几种常见的步长选择算法，包括但不限于：</p>
<ol>
<li><strong>线性插值与二次插值</strong>：通过构造插值多项式，找到使得目标函数值最小的步长。</li>
<li><strong>初始步长选择</strong>：在实际应用中，初始步长的选择也非常重要。一些方法通过经验法则或预估来选择合适的初始步长。</li>
<li><strong>满足Wolfe条件的线搜索算法</strong>：具体实现了前述的Wolfe条件，确保所选步长在保证收敛性的同时，具有足够的计算效率。</li>
</ol>
<h3>详细分析</h3>
<p>在实际应用中，步长的选择不仅影响算法的收敛速度，还可能影响到收敛的稳定性和最终的解的精度。以下是一些深入的分析和讨论：</p>
<ol>
<li>
<p><strong>步长的动态调整</strong>：优化过程中，固定步长可能无法适应目标函数的变化。动态调整步长，例如通过前几次迭代的效果来调整当前步长，可以提高算法的适应性和效率。</p>
</li>
<li>
<p><strong>多种条件的结合</strong>：单一条件可能无法全面评价步长的优劣，结合多种条件（如Wolfe条件与Goldstein条件）可以更全面地评估步长选择的合理性。</p>
</li>
<li>
<p><strong>数值稳定性</strong>：在步长选择过程中，尤其是在接近最优解时，数值误差可能对步长的影响显著。细致的数值分析和高精度计算方法可以提高步长选择的可靠性。</p>
</li>
<li>
<p><strong>应用场景差异</strong>：不同的优化问题可能对步长有不同的要求。例如，在非凸优化问题中，步长的选择可能需要更加谨慎，以避免陷入局部最优。针对不同问题特性，设计适应性更强的步长选择策略是优化算法研究的重要方向。</p>
</li>
</ol>

    <h3>Python 文件</h3>
    <pre><code># 00_3.1._Step_Length

"""
Lecture: /03._Line_Search_Methods
Content: 00_3.1._Step_Length
"""

import numpy as np

def armijo_condition(f, xk, pk, alpha, c1=1e-4):
    """
    检查是否满足Armijo条件
    Args:
        f (callable): 目标函数
        xk (np.array): 当前点
        pk (np.array): 搜索方向
        alpha (float): 步长
        c1 (float): Armijo条件的参数，默认值为1e-4
    
    Returns:
        bool: 是否满足Armijo条件
    """
    return f(xk + alpha * pk) <= f(xk) + c1 * alpha * np.dot(gradient(f, xk), pk)

def wolfe_conditions(f, xk, pk, alpha, c1=1e-4, c2=0.9):
    """
    检查是否满足Wolfe条件
    Args:
        f (callable): 目标函数
        xk (np.array): 当前点
        pk (np.array): 搜索方向
        alpha (float): 步长
        c1 (float): Wolfe条件的参数，默认值为1e-4
        c2 (float): Wolfe条件的参数，默认值为0.9
    
    Returns:
        bool: 是否满足Wolfe条件
    """
    return armijo_condition(f, xk, pk, alpha, c1) and np.dot(gradient(f, xk + alpha * pk), pk) >= c2 * np.dot(gradient(f, xk), pk)

def backtracking_line_search(f, xk, pk, alpha=1, rho=0.5, c1=1e-4):
    """
    使用回溯法选择步长，满足Armijo条件
    Args:
        f (callable): 目标函数
        xk (np.array): 当前点
        pk (np.array): 搜索方向
        alpha (float): 初始步长，默认值为1
        rho (float): 步长缩减系数，默认值为0.5
        c1 (float): Armijo条件的参数，默认值为1e-4
    
    Returns:
        float: 选择的步长
    """
    while not armijo_condition(f, xk, pk, alpha, c1):
        alpha *= rho
    return alpha

def gradient(f, x, eps=1e-8):
    """
    计算目标函数的梯度
    Args:
        f (callable): 目标函数
        x (np.array): 当前点
        eps (float): 数值梯度计算的步长，默认值为1e-8
    
    Returns:
        np.array: 梯度向量
    """
    grad = np.zeros_like(x)
    for i in range(len(x)):
        x1, x2 = np.copy(x), np.copy(x)
        x1[i] -= eps
        x2[i] += eps
        grad[i] = (f(x2) - f(x1)) / (2 * eps)
    return grad

def test_function(x):
    """
    测试用的目标函数
    Args:
        x (np.array): 输入向量
    
    Returns:
        float: 目标函数值
    """
    return np.sum(x**2)

# 示例使用
xk = np.array([1.0, 1.0])
pk = -gradient(test_function, xk)
alpha = backtracking_line_search(test_function, xk, pk)

print(f"选择的步长: {alpha}")
</code></pre>
  </div>
</body>
</html>
  