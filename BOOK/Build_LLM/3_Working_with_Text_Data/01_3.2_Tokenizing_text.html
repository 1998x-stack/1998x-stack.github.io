
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>3.2 Tokenizing text</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>01_3.2_Tokenizing_text</h1>
<pre><code>Lecture: /3_Working_with_Text_Data
Content: 01_3.2_Tokenizing_text
</code></pre>
<h3>3.2 文本分词</h3>
<h4>1. 引言</h4>
<p>文本分词是自然语言处理（NLP）中的一个关键步骤，它涉及将输入文本分解成较小的单元，通常是单词或子词。分词是创建嵌入、训练语言模型和执行其他NLP任务的必要前处理步骤。</p>
<h4>2. 分词的基本概念</h4>
<p>分词的主要目的是将连续的文本字符串分割成可以独立处理的较小单元。这些单元可以是单词、子词甚至字符。分词的结果称为“token”，每个token可以看作是文本的最小语义单位。</p>
<h4>3. 分词方法</h4>
<p>分词的方法多种多样，包括基于规则的方法、统计方法和基于深度学习的方法。常见的分词方法有：</p>
<ul>
<li><strong>空格分词</strong>：按照空格或其他标点符号分割文本。</li>
<li><strong>正则表达式分词</strong>：使用正则表达式匹配特定模式进行分词。</li>
<li><strong>字节对编码（Byte Pair Encoding，BPE）</strong>：一种基于统计的方法，通过反复合并最频繁的子词对，生成固定大小的词汇表。</li>
</ul>
<h4>4. 简单分词示例</h4>
<p>为了演示基本的分词过程，我们使用Python的正则表达式库（re）进行分词。以下是一个简单的示例：</p>
<pre><code class="language-python">import re

text = &quot;Hello, world. This, is a test.&quot;
tokens = re.split(r'(\s|,|\.)', text)
tokens = [token for token in tokens if token.strip()]
print(tokens)
</code></pre>
<p>输出结果为：</p>
<pre><code>['Hello', 'world', 'This', 'is', 'a', 'test']
</code></pre>
<p>在这个示例中，文本被分割成单词和标点符号，并去除了空白字符。</p>
<h4>5. 高级分词方法</h4>
<p>现代大型语言模型（如GPT-2和GPT-3）通常使用更复杂的分词方法，如字节对编码（BPE）。BPE通过迭代地合并最频繁的字符对，创建一个包含常见子词和单词的词汇表。BPE的优点是它能够有效地处理未见过的单词，并生成更紧凑的词汇表示。</p>
<h4>6. 特殊标记的使用</h4>
<p>在训练大型语言模型时，使用特殊标记（如&lt;|endoftext|&gt;和&lt;|unk|&gt;）可以增强模型对上下文和其他相关信息的理解。以下是两个常见的特殊标记：</p>
<ul>
<li><strong>&lt;|unk|&gt;</strong>：表示未见过的单词。</li>
<li><strong>&lt;|endoftext|&gt;</strong>：用于标记文本的结束，特别是在训练多个独立文档时，这种标记可以帮助模型理解这些文本实际上是无关的。</li>
</ul>
<h4>7. 分词过程中的挑战</h4>
<p>分词过程中的一个主要挑战是处理未知单词和多义词。使用BPE等高级分词方法可以部分解决这个问题，但仍需要精心设计和调整分词器以适应特定的任务和数据集。</p>
<h4>8. 实践中的分词器</h4>
<p>在实践中，分词器通常包含两个主要方法：编码（encode）和解码（decode）。编码方法将文本分割成tokens并转换为token IDs；解码方法则将token IDs转换回文本。以下是一个简单的分词器类的示例：</p>
<pre><code class="language-python">class SimpleTokenizer:
    def __init__(self, vocab):
        self.str_to_int = vocab
        self.int_to_str = {i: s for s, i in vocab.items()}

    def encode(self, text):
        tokens = re.split(r'([,.?_!&quot;()\']|\s)', text)
        tokens = [token.strip() for token in tokens if token.strip()]
        token_ids = [self.str_to_int.get(token, self.str_to_int['&lt;|unk|&gt;']) for token in tokens]
        return token_ids

    def decode(self, token_ids):
        tokens = [self.int_to_str[token_id] for token_id in token_ids]
        text = ' '.join(tokens)
        return re.sub(r'\s+([,.?!&quot;()\'])', r'\1', text)
</code></pre>
<h4>9. 结论</h4>
<p>文本分词是NLP中的基础步骤，对于构建和训练大型语言模型至关重要。通过理解和应用不同的分词方法，可以提高模型的性能和准确性。同时，需要不断调整和优化分词器，以适应不同的应用场景和数据集。</p>

    <h3>Python 文件</h3>
    <pre><code># 01_3.2_Tokenizing_text

"""
Lecture: /3_Working_with_Text_Data
Content: 01_3.2_Tokenizing_text
"""

import re
from typing import List, Dict

class TextTokenizer:
    """
    文本分词类，用于将输入文本分解为tokens

    Attributes:
        vocab (Dict[str, int]): 词汇表，将单词映射到索引
        str_to_int (Dict[str, int]): 单词到索引的映射
        int_to_str (Dict[int, str]): 索引到单词的映射
    """

    def __init__(self, vocab: Dict[str, int] = None):
        """
        初始化TextTokenizer类

        Args:
            vocab (Dict[str, int], optional): 初始化词汇表。默认值为None
        """
        if vocab is None:
            vocab = {"<|unk|>": 0}
        self.vocab = vocab
        self.str_to_int = vocab
        self.int_to_str = {i: s for s, i in vocab.items()}

    def build_vocab(self, sentences: List[str]):
        """
        构建词汇表和索引映射

        Args:
            sentences (List[str]): 句子列表，每个句子是一个字符串
        """
        word_set = set()
        for sentence in sentences:
            words = re.split(r'\s+', sentence)
            word_set.update(words)

        self.str_to_int = {word: idx for idx, word in enumerate(word_set, start=len(self.vocab))}
        self.int_to_str = {idx: word for word, idx in self.str_to_int.items()}
        self.str_to_int.update(self.vocab)  # 保留原始词汇表
        self.int_to_str.update({0: "<|unk|>"})  # 确保<|unk|>在索引中
        print(f"词汇表构建完成，共包含{len(self.str_to_int)}个单词。")

    def encode(self, text: str) -> List[int]:
        """
        将文本编码为token IDs

        Args:
            text (str): 输入文本

        Returns:
            List[int]: token ID列表
        """
        tokens = re.split(r'(\s|,|\.)', text)
        tokens = [token.strip() for token in tokens if token.strip()]
        token_ids = [self.str_to_int.get(token, self.str_to_int['<|unk|>']) for token in tokens]
        return token_ids

    def decode(self, token_ids: List[int]) -> str:
        """
        将token IDs解码为文本

        Args:
            token_ids (List[int]): token ID列表

        Returns:
            str: 解码后的文本
        """
        tokens = [self.int_to_str[token_id] for token_id in token_ids]
        text = ' '.join(tokens)
        return re.sub(r'\s+([,.?!"()\'])', r'\1', text)

    def most_common_tokens(self, top_n: int = 10) -> List[str]:
        """
        获取出现频率最高的前n个token

        Args:
            top_n (int): 返回的token数量

        Returns:
            List[str]: 最常见的token列表
        """
        token_counts = {token: self.vocab.count(token) for token in self.vocab}
        sorted_tokens = sorted(token_counts, key=token_counts.get, reverse=True)
        return sorted_tokens[:top_n]

# 示例用法
if __name__ == "__main__":
    sentences = [
        "Hello, world.",
        "Word embeddings are useful.",
        "Deep learning is a subset of machine learning.",
        "Machine learning is a field of artificial intelligence."
    ]

    tokenizer = TextTokenizer()
    tokenizer.build_vocab(sentences)

    text = "Machine learning is amazing."
    encoded = tokenizer.encode(text)
    print(f"文本 '{text}' 的编码结果是：{encoded}")

    decoded = tokenizer.decode(encoded)
    print(f"编码结果解码后的文本是：'{decoded}'")

    common_tokens = tokenizer.most_common_tokens(top_n=5)
    print(f"最常见的五个token是：{common_tokens}")</code></pre>
  </div>
</body>
</html>
  