
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>4.3 Multi head attention mechanisms</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>02_4.3_Multi-head_attention_mechanisms</h1>
<pre><code>Lecture: /4_Coding_Attention_Mechanisms
Content: 02_4.3_Multi-head_attention_mechanisms
</code></pre>
<h3>4.3 多头注意力机制</h3>
<h4>背景介绍</h4>
<p>多头注意力机制（Multi-Head Attention Mechanism）是Transformer模型中的核心组件，它在自注意力机制（Self-Attention Mechanism）的基础上，通过并行计算多个注意力头来捕获不同子空间中的信息。这种方法增强了模型的表达能力和学习复杂模式的能力，使其在自然语言处理（NLP）任务中表现出色。</p>
<h4>多头注意力机制的基本原理</h4>
<p>多头注意力机制通过将输入向量分割成多个头，每个头分别计算自注意力，然后将这些头的输出拼接起来，经过线性变换后生成最终的输出。</p>
<h5>具体步骤</h5>
<ol>
<li>
<p><strong>线性变换</strong>：</p>
<ul>
<li>输入序列通过线性变换生成多个查询、键和值向量。</li>
</ul>
</li>
<li>
<p><strong>并行计算自注意力</strong>：</p>
<ul>
<li>对每个查询向量，计算其与对应键向量的点积，得到注意力分数，并通过Softmax函数得到注意力权重。</li>
<li>使用注意力权重对对应值向量进行加权求和，得到每个头的输出。</li>
</ul>
</li>
<li>
<p><strong>连接和线性变换</strong>：</p>
<ul>
<li>将所有头的输出连接起来，通过线性变换生成最终的输出向量。</li>
</ul>
</li>
</ol>
<h5>数学公式</h5>
<p>假设输入序列为$X$，查询、键和值向量分别为$Q$、$K$和$V$，则多头注意力的计算过程为：</p>
<ol>
<li>
<p>生成多个查询、键和值向量：
$$ Q_i = XW_i^Q, \quad K_i = XW_i^K, \quad V_i = XW_i^V $$
其中，$W_i^Q, W_i^K, W_i^V$为线性变换的权重矩阵，$i$表示第$i$个头。</p>
</li>
<li>
<p>计算每个头的自注意力：
$$ \text{Attention}_i(Q_i, K_i, V_i) = \text{softmax}\left(\frac{Q_iK_i^T}{\sqrt{d_k}}\right)V_i $$
其中，$d_k$为键向量的维度。</p>
</li>
<li>
<p>将所有头的输出连接起来，并通过线性变换生成最终的输出：
$$ \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \ldots, \text{head}_h)W^O $$
其中，$\text{head}_i = \text{Attention}_i(Q_i, K_i, V_i)$，$h$为头的数量，$W^O$为线性变换的权重矩阵。</p>
</li>
</ol>
<h4>多头注意力机制的优点</h4>
<ol>
<li><strong>捕获多样化信息</strong>：通过并行计算多个头，多头注意力机制能够捕获不同子空间中的信息，增强模型的表达能力。</li>
<li><strong>提高模型性能</strong>：多头注意力机制可以学习到更复杂的模式，提高模型在各种任务中的性能。</li>
<li><strong>增强稳定性</strong>：多个头的并行计算可以提高模型的稳定性，减少单一头计算结果的不确定性。</li>
</ol>
<h4>多头注意力机制在Transformer中的应用</h4>
<p>在Transformer模型中，多头注意力机制被广泛应用于编码器和解码器的每一层。每一层都由多头自注意力机制和前馈神经网络组成。</p>
<h5>编码器（Encoder）</h5>
<p>编码器由多个相同的层叠加而成，每层包括一个多头自注意力机制和一个前馈神经网络。输入序列首先通过嵌入层，转化为固定维度的向量，然后依次通过每一层进行处理。</p>
<h5>解码器（Decoder）</h5>
<p>解码器结构与编码器类似，但在每层多头自注意力机制后增加了一个用于处理编码器输出的注意力机制。解码器接收目标序列作为输入，通过层层处理生成最终的输出序列。</p>
<h5>Transformer的架构</h5>
<p>Transformer模型的整体架构如下：</p>
<ul>
<li><strong>输入嵌入</strong>：将输入序列转换为固定维度的向量表示。</li>
<li><strong>位置编码</strong>：为输入向量添加位置编码，保留序列的位置信息。</li>
<li><strong>编码器层</strong>：由多个相同的编码器层叠加而成，每层包含多头自注意力机制和前馈神经网络。</li>
<li><strong>解码器层</strong>：由多个相同的解码器层叠加而成，每层包含多头自注意力机制、用于处理编码器输出的注意力机制和前馈神经网络。</li>
<li><strong>输出层</strong>：通过线性变换和Softmax函数生成最终的输出序列。</li>
</ul>
<h4>多头注意力机制的挑战</h4>
<ol>
<li><strong>计算复杂度高</strong>：多头注意力机制需要并行计算多个头，每个头都需要进行自注意力计算，计算复杂度较高。</li>
<li><strong>内存消耗大</strong>：由于并行计算多个头，多头注意力机制需要更多的内存来存储中间结果。</li>
<li><strong>训练难度大</strong>：多头注意力机制包含多个参数，需要更大的数据集和更长的训练时间。</li>
</ol>
<h4>总结</h4>
<p>多头注意力机制是Transformer模型中的核心组件，通过并行计算多个注意力头，增强了模型的表达能力和学习复杂模式的能力。多头注意力机制在自然语言处理任务中表现出色，被广泛应用于机器翻译、文本生成、情感分析等领域。尽管多头注意力机制的计算复杂度较高，但其优越的性能和灵活性使其成为现代深度学习模型的重要组成部分。</p>
<hr>
<h3>多头注意力机制的优势</h3>
<p>多头注意力机制（Multi-Head Attention Mechanism）在深度学习中，尤其是在Transformer模型中，具有显著的优势。以下是多头注意力机制的几个主要优势：</p>
<h4>1. 捕获多样化信息</h4>
<p>多头注意力机制通过并行计算多个注意力头，每个头在不同的子空间中独立学习。这意味着模型可以在不同的语义层次上捕获多样化的信息。每个头可以关注输入序列中的不同部分，从而使模型能够捕捉到更丰富和更复杂的特征。</p>
<h4>2. 提高模型性能</h4>
<p>多头注意力机制能够增强模型的表达能力，使其在处理复杂任务时表现更好。例如，在机器翻译任务中，不同的注意力头可以分别关注源语言句子中的不同词汇，从而更准确地生成目标语言句子。</p>
<h4>3. 增强稳定性</h4>
<p>由于多头注意力机制涉及多个注意力头的并行计算，相比于单一注意力机制，其计算结果更加稳定。多个头的注意力权重通过平均或拼接的方式组合，减少了单一头计算结果的不确定性和噪声，提高了模型的整体鲁棒性。</p>
<h4>4. 提高训练效率</h4>
<p>多头注意力机制可以并行计算多个注意力头，从而提高训练效率。相比于传统的RNN模型，Transformer模型中的多头注意力机制能够更高效地处理长序列数据，并且可以利用现代GPU进行并行计算，大幅度加快训练速度。</p>
<h4>5. 长距离依赖建模</h4>
<p>多头注意力机制能够有效地建模长距离依赖关系。在处理长序列时，传统的RNN模型容易遇到梯度消失或梯度爆炸的问题，而多头注意力机制通过自注意力机制，可以直接捕捉序列中任意位置之间的依赖关系，解决了这一问题。</p>
<h4>6. 提高模型可解释性</h4>
<p>多头注意力机制提供了对模型决策过程的可解释性。通过观察不同注意力头的注意力权重，可以了解模型在处理每个输入时关注的具体部分。这对于分析和调试模型非常有帮助，也使得模型的决策过程更加透明。</p>
<h4>7. 灵活性和可扩展性</h4>
<p>多头注意力机制具有很高的灵活性和可扩展性。通过调整注意力头的数量和维度，可以根据具体任务和数据集的需求，灵活地调整模型的复杂度和性能。多头注意力机制也易于与其他模型组件结合，形成更复杂的模型架构。</p>
<h3>结论</h3>
<p>多头注意力机制在深度学习中的应用，为处理复杂任务提供了强大的工具。其捕获多样化信息、提高模型性能、增强稳定性和提高训练效率的能力，使其在自然语言处理和计算机视觉等领域取得了显著的成功。通过理解和利用多头注意力机制的优势，研究人员和工程师能够构建出更高效、更强大和更具解释性的深度学习模型。</p>

    <h3>Python 文件</h3>
    <pre><code># 02_4.3_Multi-head_attention_mechanisms

"""
Lecture: /4_Coding_Attention_Mechanisms
Content: 02_4.3_Multi-head_attention_mechanisms
"""

import torch
import torch.nn as nn
from typing import Tuple, Optional

class MultiHeadAttention(nn.Module):
    def __init__(self, embed_size: int, heads: int):
        """
        多头注意力机制的初始化方法。

        参数:
        embed_size (int): 嵌入向量的维度。
        heads (int): 多头注意力机制的头数。
        """
        super(MultiHeadAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads

        assert (
            self.head_dim * heads == embed_size
        ), "Embedding size needs to be divisible by heads"

        self.values = nn.Linear(self.head_dim, embed_size, bias=False)
        self.keys = nn.Linear(self.head_dim, embed_size, bias=False)
        self.queries = nn.Linear(self.head_dim, embed_size, bias=False)
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)

    def forward(self, values: torch.Tensor, keys: torch.Tensor, query: torch.Tensor, mask: Optional[torch.Tensor]) -> torch.Tensor:
        """
        前向传播方法。

        参数:
        values (torch.Tensor): 值向量，形状为(batch_size, value_len, embed_size)。
        keys (torch.Tensor): 键向量，形状为(batch_size, key_len, embed_size)。
        query (torch.Tensor): 查询向量，形状为(batch_size, query_len, embed_size)。
        mask (torch.Tensor, optional): 掩码张量，形状为(batch_size, 1, 1, key_len)。

        返回:
        torch.Tensor: 多头注意力机制的输出，形状为(batch_size, query_len, embed_size)。
        """
        N = query.shape[0]
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]

        # 将输入分割成多个头
        values = values.reshape(N, value_len, self.heads, self.head_dim)
        keys = keys.reshape(N, key_len, self.heads, self.head_dim)
        queries = query.reshape(N, query_len, self.heads, self.head_dim)

        values = self.values(values)
        keys = self.keys(keys)
        queries = self.queries(queries)

        # 计算注意力得分
        energy = torch.einsum("nqhd,nkhd->nhqk", [queries, keys])
        
        if mask is not None:
            energy = energy.masked_fill(mask == 0, float("-1e20"))

        # 计算注意力权重
        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)

        # 计算加权和值
        out = torch.einsum("nhql,nlhd->nqhd", [attention, values]).reshape(N, query_len, self.heads * self.head_dim)

        out = self.fc_out(out)

        return out

# 示例使用
if __name__ == "__main__":
    embed_size = 256
    heads = 8
    query_len = 10
    key_len = 10
    value_len = 10
    batch_size = 64

    values = torch.rand((batch_size, value_len, embed_size))
    keys = torch.rand((batch_size, key_len, embed_size))
    query = torch.rand((batch_size, query_len, embed_size))
    mask = torch.ones((batch_size, 1, 1, key_len))

    multi_head_attention = MultiHeadAttention(embed_size, heads)
    out = multi_head_attention(values, keys, query, mask)
    print(f"Output shape: {out.shape}")
</code></pre>
  </div>
</body>
</html>
  