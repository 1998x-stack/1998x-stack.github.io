
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>2.3.5 Stochastic Games</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>04_2.3.5_Stochastic_Games</h1>
<pre><code>
Lecture: 2_Problem-solving/2.3_Adversarial_Search
Content: 04_2.3.5_Stochastic_Games

</code></pre>
<h3>2.3.5 随机博弈</h3>
<p>在本节中，我们深入探讨了随机博弈（Stochastic Games）的概念及其应用。随机博弈是指在博弈过程中，除了玩家的策略外，还有随机事件会影响游戏的结果。这类博弈在很多实际问题中都有广泛的应用，例如赌博游戏、金融市场中的投资决策等。以下是对这一章的详细分析：</p>
<h4>1. 引言</h4>
<p>随机博弈是一类包含随机事件的博弈，其中每个状态转换不仅取决于玩家的动作，还受到某种随机过程的影响。这种博弈模型在处理非确定性环境中的决策问题时尤为重要。经典的确定性博弈模型无法处理由随机性引入的不确定性，而随机博弈提供了一个框架来解决这些问题。</p>
<h4>2. 随机博弈的定义</h4>
<p>一个随机博弈通常由以下几部分组成：</p>
<ul>
<li><strong>状态空间（State Space）</strong>：表示游戏中所有可能的状态集合。</li>
<li><strong>动作空间（Action Space）</strong>：表示每个玩家在不同状态下可以采取的动作集合。</li>
<li><strong>状态转移概率（State Transition Probabilities）</strong>：给定当前状态和动作，转移到下一个状态的概率分布。</li>
<li><strong>奖励函数（Reward Function）</strong>：每个玩家在不同状态和动作组合下获得的奖励。</li>
</ul>
<p>在这种博弈中，每个玩家的目标是通过选择合适的策略来最大化其期望收益。策略的选择不仅取决于当前状态，还需要考虑到未来的随机事件对结果的影响。</p>
<h4>3. 马尔可夫决策过程（Markov Decision Process, MDP）</h4>
<p>随机博弈的一个典型模型是马尔可夫决策过程（MDP）。MDP 由以下元素构成：</p>
<ul>
<li><strong>状态集合（S）</strong>：所有可能的状态。</li>
<li><strong>动作集合（A）</strong>：所有可能的动作。</li>
<li><strong>转移概率（P）</strong>：状态转移的概率，P(s'|s, a) 表示在状态 s 采取动作 a 转移到状态 s' 的概率。</li>
<li><strong>奖励函数（R）</strong>：R(s, a) 表示在状态 s 采取动作 a 所获得的即时奖励。</li>
</ul>
<p>MDP 的目标是找到一个策略 π，使得从任意初始状态开始，期望总收益最大化。常用的方法有动态规划（Dynamic Programming）和蒙特卡罗方法（Monte Carlo Methods）。</p>
<h4>4. 策略评估与改进</h4>
<p>在随机博弈中，策略的评估和改进是关键步骤。常用的方法包括：</p>
<ul>
<li><strong>策略评估（Policy Evaluation）</strong>：计算在给定策略下的状态值函数 Vπ(s)，表示在状态 s 下遵循策略 π 的期望收益。</li>
<li><strong>策略改进（Policy Improvement）</strong>：基于当前策略的状态值函数，改进策略以提高期望收益。</li>
</ul>
<p>策略迭代（Policy Iteration）和价值迭代（Value Iteration）是常用的策略优化方法。策略迭代通过交替进行策略评估和策略改进，直到策略收敛。价值迭代则直接更新状态值函数，逐步逼近最优策略。</p>
<h4>5. Q学习（Q-Learning）</h4>
<p>Q学习是一种无模型的强化学习方法，广泛应用于随机博弈中。其核心是通过学习状态-动作值函数 Q(s, a)，在每个状态选择能最大化长期收益的动作。Q学习算法通过不断更新 Q值来逼近最优策略，其更新规则如下：
$$ Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)] $$
其中，α 是学习率，γ 是折扣因子，r 是即时奖励，s' 是执行动作 a 后的下一状态。</p>
<h4>6. 应用实例</h4>
<p>随机博弈在许多实际应用中具有重要意义。例如：</p>
<ul>
<li><strong>金融投资</strong>：在不确定的市场环境中，投资者需要根据市场变化和风险选择最优投资策略。</li>
<li><strong>机器人路径规划</strong>：在动态和不确定的环境中，机器人需要实时决策，找到最优路径以完成任务。</li>
<li><strong>在线广告投放</strong>：广告商需要根据用户行为和市场反馈，动态调整广告策略以最大化收益。</li>
</ul>
<h3>总结</h3>
<p>随机博弈提供了处理非确定性环境中决策问题的有效框架。通过马尔可夫决策过程、策略迭代、价值迭代和Q学习等方法，研究者能够在不确定的环境中找到最优策略。这些方法在实际应用中展现出巨大的潜力，为解决复杂的现实问题提供了有效的工具。</p>
<p>通过深入理解和应用这些决策方法，研究者和工程师可以开发出更加智能和高效的系统，以应对随机性和动态环境下的各种挑战。</p>

    <h3>Python 文件</h3>
    <pre><code># 04_2.3.5_Stochastic_Games

"""

Lecture: 2_Problem-solving/2.3_Adversarial_Search
Content: 04_2.3.5_Stochastic_Games

"""

import numpy as np
from typing import List, Tuple, Dict, Callable, Any, Optional

class StochasticGameState:
    """
    随机博弈状态类，用于表示博弈中的一个状态。
    """
    def __init__(self, state: Any, player: int):
        """
        初始化游戏状态。

        参数:
        - state (Any): 当前的游戏状态，可以是任意类型。
        - player (int): 当前玩家（1 或 -1）。
        """
        self.state = state
        self.player = player

    def get_legal_actions(self) -> List[Any]:
        """
        获取当前状态下的所有合法动作。

        返回:
        - List[Any]: 合法动作的列表。
        """
        raise NotImplementedError("子类应实现 get_legal_actions 方法。")

    def apply_action(self, action: Any) -> 'StochasticGameState':
        """
        在当前状态下应用一个动作，返回新的游戏状态。

        参数:
        - action (Any): 要应用的动作。

        返回:
        - StochasticGameState: 应用动作后的新游戏状态。
        """
        raise NotImplementedError("子类应实现 apply_action 方法。")

    def is_terminal(self) -> bool:
        """
        判断当前状态是否为终局状态。

        返回:
        - bool: 如果是终局状态，返回 True；否则返回 False。
        """
        raise NotImplementedError("子类应实现 is_terminal 方法。")

    def get_reward(self) -> float:
        """
        获取当前状态的奖励值（仅对终局状态调用）。

        返回:
        - float: 当前状态的奖励值。
        """
        raise NotImplementedError("子类应实现 get_reward 方法。")

class MarkovDecisionProcess:
    """
    马尔可夫决策过程（MDP）类。
    """
    def __init__(self, states: List[Any], actions: List[Any], transition_probabilities: Dict[Tuple[Any, Any, Any], float], rewards: Dict[Tuple[Any, Any], float], discount_factor: float = 0.9):
        """
        初始化MDP。

        参数:
        - states (List[Any]): 状态列表。
        - actions (List[Any]): 动作列表。
        - transition_probabilities (Dict[Tuple[Any, Any, Any], float]): 状态转移概率，格式为{(s, a, s'): p}。
        - rewards (Dict[Tuple[Any, Any], float]): 奖励函数，格式为{(s, a): r}。
        - discount_factor (float): 折扣因子，默认为0.9。
        """
        self.states = states
        self.actions = actions
        self.transition_probabilities = transition_probabilities
        self.rewards = rewards
        self.discount_factor = discount_factor

    def value_iteration(self, epsilon: float = 1e-6) -> Dict[Any, float]:
        """
        价值迭代算法，用于求解MDP。

        参数:
        - epsilon (float): 收敛阈值。

        返回:
        - Dict[Any, float]: 最优状态值函数。
        """
        V = {s: 0 for s in self.states}
        while True:
            delta = 0
            for s in self.states:
                v = V[s]
                V[s] = max(sum(self.transition_probabilities.get((s, a, s_prime), 0) * (self.rewards.get((s, a), 0) + self.discount_factor * V[s_prime]) for s_prime in self.states) for a in self.actions)
                delta = max(delta, abs(v - V[s]))
            if delta < epsilon:
                break
        return V

    def extract_policy(self, V: Dict[Any, float]) -> Dict[Any, Any]:
        """
        根据最优状态值函数提取最优策略。

        参数:
        - V (Dict[Any, float]): 最优状态值函数。

        返回:
        - Dict[Any, Any]: 最优策略。
        """
        policy = {}
        for s in self.states:
            policy[s] = max(self.actions, key=lambda a: sum(self.transition_probabilities.get((s, a, s_prime), 0) * (self.rewards.get((s, a), 0) + self.discount_factor * V[s_prime]) for s_prime in self.states))
        return policy

# 示例：随机博弈中的井字棋状态类
class TicTacToeState(StochasticGameState):
    """
    随机博弈中的井字棋游戏状态类。
    """
    def __init__(self, board: np.ndarray, player: int):
        super().__init__(board, player)

    def get_legal_actions(self) -> List[Tuple[int, int]]:
        return [(i, j) for i in range(3) for j in range(3) if self.state[i, j] == 0]

    def apply_action(self, action: Tuple[int, int]) -> 'TicTacToeState':
        new_board = np.copy(self.state)
        new_board[action] = self.player
        return TicTacToeState(new_board, -self.player)

    def is_terminal(self) -> bool:
        for i in range(3):
            if abs(np.sum(self.state[i, :])) == 3 or abs(np.sum(self.state[:, i])) == 3:
                return True
        if abs(np.sum(self.state.diagonal())) == 3 or abs(np.sum(np.fliplr(self.state).diagonal())) == 3:
            return True
        if not np.any(self.state == 0):
            return True
        return False

    def get_reward(self) -> float:
        for i in range(3):
            if np.sum(self.state[i, :]) == 3 or np.sum(self.state[:, i]) == 3:
                return 1.0
            if np.sum(self.state[i, :]) == -3 or np.sum(self.state[:, i]) == -3:
                return -1.0
        if np.sum(self.state.diagonal()) == 3 or np.sum(np.fliplr(self.state).diagonal()) == 3:
            return 1.0
        if np.sum(self.state.diagonal()) == -3 or np.sum(np.fliplr(self.state).diagonal()) == -3:
            return -1.0
        return 0.0

# 示例用法：
if __name__ == "__main__":
    # 定义井字棋初始状态
    initial_board = np.zeros((3, 3), dtype=int)
    initial_state = TicTacToeState(initial_board, 1)

    # 定义状态、动作、转移概率和奖励函数
    states = [initial_state]
    actions = initial_state.get_legal_actions()
    transition_probabilities = {}  # 这里需要定义具体的转移概率
    rewards = {}  # 这里需要定义具体的奖励函数

    # 创建MDP实例
    mdp = MarkovDecisionProcess(states, actions, transition_probabilities, rewards)

    # 执行价值迭代
    V = mdp.value_iteration()
    print("最优状态值函数:", V)

    # 提取最优策略
    policy = mdp.extract_policy(V)
    print("最优策略:", policy)
</code></pre>
  </div>
</body>
</html>
  