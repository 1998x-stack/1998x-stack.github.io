
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>3.8 小结</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>07_3.8_小结</h1>
<pre><code>Lecture: /03._有限马尔可夫决策过程
Content: 07_3.8_小结
</code></pre>
<h3>07_3.8_小结</h3>
<p>在第三章中，我们详细探讨了有限马尔可夫决策过程（Finite Markov Decision Processes, FMDP）的基本概念和方法。本节小结将回顾并总结这些关键点，以便帮助读者更好地理解和应用这些知识。</p>
<h4>有限马尔可夫决策过程的定义</h4>
<p>FMDP 是一种模型，用于描述智能体与环境之间的相互作用。模型由以下五个元素组成：</p>
<ol>
<li>状态集 $ S $：描述环境的所有可能状态。</li>
<li>动作集 $ A $：描述智能体在每个状态下可以执行的所有可能动作。</li>
<li>状态转移概率 $ P(s'|s, a) $：描述在状态 $ s $ 下执行动作 $ a $ 后转移到状态 $ s' $ 的概率。</li>
<li>奖励函数 $ R(s, a) $：描述智能体在状态 $ s $ 下执行动作 $ a $ 所得到的即时奖励。</li>
<li>折扣因子 $ \gamma $：描述未来奖励的当前价值。</li>
</ol>
<p>这些元素共同定义了一个FMDP，通过不断优化策略，智能体可以在与环境的互动中获得最大的累积奖励。</p>
<h4>策略和值函数</h4>
<p>策略 $ \pi $ 定义了智能体在每个状态下选择动作的规则，可以是确定性的，也可以是随机的。值函数用于评估给定策略的优劣：</p>
<ul>
<li>状态值函数 $ V^\pi(s) $：在策略 $ \pi $ 下，从状态 $ s $ 开始，智能体期望获得的累积奖励。</li>
<li>动作值函数 $ Q^\pi(s, a) $：在策略 $ \pi $ 下，从状态 $ s $ 开始执行动作 $ a $ 后，智能体期望获得的累积奖励。</li>
</ul>
<h4>贝尔曼方程</h4>
<p>贝尔曼方程是值函数的递归定义，表示当前状态的值与后续状态的值之间的关系：
$$ V^\pi(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s, a) [R(s, a) + \gamma V^\pi(s')] $$
$$ Q^\pi(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s, a) \sum_{a'} \pi(a'|s') Q^\pi(s', a') $$</p>
<p>这些方程是求解最优策略和最优值函数的基础。</p>
<h4>最优策略和最优值函数</h4>
<p>最优策略 $ \pi^* $ 和最优值函数 $ V^* $ 满足以下条件：</p>
<ul>
<li>最优状态值函数 $ V^* $ 满足：
$$ V^<em>(s) = \max_{a} \sum_{s'} P(s'|s, a) [R(s, a) + \gamma V^</em>(s')] $$</li>
<li>最优动作值函数 $ Q^* $ 满足：
$$ Q^<em>(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s, a) \max_{a'} Q^</em>(s', a') $$</li>
</ul>
<p>最优策略 $ \pi^* $ 是使得值函数最大的策略，通常通过策略迭代或值迭代方法求得。</p>
<h4>策略迭代和值迭代</h4>
<ul>
<li>策略迭代：交替进行策略评估和策略改进，直到收敛到最优策略。</li>
<li>值迭代：直接通过迭代更新贝尔曼方程，直到值函数收敛，然后从中提取最优策略。</li>
</ul>
<h4>总结</h4>
<p>有限马尔可夫决策过程提供了一种框架，使智能体能够通过优化其行为策略，在与环境的互动中实现目标。通过理解和应用FMDP的核心概念，如状态、动作、奖励、转移概率、值函数和贝尔曼方程，研究人员和工程师可以设计出高效的算法，解决复杂的决策问题。</p>
<p>在接下来的章节中，我们将探讨动态规划、蒙特卡洛方法以及时序差分学习等解决FMDP问题的方法，这些方法各有优劣，适用于不同的应用场景。</p>

    <h3>Python 文件</h3>
    <pre><code># 07_3.8_小结

"""
Lecture: /03._有限马尔可夫决策过程
Content: 07_3.8_小结
"""

</code></pre>
  </div>
</body>
</html>
  