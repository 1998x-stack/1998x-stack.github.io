# 02_2.3_10臂测试平台

"""
Lecture: /02._表格解法方法
Content: 02_2.3_10臂测试平台
"""

## 2.3 10臂测试平台

### 引言

为了大致评估贪婪方法和ε-贪婪动作值方法的相对有效性，作者设计了一组测试问题。这个测试问题组包含2000个随机生成的10臂强盗问题，每个强盗问题都有10个动作，每个动作的真实值 $q^*(a)$ 都是从均值为0、方差为1的正态分布中抽取的 。

### 测试平台的设计

#### 1. 动作值的分布

在每个强盗问题中，10个动作的真实值 $q^*(a)$ 是从均值为0、方差为1的正态分布中抽取的。每次选择动作 $A_t$ 后，实际的奖励 $R_t$ 是从均值为 $q^*(A_t)$ 和方差为1的正态分布中抽取的。这些分布在图2.1中用灰色显示  。

#### 2. 测试运行

对于每种学习方法，可以在1000个时间步内测量其随着经验增加而改善的性能和行为。这构成了一次运行。重复这个过程2000次，每次都使用一个不同的强盗问题，我们可以获得学习算法的平均行为的度量  。

### 实验对比

图2.2比较了贪婪方法和两种ε-贪婪方法（$\epsilon=0.01$ 和 $\epsilon=0.1$）在10臂测试平台上的表现。所有方法都使用样本平均技术来形成其动作值估计。上图显示了预期奖励随着经验的增加而增加的情况。贪婪方法在最初稍微快于其他方法，但随后在较低水平上趋于平稳。在这个测试平台上，贪婪方法的每步奖励仅约为1，而最佳可能值约为1.55。贪婪方法在长期表现显著较差，因为它经常会陷入执行次优动作的困境  。

### 关键分析

#### 1. 贪婪方法的局限性

贪婪方法总是选择当前估计值最大的动作，因此利用当前知识最大化即时奖励。然而，这种方法忽略了对其他动作的探索，可能会错过那些可能更优的动作。这导致在长期内，贪婪方法的性能较差，因为它可能会一直选择次优动作而未能发现最优动作 。

#### 2. ε-贪婪方法的优势

ε-贪婪方法通过引入一个小概率$\epsilon$，在每个时间步随机选择一个动作，从而在探索和利用之间取得平衡。这种方法确保了每个动作都会被采样无数次，从而使所有的 $Q_t(a)$ 收敛到 $q^*(a)$。实验结果显示，$\epsilon=0.1$ 的方法在长期表现最好，因为它能够更好地探索动作空间并发现最优动作 。

#### 3. 实验结果分析

- **平均奖励**：上图显示，贪婪方法在初期略快于ε-贪婪方法，但随后在较低水平上趋于平稳。ε-贪婪方法在长期表现更好，尤其是$\epsilon=0.1$ 的方法，每步奖励接近最佳可能值1.55。
- **最优动作的选择比例**：下图显示，贪婪方法仅在约三分之一的任务中找到了最优动作，而ε-贪婪方法随着时间的推移，选择最优动作的概率接近1  。

### 结论

10臂测试平台展示了贪婪方法和ε-贪婪方法在多臂强盗问题中的相对有效性。实验结果表明，ε-贪婪方法在长期表现优于贪婪方法，因为它能够在探索和利用之间取得平衡，更好地发现最优动作。这一结果强调了在强化学习中，探索对于发现最优策略的重要性。

通过10臂测试平台的实验，我们可以更清楚地理解不同动作值方法的优缺点，并为实际应用中选择合适的强化学习算法提供参考  。