# 04_2.5_跟踪非平稳问题

"""
Lecture: /02._表格解法方法
Content: 04_2.5_跟踪非平稳问题
"""

## 2.5 跟踪非平稳问题

### 引言

在强化学习中，我们经常面临非平稳的环境，其中奖励的概率会随时间变化。在这种情况下，传统的基于样本平均的动作值估计方法可能无法适应环境的变化。为了有效地应对非平稳问题，我们需要采用能够及时反映最新奖励信息的方法。

### 非平稳问题的应对策略

#### 1. 使用常数步长参数

一个应对非平稳问题的有效方法是使用常数步长参数来更新动作值估计。常数步长参数可以使得新的奖励对估计值的影响较大，从而更好地跟踪奖励分布的变化。

增量更新规则可以表示为：
$$ Q_{n+1}(a) = Q_n(a) + \alpha (R_n - Q_n(a)) $$
其中，$\alpha$ 是一个常数步长参数，取值范围为 $0 < \alpha \leq 1$。

这种方法使得 $Q_{n+1}(a)$ 成为过去奖励和初始估计 $Q_1$ 的加权平均值：
$$ Q_{n+1}(a) = \alpha R_n + (1 - \alpha)Q_n(a) $$

进一步展开可以得到：
$$ Q_{n+1}(a) = \alpha R_n + (1 - \alpha)\alpha R_{n-1} + (1 - \alpha)^2\alpha R_{n-2} + \cdots + (1 - \alpha)^{n-1}\alpha R_1 + (1 - \alpha)^n Q_1 $$

这种加权平均称为指数递减平均，因为随着时间的推移，旧的奖励权重会按指数衰减。

#### 2. 可变步长参数

有时，使用可变步长参数可能更加方便。设 $\alpha_n(a)$ 表示在第 $n$ 次选择动作 $a$ 后使用的步长参数。传统的样本平均法使用的是 $\alpha_n(a) = \frac{1}{n}$，这种方法能够保证估计值收敛到真实值。

然而，对于非平稳环境，常数步长参数 $\alpha_n(a) = \alpha$ 更为合适。虽然这种方法无法保证完全收敛，但能够持续跟踪最新的奖励变化。

### 实验与结果

在实验中，使用了10臂强盗问题来测试不同方法在非平稳环境中的表现。具体做法是让每个动作的真实值 $q^*(a)$ 以独立的随机步长变化（每步添加一个均值为0、标准差为0.01的正态分布增量）。

实验结果表明，使用常数步长参数的方法在非平稳环境中表现优于传统的样本平均法。随着时间的推移，常数步长参数方法能够更好地适应奖励分布的变化，从而在长期内获得更高的平均奖励。

### 结论

在面对非平稳问题时，使用常数步长参数更新动作值估计是一种有效的方法。它能够通过给予最新奖励更大的权重，从而更好地跟踪环境变化。尽管这种方法不能保证估计值的完全收敛，但在实际应用中往往能取得更好的表现。未来的研究可以进一步优化步长参数的选择，以提高算法的适应性和稳定性。