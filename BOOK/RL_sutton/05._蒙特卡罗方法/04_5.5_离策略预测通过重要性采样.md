# 04_5.5_离策略预测通过重要性采样

"""
Lecture: /05._蒙特卡罗方法
Content: 04_5.5_离策略预测通过重要性采样
"""

### 04_5.5 离策略预测通过重要性采样

#### 离策略预测简介

离策略预测（Off-policy Prediction）是一种在行为策略（behavior policy）与目标策略（target policy）不相同的情况下估计目标策略值函数的方法。其主要目的是在不改变当前策略的情况下，通过从其他策略生成的数据来学习和评估目标策略。为实现这一点，通常使用重要性采样（Importance Sampling）技术来调整回报的期望值，使其反映目标策略的期望。

#### 重要性采样的核心概念

1. **重要性采样**：
   - 重要性采样是一种调整样本权重以估计目标分布期望值的技术。具体而言，使用行为策略生成的回报通过乘以重要性采样比率来调整，从而得到目标策略下的期望回报。

2. **普通重要性采样**：
   - 在普通重要性采样中，返回值 $ G_t $ 通过重要性采样比率 $ \rho_{t:T-1} $ 进行调整，然后对所有返回值进行简单平均。公式如下：
     $$
     V(s) = \frac{1}{|T(s)|} \sum_{t \in T(s)} \rho_{t:T-1} G_t
     $$
   - 这种方法在理论上是无偏的，但由于比率的方差可能非常大，实际应用中会导致较高的方差。

3. **加权重要性采样**：
   - 加权重要性采样使用加权平均的方法，通过对比率进行归一化处理来减少方差。公式如下：
     $$
     V(s) = \frac{\sum_{t \in T(s)} \rho_{t:T-1} G_t}{\sum_{t \in T(s)} \rho_{t:T-1}}
     $$
   - 虽然这种方法在统计意义上是有偏的，但其偏差随着样本数量的增加逐渐减小，且在实践中方差明显较低，更加稳定。

#### 离策略预测的重要性采样算法

离策略预测的重要性采样算法可以分为以下几个步骤：

1. **初始化**：
   - 随机初始化状态值函数 $ V(s) $ 或动作值函数 $ Q(s, a) $。
   - 初始化累计权重和返回值列表。

2. **生成序列**：
   - 使用行为策略生成一个完整的序列，记录状态、动作和相应的奖励。

3. **计算重要性采样比率**：
   - 对于每一个状态-动作对，计算重要性采样比率 $ \rho_{t:T-1} $，该比率为目标策略和行为策略在当前序列下的概率之比。

4. **更新值函数**：
   - 根据计算得到的比率和返回值，使用普通重要性采样或加权重要性采样的方法更新状态值函数或动作值函数。

5. **重复**：
   - 反复进行生成序列、计算比率和更新值函数，直到值函数收敛。

#### 重要性采样的实际应用

**示例：二十一点（Blackjack）**：
- 在二十一点游戏中，玩家的目标是在不超过21点的情况下，尽量接近21点。通过离策略预测，可以在使用随机策略（行为策略）的同时评估特定策略（目标策略）的效果。实验结果表明，加权重要性采样在估计游戏状态值时，比普通重要性采样具有更低的方差，更加稳定。

#### 重要性采样的优缺点

**优点**：
1. **无偏性**：普通重要性采样在理论上是无偏的，即其期望值等于目标策略的真实值。
2. **减少方差**：加权重要性采样通过归一化处理有效减少了方差，实际应用中更加稳定。

**缺点**：
1. **高方差**：普通重要性采样的方差可能非常高，导致收敛速度较慢。
2. **偏差问题**：加权重要性采样在统计意义上是有偏的，但偏差随着样本数量的增加逐渐减小。

#### 结论

离策略预测通过重要性采样方法在不改变当前行为策略的情况下，有效评估和学习目标策略。通过调整返回值的期望，普通重要性采样和加权重要性采样分别在无偏性和方差控制上提供了不同的优势。加权重要性采样由于其更低的方差，在实际应用中更受青睐。

### 总结

离策略预测通过重要性采样方法能够有效解决行为策略与目标策略不同时的值函数估计问题。通过调整返回值的期望，普通重要性采样提供了无偏的估计，而加权重要性采样通过减少方差提供了更稳定的估计。应用如二十一点游戏展示了其在实际问题中的有效性。总的来说，离策略预测通过重要性采样在解决强化学习问题中具有重要作用和广泛应用前景。