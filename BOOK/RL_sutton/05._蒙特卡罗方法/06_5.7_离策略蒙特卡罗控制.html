
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>5.7 离策略蒙特卡罗控制</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>06_5.7_离策略蒙特卡罗控制</h1>
<pre><code>Lecture: /05._蒙特卡罗方法
Content: 06_5.7_离策略蒙特卡罗控制
</code></pre>
<h3>06_5.7 离策略蒙特卡罗控制</h3>
<h4>离策略蒙特卡罗控制简介</h4>
<p>离策略蒙特卡罗控制（Off-policy Monte Carlo Control）是一种用于强化学习中策略优化的方法。与策略内（on-policy）方法不同，离策略方法分离了生成行为和评估行为的策略。行为策略（behavior policy）用于生成样本数据，而目标策略（target policy）则用于评估和改进。离策略方法的主要优势在于，目标策略可以是确定性的（例如贪心策略），而行为策略可以是软性策略（soft policy），从而确保所有动作都有非零的选择概率。</p>
<h4>关键概念</h4>
<ol>
<li>
<p><strong>行为策略与目标策略</strong>：</p>
<ul>
<li><strong>行为策略</strong>：用于生成数据的策略，通常是软性策略，确保探索所有可能的动作。</li>
<li><strong>目标策略</strong>：需要评估和改进的策略，通常是确定性的，例如贪心策略。</li>
</ul>
</li>
<li>
<p><strong>重要性采样</strong>：</p>
<ul>
<li>由于行为策略与目标策略不同，需要使用重要性采样（Importance Sampling）来调整回报，以估计目标策略下的值函数。重要性采样比率定义为目标策略和行为策略的概率比率。</li>
</ul>
</li>
<li>
<p><strong>加权重要性采样</strong>：</p>
<ul>
<li>加权重要性采样通过对返回值进行加权平均，减少了方差。加权重要性采样的更新规则如下：
$$
Q(s, a) = Q(s, a) + \frac{W}{C(s, a)} [G - Q(s, a)]
$$
其中，$W$ 是重要性采样比率，$C(s, a)$ 是累计权重，$G$ 是回报。</li>
</ul>
</li>
</ol>
<h4>算法步骤</h4>
<ol>
<li>
<p><strong>初始化</strong>：</p>
<ul>
<li>初始化动作值函数 $Q(s, a)$ 和累计权重 $C(s, a)$。</li>
<li>初始策略为贪心策略，根据 $Q(s, a)$ 选择动作。</li>
</ul>
</li>
<li>
<p><strong>生成序列</strong>：</p>
<ul>
<li>使用行为策略生成一个完整的序列，记录状态、动作和相应的奖励。</li>
</ul>
</li>
<li>
<p><strong>计算重要性采样比率</strong>：</p>
<ul>
<li>对于每个状态-动作对，计算重要性采样比率 $W$，用于调整回报。</li>
</ul>
</li>
<li>
<p><strong>更新动作值函数</strong>：</p>
<ul>
<li>根据重要性采样比率和回报，使用加权重要性采样的方法更新动作值函数。</li>
</ul>
</li>
<li>
<p><strong>策略改进</strong>：</p>
<ul>
<li>根据当前的动作值函数 $Q(s, a)$ 更新策略，使其贪心于 $Q$。</li>
</ul>
</li>
<li>
<p><strong>重复</strong>：</p>
<ul>
<li>反复进行生成序列、计算比率、更新值函数和策略改进，直到策略收敛。</li>
</ul>
</li>
</ol>
<h4>算法示例</h4>
<p>以下是离策略蒙特卡罗控制算法的伪代码示例：</p>
<pre><code class="language-plaintext">初始化:
  对于所有状态-动作对 (s, a):
    Q(s, a) &lt;- 任意值
    C(s, a) &lt;- 0
    π(s) &lt;- argmax_a Q(s, a) (打破平局)

循环（对于每个序列）:
  使用软性行为策略 b 生成一个序列: S0, A0, R1, ..., ST-1, AT-1, RT

  G &lt;- 0
  W &lt;- 1

  对于序列中的每一步 t = T-1, T-2, ..., 0:
    G &lt;- G + Rt+1
    C(St, At) &lt;- C(St, At) + W
    Q(St, At) &lt;- Q(St, At) + (W / C(St, At)) * (G - Q(St, At))
    π(St) &lt;- argmax_a Q(St, a) (打破平局)
    如果 At ≠ π(St):
      退出内循环（进行下一个序列）
    W &lt;- W * (π(At|St) / b(At|St))
</code></pre>
<h4>优缺点分析</h4>
<p><strong>优点</strong>：</p>
<ol>
<li><strong>高效探索</strong>：通过使用软性行为策略，确保了所有状态-动作对都有非零的选择概率，促进了充分探索。</li>
<li><strong>目标策略优化</strong>：目标策略可以是确定性的，从而实现策略的最优化。</li>
</ol>
<p><strong>缺点</strong>：</p>
<ol>
<li><strong>学习速度慢</strong>：由于学习仅来自于序列末尾的行为，如果非贪心动作较多，学习速度可能较慢。</li>
<li><strong>高方差问题</strong>：尽管加权重要性采样减少了方差，但在某些情况下，方差仍可能较高，影响收敛速度。</li>
</ol>
<h4>应用实例</h4>
<p>在实际应用中，离策略蒙特卡罗控制可以用于各种强化学习任务。例如，在广告推荐系统中，可以使用行为策略生成用户点击数据，同时评估和改进目标策略，以优化广告点击率。</p>
<h3>结论</h3>
<p>离策略蒙特卡罗控制通过分离生成数据的策略和评估策略，实现了策略的高效优化。尽管存在学习速度慢和高方差等问题，但通过适当的策略选择和重要性采样方法，可以有效解决这些问题，广泛应用于强化学习任务。</p>

    <h3>Python 文件</h3>
    <pre><code># 06_5.7_离策略蒙特卡罗控制

"""
Lecture: /05._蒙特卡罗方法
Content: 06_5.7_离策略蒙特卡罗控制
"""

import numpy as np
from typing import Dict, Tuple, List

class OffPolicyMonteCarloControl:
    """
    离策略蒙特卡罗控制类，包含用于求解马尔可夫决策过程（MDP）的策略评估和改进算法。

    Attributes:
        states: 状态集合
        actions: 动作集合
        transition_probabilities: 状态转移概率矩阵
        rewards: 奖励矩阵
        gamma: 折扣因子
        behavior_policy: 行为策略
        target_policy: 目标策略
    """

    def __init__(self, states: List[int], actions: List[int],
                 transition_probabilities: Dict[Tuple[int, int, int], float],
                 rewards: Dict[Tuple[int, int, int], float], gamma: float,
                 behavior_policy: Dict[int, Dict[int, float]]):
        """
        初始化离策略蒙特卡罗控制类。

        参数:
            states: 状态集合
            actions: 动作集合
            transition_probabilities: 状态转移概率矩阵
            rewards: 奖励矩阵
            gamma: 折扣因子
            behavior_policy: 行为策略
        """
        self.states = states
        self.actions = actions
        self.transition_probabilities = transition_probabilities
        self.rewards = rewards
        self.gamma = gamma
        self.behavior_policy = behavior_policy
        self.q_value = {(s, a): 0 for s in states for a in actions}
        self.cumulative_weights = {(s, a): 0 for s in states for a in actions}
        self.target_policy = {s: np.random.choice(actions) for s in states}

    def generate_episode(self) -> List[Tuple[int, int, float]]:
        """
        生成一个遵循行为策略的完整序列。

        返回:
            List[Tuple[int, int, float]]: 序列列表，包含状态、动作和奖励
        """
        episode = []
        state = np.random.choice(self.states)
        while True:
            action = np.random.choice(self.actions, p=[self.behavior_policy[state][a] for a in self.actions])
            next_state = np.random.choice(self.states, p=[self.transition_probabilities[(state, action, s)] for s in self.states])
            reward = self.rewards[(state, action, next_state)]
            episode.append((state, action, reward))
            if next_state == None:  # 假设终止状态用 None 表示
                break
            state = next_state
        return episode

    def update_q_value(self, episode: List[Tuple[int, int, float]]) -> None:
        """
        使用生成的序列更新动作值函数。

        参数:
            episode: 序列列表，包含状态、动作和奖励
        """
        G = 0
        W = 1
        for state, action, reward in reversed(episode):
            G = self.gamma * G + reward
            self.cumulative_weights[(state, action)] += W
            self.q_value[(state, action)] += (W / self.cumulative_weights[(state, action)]) * (G - self.q_value[(state, action)])
            self.target_policy[state] = max(self.actions, key=lambda a: self.q_value[(state, a)])
            if action != self.target_policy[state]:
                break
            W *= 1.0 / self.behavior_policy[state][action]

    def off_policy_monte_carlo_control(self, num_episodes: int) -> None:
        """
        离策略蒙特卡罗控制算法，通过策略评估和策略改进找到最优策略。

        参数:
            num_episodes: 运行的序列数量
        """
        for _ in range(num_episodes):
            episode = self.generate_episode()
            self.update_q_value(episode)

# 测试代码
if __name__ == "__main__":
    # 定义状态、动作、转移概率和奖励
    states = [0, 1, 2, 3]
    actions = [0, 1]
    transition_probabilities = {
        (0, 0, 0): 0.7, (0, 0, 1): 0.3,
        (0, 1, 0): 0.4, (0, 1, 1): 0.6,
        (1, 0, 2): 0.8, (1, 0, 3): 0.2,
        (1, 1, 2): 0.5, (1, 1, 3): 0.5,
        (2, 0, 0): 0.9, (2, 0, 1): 0.1,
        (2, 1, 0): 0.2, (2, 1, 1): 0.8,
        (3, 0, 2): 0.6, (3, 0, 3): 0.4,
        (3, 1, 2): 0.3, (3, 1, 3): 0.7,
    }
    rewards = {
        (0, 0, 0): 5, (0, 0, 1): 10,
        (0, 1, 0): 2, (0, 1, 1): 7,
        (1, 0, 2): 15, (1, 0, 3): 20,
        (1, 1, 2): 9, (1, 1, 3): 13,
        (2, 0, 0): 6, (2, 0, 1): 8,
        (2, 1, 0): 3, (2, 1, 1): 4,
        (3, 0, 2): 12, (3, 0, 3): 18,
        (3, 1, 2): 14, (3, 1, 3): 11,
    }
    gamma = 0.9

    # 定义行为策略
    behavior_policy = {
        0: {0: 0.5, 1: 0.5},
        1: {0: 0.5, 1: 0.5},
        2: {0: 0.5, 1: 0.5},
        3: {0: 0.5, 1: 0.5},
    }

    num_episodes = 1000

    off_policy_mc = OffPolicyMonteCarloControl(states, actions, transition_probabilities, rewards, gamma, behavior_policy)
    off_policy_mc.off_policy_monte_carlo_control(num_episodes)

    print("最终动作值函数:", off_policy_mc.q_value)
    print("最终策略:", off_policy_mc.target_policy)
</code></pre>
  </div>
</body>
</html>
  