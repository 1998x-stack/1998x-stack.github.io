
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>概率 PCA 和因子分析</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h4>00. 概率 PCA 和因子分析（Probabilistic PCA and Factor Analysis）</h4>
<h5>背景介绍</h5>
<p>概率 PCA（Probabilistic PCA, PPCA）和因子分析（Factor Analysis, FA）是用于降维和数据表示的线性因子模型。它们通过假设观测数据是由潜在因子的线性组合加上噪声生成的，从而找到数据的低维表示。</p>
<h5>方法定义和数学原理</h5>
<p><strong>定义：</strong></p>
<ol>
<li>
<p><strong>概率 PCA（Probabilistic PCA, PPCA）：</strong>
PPCA 假设数据是由潜在因子的线性组合加上各向同性高斯噪声生成的。</p>
<p>数学表示为：
$$
x = W h + \mu + \epsilon
$$
其中，$ x $ 是观测数据，$ W $ 是因子载荷矩阵，$ h $ 是潜在因子，$ \mu $ 是均值向量，$ \epsilon $ 是各向同性高斯噪声，满足 $ \epsilon \sim \mathcal{N}(0, \sigma^2 I) $。</p>
</li>
<li>
<p><strong>因子分析（Factor Analysis, FA）：</strong>
FA 假设数据是由潜在因子的线性组合加上各变量独立的高斯噪声生成的。</p>
<p>数学表示为：
$$
x = W h + \mu + \epsilon
$$
其中，噪声 $ \epsilon $ 满足 $ \epsilon \sim \mathcal{N}(0, \Psi) $，且 $ \Psi $ 是对角矩阵。</p>
</li>
</ol>
<h5>应用示例</h5>
<p>PPCA 和 FA 在数据降维、特征提取和噪声去除等任务中有广泛应用。例如，在图像处理和自然语言处理领域，可以通过 PPCA 和 FA 提取低维特征，减少计算复杂度。</p>
<h3>TASK 3: 使用 Numpy 和 Scipy 从头实现代码</h3>
<h4>代码实现</h4>
<pre><code class="language-python">import numpy as np
from scipy.linalg import svd
from scipy.optimize import minimize
from typing import Tuple

class ProbabilisticPCA:
    def __init__(self, n_components: int):
        ```
        初始化概率 PCA 模型
        
        Args:
            n_components (int): 降维后的维数
        ```
        self.n_components = n_components
        self.W = None
        self.mu = None
        self.sigma2 = None

    def fit(self, X: np.ndarray):
        ```
        拟合概率 PCA 模型
        
        Args:
            X (np.ndarray): 输入数据，形状为 (n_samples, n_features)
        ```
        n_samples, n_features = X.shape
        self.mu = np.mean(X, axis=0)
        X_centered = X - self.mu
        U, S, Vt = svd(X_centered, full_matrices=False)
        S2 = S ** 2 / n_samples
        self.W = Vt.T[:, :self.n_components] * np.sqrt(S2[:self.n_components] - S2[self.n_components:].mean())
        self.sigma2 = S2[self.n_components:].mean()

    def transform(self, X: np.ndarray) -&gt; np.ndarray:
        ```
        将数据转换到低维空间
        
        Args:
            X (np.ndarray): 输入数据，形状为 (n_samples, n_features)
        
        Returns:
            np.ndarray: 转换后的数据，形状为 (n_samples, n_components)
        ```
        X_centered = X - self.mu
        return X_centered @ self.W

class FactorAnalysis:
    def __init__(self, n_components: int):
        ```
        初始化因子分析模型
        
        Args:
            n_components (int): 降维后的维数
        ```
        self.n_components = n_components
        self.W = None
        self.mu = None
        self.psi = None

    def fit(self, X: np.ndarray):
        ```
        拟合因子分析模型
        
        Args:
            X (np.ndarray): 输入数据，形状为 (n_samples, n_features)
        ```
        n_samples, n_features = X.shape
        self.mu = np.mean(X, axis=0)
        X_centered = X - self.mu
        U, S, Vt = svd(X_centered, full_matrices=False)
        S2 = S ** 2 / n_samples
        self.W = Vt.T[:, :self.n_components] * np.sqrt(S2[:self.n_components] - S2[self.n_components:].mean())
        self.psi = np.diag(S2[self.n_components:].mean() * np.ones(n_features))

    def transform(self, X: np.ndarray) -&gt; np.ndarray:
        ```
        将数据转换到低维空间
        
        Args:
            X (np.ndarray): 输入数据，形状为 (n_samples, n_features)
        
        Returns:
            np.ndarray: 转换后的数据，形状为 (n_samples, n_components)
        ```
        X_centered = X - self.mu
        return X_centered @ self.W

# 示例数据
np.random.seed(42)
X = np.random.rand(100, 5)

# 拟合概率 PCA 模型
ppca = ProbabilisticPCA(n_components=2)
ppca.fit(X)
X_transformed_ppca = ppca.transform(X)
print(&quot;PPCA Transformed Data:\n&quot;, X_transformed_ppca)

# 拟合因子分析模型
fa = FactorAnalysis(n_components=2)
fa.fit(X)
X_transformed_fa = fa.transform(X)
print(&quot;FA Transformed Data:\n&quot;, X_transformed_fa)
</code></pre>
<h3>代码逐步分析</h3>
<ol>
<li><strong>ProbabilisticPCA 类：</strong> 该类定义了概率 PCA 模型，包括模型初始化、拟合和转换方法。</li>
<li><strong>FactorAnalysis 类：</strong> 该类定义了因子分析模型，包括模型初始化、拟合和转换方法。</li>
<li><strong>示例数据：</strong> 使用随机生成的数据演示 PPCA 和 FA 的效果。</li>
</ol>
<h4>多角度分析概率 PCA 和因子分析方法的应用</h4>
<p><strong>角度一：降维</strong>
问：PPCA 和 FA 如何实现数据降维？
答：通过假设数据是由潜在因子的线性组合加上噪声生成的，这两种方法可以找到数据的低维表示，从而实现降维。</p>
<p><strong>角度二：特征提取</strong>
问：PPCA 和 FA 如何进行特征提取？
答：通过学习潜在因子，这两种方法可以提取数据的主要特征，去除噪声和冗余信息。</p>
<p><strong>角度三：计算效率</strong>
问：PPCA 和 FA 的计算效率如何？
答：这两种方法都涉及 SVD 分解，其计算复杂度较高，但通过适当的优化可以在实际应用中达到较好的计算效率。</p>
<h3>总结</h3>
<p>概率 PCA 和因子分析是用于降维和特征提取的有效方法，通过假设数据由潜在因子的线性组合生成，可以提高模型的泛化能力和鲁棒性。在实际应用中，掌握并应用这些技术对于构建高效、可靠的机器学习模型具有重要意义。</p>

    <h3>Python 文件</h3>
    <pre><code># 00_概率 PCA 和因子分析
"""
Lecture: 3_深度学习研究/13_线性因子模型
Content: 00_概率 PCA 和因子分析
"""
</code></pre>
  </div>
</body>
</html>
  