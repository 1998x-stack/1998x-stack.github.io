#### 07. 去噪自编码器

##### 背景介绍
去噪自编码器（Denoising Autoencoder, DAE）是一类通过添加噪声来增强模型鲁棒性的自编码器。它们不仅用于特征提取，还用于去噪和生成数据。

##### 方法定义和数学原理
**定义：**

去噪自编码器通过将输入数据添加噪声，再训练模型重建原始未损坏的数据来工作。

**数学原理：**

1. **编码器：** 将损坏的输入数据 $ x̃ $ 映射到隐藏表示 $ h $。
   $$
   h = f(x̃)
   $$

2. **解码器：** 将隐藏表示 $ h $ 重新映射回原始数据 $ x $。
   $$
   x̂ = g(h)
   $$

3. **损失函数：** 最小化重构误差，通常是均方误差。
   $$
   \mathcal{L}(x, x̂) = \|x - x̂\|^2
   $$

##### 算法步骤
1. **数据预处理：** 添加噪声到输入数据。
2. **前向传播：** 通过编码器将损坏的数据编码为隐藏表示，再通过解码器重构原始数据。
3. **计算损失：** 计算重构误差。
4. **反向传播：** 计算梯度并更新参数。
5. **重复步骤2-4，直到收敛。

##### 应用示例
去噪自编码器在图像去噪、数据增强和特征提取方面有广泛应用。

### 代码实现

#### 使用 Numpy 和 Scipy 从头实现代码

```python
import numpy as np
from typing import Tuple

class DenoisingAutoencoder:
    def __init__(self, input_dim: int, hidden_dim: int, learning_rate: float = 0.01, max_iter: int = 1000, noise_factor: float = 0.1):
        """
        初始化去噪自编码器模型
        
        Args:
            input_dim (int): 输入数据的维数
            hidden_dim (int): 隐藏层的维数
            learning_rate (float): 学习率
            max_iter (int): 最大迭代次数
            noise_factor (float): 噪声因子
        """
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.learning_rate = learning_rate
        self.max_iter = max_iter
        self.noise_factor = noise_factor
        
        # 初始化权重和偏置
        self.weights = {
            'encoder': np.random.randn(input_dim, hidden_dim) * 0.01,
            'decoder': np.random.randn(hidden_dim, input_dim) * 0.01
        }
        self.biases = {
            'encoder': np.zeros(hidden_dim),
            'decoder': np.zeros(input_dim)
        }

    def _sigmoid(self, x: np.ndarray) -> np.ndarray:
        return 1 / (1 + np.exp(-x))

    def _sigmoid_derivative(self, x: np.ndarray) -> np.ndarray:
        return x * (1 - x)

    def _add_noise(self, x: np.ndarray) -> np.ndarray:
        """
        添加噪声到输入数据
        
        Args:
            x (np.ndarray): 输入数据
        
        Returns:
            np.ndarray: 加噪后的数据
        """
        noise = self.noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x.shape)
        return x + noise

    def fit(self, X: np.ndarray):
        """
        训练去噪自编码器模型
        
        Args:
            X (np.ndarray): 输入数据，形状为 (n_samples, input_dim)
        """
        for epoch in range(self.max_iter):
            # 添加噪声到输入数据
            noisy_X = self._add_noise(X)
            
            # 前向传播
            hidden = self._sigmoid(np.dot(noisy_X, self.weights['encoder']) + self.biases['encoder'])
            output = self._sigmoid(np.dot(hidden, self.weights['decoder']) + self.biases['decoder'])
            
            # 计算重构误差
            loss = np.mean((X - output) ** 2)
            
            # 反向传播
            output_error = X - output
            output_delta = output_error * self._sigmoid_derivative(output)
            
            hidden_error = np.dot(output_delta, self.weights['decoder'].T)
            hidden_delta = hidden_error * self._sigmoid_derivative(hidden)
            
            # 更新权重和偏置
            self.weights['decoder'] += self.learning_rate * np.dot(hidden.T, output_delta)
            self.biases['decoder'] += self.learning_rate * np.sum(output_delta, axis=0)
            self.weights['encoder'] += self.learning_rate * np.dot(noisy_X.T, hidden_delta)
            self.biases['encoder'] += self.learning_rate * np.sum(hidden_delta, axis=0)
            
            if (epoch + 1) % 100 == 0:
                print(f"Epoch [{epoch+1}/{self.max_iter}], Loss: {loss:.4f}")

    def transform(self, X: np.ndarray) -> np.ndarray:
        """
        将输入数据编码为隐藏表示
        
        Args:
            X (np.ndarray): 输入数据，形状为 (n_samples, input_dim)
        
        Returns:
            np.ndarray: 编码后的隐藏表示，形状为 (n_samples, hidden_dim)
        """
        hidden = self._sigmoid(np.dot(X, self.weights['encoder']) + self.biases['encoder'])
        return hidden

    def reconstruct(self, X: np.ndarray) -> np.ndarray:
        """
        重构输入数据
        
        Args:
            X (np.ndarray): 输入数据，形状为 (n_samples, input_dim)
        
        Returns:
            np.ndarray: 重构后的数据，形状为 (n_samples, input_dim)
        """
        hidden = self.transform(X)
        output = self._sigmoid(np.dot(hidden, self.weights['decoder']) + self.biases['decoder'])
        return output

# 示例数据
np.random.seed(42)
X = np.random.rand(100, 20)

# 初始化去噪自编码器模型
autoencoder = DenoisingAutoencoder(input_dim=20, hidden_dim=10, learning_rate=0.01, max_iter=1000)

# 训练模型
autoencoder.fit(X)

# 编码和重构数据
encoded_X = autoencoder.transform(X)
reconstructed_X = autoencoder.reconstruct(X)

print("编码后的数据:\n", encoded_X)
print("重构后的数据:\n", reconstructed_X)
```

### 代码逐步分析

1. **DenoisingAutoencoder 类：** 定义了去噪自编码器模型，包括初始化、前向传播、反向传播、训练、编码和重构方法。
2. **_add_noise 方法：** 添加噪声到输入数据，以增强模型的鲁棒性。
3. **fit 方法：** 实现了自编码器模型的训练过程，包括前向传播、计算重构误差、反向传播和参数更新。
4. **transform 方法：** 将输入数据编码为隐藏表示。
5. **reconstruct 方法：** 将隐藏表示重构为原始数据。
6. **示例数据：** 使用随机生成的数据演示自编码器的效果。

#### 多角度分析去噪自编码器方法的应用

**角度一：鲁棒性**
问：去噪自编码器如何提高模型的鲁棒性？
答：通过在训练过程中向输入数据添加噪声，去噪自编码器能够更好地应对噪声和数据的随机变动，提高模型的鲁棒性。
