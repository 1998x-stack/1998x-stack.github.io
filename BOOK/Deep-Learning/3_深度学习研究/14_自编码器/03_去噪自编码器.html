
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>去噪自编码器</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h4>03. 去噪自编码器（Denoising Autoencoder）</h4>
<h5>背景介绍</h5>
<p>去噪自编码器是一种通过在训练过程中对输入数据添加噪声并训练模型去还原原始数据的自编码器。这种方法不仅可以提高模型的鲁棒性，还可以帮助模型学习到更好的特征表示。</p>
<h5>方法定义和数学原理</h5>
<p><strong>定义：</strong></p>
<p>去噪自编码器的目标是通过最小化重构误差来学习数据的低维表示，同时通过引入噪声来避免过拟合。数学上，去噪自编码器的优化问题可以表示为：</p>
<p>$$
\min_{f, g} L(x, g(f(x̃)))
$$</p>
<p>其中，$ L $ 是重构误差，通常选择为均方误差，$ x̃ $ 是被添加噪声后的输入数据。</p>
<p><strong>数学原理：</strong></p>
<ol>
<li><strong>重构误差：</strong> 确保自编码器能够重构输入数据 $ x $。
$$
L(x, g(f(x̃))) = |x - g(f(x̃))|^2
$$</li>
<li><strong>添加噪声：</strong> 向输入数据添加噪声，以避免模型学习到简单的恒等函数。
$$
x̃ = x + \text{noise}
$$</li>
</ol>
<p><strong>算法步骤：</strong></p>
<ol>
<li><strong>初始化：</strong> 初始化编码器和解码器的参数。</li>
<li><strong>前向传播：</strong> 计算添加噪声后的编码 $ h $ 和重构 $ r $。</li>
<li><strong>计算损失：</strong> 计算重构误差。</li>
<li><strong>反向传播：</strong> 计算梯度并更新参数。</li>
<li>**重复步骤2-4，直到收敛。</li>
</ol>
<h5>应用示例</h5>
<p>去噪自编码器在图像处理中的典型应用是图像去噪和特征提取。在图像去噪中，可以通过向图像添加噪声并训练模型去还原原始图像来提高模型的鲁棒性；在特征提取中，可以通过去噪自编码器来学习图像的主要特征。</p>
<h3>TASK 3: 使用 Numpy 和 Scipy 从头实现代码</h3>
<h4>代码实现</h4>
<pre><code class="language-python">import numpy as np
from typing import Tuple

class DenoisingAutoencoder:
    def __init__(self, input_dim: int, hidden_dim: int, learning_rate: float = 0.01, max_iter: int = 1000, noise_factor: float = 0.5):
        ```
        初始化去噪自编码器模型
        
        Args:
            input_dim (int): 输入数据的维数
            hidden_dim (int): 编码层的维数
            learning_rate (float): 学习率
            max_iter (int): 最大迭代次数
            noise_factor (float): 噪声因子
        ```
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.learning_rate = learning_rate
        self.max_iter = max_iter
        self.noise_factor = noise_factor
        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.01
        self.b1 = np.zeros(hidden_dim)
        self.W2 = np.random.randn(hidden_dim, input_dim) * 0.01
        self.b2 = np.zeros(input_dim)

    def _sigmoid(self, x: np.ndarray) -&gt; np.ndarray:
        return 1 / (1 + np.exp(-x))

    def _sigmoid_derivative(self, x: np.ndarray) -&gt; np.ndarray:
        return x * (1 - x)

    def _add_noise(self, X: np.ndarray) -&gt; np.ndarray:
        ```
        向输入数据添加噪声
        
        Args:
            X (np.ndarray): 输入数据
        
        Returns:
            np.ndarray: 添加噪声后的数据
        ```
        noise = self.noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X.shape)
        return X + noise

    def fit(self, X: np.ndarray):
        ```
        训练去噪自编码器模型
        
        Args:
            X (np.ndarray): 输入数据，形状为 (n_samples, input_dim)
        ```
        for _ in range(self.max_iter):
            # 向输入数据添加噪声
            X_noisy = self._add_noise(X)
            
            # 前向传播
            hidden = self._sigmoid(np.dot(X_noisy, self.W1) + self.b1)
            output = self._sigmoid(np.dot(hidden, self.W2) + self.b2)
            
            # 计算重构误差
            loss = X - output
            
            # 计算总损失
            total_loss = np.sum(loss ** 2) / 2
            
            # 反向传播
            output_error = loss * self._sigmoid_derivative(output)
            hidden_error = np.dot(output_error, self.W2.T) * self._sigmoid_derivative(hidden)
            
            # 更新权重和偏置
            self.W2 += self.learning_rate * np.dot(hidden.T, output_error)
            self.b2 += self.learning_rate * np.sum(output_error, axis=0)
            self.W1 += self.learning_rate * np.dot(X.T, hidden_error)
            self.b1 += self.learning_rate * np.sum(hidden_error, axis=0)

    def transform(self, X: np.ndarray) -&gt; np.ndarray:
        ```
        将输入数据编码为低维表示
        
        Args:
            X (np.ndarray): 输入数据，形状为 (n_samples, input_dim)
        
        Returns:
            np.ndarray: 编码后的低维表示，形状为 (n_samples, hidden_dim)
        ```
        return self._sigmoid(np.dot(X, self.W1) + self.b1)

    def reconstruct(self, X: np.ndarray) -&gt; np.ndarray:
        ```
        重构输入数据
        
        Args:
            X (np.ndarray): 输入数据，形状为 (n_samples, input_dim)
        
        Returns:
            np.ndarray: 重构后的数据，形状为 (n_samples, input_dim)
        ```
        hidden = self.transform(X)
        return self._sigmoid(np.dot(hidden, self.W2) + self.b2)

# 示例数据
np.random.seed(42)
X = np.random.rand(100, 20)

# 拟合去噪自编码器模型
autoencoder = DenoisingAutoencoder(input_dim=20, hidden_dim=10, learning_rate=0.01, max_iter=1000, noise_factor=0.5)
autoencoder.fit(X)
encoded_X = autoencoder.transform(X)
reconstructed_X = autoencoder.reconstruct(X)

print(&quot;编码后的数据:\n&quot;, encoded_X)
print(&quot;重构后的数据:\n&quot;, reconstructed_X)
</code></pre>
<h3>代码逐步分析</h3>
<ol>
<li><strong>DenoisingAutoencoder 类：</strong> 定义了去噪自编码器模型，包括初始化、前向传播、反向传播、训练、编码和重构方法。</li>
<li><strong>fit 方法：</strong> 实现了去噪自编码器模型的训练过程，包括向输入数据添加噪声、前向传播、重构误差的计算、反向传播和参数更新。</li>
<li><strong>transform 方法：</strong> 将输入数据编码为低维表示。</li>
<li><strong>reconstruct 方法：</strong> 将低维表示重构为原始数据。</li>
<li><strong>示例数据：</strong> 使用随机生成的数据演示去噪自编码器的效果。</li>
</ol>
<h4>多角度分析去噪自编码器方法的应用</h4>
<p><strong>角度一：鲁棒性</strong>
问：去噪自编码器如何提高模型的鲁棒性？
答：通过向输入数据添加噪声并训练模型还原原始数据，去噪自编码器可以提高模型对噪声的鲁棒性。</p>
<p><strong>角度二：特征提取</strong>
问：去噪自编码器如何进行特征提取？
答：通过学习输入数据的低维表示，去噪自编码器可以提取数据中的重要特征，从而减少冗余信息。</p>
<p><strong>角度三：计算效率</strong>
问：去噪自编码器的计算效率如何？
答：去噪自编码器的计算效率较高，因为其训练过程主要涉及前向传播和反向传播，计算复杂度较低。</p>
<h3>总结</h3>
<p>去噪自编码器是一种强大的数据降维和特征提取技术，通过向输入数据添加噪声并训练模型还原原始数据，可以提高模型的鲁棒性和泛化能力。在实际应用中，掌握并应用去噪自编码器技术对于构建高效、可靠的数据分析和机器学习模型具有重要意义。</p>

    <h3>Python 文件</h3>
    <pre><code># 03_去噪自编码器
"""
Lecture: 3_深度学习研究/14_自编码器
Content: 03_去噪自编码器
"""
</code></pre>
  </div>
</body>
</html>
  