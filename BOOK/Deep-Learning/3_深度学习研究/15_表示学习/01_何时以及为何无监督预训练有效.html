
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>何时以及为何无监督预训练有效</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h4>背景介绍</h4>
<p>无监督预训练是一种在深度学习中用于初始化深层神经网络的方法，特别是在数据有限的情况下，这种方法有助于提高模型的性能。无监督预训练可以显著减少测试误差，特别是在分类任务中。这种方法的有效性最早在2006年被发现，并在接下来的研究中得到进一步验证。</p>
<h4>详细展开</h4>
<p><strong>何时无监督预训练有效？</strong></p>
<ol>
<li><strong>在分类任务中</strong>：无监督预训练通常能够在测试误差上获得显著提升。研究表明，网络越深，无监督预训练的效果越好。</li>
<li><strong>在小样本学习中</strong>：当标注样本数量较少时，无监督预训练能够显著提升模型的性能，因为它可以从大量未标注数据中学习有用的特征。</li>
<li><strong>在复杂任务中</strong>：对于复杂的学习任务，无监督预训练可以帮助模型发现输入数据的潜在结构，从而提高模型的泛化能力。</li>
</ol>
<p><strong>为何无监督预训练有效？</strong></p>
<ol>
<li><strong>参数初始化</strong>：无监督预训练可以将模型的参数初始化到一个较优的区域，使得模型更容易收敛到一个好的局部最优解。</li>
<li><strong>正则化效果</strong>：无监督预训练可以作为一种正则化手段，减少模型的方差，从而降低过拟合的风险。</li>
<li><strong>学习有用特征</strong>：无监督预训练鼓励模型学习生成数据的潜在原因相关的特征，这些特征在后续的监督学习任务中同样有用。</li>
</ol>
<h3>实现贪心逐层无监督预训练的代码</h3>
<p>下面的代码使用Numpy和Scipy实现了一个简单的贪心逐层无监督预训练过程。</p>
<pre><code class="language-python">import numpy as np
from typing import List

class GreedyLayerWisePretraining:
    def __init__(self, layer_sizes: List[int], learning_rate: float = 0.01, max_iter: int = 1000):
        ```
        初始化贪心逐层无监督预训练模型
        
        Args:
            layer_sizes (List[int]): 每层的神经元数量
            learning_rate (float): 学习率
            max_iter (int): 最大迭代次数
        ```
        self.layer_sizes = layer_sizes
        self.learning_rate = learning_rate
        self.max_iter = max_iter
        self.weights = []
        self.biases = []
        self.init_weights()

    def init_weights(self):
        ```初始化权重和偏置```
        for i in range(len(self.layer_sizes) - 1):
            weight = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * 0.01
            bias = np.zeros(self.layer_sizes[i+1])
            self.weights.append(weight)
            self.biases.append(bias)

    def sigmoid(self, x: np.ndarray) -&gt; np.ndarray:
        ```Sigmoid激活函数```
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x: np.ndarray) -&gt; np.ndarray:
        ```Sigmoid激活函数的导数```
        return x * (1 - x)

    def train_layer(self, X: np.ndarray, layer_idx: int):
        ```训练单层```
        for epoch in range(self.max_iter):
            # 前向传播
            z = np.dot(X, self.weights[layer_idx]) + self.biases[layer_idx]
            a = self.sigmoid(z)
            
            # 计算损失（这里使用均方误差作为损失函数）
            loss = np.mean((X - a) ** 2)
            
            # 反向传播
            error = X - a
            delta = error * self.sigmoid_derivative(a)
            
            # 更新权重和偏置
            self.weights[layer_idx] += self.learning_rate * np.dot(X.T, delta)
            self.biases[layer_idx] += self.learning_rate * np.sum(delta, axis=0)
            
            if (epoch + 1) % 100 == 0:
                print(f&quot;Layer {layer_idx+1}, Epoch [{epoch+1}/{self.max_iter}], Loss: {loss:.4f}&quot;)

    def pretrain(self, X: np.ndarray):
        ```贪心逐层无监督预训练```
        input_data = X
        for layer_idx in range(len(self.layer_sizes) - 1):
            print(f&quot;Pretraining Layer {layer_idx+1}&quot;)
            self.train_layer(input_data, layer_idx)
            # 生成下一层的输入
            input_data = self.sigmoid(np.dot(input_data, self.weights[layer_idx]) + self.biases[layer_idx])

# 示例数据
np.random.seed(42)
X = np.random.rand(100, 20)

# 初始化贪心逐层无监督预训练模型
pretrainer = GreedyLayerWisePretraining(layer_sizes=[20, 15, 10], learning_rate=0.01, max_iter=1000)

# 进行预训练
pretrainer.pretrain(X)
</code></pre>
<h3>代码逐步分析</h3>
<ol>
<li><strong>GreedyLayerWisePretraining 类</strong>：定义了贪心逐层无监督预训练模型，包括初始化、权重初始化、单层训练和整体预训练的方法。</li>
<li><strong>init_weights 方法</strong>：初始化每一层的权重和偏置。</li>
<li><strong>sigmoid 方法</strong>：定义了Sigmoid激活函数。</li>
<li><strong>sigmoid_derivative 方法</strong>：定义了Sigmoid激活函数的导数。</li>
<li><strong>train_layer 方法</strong>：实现了单层的训练过程，包括前向传播、计算损失、反向传播和参数更新。</li>
<li><strong>pretrain 方法</strong>：实现了贪心逐层无监督预训练的整体流程，通过逐层训练每一层网络。</li>
</ol>
<h3>结果</h3>
<ol>
<li><strong>单层训练</strong>：逐层训练每一层网络，保证每一层的权重初始化合理，从而为后续的有监督训练提供良好的初始值。</li>
<li><strong>贪心逐层无监督预训练</strong>：通过逐层训练，解决深层网络训练中的梯度消失问题，提高整体训练效果。</li>
</ol>
<h3>总结</h3>
<p>无监督预训练在深度学习中是一种重要的技术，通过逐层训练网络来初始化权重，从而为后续的有监督训练提供良好的初始值。掌握这种技术对于构建高效、稳定的深度学习模型具有重要意义。</p>

    <h3>Python 文件</h3>
    <pre><code># 01_何时以及为何无监督预训练有效
"""
Lecture: 3_深度学习研究/15_表示学习
Content: 01_何时以及为何无监督预训练有效
"""
import torch
import torch.nn as nn
import torch.optim as optim
from typing import List, Tuple

class GreedyLayerWisePretraining:
    def __init__(self, layer_sizes: List[int], learning_rate: float = 0.01, max_iter: int = 1000):
        """
        初始化贪心逐层无监督预训练模型
        
        Args:
            layer_sizes (List[int]): 每层的神经元数量
            learning_rate (float): 学习率
            max_iter (int): 最大迭代次数
        """
        self.layer_sizes = layer_sizes
        self.learning_rate = learning_rate
        self.max_iter = max_iter
        self.models = self.init_models()

    def init_models(self) -> List[Tuple[nn.Module, nn.Module]]:
        """
        初始化每一层的自编码器模型
        
        Returns:
            List[Tuple[nn.Module, nn.Module]]: 每一层的自编码器模型（编码器，解码器）
        """
        models = []
        for i in range(len(self.layer_sizes) - 1):
            encoder = nn.Sequential(
                nn.Linear(self.layer_sizes[i], self.layer_sizes[i+1]),
                nn.Sigmoid()
            )
            decoder = nn.Sequential(
                nn.Linear(self.layer_sizes[i+1], self.layer_sizes[i]),
                nn.Sigmoid()
            )
            models.append((encoder, decoder))
        return models

    def train_layer(self, data: torch.Tensor, model: Tuple[nn.Module, nn.Module]):
        """
        训练单层
        
        Args:
            data (torch.Tensor): 输入数据
            model (Tuple[nn.Module, nn.Module]): 自编码器模型 (encoder, decoder)
        """
        encoder, decoder = model
        criterion = nn.MSELoss()
        optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=self.learning_rate)
        
        for epoch in range(self.max_iter):
            # 前向传播
            encoded = encoder(data)
            decoded = decoder(encoded)
            loss = criterion(decoded, data)
            
            # 反向传播和优化
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            if (epoch + 1) % 100 == 0:
                print(f"Epoch [{epoch+1}/{self.max_iter}], Loss: {loss.item():.4f}")

    def pretrain(self, data: torch.Tensor):
        """
        贪心逐层无监督预训练
        
        Args:
            data (torch.Tensor): 输入数据
        """
        input_data = data
        for idx, model in enumerate(self.models):
            print(f"Pretraining Layer {idx+1}")
            self.train_layer(input_data, model)
            # 冻结已训练层参数
            for param in model[0].parameters():
                param.requires_grad = False
            for param in model[1].parameters():
                param.requires_grad = False
            # 生成下一层的输入
            input_data = model[0](input_data).detach()  # 只取encoder的输出并detach梯度

# 示例数据
torch.manual_seed(42)
X = torch.rand(100, 20)

# 初始化贪心逐层无监督预训练模型
pretrainer = GreedyLayerWisePretraining(layer_sizes=[20, 15, 10], learning_rate=0.01, max_iter=1000)

# 进行预训练
pretrainer.pretrain(X)
</code></pre>
  </div>
</body>
</html>
  