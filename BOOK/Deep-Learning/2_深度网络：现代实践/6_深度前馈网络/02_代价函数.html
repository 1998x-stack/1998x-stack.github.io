
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>代价函数</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>02_代价函数</h1>
<pre><code>Lecture: 2_深度网络：现代实践/6_深度前馈网络
Content: 02_代价函数
</code></pre>
<h2>02_代价函数</h2>
<h3>任务分解：</h3>
<ol>
<li><strong>背景介绍</strong></li>
<li><strong>代价函数的定义</strong></li>
<li><strong>常见代价函数</strong></li>
<li><strong>代价函数的选择</strong></li>
<li><strong>代价函数在神经网络中的实现</strong></li>
<li><strong>可视化代价函数</strong></li>
</ol>
<h3>1. 背景介绍</h3>
<p><strong>步骤：</strong></p>
<ul>
<li>解释代价函数在机器学习和神经网络中的作用。</li>
<li>强调代价函数在优化过程中作为目标函数的作用。
<strong>解释：</strong>
代价函数（也称损失函数或误差函数）是评估模型预测与真实值之间差距的函数。在机器学习中，代价函数用于指导模型参数的调整，通过最小化代价函数来优化模型性能。</li>
</ul>
<h3>2. 代价函数的定义</h3>
<p><strong>步骤：</strong></p>
<ul>
<li>提供代价函数的数学定义。</li>
<li>说明代价函数如何度量模型预测的好坏。
<strong>解释：</strong>
代价函数 $ J(\theta) $ 是模型参数 $ \theta $ 的函数，表示模型预测与真实值之间的差异。数学上，它通常表示为：
$$ J(\theta) = \frac{1}{m} \sum_{i=1}^{m} L(\hat{y}_i, y_i) $$
其中，$ \hat{y}_i $ 是模型的预测值，$ y_i $ 是真实值，$ L $ 是单个样本的损失，$ m $ 是样本数量。</li>
</ul>
<h3>3. 常见代价函数</h3>
<p><strong>步骤：</strong></p>
<ul>
<li>
<p>介绍回归问题中的均方误差（MSE）。</p>
</li>
<li>
<p>介绍分类问题中的交叉熵损失（Cross-Entropy Loss）。</p>
</li>
<li>
<p>提及其他常见的代价函数，如绝对误差（MAE）等。
<strong>解释：</strong></p>
</li>
<li>
<p><strong>均方误差（MSE）</strong>：用于回归问题，计算预测值与真实值之间差值的平方和的平均值。</p>
<p>$$ MSE = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}_i - y_i)^2 $$</p>
</li>
<li>
<p><strong>交叉熵损失（Cross-Entropy Loss）</strong>：用于分类问题，计算预测概率分布与真实标签分布之间的距离。</p>
<p>$$ Cross_Entropy = -\frac{1}{m} \sum_{i=1}^{m} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)] $$</p>
</li>
</ul>
<h3>4. 代价函数的选择</h3>
<p><strong>步骤：</strong></p>
<ul>
<li>讨论如何根据具体问题选择合适的代价函数。</li>
<li>说明不同代价函数对模型性能的影响。
<strong>解释：</strong>
选择合适的代价函数取决于具体的任务和数据特性。例如，对于回归问题，均方误差（MSE）通常是首选，而对于分类问题，交叉熵损失（Cross-Entropy Loss）更为合适。选择不当的代价函数可能会导致模型无法有效学习。</li>
</ul>
<h3>5. 代价函数在神经网络中的实现</h3>
<p><strong>步骤：</strong></p>
<ul>
<li>使用 PyTorch 实现常见代价函数。</li>
<li>演示如何在训练过程中使用代价函数。
<strong>代码：</strong></li>
</ul>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
# 定义神经网络模型
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(2, 2)
        self.fc2 = nn.Linear(2, 1)
        self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        x = self.sigmoid(self.fc1(x))
        x = self.sigmoid(self.fc2(x))
        return x
# 初始化模型
model = SimpleNN()
# 定义交叉熵损失函数和优化器
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)
# 准备数据
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])
# 训练模型
epochs = 10000
losses = []
for epoch in range(epochs):
    # 前向传播
    outputs = model(torch.tensor(X, dtype=torch.float))
    loss = criterion(outputs, torch.tensor(y, dtype=torch.float))
    losses.append(loss.item())
    
    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 1000 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')
# 评估模型
with torch.no_grad():
    predicted = model(torch.tensor(X, dtype=torch.float)).round()
    accuracy = (predicted.numpy() == y).mean()
    print(f'Accuracy: {accuracy * 100:.2f}%')
</code></pre>
<h3>6. 可视化代价函数</h3>
<p><strong>步骤：</strong></p>
<ul>
<li>可视化训练损失的变化。</li>
<li>展示代价函数如何随着训练迭代的增加而变化。
<strong>代码：</strong></li>
</ul>
<pre><code class="language-python">import matplotlib.pyplot as plt
# 可视化训练损失
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.show()
</code></pre>
<h3>代价函数误用的影响</h3>
<h3>1. 在分类问题中使用均方误差（MSE）</h3>
<p><strong>问题：</strong>
均方误差（MSE）是用于回归问题的损失函数。当在分类问题中使用 MSE 时，模型可能无法正确学习数据的分类特征。
<strong>影响：</strong></p>
<ul>
<li><strong>损失函数不适用</strong>：MSE 衡量的是预测值与真实值之间的平方差，而分类问题的目标是最大化预测概率与真实标签的匹配度。MSE 在这种情况下不能有效地捕捉概率分布之间的差异。</li>
<li><strong>梯度消失问题</strong>：由于 MSE 的梯度在接近正确分类时会变得非常小，这会导致学习过程变得缓慢甚至停滞。</li>
<li><strong>效果不佳</strong>：最终模型的准确性可能会很低，因为它无法正确地最小化分类错误。
<strong>示例：</strong></li>
</ul>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
# 定义神经网络模型
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(2, 2)
        self.fc2 = nn.Linear(2, 1)
        self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        x = self.sigmoid(self.fc1(x))
        x = self.sigmoid(self.fc2(x))
        return x
# 初始化模型
model = SimpleNN()
# 定义均方误差损失函数和优化器
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)
# 准备数据
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])
# 训练模型
epochs = 10000
losses = []
for epoch in range(epochs):
    # 前向传播
    outputs = model(torch.tensor(X, dtype=torch.float))
    loss = criterion(outputs, torch.tensor(y, dtype=torch.float))
    losses.append(loss.item())
    
    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 1000 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')
# 评估模型
with torch.no_grad():
    predicted = model(torch.tensor(X, dtype=torch.float)).round()
    accuracy = (predicted.numpy() == y).mean()
    print(f'Accuracy: {accuracy * 100:.2f}%')
</code></pre>
<p>在上述示例中，由于使用了不适合分类问题的均方误差（MSE），模型可能无法很好地分类数据。</p>
<h3>2. 在回归问题中使用交叉熵损失（Cross-Entropy Loss）</h3>
<p><strong>问题：</strong>
交叉熵损失（Cross-Entropy Loss）是用于分类问题的损失函数。当在回归问题中使用交叉熵损失时，模型可能无法正确预测连续值。
<strong>影响：</strong></p>
<ul>
<li><strong>损失函数不适用</strong>：交叉熵损失衡量的是概率分布之间的差异，而回归问题的目标是预测连续的数值。交叉熵在这种情况下不能有效地衡量预测值与真实值之间的差距。</li>
<li><strong>梯度计算错误</strong>：由于交叉熵损失函数设计用于概率值（通常在 [0, 1] 之间），在回归问题中使用时，梯度计算可能会出现问题。</li>
<li><strong>效果不佳</strong>：最终模型的预测值可能会不准确，因为它无法正确地最小化回归误差。
<strong>示例：</strong></li>
</ul>
<pre><code class="language-python"># 定义回归问题的简单数据
X = np.array([[1], [2], [3], [4]])
y = np.array([[2], [3], [4], [5]])
# 定义神经网络模型
class RegressionNN(nn.Module):
    def __init__(self):
        super(RegressionNN, self).__init__()
        self.fc1 = nn.Linear(1, 1)
    def forward(self, x):
        return self.fc1(x)
# 初始化模型
model = RegressionNN()
# 定义交叉熵损失函数和优化器（错误使用）
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
# 训练模型
epochs = 10000
losses = []
for epoch in range(epochs):
    # 前向传播
    outputs = model(torch.tensor(X, dtype=torch.float))
    loss = criterion(outputs, torch.tensor(y, dtype=torch.float))  # 错误的损失函数
    losses.append(loss.item())
    
    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 1000 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')
# 评估模型
with torch.no_grad():
    predicted = model(torch.tensor(X, dtype=torch.float))
    print('Predictions:', predicted.numpy())
</code></pre>
<p>在上述示例中，由于使用了不适合回归问题的交叉熵损失（Cross-Entropy Loss），模型可能无法正确预测连续值。</p>
<h3>结论</h3>
<p>在不同类型的问题中使用不适当的代价函数会导致模型性能不佳，甚至无法收敛。因此，选择合适的代价函数对于模型的成功训练至关重要。</p>

    <h3>Python 文件</h3>
    <pre><code># 02_代价函数
"""
Lecture: 2_深度网络：现代实践/6_深度前馈网络
Content: 02_代价函数
"""
</code></pre>
  </div>
</body>
</html>
  