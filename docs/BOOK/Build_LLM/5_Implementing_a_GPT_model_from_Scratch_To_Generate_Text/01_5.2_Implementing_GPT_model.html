
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>5.2 Implementing GPT model</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>01_5.2_Implementing_GPT_model</h1>
<pre><code>Lecture: /5_Implementing_a_GPT_model_from_Scratch_To_Generate_Text
Content: 01_5.2_Implementing_GPT_model
</code></pre>
<h3>5.2 实现GPT模型</h3>
<h4>背景介绍</h4>
<p>实现生成预训练变换器（Generative Pre-trained Transformer，简称GPT）模型需要深入理解其架构和工作原理。GPT模型是基于Transformer架构的生成模型，通过大量的无监督文本数据进行预训练，并在特定任务上进行微调。它通过多层解码器堆叠来生成高质量的文本。</p>
<h4>实现GPT模型的关键步骤</h4>
<ol>
<li>
<p><strong>准备数据</strong>：</p>
<ul>
<li>将文本数据转化为模型可以处理的格式，包括标记化和构建词汇表。</li>
</ul>
</li>
<li>
<p><strong>构建模型</strong>：</p>
<ul>
<li>构建GPT模型的架构，包括输入嵌入、位置编码、多头自注意力机制、前馈神经网络等。</li>
</ul>
</li>
<li>
<p><strong>模型训练</strong>：</p>
<ul>
<li>通过无监督预训练和有监督微调训练模型。</li>
</ul>
</li>
<li>
<p><strong>文本生成</strong>：</p>
<ul>
<li>使用训练好的模型生成文本，根据给定的上下文预测下一个单词。</li>
</ul>
</li>
</ol>
<h4>具体步骤详解</h4>
<h5>1. 准备数据</h5>
<p>数据准备是实现GPT模型的第一步，包括数据清洗、标记化和构建词汇表。</p>
<ul>
<li><strong>数据清洗</strong>：清理原始文本数据，去除无关字符和符号。</li>
<li><strong>标记化</strong>：将文本数据分割为单词或子词。可以使用Byte Pair Encoding（BPE）等方法进行标记化。</li>
<li><strong>构建词汇表</strong>：根据标记化的结果构建词汇表，分配唯一的标记ID。</li>
</ul>
<h5>2. 构建模型</h5>
<p>构建GPT模型的架构是实现的核心部分，包括以下几个组件：</p>
<ul>
<li><strong>输入嵌入</strong>：将输入文本转化为嵌入向量。</li>
<li><strong>位置编码</strong>：为嵌入向量添加位置编码，保留输入序列的位置信息。</li>
<li><strong>解码器层</strong>：由多个相同的子层堆叠而成，每个子层包括多头自注意力机制、前馈神经网络、残差连接和层归一化。</li>
<li><strong>输出层</strong>：将解码器的输出转化为预测的单词分布。</li>
</ul>
<h5>3. 模型训练</h5>
<p>GPT模型的训练包括两个阶段：预训练和微调。</p>
<ul>
<li><strong>预训练</strong>：通过大量的无监督文本数据进行训练，目标是通过上下文预测下一个单词。</li>
<li><strong>微调</strong>：在特定任务的数据集上进行训练，使模型更好地适应特定任务。</li>
</ul>
<p>预训练的损失函数为交叉熵损失：
$$ \mathcal{L} = -\sum_{t=1}^{T} \log P(x_t \mid x_1, x_2, \ldots, x_{t-1}; \theta) $$</p>
<h5>4. 文本生成</h5>
<p>使用训练好的GPT模型生成文本，根据给定的上下文预测下一个单词，生成连贯的文本序列。</p>
<ul>
<li><strong>输入处理</strong>：将输入的上下文转化为模型可以处理的格式。</li>
<li><strong>逐步生成</strong>：逐步预测下一个单词，并将其作为新的输入，继续生成后续单词。</li>
<li><strong>输出文本</strong>：将生成的标记ID序列转化为文本。</li>
</ul>
<h4>GPT模型的实现细节</h4>
<p>在实际实现中，需要注意以下几个细节：</p>
<ol>
<li>
<p><strong>模型参数初始化</strong>：</p>
<ul>
<li>使用适当的方法初始化模型参数，如Xavier初始化或He初始化。</li>
</ul>
</li>
<li>
<p><strong>优化器和学习率调度</strong>：</p>
<ul>
<li>选择合适的优化器（如Adam）和学习率调度策略，确保模型训练的稳定性和收敛性。</li>
</ul>
</li>
<li>
<p><strong>处理长序列问题</strong>：</p>
<ul>
<li>使用分段训练或梯度截断等方法处理长序列输入，避免梯度消失或爆炸。</li>
</ul>
</li>
<li>
<p><strong>生成质量控制</strong>：</p>
<ul>
<li>在文本生成过程中，使用温度采样、Top-k采样或Top-p采样等方法控制生成的多样性和质量。</li>
</ul>
</li>
</ol>
<h4>总结</h4>
<p>实现GPT模型需要深入理解其架构和工作原理，包括数据准备、模型构建、训练和文本生成等关键步骤。在实际应用中，合理的模型参数初始化、优化器选择和生成质量控制等细节对于模型的性能至关重要。通过详细分析和实现GPT模型，我们可以更好地理解这一强大的生成模型，为构建高效的自然语言处理应用打下坚实的基础。</p>

    <h3>Python 文件</h3>
    <pre><code># 01_5.2_Implementing_GPT_model

"""
Lecture: /5_Implementing_a_GPT_model_from_Scratch_To_Generate_Text
Content: 01_5.2_Implementing_GPT_model
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from typing import Optional, Tuple, List

class GPTSelfAttention(nn.Module):
    def __init__(self, embed_size: int, heads: int):
        """
        GPT模型中的自注意力机制。

        参数:
        embed_size (int): 嵌入向量的维度。
        heads (int): 多头注意力机制的头数。
        """
        super(GPTSelfAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads

        assert self.head_dim * heads == embed_size, "Embedding size needs to be divisible by heads"

        self.values = nn.Linear(self.head_dim, embed_size, bias=False)
        self.keys = nn.Linear(self.head_dim, embed_size, bias=False)
        self.queries = nn.Linear(self.head_dim, embed_size, bias=False)
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)

    def forward(self, values: torch.Tensor, keys: torch.Tensor, query: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        前向传播方法。

        参数:
        values (torch.Tensor): 值向量，形状为(batch_size, value_len, embed_size)。
        keys (torch.Tensor): 键向量，形状为(batch_size, key_len, embed_size)。
        query (torch.Tensor): 查询向量，形状为(batch_size, query_len, embed_size)。
        mask (torch.Tensor, optional): 掩码张量，形状为(batch_size, 1, 1, key_len)。

        返回:
        torch.Tensor: 多头自注意力机制的输出，形状为(batch_size, query_len, embed_size)。
        """
        N = query.shape[0]
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]

        values = values.reshape(N, value_len, self.heads, self.head_dim)
        keys = keys.reshape(N, key_len, self.heads, self.head_dim)
        queries = query.reshape(N, query_len, self.heads, self.head_dim)

        values = self.values(values)
        keys = self.keys(keys)
        queries = self.queries(queries)

        energy = torch.einsum("nqhd,nkhd->nhqk", [queries, keys])
        
        if mask is not None:
            energy = energy.masked_fill(mask == 0, float("-1e20"))

        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)

        out = torch.einsum("nhql,nlhd->nqhd", [attention, values]).reshape(N, query_len, self.heads * self.head_dim)

        out = self.fc_out(out)

        return out


class GPTBlock(nn.Module):
    def __init__(self, embed_size: int, heads: int, dropout: float, forward_expansion: int):
        """
        GPT模型中的单个解码器层。

        参数:
        embed_size (int): 嵌入向量的维度。
        heads (int): 多头注意力机制的头数。
        dropout (float): dropout概率。
        forward_expansion (int): 前馈神经网络的扩展维度。
        """
        super(GPTBlock, self).__init__()
        self.attention = GPTSelfAttention(embed_size, heads)
        self.norm1 = nn.LayerNorm(embed_size)
        self.norm2 = nn.LayerNorm(embed_size)

        self.feed_forward = nn.Sequential(
            nn.Linear(embed_size, forward_expansion * embed_size),
            nn.ReLU(),
            nn.Linear(forward_expansion * embed_size, embed_size)
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        前向传播方法。

        参数:
        x (torch.Tensor): 输入张量，形状为(batch_size, seq_len, embed_size)。
        mask (torch.Tensor, optional): 掩码张量，形状为(batch_size, 1, 1, key_len)。

        返回:
        torch.Tensor: 解码器层的输出，形状为(batch_size, seq_len, embed_size)。
        """
        attention = self.attention(x, x, x, mask)
        x = self.dropout(self.norm1(attention + x))
        forward = self.feed_forward(x)
        out = self.dropout(self.norm2(forward + x))
        return out


class GPT(nn.Module):
    def __init__(self, vocab_size: int, embed_size: int, num_layers: int, heads: int, device: str, forward_expansion: int, dropout: float, max_length: int):
        """
        GPT模型。

        参数:
        vocab_size (int): 词汇表大小。
        embed_size (int): 嵌入向量的维度。
        num_layers (int): 解码器层数。
        heads (int): 多头注意力机制的头数。
        device (str): 设备（'cpu'或'cuda'）。
        forward_expansion (int): 前馈神经网络的扩展维度。
        dropout (float): dropout概率。
        max_length (int): 输入序列的最大长度。
        """
        super(GPT, self).__init__()
        self.embed_size = embed_size
        self.device = device
        self.word_embedding = nn.Embedding(vocab_size, embed_size)
        self.position_embedding = nn.Embedding(max_length, embed_size)

        self.layers = nn.ModuleList(
            [GPTBlock(embed_size, heads, dropout, forward_expansion) for _ in range(num_layers)]
        )
        self.fc_out = nn.Linear(embed_size, vocab_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        前向传播方法。

        参数:
        x (torch.Tensor): 输入张量，形状为(batch_size, seq_len)。
        mask (torch.Tensor, optional): 掩码张量，形状为(batch_size, 1, 1, key_len)。

        返回:
        torch.Tensor: 模型的输出，形状为(batch_size, seq_len, vocab_size)。
        """
        N, seq_len = x.shape
        positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)
        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))

        for layer in self.layers:
            out = layer(out, mask)

        out = self.fc_out(out)

        return out

# 示例使用
if __name__ == "__main__":
    # 定义模型参数
    vocab_size = 10000
    embed_size = 256
    num_layers = 6
    heads = 8
    device = "cuda" if torch.cuda.is_available() else "cpu"
    forward_expansion = 4
    dropout = 0.1
    max_length = 100

    # 初始化模型
    model = GPT(vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length).to(device)

    # 打印模型结构
    print(model)</code></pre>
  </div>
</body>
</html>
  