
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>2.6.1 Weighting and Regularization</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>00_2.6.1_Weighting_and_Regularization</h1>
<pre><code>
Lecture: 2._Chapters/2.6_Modified_Least_Squares_Problems_and_Methods
Content: 00_2.6.1_Weighting_and_Regularization

</code></pre>
<h3>详细分析权重与正则化</h3>
<p>在《Matrix Computations》一书的第6章，第6.1节对权重和正则化进行了详细讨论。以下是对第2.6.1节“权重与正则化”的详细分析：</p>
<h4>1. 引言与背景</h4>
<p>在最小二乘问题中，权重与正则化是两个重要的修改方式。权重方法主要处理每个方程在最小化 $|Ax - b|_2^2$ 中的重要性，而正则化方法则是在矩阵 $A$ 病态时，控制解向量 $x$ 的大小。</p>
<h4>2. 行权重</h4>
<p>在普通最小二乘问题中，目标是最小化 $|Ax - b|_2^2$，即：
$$ |Ax - b|<em i="1">2^2 = \sum</em>^m (a_i^T x - b_i)^2 $$
其中 $A \in \mathbb{R}^{m \times n}$，$b \in \mathbb{R}^m$，$a_i$ 是矩阵 $A$ 的第 $i$ 行。在加权最小二乘问题中，我们引入一个对角权重矩阵 $D = \text{diag}(d_1, \ldots, d_m)$，并最小化：
$$ |D(Ax - b)|<em i="1">2^2 = \sum</em>^m d_i^2 (a_i^T x - b_i)^2 $$
这种方式可以改变每个方程在最小化中的权重。通过引入权重矩阵 $D$，我们可以将问题转化为带有加权矩阵的新问题。</p>
<h4>3. 列权重</h4>
<p>列权重的方法是通过调整矩阵 $A$ 的列来反映其不确定性。假设 $G \in \mathbb{R}^{n \times n}$ 是非奇异的，定义 $G$-范数为：
$$ |x|_G = |G^{-1} x|_2 $$
我们最小化 $|(AG^{-1})y - b|_2$，得到的解为 $\hat{x} = G^{-1} \hat{y}$。这种方法在处理不同列尺度的矩阵时非常有效。</p>
<h4>4. 正则化</h4>
<p>正则化是处理病态问题的一种重要方法。岭回归（Ridge Regression）和Tikhonov正则化是两种常见的正则化方法：</p>
<p><strong>岭回归</strong>：
岭回归的目标是最小化以下目标函数：
$$ |Ax - b|_2^2 + \lambda |x|_2^2 $$
通过引入正则化参数 $\lambda$，控制解的范数。正常方程为：
$$ (A^T A + \lambda I)x = A^T b $$
通过SVD，可以将问题转换为对角形式，并通过选择适当的 $\lambda$ 来获得稳定的解。</p>
<p><strong>Tikhonov正则化</strong>：
Tikhonov正则化的目标是最小化以下目标函数：
$$ |Ax - b|_2^2 + \lambda |Bx|_2^2 $$
其中 $B$ 是正则化矩阵。正常方程为：
$$ (A^T A + \lambda B^T B)x = A^T b $$
通过广义奇异值分解（GSVD），可以同时对角化 $A$ 和 $B$，从而简化问题。</p>
<h4>5. 算法实现与分析</h4>
<p><strong>算法6.1.1</strong>：利用行权重解决加权最小二乘问题</p>
<ol>
<li>构建加权矩阵 $D$ 并计算加权后的矩阵 $A$ 和向量 $b$。</li>
<li>使用QR分解或SVD求解加权最小二乘问题。</li>
</ol>
<p><strong>算法6.1.2</strong>：利用列权重解决加权最小二乘问题</p>
<ol>
<li>构建列权重矩阵 $G$ 并计算加权后的矩阵 $AG^{-1}$。</li>
<li>使用QR分解或SVD求解加权最小二乘问题，并将解转换回原始空间。</li>
</ol>
<p><strong>算法6.1.3</strong>：岭回归的正则化求解</p>
<ol>
<li>计算矩阵 $A$ 的SVD分解。</li>
<li>通过选择适当的正则化参数 $\lambda$，计算稳定的最小二乘解。</li>
</ol>
<p><strong>算法6.1.4</strong>：Tikhonov正则化的求解</p>
<ol>
<li>计算矩阵 $A$ 和 $B$ 的GSVD分解。</li>
<li>通过选择适当的正则化参数 $\lambda$，计算稳定的最小二乘解。</li>
</ol>
<h3>结论</h3>
<p>权重与正则化方法在最小二乘问题中具有重要作用。通过合理选择权重和正则化参数，可以在保持计算稳定性的同时，提高解的准确性和可靠性。这些方法在处理病态矩阵和不确定性较大的数据时尤为有效。</p>

    <h3>Python 文件</h3>
    <pre><code># 00_2.6.1_Weighting_and_Regularization

"""

Lecture: 2._Chapters/2.6_Modified_Least_Squares_Problems_and_Methods
Content: 00_2.6.1_Weighting_and_Regularization

"""

import numpy as np
from typing import Tuple

def weighted_least_squares_row_weight(A: np.ndarray, b: np.ndarray, D: np.ndarray) -> Tuple[np.ndarray, float]:
    """
    使用行权重解决加权最小二乘问题

    Args:
    - A (np.ndarray): 系数矩阵 A，形状为 (m, n)
    - b (np.ndarray): 右侧向量 b，形状为 (m,)
    - D (np.ndarray): 对角行权重矩阵 D，形状为 (m, m)

    Returns:
    - x (np.ndarray): 解向量 x，形状为 (n,)
    - residual_norm (float): 残差范数 \|Ax - b\|_2
    """
    weighted_A = D @ A
    weighted_b = D @ b
    x = np.linalg.lstsq(weighted_A, weighted_b, rcond=None)[0]
    residual = A @ x - b
    residual_norm = np.linalg.norm(residual, ord=2)
    return x, residual_norm

# 示例用法
def test1():
    A = np.array([[1, 2], [3, 4], [5, 6]])
    b = np.array([1, 2, 3])
    D = np.diag([0.5, 1.0, 1.5])
    x, residual_norm = weighted_least_squares_row_weight(A, b, D)
    print("解向量 x:", x)
    print("残差范数:", residual_norm)

def weighted_least_squares_column_weight(A: np.ndarray, b: np.ndarray, G: np.ndarray) -> Tuple[np.ndarray, float]:
    """
    使用列权重解决加权最小二乘问题

    Args:
    - A (np.ndarray): 系数矩阵 A，形状为 (m, n)
    - b (np.ndarray): 右侧向量 b，形状为 (m,)
    - G (np.ndarray): 非奇异列权重矩阵 G，形状为 (n, n)

    Returns:
    - x (np.ndarray): 解向量 x，形状为 (n,)
    - residual_norm (float): 残差范数 \|Ax - b\|_2
    """
    weighted_A = A @ np.linalg.inv(G)
    weighted_b = b
    x = np.linalg.lstsq(weighted_A, weighted_b, rcond=None)[0]
    residual = A @ x - b
    residual_norm = np.linalg.norm(residual, ord=2)
    return x, residual_norm

# 示例用法
def test2():
    A = np.array([[1, 2], [3, 4], [5, 6]])
    b = np.array([1, 2, 3])
    G = np.array([[0.5, 0], [0, 2.0]])
    x, residual_norm = weighted_least_squares_column_weight(A, b, G)
    print("解向量 x:", x)
    print("残差范数:", residual_norm)

def ridge_regression(A: np.ndarray, b: np.ndarray, lambda_value: float) -> Tuple[np.ndarray, float]:
    """
    岭回归的正则化求解

    Args:
    - A (np.ndarray): 系数矩阵 A，形状为 (m, n)
    - b (np.ndarray): 右侧向量 b，形状为 (m,)
    - lambda_value (float): 正则化参数 lambda

    Returns:
    - x (np.ndarray): 解向量 x，形状为 (n,)
    - residual_norm (float): 残差范数 \|Ax - b\|_2
    """
    m, n = A.shape
    regularized_A = np.vstack((A, np.sqrt(lambda_value) * np.eye(n)))
    regularized_b = np.concatenate((b, np.zeros(n)))
    x = np.linalg.lstsq(regularized_A, regularized_b, rcond=None)[0]
    residual = A @ x - b
    residual_norm = np.linalg.norm(residual, ord=2)
    return x, residual_norm

# 示例用法
def test3():
    A = np.array([[1, 2], [3, 4]])
    b = np.array([1, 2])
    lambda_value = 0.1
    x, residual_norm = ridge_regression(A, b, lambda_value)
    print("解向量 x:", x)
    print("残差范数:", residual_norm)

def tikhonov_regularization(A: np.ndarray, b: np.ndarray, B: np.ndarray, lambda_value: float) -> Tuple[np.ndarray, float]:
    """
    Tikhonov正则化的求解

    Args:
    - A (np.ndarray): 系数矩阵 A，形状为 (m, n)
    - b (np.ndarray): 右侧向量 b，形状为 (m,)
    - B (np.ndarray): 正则化矩阵 B，形状为 (p, n)
    - lambda_value (float): 正则化参数 lambda

    Returns:
    - x (np.ndarray): 解向量 x，形状为 (n,)
    - residual_norm (float): 残差范数 \|Ax - b\|_2
    """
    m, n = A.shape
    p = B.shape[0]
    regularized_A = np.vstack((A, np.sqrt(lambda_value) * B))
    regularized_b = np.concatenate((b, np.zeros(p)))
    x = np.linalg.lstsq(regularized_A, regularized_b, rcond=None)[0]
    residual = A @ x - b
    residual_norm = np.linalg.norm(residual, ord=2)
    return x, residual_norm

# 示例用法
def test4():
    A = np.array([[1, 2], [3, 4]])
    b = np.array([1, 2])
    B = np.array([[1, 0], [0, 1]])
    lambda_value = 0.1
    x, residual_norm = tikhonov_regularization(A, b, B, lambda_value)
    print("解向量 x:", x)
    print("残差范数:", residual_norm)

if __name__ == "__main__":
    test1()
    test2()
    test3()
    test4()</code></pre>
  </div>
</body>
</html>
  