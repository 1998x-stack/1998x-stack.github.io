
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>受限玻尔兹曼机</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h3>详细展开 01_受限玻尔兹曼机</h3>
<h4>背景介绍</h4>
<p><strong>步骤：</strong></p>
<ol>
<li>解释受限玻尔兹曼机（Restricted Boltzmann Machine, RBM）的背景和重要性。</li>
<li>强调其在深度学习和机器学习中的作用。</li>
</ol>
<p><strong>解释：</strong></p>
<p>受限玻尔兹曼机（RBM）是一种无向图模型，由一层可见单元和一层隐藏单元组成。RBM 最早由 Paul Smolensky 在 1986 年提出，后来由 Geoffrey Hinton 进一步推广。RBM 是深度概率模型中最常见的组件之一，可以堆叠形成深度信念网络（DBN）和深度玻尔兹曼机（DBM）。</p>
<h4>受限玻尔兹曼机的定义和数学原理</h4>
<p><strong>步骤：</strong></p>
<ol>
<li>介绍受限玻尔兹曼机的定义。</li>
<li>说明其基本原理和算法步骤。</li>
</ol>
<p><strong>解释：</strong></p>
<p><strong>受限玻尔兹曼机：</strong> RBM 的联合概率分布可以通过能量函数 $E(v,h)$ 定义为：</p>
<p>$$ P(v,h) = \frac{\exp(-E(v,h))}{Z} $$</p>
<p>其中 $Z$ 是配分函数，用于确保概率分布的归一化，定义为：</p>
<p>$$ Z = \sum_{v} \sum_{h} \exp(-E(v,h)) $$</p>
<p>RBM 的能量函数由以下形式给出：</p>
<p>$$ E(v,h) = -b^T v - c^T h - v^T W h $$</p>
<p>其中，$v$ 和 $h$ 分别表示可见单元和隐藏单元的状态，$b$ 和 $c$ 分别表示可见单元和隐藏单元的偏置，$W$ 是权重矩阵。</p>
<h4>受限玻尔兹曼机的训练和推断</h4>
<p><strong>步骤：</strong></p>
<ol>
<li>讨论受限玻尔兹曼机的训练方法。</li>
<li>说明推断过程中的挑战和解决方案。</li>
</ol>
<p><strong>解释：</strong></p>
<p><strong>训练受限玻尔兹曼机：</strong> RBM 的训练通常基于最大似然估计。由于配分函数 $Z$ 的计算复杂性，梯度下降法需要使用近似技术，如对比散度（Contrastive Divergence, CD）来近似计算梯度。</p>
<p><strong>推断：</strong> 推断过程涉及在给定观测数据的情况下估计隐藏单元的分布。这可以通过 Gibbs 采样等 MCMC 方法实现。</p>
<h3>实现受限玻尔兹曼机的方法的代码示例</h3>
<p><strong>步骤：</strong></p>
<ol>
<li>使用 Numpy 和 Scipy 实现受限玻尔兹曼机的方法。</li>
<li>演示如何在实际应用中使用这些方法提高模型性能。</li>
</ol>
<p><strong>代码：</strong></p>
<pre><code class="language-python">import numpy as np

class RBM:
    def __init__(self, num_visible: int, num_hidden: int):
        ```初始化受限玻尔兹曼机
        
        Args:
            num_visible (int): 可见单元的数量
            num_hidden (int): 隐藏单元的数量
        ```
        self.num_visible = num_visible
        self.num_hidden = num_hidden
        self.weights = np.random.randn(num_visible, num_hidden) * 0.1
        self.visible_bias = np.zeros(num_visible)
        self.hidden_bias = np.zeros(num_hidden)

    def energy(self, v: np.ndarray, h: np.ndarray) -&gt; float:
        ```计算能量函数
        
        Args:
            v (np.ndarray): 可见单元状态
            h (np.ndarray): 隐藏单元状态
        
        Returns:
            float: 能量值
        ```
        return -np.dot(v, self.visible_bias) - np.dot(h, self.hidden_bias) - np.dot(v, np.dot(self.weights, h))

    def sample_hidden(self, v: np.ndarray) -&gt; np.ndarray:
        ```给定可见单元状态采样隐藏单元状态
        
        Args:
            v (np.ndarray): 可见单元状态
        
        Returns:
            np.ndarray: 隐藏单元状态
        ```
        activation = np.dot(v, self.weights) + self.hidden_bias
        probabilities = 1 / (1 + np.exp(-activation))
        return (np.random.rand(self.num_hidden) &lt; probabilities).astype(int)

    def sample_visible(self, h: np.ndarray) -&gt; np.ndarray:
        ```给定隐藏单元状态采样可见单元状态
        
        Args:
            h (np.ndarray): 隐藏单元状态
        
        Returns:
            np.ndarray: 可见单元状态
        ```
        activation = np.dot(h, self.weights.T) + self.visible_bias
        probabilities = 1 / (1 + np.exp(-activation))
        return (np.random.rand(self.num_visible) &lt; probabilities).astype(int)

    def contrastive_divergence(self, data: np.ndarray, learning_rate: float = 0.1, k: int = 1):
        ```对比散度算法更新权重
        
        Args:
            data (np.ndarray): 训练数据
            learning_rate (float): 学习率
            k (int): Gibbs 采样步数
        ```
        num_samples = data.shape[0]
        for sample in data:
            v0 = sample
            h0 = self.sample_hidden(v0)
            vk, hk = v0, h0
            for _ in range(k):
                vk = self.sample_visible(hk)
                hk = self.sample_hidden(vk)
            positive_grad = np.outer(v0, h0)
            negative_grad = np.outer(vk, hk)
            self.weights += learning_rate * (positive_grad - negative_grad) / num_samples
            self.visible_bias += learning_rate * (v0 - vk) / num_samples
            self.hidden_bias += learning_rate * (h0 - hk) / num_samples

# 示例数据
np.random.seed(42)
data = (np.random.rand(100, 6) &gt; 0.5).astype(int)

# 创建受限玻尔兹曼机实例
rbm = RBM(num_visible=6, num_hidden=3)

# 使用对比散度训练受限玻尔兹曼机
rbm.contrastive_divergence(data, learning_rate=0.1, k=1)

print(&quot;Trained weights:\n&quot;, rbm.weights)
</code></pre>
<h4>多角度分析受限玻尔兹曼机的方法应用</h4>
<p><strong>步骤：</strong></p>
<ol>
<li>从多个角度分析受限玻尔兹曼机的方法应用。</li>
<li>通过自问自答方式深入探讨这些方法的不同方面。</li>
</ol>
<p><strong>解释：</strong></p>
<p><strong>角度一：计算效率</strong>
问：受限玻尔兹曼机的训练计算效率如何？
答：由于配分函数 $Z$ 的计算复杂性，受限玻尔兹曼机的训练通常需要使用近似方法，如对比散度（CD）。这些方法在大多数应用中效果良好，但在高维数据集上仍可能计算密集。</p>
<p><strong>角度二：适用范围</strong>
问：受限玻尔兹曼机适用于哪些类型的问题？
答：受限玻尔兹曼机适用于各种需要建模复杂高维数据分布的问题，如图像、语音和文本数据的建模。</p>
<p><strong>角度三：收敛性</strong>
问：如何判断受限玻尔兹曼机训练的收敛性？
答：可以通过监测对比散度的损失函数值，或者通过样本的能量值来判断模型的收敛性。当损失函数值趋于稳定或能量值变化不大时，通常认为模型已经收敛。</p>
<h4>总结</h4>
<p><strong>步骤：</strong></p>
<ol>
<li>总结受限玻尔兹曼机在统计推断和机器学习中的重要性。</li>
<li>强调掌握这些技术对构建高效模型的关键作用。</li>
</ol>
<p><strong>解释：</strong></p>
<p>受限玻尔兹曼机是统计推断和机器学习中的重要工具，通过建模复杂的高维数据分布，可以有效捕捉数据中的模式和结构。掌握受限玻尔兹曼机及其近似训练方法对于构建高效、可靠的深度学习和机器学习模型具有重要意义。</p>

    <h3>Python 文件</h3>
    <pre><code># 01_受限玻尔兹曼机
"""
Lecture: 3_深度学习研究/20_深度生成模型
Content: 01_受限玻尔兹曼机
"""
import torch
import torch.nn.functional as F
import numpy as np

class RBM(torch.nn.Module):
    def __init__(self, num_visible: int, num_hidden: int):
        """初始化受限玻尔兹曼机
        
        Args:
            num_visible (int): 可见单元的数量
            num_hidden (int): 隐藏单元的数量
        """
        super(RBM, self).__init__()
        self.num_visible = num_visible
        self.num_hidden = num_hidden
        self.weights = torch.nn.Parameter(torch.randn(num_visible, num_hidden) * 0.1)
        self.visible_bias = torch.nn.Parameter(torch.zeros(num_visible))
        self.hidden_bias = torch.nn.Parameter(torch.zeros(num_hidden))

    def energy(self, v: torch.Tensor, h: torch.Tensor) -> torch.Tensor:
        """计算能量函数
        
        Args:
            v (torch.Tensor): 可见单元状态
            h (torch.Tensor): 隐藏单元状态
        
        Returns:
            torch.Tensor: 能量值
        """
        return -torch.matmul(v, self.visible_bias) - torch.matmul(h, self.hidden_bias) - torch.matmul(v, torch.matmul(self.weights, h))

    def sample_hidden(self, v: torch.Tensor) -> torch.Tensor:
        """给定可见单元状态采样隐藏单元状态
        
        Args:
            v (torch.Tensor): 可见单元状态
        
        Returns:
            torch.Tensor: 隐藏单元状态
        """
        activation = torch.matmul(v, self.weights) + self.hidden_bias
        probabilities = torch.sigmoid(activation)
        return torch.bernoulli(probabilities)

    def sample_visible(self, h: torch.Tensor) -> torch.Tensor:
        """给定隐藏单元状态采样可见单元状态
        
        Args:
            h (torch.Tensor): 隐藏单元状态
        
        Returns:
            torch.Tensor: 可见单元状态
        """
        activation = torch.matmul(h, self.weights.t()) + self.visible_bias
        probabilities = torch.sigmoid(activation)
        return torch.bernoulli(probabilities)

    def contrastive_divergence(self, data: torch.Tensor, learning_rate: float = 0.1, k: int = 1):
        """对比散度算法更新权重
        
        Args:
            data (torch.Tensor): 训练数据
            learning_rate (float): 学习率
            k (int): Gibbs 采样步数
        """
        num_samples = data.size(0)
        for sample in data:
            v0 = sample
            h0 = self.sample_hidden(v0)
            vk, hk = v0, h0
            for _ in range(k):
                vk = self.sample_visible(hk)
                hk = self.sample_hidden(vk)
            positive_grad = torch.matmul(v0.unsqueeze(1), h0.unsqueeze(0))
            negative_grad = torch.matmul(vk.unsqueeze(1), hk.unsqueeze(0))
            self.weights.data += learning_rate * (positive_grad - negative_grad) / num_samples
            self.visible_bias.data += learning_rate * (v0 - vk) / num_samples
            self.hidden_bias.data += learning_rate * (h0 - hk) / num_samples

# 示例数据
np.random.seed(42)
data_np = (np.random.rand(100, 6) > 0.5).astype(np.float32)
data = torch.tensor(data_np)

# 创建受限玻尔兹曼机实例
rbm = RBM(num_visible=6, num_hidden=3)

# 使用对比散度训练受限玻尔兹曼机
rbm.contrastive_divergence(data, learning_rate=0.1, k=1)

print("Trained weights:\n", rbm.weights)
</code></pre>
  </div>
</body>
</html>
  