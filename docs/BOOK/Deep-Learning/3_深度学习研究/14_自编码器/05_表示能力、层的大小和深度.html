
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>表示能力、层的大小和深度</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h4>05. 表示能力、层的大小和深度</h4>
<h5>背景介绍</h5>
<p>深度学习中的自编码器通常由单层的编码器和解码器组成，但深度编码器和解码器可以提供更多优势。深度网络可以通过多层次的特征提取，提高模型的表达能力和泛化能力。</p>
<h5>方法定义和数学原理</h5>
<p><strong>定义：</strong></p>
<p>深度自编码器通过增加编码器和解码器的层数来提高模型的表示能力。数学上，深度自编码器的优化问题可以表示为：</p>
<p>$$
\min_{\theta} L(x, g(f(x; \theta_e); \theta_d))
$$</p>
<p>其中，$ L $ 是重构误差，$ \theta_e $ 是编码器的参数，$ \theta_d $ 是解码器的参数。</p>
<p><strong>数学原理：</strong></p>
<ol>
<li>
<p><strong>损失函数：</strong> 确保自编码器能够重构输入数据 $ x $。
$$
L(x, g(f(x; \theta_e); \theta_d)) = |x - g(f(x; \theta_e); \theta_d)|^2
$$</p>
</li>
<li>
<p><strong>深度编码器：</strong> 多层的编码器和解码器能够提取更高层次的特征。
$$
f(x; \theta_e) = f_n(f_{n-1}(...f_1(x; \theta_1)...; \theta_{n-1}); \theta_n)
$$
$$
g(h; \theta_d) = g_1(g_2(...g_m(h; \theta_{d_m})...; \theta_{d_2}); \theta_{d_1})
$$</p>
</li>
</ol>
<p><strong>算法步骤：</strong></p>
<ol>
<li><strong>初始化：</strong> 初始化编码器和解码器的参数。</li>
<li><strong>前向传播：</strong> 计算输入数据的编码和重构。</li>
<li><strong>计算损失：</strong> 计算重构误差。</li>
<li><strong>反向传播：</strong> 计算梯度并更新参数。</li>
<li>**重复步骤2-4，直到收敛。</li>
</ol>
<h5>应用示例</h5>
<p>深度自编码器在图像处理中的典型应用是图像压缩和去噪。通过多层的编码器和解码器，可以提取更高层次的特征，从而提高图像的压缩效率和去噪效果。</p>
<h3>TASK 3: 使用 Numpy 和 Scipy 从头实现代码</h3>
<h4>代码实现</h4>
<pre><code class="language-python">import numpy as np
from typing import Tuple

class DeepAutoencoder:
    def __init__(self, input_dim: int, hidden_dims: Tuple[int, ...], learning_rate: float = 0.01, max_iter: int = 1000):
        ```
        初始化深度自编码器模型
        
        Args:
            input_dim (int): 输入数据的维数
            hidden_dims (Tuple[int, ...]): 每一隐藏层的维数
            learning_rate (float): 学习率
            max_iter (int): 最大迭代次数
        ```
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.learning_rate = learning_rate
        self.max_iter = max_iter
        
        # 初始化权重和偏置
        self.weights = []
        self.biases = []
        self._initialize_weights()

    def _initialize_weights(self):
        # 初始化编码器权重和偏置
        layer_dims = [self.input_dim] + list(self.hidden_dims)
        for i in range(len(layer_dims) - 1):
            weight = np.random.randn(layer_dims[i], layer_dims[i + 1]) * 0.01
            bias = np.zeros(layer_dims[i + 1])
            self.weights.append(weight)
            self.biases.append(bias)
        
        # 初始化解码器权重和偏置
        for i in range(len(layer_dims) - 2, -1, -1):
            weight = np.random.randn(layer_dims[i + 1], layer_dims[i]) * 0.01
            bias = np.zeros(layer_dims[i])
            self.weights.append(weight)
            self.biases.append(bias)

    def _sigmoid(self, x: np.ndarray) -&gt; np.ndarray:
        return 1 / (1 + np.exp(-x))

    def _sigmoid_derivative(self, x: np.ndarray) -&gt; np.ndarray:
        return x * (1 - x)

    def fit(self, X: np.ndarray):
        ```
        训练深度自编码器模型
        
        Args:
            X (np.ndarray): 输入数据，形状为 (n_samples, input_dim)
        ```
        for _ in range(self.max_iter):
            # 前向传播
            activations = [X]
            for i in range(len(self.hidden_dims)):
                z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
                activation = self._sigmoid(z)
                activations.append(activation)
            
            for i in range(len(self.hidden_dims)):
                z = np.dot(activations[-1], self.weights[len(self.hidden_dims) + i]) + self.biases[len(self.hidden_dims) + i]
                activation = self._sigmoid(z)
                activations.append(activation)
            
            # 计算重构误差
            loss = X - activations[-1]
            total_loss = np.sum(loss ** 2) / 2
            
            # 反向传播
            errors = [loss * self._sigmoid_derivative(activations[-1])]
            for i in range(len(self.hidden_dims) - 1, -1, -1):
                error = np.dot(errors[-1], self.weights[len(self.hidden_dims) + i].T) * self._sigmoid_derivative(activations[len(self.hidden_dims) + i])
                errors.append(error)
            
            errors.reverse()
            for i in range(len(self.hidden_dims) - 1, -1, -1):
                error = np.dot(errors[-1], self.weights[i].T) * self._sigmoid_derivative(activations[i])
                errors.append(error)
            
            errors.reverse()
            # 更新权重和偏置
            for i in range(len(self.hidden_dims)):
                self.weights[i] += self.learning_rate * np.dot(activations[i].T, errors[i])
                self.biases[i] += self.learning_rate * np.sum(errors[i], axis=0)
            
            for i in range(len(self.hidden_dims)):
                self.weights[len(self.hidden_dims) + i] += self.learning_rate * np.dot(activations[len(self.hidden_dims) + i].T, errors[len(self.hidden_dims) + i])
                self.biases[len(self.hidden_dims) + i] += self.learning_rate * np.sum(errors[len(self.hidden_dims) + i], axis=0)

    def transform(self, X: np.ndarray) -&gt; np.ndarray:
        ```
        将输入数据编码为低维表示
        
        Args:
            X (np.ndarray): 输入数据，形状为 (n_samples, input_dim)
        
        Returns:
            np.ndarray: 编码后的低维表示，形状为 (n_samples, hidden_dims[-1])
        ```
        activation = X
        for i in range(len(self.hidden_dims)):
            z = np.dot(activation, self.weights[i]) + self.biases[i]
            activation = self._sigmoid(z)
        return activation

    def reconstruct(self, X: np.ndarray) -&gt; np.ndarray:
        ```
        重构输入数据
        
        Args:
            X (np.ndarray): 输入数据，形状为 (n_samples, input_dim)
        
        Returns:
            np.ndarray: 重构后的数据，形状为 (n_samples, input_dim)
        ```
        activation = self.transform(X)
        for i in range(len(self.hidden_dims), len(self.weights)):
            z = np.dot(activation, self.weights[i]) + self.biases[i]
            activation = self._sigmoid(z)
        return activation

# 示例数据
np.random.seed(42)
X = np.random.rand(100, 20)

# 拟合深度自编码器模型
autoencoder = DeepAutoencoder(input_dim=20, hidden_dims=(15, 10, 5), learning_rate=0.01, max_iter=1000)
autoencoder.fit(X)
encoded_X = autoencoder.transform(X)
reconstructed_X = autoencoder.reconstruct(X)

print(&quot;编码后的数据:\n&quot;, encoded_X)
print(&quot;重构后的数据:\n&quot;, reconstructed_X)
</code></pre>
<h3>代码逐步分析</h3>
<ol>
<li><strong>DeepAutoencoder 类：</strong> 定义了深度自编码器模型，包括初始化、前向传播、反向传播、训练、编码和重构方法。</li>
<li><strong>_initialize_weights 方法：</strong> 初始化编码器和解码器的权重和偏置。</li>
<li><strong>fit 方法：</strong> 实现了自编码器模型的训练过程，包括前向传播、计算重构误差、反向传播和参数更新。</li>
<li><strong>transform 方法：</strong> 将输入数据编码为低维表示。</li>
</ol>

    <h3>Python 文件</h3>
    <pre><code># 05_表示能力、层的大小和深度
"""
Lecture: 3_深度学习研究/14_自编码器
Content: 05_表示能力、层的大小和深度
"""
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Tuple

class DeepAutoencoder(nn.Module):
    def __init__(self, input_dim: int, hidden_dims: Tuple[int, ...]):
        """
        初始化深度自编码器模型
        
        Args:
            input_dim (int): 输入数据的维数
            hidden_dims (Tuple[int, ...]): 每一隐藏层的维数
        """
        super(DeepAutoencoder, self).__init__()
        
        # 编码器
        encoder_layers = []
        prev_dim = input_dim
        for dim in hidden_dims:
            encoder_layers.append(nn.Linear(prev_dim, dim))
            encoder_layers.append(nn.ReLU())
            prev_dim = dim
        self.encoder = nn.Sequential(*encoder_layers)
        
        # 解码器
        decoder_layers = []
        for dim in reversed(hidden_dims[:-1]):
            decoder_layers.append(nn.Linear(prev_dim, dim))
            decoder_layers.append(nn.ReLU())
            prev_dim = dim
        decoder_layers.append(nn.Linear(prev_dim, input_dim))
        decoder_layers.append(nn.ReLU())
        self.decoder = nn.Sequential(*decoder_layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        前向传播
        
        Args:
            x (torch.Tensor): 输入数据，形状为 (n_samples, input_dim)
        
        Returns:
            torch.Tensor: 重构后的数据，形状为 (n_samples, input_dim)
        """
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

def train_autoencoder(model: DeepAutoencoder, data: torch.Tensor, num_epochs: int = 1000, learning_rate: float = 0.01):
    """
    训练深度自编码器模型
    
    Args:
        model (DeepAutoencoder): 自编码器模型
        data (torch.Tensor): 训练数据，形状为 (n_samples, input_dim)
        num_epochs (int): 训练周期数
        learning_rate (float): 学习率
    """
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    for epoch in range(num_epochs):
        # 前向传播
        output = model(data)
        loss = criterion(output, data)
        
        # 反向传播和优化
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if (epoch+1) % 100 == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

def transform(model: DeepAutoencoder, data: torch.Tensor) -> torch.Tensor:
    """
    将输入数据编码为低维表示
    
    Args:
        model (DeepAutoencoder): 自编码器模型
        data (torch.Tensor): 输入数据，形状为 (n_samples, input_dim)
    
    Returns:
        torch.Tensor: 编码后的低维表示，形状为 (n_samples, hidden_dims[-1])
    """
    with torch.no_grad():
        return model.encoder(data)

def reconstruct(model: DeepAutoencoder, data: torch.Tensor) -> torch.Tensor:
    """
    重构输入数据
    
    Args:
        model (DeepAutoencoder): 自编码器模型
        data (torch.Tensor): 输入数据，形状为 (n_samples, input_dim)
    
    Returns:
        torch.Tensor: 重构后的数据，形状为 (n_samples, input_dim)
    """
    with torch.no_grad():
        return model(data)

# 示例数据
torch.manual_seed(42)
X = torch.rand(100, 20)

# 初始化深度自编码器模型
autoencoder = DeepAutoencoder(input_dim=20, hidden_dims=(15, 10, 5))

# 训练模型
train_autoencoder(autoencoder, X)

# 编码和重构数据
encoded_X = transform(autoencoder, X)
reconstructed_X = reconstruct(autoencoder, X)

print("编码后的数据:\n", encoded_X)
print("重构后的数据:\n", reconstructed_X)
</code></pre>
  </div>
</body>
</html>
  