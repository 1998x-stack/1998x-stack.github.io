
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>7.1.2 Relation to logistic regression</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>01_7.1.2_Relation_to_logistic_regression</h1>
<pre><code>Lecture: 7_Sparse_Kernel_Machines/7.1_Maximum_Margin_Classifiers
Content: 01_7.1.2_Relation_to_logistic_regression
</code></pre>
<h2>详细分析第7.1.2节：与逻辑回归的关系</h2>
<h3>引言</h3>
<p>逻辑回归和支持向量机（SVM）是两种常见的分类方法，尽管它们在理论基础和损失函数上有所不同，但在某些情况下，它们具有相似的表现和连接。第7.1.2节详细探讨了逻辑回归和最大间隔分类器（SVM）之间的关系，揭示了它们在分类问题中的联系和区别。</p>
<h3>逻辑回归</h3>
<p>逻辑回归是一种广泛使用的分类算法，通过线性模型和Sigmoid函数将输入映射到概率值，进而进行二分类决策。其目标是最大化似然函数，从而找到最优参数。</p>
<h4>数学形式</h4>
<p>假设输入特征为 $ x $，模型参数为 $ w $ 和偏置 $ b $，逻辑回归模型的输出为：
$$ p(y=1|x) = \sigma(w^T x + b) $$
其中，$\sigma(z)$ 是Sigmoid函数，定义为：
$$ \sigma(z) = \frac{1}{1 + \exp(-z)} $$</p>
<h4>似然函数</h4>
<p>对于给定的训练数据集 ${(x_i, y_i)}<em i="1">{i=1}^{N}$，逻辑回归的对数似然函数为：
$$ \ln L(w, b) = \sum</em>^{N} \left[ y_i \ln \sigma(w^T x_i + b) + (1 - y_i) \ln (1 - \sigma(w^T x_i + b)) \right] $$</p>
<h4>损失函数</h4>
<p>逻辑回归通常通过最小化负对数似然损失函数进行优化：
$$ J(w, b) = -\ln L(w, b) = -\sum_{i=1}^{N} \left[ y_i \ln \sigma(w^T x_i + b) + (1 - y_i) \ln (1 - \sigma(w^T x_i + b)) \right] $$</p>
<h3>支持向量机</h3>
<p>支持向量机（SVM）通过寻找最大化间隔的超平面来实现分类。其核心思想是找到一个最佳的决策边界，使得不同类别的数据点尽可能分开。</p>
<h4>数学形式</h4>
<p>对于二分类问题，SVM 的决策函数为：
$$ f(x) = w^T x + b $$
目标是最大化间隔，同时允许少量的分类错误（软间隔），优化问题为：
$$ \min_{w, b, \xi} \frac{1}{2} |w|^2 + C \sum_{i=1}^{N} \xi_i $$
约束条件为：
$$ y_i (w^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \quad \forall i $$</p>
<h3>逻辑回归与SVM的关系</h3>
<p>尽管逻辑回归和SVM在损失函数和优化目标上有所不同，但它们都试图找到一个线性决策边界来进行分类。以下是它们的主要联系：</p>
<h4>损失函数的相似性</h4>
<ul>
<li><strong>逻辑回归</strong>：使用对数损失函数，对分类错误的惩罚是对数的。</li>
<li><strong>SVM</strong>：使用铰链损失函数，对分类错误的惩罚是线性的。</li>
</ul>
<p>在某些情况下，特别是当使用大数据集时，逻辑回归和SVM的性能可能非常相似。</p>
<h4>正则化的相似性</h4>
<ul>
<li><strong>逻辑回归</strong>：通过对参数 $ w $ 的 $ L2 $ 正则化控制模型复杂度。</li>
<li><strong>SVM</strong>：通过约束 $ |w|^2 $ 控制模型复杂度。</li>
</ul>
<h4>概率输出</h4>
<p>逻辑回归直接输出概率值，而SVM的输出需要通过Platt缩放等方法进行概率化处理。</p>
<h3>实例分析</h3>
<p>假设我们有一个二维分类问题，输入变量为 $ x = [x_1, x_2] $，目标变量为 $ y $。我们使用逻辑回归和SVM进行分类，并比较它们的性能。</p>
<h4>数据生成</h4>
<p>生成数据如下：
$$ x_i \sim \mathcal{U}(0, 1) $$
$$ y_i = \begin{cases}
1, &amp; \text{if } x_1 + x_2 + \epsilon_i &gt; 1 \
0, &amp; \text{otherwise}
\end{cases} $$
其中，$ \epsilon_i \sim \mathcal{N}(0, 0.1) $。</p>
<h4>模型训练</h4>
<ol>
<li><strong>逻辑回归</strong>：使用梯度下降法最小化负对数似然损失函数，训练逻辑回归模型。</li>
<li><strong>SVM</strong>：使用梯度下降法或其他优化算法最小化铰链损失函数，训练SVM模型。</li>
</ol>
<h4>结果分析</h4>
<p>通过比较逻辑回归和SVM的分类效果，可以发现它们在分类边界和误差率方面的差异和相似性。绘制分类边界和支持向量，展示模型的分类效果。</p>
<h3>优势与应用</h3>
<ul>
<li><strong>逻辑回归</strong>：适用于概率输出和解释性较强的场景，如医学诊断、市场营销等。</li>
<li><strong>SVM</strong>：适用于高维数据和需要最大化分类间隔的场景，如图像分类、文本分类等。</li>
</ul>
<h3>小结</h3>
<p>逻辑回归和SVM在理论基础和应用场景上有所不同，但在分类任务中它们具有一定的联系。通过理解它们的相似性和区别，可以更好地选择和应用这两种方法。</p>

    <h3>Python 文件</h3>
    <pre><code># 01_7.1.2_Relation_to_logistic_regression

"""
Lecture: 7_Sparse_Kernel_Machines/7.1_Maximum_Margin_Classifiers
Content: 01_7.1.2_Relation_to_logistic_regression
"""

</code></pre>
  </div>
</body>
</html>
  