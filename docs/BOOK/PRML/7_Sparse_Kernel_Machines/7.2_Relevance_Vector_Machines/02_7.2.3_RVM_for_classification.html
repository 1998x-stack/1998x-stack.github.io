
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>7.2.3 RVM for classification</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>02_7.2.3_RVM_for_classification</h1>
<pre><code>Lecture: 7_Sparse_Kernel_Machines/7.2_Relevance_Vector_Machines
Content: 02_7.2.3_RVM_for_classification
</code></pre>
<h3>7.2.3 分类中的相关向量机（RVM）</h3>
<h4>概述</h4>
<p>在《模式识别与机器学习》中，7.2.3节详细介绍了如何将相关向量机（Relevance Vector Machine, RVM）框架扩展到分类问题中。与回归模型不同，分类问题中使用了概率线性分类模型，并通过自动相关性确定（Automatic Relevance Determination, ARD）先验来实现稀疏性。</p>
<h4>模型形式</h4>
<p>对于二分类问题，目标变量 $ t \in {0, 1} $。模型形式为：</p>
<p>$$ y(x, w) = \sigma(w^T \phi(x)) $$</p>
<p>其中，$ \sigma(\cdot) $ 是逻辑Sigmoid函数，定义为：</p>
<p>$$ \sigma(a) = \frac{1}{1 + e^{-a}} $$</p>
<p>通过对权重向量 $ w $ 引入高斯先验，得到模型形式为：</p>
<p>$$ p(w | \alpha) = \mathcal{N}(w | 0, A^{-1}) $$</p>
<p>其中，$ A $ 是一个对角矩阵，其对角元素为各个权重参数的精度超参数 $ \alpha_i $。</p>
<h4>贝叶斯推断</h4>
<p>与回归模型不同，分类模型中无法对参数向量 $ w $ 进行解析积分。因此，使用拉普拉斯近似法来处理，即用一个高斯分布来近似后验分布。这种方法在第4章中已经用于贝叶斯逻辑回归。</p>
<p>首先，初始化超参数向量 $ \alpha $。对于给定的 $ \alpha $ 值，构建后验分布的高斯近似，从而得到边缘似然的近似值。最大化该近似边缘似然可以重新估计 $ \alpha $ 值，重复该过程直到收敛。</p>
<h4>拉普拉斯近似</h4>
<p>拉普拉斯近似的详细步骤如下：</p>
<ol>
<li><strong>后验分布模式</strong>：对于固定的 $ \alpha $ 值，通过最大化 $ \ln p(w|t, \alpha) $ 得到后验分布的模式，即：</li>
</ol>
<p>$$ \ln p(w|t, \alpha) = \ln {p(t|w)p(w|\alpha)} - \ln p(t|\alpha) $$</p>
<ol start="2">
<li><strong>最大化对数后验分布</strong>：可以使用迭代重加权最小二乘法（IRLS）进行最大化，对数后验分布的梯度向量和Hessian矩阵为：</li>
</ol>
<p>$$ \nabla \ln p(w|t, \alpha) = \Phi^T(t - y) - Aw $$</p>
<p>$$ \nabla \nabla \ln p(w|t, \alpha) = -(\Phi^T B \Phi + A) $$</p>
<p>其中，$ B $ 是对角矩阵，其元素为 $ b_n = y_n (1 - y_n) $，$ y $ 是模型输出向量。</p>
<ol start="3">
<li><strong>近似边缘似然</strong>：使用拉普拉斯近似法计算边缘似然：</li>
</ol>
<p>$$ \ln p(t|\alpha) \approx \ln p(t|w_{MAP}) - \frac{1}{2} w_{MAP}^T A w_{MAP} - \frac{1}{2} \ln |H| + \text{const} $$</p>
<p>其中，$ w_{MAP} $ 为后验分布的模式，$ H $ 为后验分布的Hessian矩阵。</p>
<h4>多分类扩展</h4>
<p>对于多分类问题，使用K个线性模型，通过softmax函数组合输出：</p>
<p>$$ y_k(x) = \frac{\exp(a_k)}{\sum_j \exp(a_j)} $$</p>
<p>其中，$ a_k = w_k^T x $。</p>
<p>对数似然函数为：</p>
<p>$$ \ln p(T|w_1, \ldots, w_K) = \sum_{n=1}^N \sum_{k=1}^K t_{nk} \ln y_{nk} $$</p>
<p>目标值 $ t_{nk} $ 使用K分类编码。使用拉普拉斯近似法优化超参数，这种方法比SVM中的成对分类方法更为原则性，并为新数据点提供概率预测。</p>
<h4>优缺点</h4>
<p>RVM的主要优点在于稀疏性和概率预测。然而，训练时间相对较长。尽管如此，RVM通过自动确定模型复杂度参数，避免了交叉验证的需求，在处理测试数据时计算时间较短。训练时需要对基函数数量 $ M $ 进行矩阵求逆，计算复杂度为 $ O(M^3) $，这比SVM的训练时间要长。</p>

    <h3>Python 文件</h3>
    <pre><code># 02_7.2.3_RVM_for_classification

"""
Lecture: 7_Sparse_Kernel_Machines/7.2_Relevance_Vector_Machines
Content: 02_7.2.3_RVM_for_classification
"""

</code></pre>
  </div>
</body>
</html>
  