
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>4.5.2 Predictive distribution</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>01_4.5.2_Predictive_distribution</h1>
<pre><code>Lecture: 4_Linear_Models_for_Classification/4.5_Bayesian_Logistic_Regression
Content: 01_4.5.2_Predictive_distribution
</code></pre>
<h3>深入解析PRML中的4.5.2节：预测分布</h3>
<p>在《模式识别与机器学习》（Pattern Recognition and Machine Learning, PRML）的第4.5节，作者介绍了贝叶斯逻辑回归（Bayesian Logistic Regression）。具体来说，第4.5.2节探讨了预测分布（Predictive Distribution）。以下是对这一节内容的详细分析。</p>
<h4>贝叶斯逻辑回归的背景</h4>
<p>在贝叶斯逻辑回归中，我们不仅希望找到模型参数的点估计，更希望能够表达参数的不确定性。这是通过后验分布 $ p(\mathbf{w}|D) $ 来实现的，其中 $\mathbf{w}$ 是模型参数，$D$ 是数据集。贝叶斯方法的核心是利用后验分布对未知参数进行推断，从而进行预测。</p>
<h4>预测分布的定义</h4>
<p>在贝叶斯框架下，预测新数据点的标签时，我们使用预测分布 $ p(t|\mathbf{x}, D) $，其中 $\mathbf{x}$ 是新的输入数据点，$t$ 是对应的标签。预测分布通过对参数的后验分布进行积分得到：</p>
<p>$$ p(t|\mathbf{x}, D) = \int p(t|\mathbf{x}, \mathbf{w}) p(\mathbf{w}|D) d\mathbf{w} $$</p>
<p>其中， $ p(t|\mathbf{x}, \mathbf{w}) $ 是给定参数 $\mathbf{w}$ 和输入 $\mathbf{x}$ 时的似然函数， $ p(\mathbf{w}|D) $ 是参数的后验分布。</p>
<h4>近似方法</h4>
<p>直接计算上述积分通常是不可行的，因此需要使用近似方法。在拉普拉斯近似中，我们假设后验分布在其最大后验估计（MAP估计）点附近呈高斯分布，从而将积分转化为高斯积分。</p>
<ol>
<li>
<p><strong>找到MAP估计</strong>：
首先，通过最大化对数后验概率找到MAP估计 $\mathbf{w}_{\text{MAP}}$：</p>
<p>$$ \mathbf{w}<em>{\text{MAP}} = \arg\max</em>{\mathbf{w}} \left{ \ln p(\mathbf{w}|D) \right} $$</p>
</li>
<li>
<p><strong>构造高斯近似</strong>：
在 $\mathbf{w}<em>{\text{MAP}}$ 附近，用高斯分布近似后验分布，其均值为 $\mathbf{w}</em>{\text{MAP}}$，协方差矩阵为负对数后验分布的二阶导数的逆：</p>
<p>$$ \Sigma = \left( - \nabla \nabla \ln p(\mathbf{w}|D) \big|<em>{\mathbf{w} = \mathbf{w}</em>{\text{MAP}}} \right)^{-1} $$</p>
</li>
<li>
<p><strong>计算预测分布</strong>：
使用高斯近似后验分布来计算预测分布：</p>
<p>$$ p(t=1|\mathbf{x}, D) \approx \sigma \left( \mathbf{w}_{\text{MAP}}^T \mathbf{x} \right) $$</p>
<p>其中，$\sigma(z)$ 是逻辑函数（Sigmoid函数），定义为：</p>
<p>$$ \sigma(z) = \frac{1}{1 + \exp(-z)} $$</p>
</li>
</ol>
<h4>实际应用</h4>
<ol>
<li>
<p><strong>分类问题</strong>：
在分类问题中，预测分布用于计算新数据点属于某个类别的概率。通过计算这些概率，可以对新数据点进行分类。</p>
</li>
<li>
<p><strong>不确定性估计</strong>：
贝叶斯方法不仅提供了点预测，还提供了预测的不确定性估计。这对于风险管理和决策制定具有重要意义。</p>
</li>
<li>
<p><strong>模型选择</strong>：
通过比较不同模型的预测分布，可以进行模型选择。具有更高预测分布值的模型通常具有更好的泛化性能。</p>
</li>
</ol>
<h4>优点与局限性</h4>
<p><strong>优点</strong>：</p>
<ol>
<li><strong>不确定性量化</strong>：贝叶斯方法通过预测分布量化了预测的不确定性。</li>
<li><strong>鲁棒性</strong>：通过整合先验信息，贝叶斯方法对噪声和异常值具有更好的鲁棒性。</li>
<li><strong>模型选择</strong>：贝叶斯方法提供了自然的模型比较和选择框架。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>计算复杂性</strong>：直接计算预测分布的积分通常是不可行的，需要使用近似方法。</li>
<li><strong>高斯假设</strong>：拉普拉斯近似假设后验分布在MAP点附近呈高斯分布，这在某些情况下可能不成立。</li>
</ol>
<h3>结论</h3>
<p>通过以上分析可以看出，预测分布在贝叶斯逻辑回归中具有重要意义。它不仅提供了点预测，还量化了预测的不确定性，使得贝叶斯方法在分类和模型选择中具有独特的优势。掌握预测分布的理论和应用，有助于在实际问题中选择合适的模型，提高分类和预测的准确性。</p>

    <h3>Python 文件</h3>
    <pre><code># 01_4.5.2_Predictive_distribution

"""
Lecture: 4_Linear_Models_for_Classification/4.5_Bayesian_Logistic_Regression
Content: 01_4.5.2_Predictive_distribution
"""

</code></pre>
  </div>
</body>
</html>
  