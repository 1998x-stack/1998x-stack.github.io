
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>4.2.3 Discrete features</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>02_4.2.3_Discrete_features</h1>
<pre><code>Lecture: 4_Linear_Models_for_Classification/4.2_Probabilistic_Generative_Models
Content: 02_4.2.3_Discrete_features
</code></pre>
<h2>详解PRML中的第4.2.3节：离散特征</h2>
<p>《模式识别与机器学习》（Pattern Recognition and Machine Learning, PRML）是由Christopher M. Bishop所著的一本经典教材，其中第4章涵盖了线性分类模型的内容。在第4.2节，作者介绍了概率生成模型（Probabilistic Generative Models）。具体来说，第4.2.3节探讨了离散特征的处理方法。以下是对这一节内容的详细分析。</p>
<h3>背景介绍</h3>
<p>在许多实际应用中，输入数据的特征可能是离散的而非连续的。例如，文本数据中的词汇特征、基因序列中的碱基特征等都是离散的。在处理这些离散特征时，我们需要对概率生成模型进行相应的调整，以便能够有效地建模和分类。</p>
<h3>离散特征的概率生成模型</h3>
<p>对于离散特征 $x_i$，我们假设它们是二进制特征，即 $x_i \in {0, 1}$。对于每一个特征，我们希望能够建模其在不同类别下的条件概率分布。为了简化问题，我们通常假设特征在给定类别条件下是相互独立的（即朴素贝叶斯假设）。</p>
<h4>朴素贝叶斯假设</h4>
<p>根据朴素贝叶斯假设，给定类别 $C_k$ 的条件下，特征 $x_i$ 的条件概率分布可以表示为：</p>
<p>$$ p(x|C_k) = \prod_{i=1}^{D} \mu_{ki}^{x_i} (1 - \mu_{ki})^{1-x_i} $$</p>
<p>其中，$\mu_{ki}$ 表示类别 $C_k$ 下特征 $x_i$ 为1的概率，$D$ 是特征的数量。这种假设极大地简化了模型的复杂性，因为我们只需要估计 $D$ 个参数，而不是 $2^D$ 个。</p>
<h3>对数线性模型</h3>
<p>在使用朴素贝叶斯假设建模离散特征后，我们可以得到对数线性模型。给定输入向量 $x$，类别 $C_k$ 的对数几率可以表示为：</p>
<p>$$ a_k(x) = \sum_{i=1}^{D} { x_i \ln \mu_{ki} + (1 - x_i) \ln (1 - \mu_{ki}) } + \ln p(C_k) $$</p>
<p>其中，$\ln p(C_k)$ 是类别的先验概率。对于二分类问题，我们可以使用逻辑回归模型来表示：</p>
<p>$$ p(C_1|x) = \frac{1}{1 + \exp(-a(x))} $$</p>
<p>其中，</p>
<p>$$ a(x) = \sum_{i=1}^{D} x_i \ln \frac{\mu_{1i}}{\mu_{2i}} + \sum_{i=1}^{D} (1 - x_i) \ln \frac{1 - \mu_{1i}}{1 - \mu_{2i}} + \ln \frac{p(C_1)}{p(C_2)} $$</p>
<h3>参数估计</h3>
<p>为了估计模型参数 $\mu_{ki}$ 和 $p(C_k)$，我们可以使用最大似然估计。对于类别 $C_k$，参数 $\mu_{ki}$ 的最大似然估计为：</p>
<p>$$ \mu_{ki} = \frac{\sum_{n \in C_k} x_{ni}}{N_k} $$</p>
<p>其中，$N_k$ 是类别 $C_k$ 中的样本数量，$x_{ni}$ 表示第 $n$ 个样本的第 $i$ 个特征。</p>
<p>先验概率 $p(C_k)$ 可以通过类别样本的比例来估计：</p>
<p>$$ p(C_k) = \frac{N_k}{N} $$</p>
<p>其中，$N$ 是总样本数。</p>
<h3>扩展到多类别和多状态特征</h3>
<p>虽然上述讨论集中于二分类问题和二进制特征，但这些方法可以扩展到多类别和多状态特征。对于多类别问题，我们可以引入一个独立的二进制分类器集合，每个分类器用于区分一个类别与其他类别（one-vs-rest方法）。对于多状态特征，我们可以将每个特征的不同状态编码为多个二进制特征，例如使用独热编码（one-hot encoding）。</p>
<h3>应用场景和优势</h3>
<p>离散特征的概率生成模型在文本分类、基因数据分析、推荐系统等领域有广泛应用。其优势在于：</p>
<ul>
<li><strong>计算简单</strong>：由于朴素贝叶斯假设，模型训练和推断的计算复杂度较低。</li>
<li><strong>鲁棒性好</strong>：在高维稀疏数据中表现良好。</li>
<li><strong>解释性强</strong>：模型参数具有明确的概率意义，便于解释。</li>
</ul>
<p>然而，朴素贝叶斯假设可能过于简化，忽略了特征之间的相互依赖性。在实际应用中，可以结合其他方法（如集成学习、特征选择）来提升模型性能。</p>

    <h3>Python 文件</h3>
    <pre><code># 02_4.2.3_Discrete_features

"""
Lecture: 4_Linear_Models_for_Classification/4.2_Probabilistic_Generative_Models
Content: 02_4.2.3_Discrete_features
"""

</code></pre>
  </div>
</body>
</html>
  