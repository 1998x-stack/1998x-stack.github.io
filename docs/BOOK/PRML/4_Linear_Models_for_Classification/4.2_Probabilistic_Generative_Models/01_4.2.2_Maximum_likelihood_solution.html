
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>4.2.2 Maximum likelihood solution</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>01_4.2.2_Maximum_likelihood_solution</h1>
<pre><code>Lecture: 4_Linear_Models_for_Classification/4.2_Probabilistic_Generative_Models
Content: 01_4.2.2_Maximum_likelihood_solution
</code></pre>
<h2>详解PRML中的第4.2.2节：最大似然解</h2>
<p>《模式识别与机器学习》（Pattern Recognition and Machine Learning, PRML）是Christopher M. Bishop所著的一本经典教材，其中第4章涵盖了线性分类模型的内容。在第4.2节，作者介绍了概率生成模型（Probabilistic Generative Models）。具体来说，第4.2.2节探讨了最大似然解的求解方法。以下是对这一节内容的详细分析。</p>
<h3>概率生成模型的背景</h3>
<p>概率生成模型是一类通过联合概率分布建模输入数据和类别标签的方法。与判别模型不同，生成模型可以直接生成数据样本，并且在处理缺失数据、异常检测和密度估计等任务中具有优势。在指定参数化形式的类条件密度函数 $ p(x|C_k) $ 之后，我们可以使用最大似然法来确定这些参数的值及先验类别概率 $ p(C_k) $。</p>
<h3>高斯判别分析与最大似然估计</h3>
<p>在这里，我们主要讨论两类问题，并假设每个类别的类条件密度为高斯分布，并具有共享的协方差矩阵。数据集 ${x_n, t_n}$ 中，$ n = 1, ..., N $，其中 $ t_n = 1 $ 表示类别 $ C_1 $，$ t_n = 0 $ 表示类别 $ C_2 $。我们用 $\pi$ 表示先验类别概率 $ p(C_1) $，那么 $ p(C_2) = 1 - \pi $。</p>
<h4>似然函数的构建</h4>
<p>对于来自类别 $ C_1 $ 的数据点 $ x_n $，我们有：
$$ p(x_n, C_1) = p(C_1) p(x_n | C_1) = \pi \mathcal{N}(x_n | \mu_1, \Sigma) $$</p>
<p>对于来自类别 $ C_2 $ 的数据点 $ x_n $，我们有：
$$ p(x_n, C_2) = p(C_2) p(x_n | C_2) = (1 - \pi) \mathcal{N}(x_n | \mu_2, \Sigma) $$</p>
<p>因此，似然函数为：
$$ p(t | \pi, \mu_1, \mu_2, \Sigma) = \prod_{n=1}^{N} [\pi \mathcal{N}(x_n | \mu_1, \Sigma)]^{t_n} [(1 - \pi) \mathcal{N}(x_n | \mu_2, \Sigma)]^{1 - t_n} $$</p>
<p>为了简化计算，我们通常最大化对数似然函数：
$$ \ln p(t | \pi, \mu_1, \mu_2, \Sigma) = \sum_{n=1}^{N} t_n \ln \pi + \sum_{n=1}^{N} (1 - t_n) \ln (1 - \pi) + \sum_{n=1}^{N} t_n \ln \mathcal{N}(x_n | \mu_1, \Sigma) + \sum_{n=1}^{N} (1 - t_n) \ln \mathcal{N}(x_n | \mu_2, \Sigma) $$</p>
<h3>最大化对数似然函数</h3>
<ol>
<li>
<p><strong>最大化 $\pi$</strong>
对数似然函数中与 $\pi$ 相关的部分为：
$$ \sum_{n=1}^{N} { t_n \ln \pi + (1 - t_n) \ln (1 - \pi) } $$</p>
<p>对 $\pi$ 求导并设为零，得到：
$$ \pi = \frac{1}{N} \sum_{n=1}^{N} t_n = \frac{N_1}{N} = \frac{N_1}{N_1 + N_2} $$</p>
<p>其中，$ N_1 $ 表示类别 $ C_1 $ 中的数据点总数，$ N_2 $ 表示类别 $ C_2 $ 中的数据点总数。因此，$\pi$ 的最大似然估计就是类别 $ C_1 $ 数据点在总数据点中的比例。</p>
</li>
<li>
<p><strong>最大化 $\mu_1$ 和 $\mu_2$</strong>
对数似然函数中与 $\mu_1$ 相关的部分为：
$$ \sum_{n=1}^{N} t_n \ln \mathcal{N}(x_n | \mu_1, \Sigma) = -\frac{1}{2} \sum_{n=1}^{N} t_n (x_n - \mu_1)^T \Sigma^{-1} (x_n - \mu_1) + \text{const} $$</p>
<p>对 $\mu_1$ 求导并设为零，得到：
$$ \mu_1 = \frac{1}{N_1} \sum_{n=1}^{N} t_n x_n $$</p>
<p>这实际上是类别 $ C_1 $ 中所有输入向量的均值。同理，对于 $\mu_2$，我们有：
$$ \mu_2 = \frac{1}{N_2} \sum_{n=1}^{N} (1 - t_n) x_n $$</p>
</li>
<li>
<p><strong>最大化协方差矩阵 $\Sigma$</strong>
对数似然函数中与 $\Sigma$ 相关的部分为：
$$ -\frac{1}{2} \sum_{n=1}^{N} t_n (x_n - \mu_1)^T \Sigma^{-1} (x_n - \mu_1) - \frac{1}{2} \sum_{n=1}^{N} (1 - t_n) (x_n - \mu_2)^T \Sigma^{-1} (x_n - \mu_2) - \frac{N}{2} \ln |\Sigma| $$</p>
<p>对 $\Sigma$ 求导并设为零，得到：
$$ \Sigma = \frac{1}{N} \left( \sum_{n \in C_1} (x_n - \mu_1)(x_n - \mu_1)^T + \sum_{n \in C_2} (x_n - \mu_2)(x_n - \mu_2)^T \right) $$</p>
</li>
</ol>
<h3>结论</h3>
<p>通过以上分析，可以看出最大似然估计在求解生成模型参数中的重要性。在高斯判别分析的背景下，最大似然方法能够有效地估计类别的先验概率、均值向量和共享的协方差矩阵。通过这些参数的估计，我们可以建立强大的分类模型，对新数据进行分类预测。</p>
<p>这种方法不仅在理论上具有重要意义，也在实际应用中广泛使用，尤其在处理高维数据和多类别分类问题时显示出其优势。</p>

    <h3>Python 文件</h3>
    <pre><code># 01_4.2.2_Maximum_likelihood_solution

"""
Lecture: 4_Linear_Models_for_Classification/4.2_Probabilistic_Generative_Models
Content: 01_4.2.2_Maximum_likelihood_solution
"""

</code></pre>
  </div>
</body>
</html>
  