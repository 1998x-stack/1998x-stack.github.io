
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>4.3.1 Fixed basis functions</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>00_4.3.1_Fixed_basis_functions</h1>
<pre><code>Lecture: 4_Linear_Models_for_Classification/4.3_Probabilistic_Discriminative_Models
Content: 00_4.3.1_Fixed_basis_functions
</code></pre>
<h3>深入解析PRML中的4.3.1节：固定基函数</h3>
<p>《模式识别与机器学习》（Pattern Recognition and Machine Learning, PRML）是由Christopher M. Bishop所著的一本经典教材，其中第4章涵盖了线性分类模型的内容。在第4.3节，作者介绍了概率判别模型（Probabilistic Discriminative Models）。具体来说，第4.3.1节探讨了固定基函数的方法。以下是对这一节内容的详细分析。</p>
<h3>固定基函数的背景</h3>
<p>在机器学习中，特别是在线性模型的背景下，基函数（Basis Function）是用于将输入数据转换为特征空间的函数。通过将输入数据映射到一个更高维度或非线性的特征空间，我们可以更容易地处理复杂的模式识别和分类问题。固定基函数意味着这些基函数在训练过程中保持不变，只调整模型参数以最小化误差。</p>
<h3>固定基函数的形式</h3>
<p>固定基函数模型的形式可以写为：</p>
<p>$$ y(x, w) = w_0 + \sum_{j=1}^{M-1} w_j \phi_j(x) $$</p>
<p>其中：</p>
<ul>
<li>$ \phi_j(x) $ 是基函数。</li>
<li>$ w_j $ 是模型参数。</li>
<li>$ M $ 是基函数的数量。</li>
</ul>
<p>为了简化表达，我们可以将模型表示为向量的形式：</p>
<p>$$ y(x, w) = w^T \phi(x) $$</p>
<p>其中，$ \phi(x) $ 是基函数向量，包含所有的基函数。</p>
<h3>常见的基函数类型</h3>
<ol>
<li>
<p><strong>多项式基函数</strong>：
多项式基函数是最简单的一类基函数，通常用于线性回归和多项式回归中。它们的形式为：</p>
<p>$$ \phi_j(x) = x^j $$</p>
</li>
<li>
<p><strong>高斯基函数</strong>：
高斯基函数广泛应用于径向基函数网络（Radial Basis Function Networks）和支持向量机（Support Vector Machines）中。它们的形式为：</p>
<p>$$ \phi_j(x) = \exp\left(-\frac{(x - \mu_j)^2}{2s^2}\right) $$</p>
<p>其中，$ \mu_j $ 是基函数的中心，$ s $ 是尺度参数。</p>
</li>
<li>
<p><strong>Sigmoid基函数</strong>：
Sigmoid基函数通常用于神经网络中。它们的形式为：</p>
<p>$$ \phi_j(x) = \sigma\left(\frac{x - \mu_j}{s}\right) $$</p>
<p>其中，$ \sigma(a) = \frac{1}{1 + \exp(-a)} $ 是Sigmoid函数。</p>
</li>
</ol>
<h3>固定基函数的优势</h3>
<ol>
<li>
<p><strong>计算简便</strong>：
固定基函数模型的参数优化通常可以通过线性回归的闭式解来实现，这使得计算非常高效。</p>
</li>
<li>
<p><strong>理论分析性强</strong>：
由于模型参数线性化，许多理论结果（如最小二乘解、贝叶斯解）可以直接应用。</p>
</li>
<li>
<p><strong>非线性映射能力</strong>：
通过选择合适的基函数，可以将输入数据映射到高维或非线性的特征空间，从而提高模型的表达能力。</p>
</li>
</ol>
<h3>固定基函数的局限性</h3>
<ol>
<li>
<p><strong>基函数选择的依赖性</strong>：
模型的性能高度依赖于基函数的选择。如果基函数不能有效地表示数据的特征，模型的性能将受到影响。</p>
</li>
<li>
<p><strong>维数灾难</strong>：
随着输入维度的增加，所需的基函数数量也会迅速增加，这可能导致计算和存储的挑战。</p>
</li>
</ol>
<h3>固定基函数在实际中的应用</h3>
<ol>
<li>
<p><strong>径向基函数网络（RBFN）</strong>：
RBFN是一种常见的神经网络模型，广泛应用于模式识别、函数逼近和时间序列预测等领域。其基函数通常为高斯函数。</p>
</li>
<li>
<p><strong>支持向量机（SVM）</strong>：
SVM是一种强大的分类和回归工具，通过核技巧将输入数据映射到高维特征空间。常用的核函数包括多项式核、高斯核和Sigmoid核等。</p>
</li>
<li>
<p><strong>贝叶斯线性回归</strong>：
在贝叶斯框架下，固定基函数模型可以通过引入先验分布和后验推断来实现，这在处理不确定性和提高模型鲁棒性方面具有优势。</p>
</li>
</ol>

    <h3>Python 文件</h3>
    <pre><code># 00_4.3.1_Fixed_basis_functions

"""
Lecture: 4_Linear_Models_for_Classification/4.3_Probabilistic_Discriminative_Models
Content: 00_4.3.1_Fixed_basis_functions
"""

</code></pre>
  </div>
</body>
</html>
  