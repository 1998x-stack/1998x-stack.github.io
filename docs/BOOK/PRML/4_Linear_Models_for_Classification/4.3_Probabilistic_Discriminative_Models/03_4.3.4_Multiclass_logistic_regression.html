
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>4.3.4 Multiclass logistic regression</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>03_4.3.4_Multiclass_logistic_regression</h1>
<pre><code>Lecture: 4_Linear_Models_for_Classification/4.3_Probabilistic_Discriminative_Models
Content: 03_4.3.4_Multiclass_logistic_regression
</code></pre>
<h2>详解PRML中的4.3.4节：多类别逻辑回归</h2>
<p>《模式识别与机器学习》（Pattern Recognition and Machine Learning, PRML）是由Christopher M. Bishop所著的一本经典教材，其中第4章涵盖了线性分类模型的内容。在第4.3节，作者介绍了概率判别模型（Probabilistic Discriminative Models）。具体来说，第4.3.4节探讨了多类别逻辑回归（Multiclass Logistic Regression）。以下是对这一节内容的详细分析。</p>
<h3>多类别逻辑回归的背景</h3>
<p>在多类别分类问题中，我们需要将输入向量 $ \phi $ 分配到 $ K $ 个类别中的一个。为了实现这一点，多类别逻辑回归使用Softmax函数将输入变量的线性组合转换为后验概率。Softmax函数是一种归一化指数函数，可以将输入映射到 $[0, 1]$ 区间，并且所有输出的和为1。</p>
<h3>Softmax函数</h3>
<p>Softmax函数的形式为：</p>
<p>$$ p(C_k|\phi) = y_k(\phi) = \frac{\exp(a_k)}{\sum_{j} \exp(a_j)} $$</p>
<p>其中，激活函数 $ a_k $ 定义为：</p>
<p>$$ a_k = w_k^T \phi $$</p>
<p>这里 $ w_k $ 是类别 $ C_k $ 的参数向量。通过这种方式，输入向量 $ \phi $ 被映射到一个线性函数，然后通过Softmax函数转换为后验概率。</p>
<h3>最大似然估计</h3>
<p>在多类别逻辑回归中，我们使用最大似然方法来确定模型参数 $ {w_k} $。首先，我们定义数据集 ${(\phi_n, t_n)}$，其中 $ t_n $ 是一个采用1-of-K编码的目标向量。如果样本 $ n $ 属于类别 $ C_k $，那么 $ t_{nk} = 1 $，否则 $ t_{nk} = 0 $。</p>
<h4>似然函数</h4>
<p>似然函数可以写成：</p>
<p>$$ p(T|w_1, \ldots, w_K) = \prod_{n=1}^{N} \prod_{k=1}^{K} p(C_k|\phi_n)^{t_{nk}} = \prod_{n=1}^{N} \prod_{k=1}^{K} y_{nk}^{t_{nk}} $$</p>
<p>其中， $ y_{nk} = y_k(\phi_n) $， $ T $ 是一个 $ N \times K $ 的目标变量矩阵，元素为 $ t_{nk} $。</p>
<h4>对数似然函数</h4>
<p>取似然函数的对数得到对数似然函数：</p>
<p>$$ E(w_1, \ldots, w_K) = - \ln p(T|w_1, \ldots, w_K) = - \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \ln y_{nk} $$</p>
<p>这就是多类别分类问题的交叉熵误差函数。</p>
<h3>梯度下降法</h3>
<p>为了最小化误差函数，我们使用梯度下降法。误差函数关于参数向量 $ w_j $ 的梯度为：</p>
<p>$$ \nabla_{w_j} E(w_1, \ldots, w_K) = \sum_{n=1}^{N} (y_{nj} - t_{nj}) \phi_n $$</p>
<p>其中， $ y_{nj} $ 是模型对样本 $ n $ 属于类别 $ C_j $ 的预测概率。</p>
<h3>牛顿-拉弗森法</h3>
<p>与二分类逻辑回归类似，我们也可以使用牛顿-拉弗森方法来优化多类别逻辑回归模型。更新公式为：</p>
<p>$$ w_{\text{new}} = w_{\text{old}} - H^{-1} \nabla E(w) $$</p>
<p>其中， $ H $ 是Hessian矩阵，其元素为：</p>
<p>$$ H_{ij} = \frac{\partial^2 E}{\partial w_i \partial w_j} $$</p>
<p>对于多类别逻辑回归，Hessian矩阵的元素为：</p>
<p>$$ \nabla_{w_k} \nabla_{w_j} E(w_1, \ldots, w_K) = - \sum_{n=1}^{N} y_{nk} (I_{kj} - y_{nj}) \phi_n \phi_n^T $$</p>
<p>这里， $ I_{kj} $ 是单位矩阵的元素。</p>
<h3>IRLS算法</h3>
<p>迭代重加权最小二乘法（IRLS）是一种基于牛顿-拉弗森方法的优化算法，适用于多类别逻辑回归问题。通过迭代更新参数向量 $ w $，每次使用新的权重向量计算修正后的加权矩阵 $ R $，直到收敛到全局最小值。</p>
<h3>结论</h3>
<p>通过以上分析可以看出，多类别逻辑回归是一种强大的分类模型，通过Softmax函数将输入映射到后验概率，并使用最大似然估计优化参数。它在处理多类别分类问题时表现优异，尤其是在高维数据和类条件密度分布复杂的情况下。掌握多类别逻辑回归的理论和应用，有助于我们在实际问题中选择合适的模型和算法，提高分类和预测的准确性。</p>

    <h3>Python 文件</h3>
    <pre><code># 03_4.3.4_Multiclass_logistic_regression

"""
Lecture: 4_Linear_Models_for_Classification/4.3_Probabilistic_Discriminative_Models
Content: 03_4.3.4_Multiclass_logistic_regression
"""

import numpy as np
from scipy.special import softmax
from scipy.optimize import minimize
from typing import Tuple

class MulticlassLogisticRegression:
    """
    多类别逻辑回归分类器

    Parameters:
    -----------
    n_iter : int
        训练数据迭代次数 (默认值为 100)
    tol : float
        收敛阈值 (默认值为 1e-6)

    Attributes:
    -----------
    W_ : np.ndarray
        权重矩阵
    cost_ : list
        每次迭代中的损失值
    """
    def __init__(self, n_iter: int = 100, tol: float = 1e-6) -> None:
        self.n_iter = n_iter
        self.tol = tol
        self.W_ = None
        self.cost_ = []

    def fit(self, X: np.ndarray, y: np.ndarray) -> None:
        """
        训练多类别逻辑回归分类器
        
        Parameters:
        -----------
        X : np.ndarray, shape = [n_samples, n_features]
            训练向量
        y : np.ndarray, shape = [n_samples]
            目标值
        """
        n_samples, n_features = X.shape
        n_classes = np.unique(y).size
        self.W_ = np.zeros((n_features + 1, n_classes))

        X_bias = np.insert(X, 0, 1, axis=1)  # 添加偏置项
        y_one_hot = np.eye(n_classes)[y]  # 转换为one-hot编码

        for _ in range(self.n_iter):
            z = np.dot(X_bias, self.W_)
            y_hat = softmax(z, axis=1)
            gradient = np.dot(X_bias.T, (y_hat - y_one_hot)) / n_samples
            H = np.dot(X_bias.T, X_bias * y_hat * (1 - y_hat)[:, np.newaxis]) / n_samples
            delta_W = np.linalg.solve(H, gradient)
            self.W_ -= delta_W
            cost = self._cost_function(y_one_hot, y_hat)
            self.cost_.append(cost)

            # 检查收敛性
            if np.linalg.norm(delta_W) < self.tol:
                break

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        返回类标预测值
        
        Parameters:
        -----------
        X : np.ndarray
            输入向量
        
        Returns:
        --------
        np.ndarray
            类标预测值
        """
        X_bias = np.insert(X, 0, 1, axis=1)  # 添加偏置项
        z = np.dot(X_bias, self.W_)
        y_hat = softmax(z, axis=1)
        return np.argmax(y_hat, axis=1)

    def _cost_function(self, y_true: np.ndarray, y_hat: np.ndarray) -> float:
        """
        计算交叉熵损失函数
        
        Parameters:
        -----------
        y_true : np.ndarray
            真实值
        y_hat : np.ndarray
            预测值
        
        Returns:
        --------
        float
            交叉熵损失值
        """
        return -np.mean(np.sum(y_true * np.log(y_hat), axis=1))

def generate_data(n_samples: int = 300) -> Tuple[np.ndarray, np.ndarray]:
    """
    生成多类别数据集
    
    Parameters:
    -----------
    n_samples : int
        样本数量 (默认值为 300)
        
    Returns:
    --------
    Tuple[np.ndarray, np.ndarray]
        特征矩阵和目标值向量
    """
    np.random.seed(0)
    X = np.random.randn(n_samples, 2)
    y = np.random.choice([0, 1, 2], size=n_samples, p=[0.3, 0.4, 0.3])
    return X, y

def main() -> None:
    """
    主函数，运行多类别逻辑回归并打印结果
    """
    X, y = generate_data()
    clf = MulticlassLogisticRegression(n_iter=100, tol=1e-6)
    clf.fit(X, y)
    predictions = clf.predict(X)
    
    print("权重矩阵 W:")
    print(clf.W_)
    print("每次迭代的损失值:")
    print(clf.cost_)

if __name__ == "__main__":
    main()
</code></pre>
  </div>
</body>
</html>
  