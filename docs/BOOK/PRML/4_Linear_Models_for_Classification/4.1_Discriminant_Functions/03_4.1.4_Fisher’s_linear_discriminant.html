
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>4.1.4 Fisher’s linear discriminant</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>03_4.1.4_Fisher’s_linear_discriminant</h1>
<pre><code>Lecture: 4_Linear_Models_for_Classification/4.1_Discriminant_Functions
Content: 03_4.1.4_Fisher’s_linear_discriminant
</code></pre>
<h3>4.1.4 Fisher线性判别分析</h3>
<p>在《模式识别与机器学习》（PRML）一书的第4章中，Bishop博士详细介绍了线性分类模型的概念。第4.1节专注于判别函数，并在4.1.4节中讨论了Fisher线性判别分析（LDA）。以下是对4.1.4节内容的详细分析。</p>
<h4>Fisher线性判别分析的基本概念</h4>
<p>Fisher线性判别分析是一种将多维数据投影到一维空间的方法，以最大化类间的分离度并最小化类内的分散度。这种方法不仅用于分类任务，还用于降维任务。</p>
<p>考虑一个两类分类问题，其中有 $ N_1 $ 个点属于类 $ C_1 $，有 $ N_2 $ 个点属于类 $ C_2 $。这两类的均值向量分别为：</p>
<p>$$ \mathbf{m}<em>1 = \frac{1}{N_1} \sum</em>{n \in C_1} \mathbf{x}_n $$
$$ \mathbf{m}<em>2 = \frac{1}{N_2} \sum</em>{n \in C_2} \mathbf{x}_n $$</p>
<h4>Fisher准则</h4>
<p>Fisher提出了一种选择投影方向 $ \mathbf{w} $ 的准则，即最大化类间方差与类内方差之比。这一准则称为Fisher准则，其表达式为：</p>
<p>$$ J(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S}_B \mathbf{w}}{\mathbf{w}^T \mathbf{S}_W \mathbf{w}} $$</p>
<p>其中，$ \mathbf{S}_B $ 是类间散布矩阵，定义为：</p>
<p>$$ \mathbf{S}_B = (\mathbf{m}_2 - \mathbf{m}_1)(\mathbf{m}_2 - \mathbf{m}_1)^T $$</p>
<p>$ \mathbf{S}_W $ 是类内散布矩阵，定义为：</p>
<p>$$ \mathbf{S}<em>W = \sum</em>{n \in C_1} (\mathbf{x}_n - \mathbf{m}_1)(\mathbf{x}_n - \mathbf{m}<em>1)^T + \sum</em>{n \in C_2} (\mathbf{x}_n - \mathbf{m}_2)(\mathbf{x}_n - \mathbf{m}_2)^T $$</p>
<p>通过对 $ J(\mathbf{w}) $ 求导并设其为0，可以得到最优投影方向 $ \mathbf{w} $ 的解析解：</p>
<p>$$ \mathbf{w} \propto \mathbf{S}_W^{-1} (\mathbf{m}_2 - \mathbf{m}_1) $$</p>
<h4>Fisher判别函数</h4>
<p>虽然Fisher线性判别不是严格意义上的判别函数，而是用于将数据投影到一维空间的方向选择，但投影后的数据可以用于构建判别函数。投影后的数据 $ y $ 可以通过如下公式计算：</p>
<p>$$ y = \mathbf{w}^T \mathbf{x} $$</p>
<p>根据投影后的值，可以选择阈值 $ y_0 $，以便分类新数据点。如果 $ y(\mathbf{x}) \geq y_0 $，则将点 $ \mathbf{x} $ 分配到类 $ C_1 $；否则分配到类 $ C_2 $。</p>
<p>例如，可以使用高斯分布对类条件密度 $ p(y|C_k) $ 建模，然后通过最大似然法找到高斯分布的参数。根据投影类的高斯近似，可以找到最优阈值。</p>
<h3>结论</h3>
<p>在第4.1.4节中，Bishop博士详细阐述了Fisher线性判别分析。这种方法通过选择最佳投影方向来最大化类间分离度并最小化类内分散度。尽管Fisher线性判别分析在理论上有很强的吸引力，但在实际应用中需要结合其他方法进行优化。</p>

    <h3>Python 文件</h3>
    <pre><code># 03_4.1.4_Fisher’s_linear_discriminant

"""
Lecture: 4_Linear_Models_for_Classification/4.1_Discriminant_Functions
Content: 03_4.1.4_Fisher’s_linear_discriminant
"""

import numpy as np
from typing import Tuple

class FisherLDA:
    """
    Fisher 线性判别分析 (LDA) 分类器

    Parameters:
    -----------
    n_components : int
        需要保留的线性判别维度数量

    Attributes:
    -----------
    W_ : np.ndarray
        投影矩阵
    """
    def __init__(self, n_components: int = 1) -> None:
        self.n_components = n_components
        self.W_ = None

    def fit(self, X: np.ndarray, y: np.ndarray) -> None:
        """
        训练LDA分类器

        Parameters:
        -----------
        X : np.ndarray, shape = [n_samples, n_features]
            训练向量
        y : np.ndarray, shape = [n_samples]
            目标值
        """
        n_features = X.shape[1]
        class_labels = np.unique(y)

        # 计算类内散布矩阵
        S_W = np.zeros((n_features, n_features))
        mean_vectors = []
        for label in class_labels:
            X_c = X[y == label]
            mean_vec = np.mean(X_c, axis=0)
            mean_vectors.append(mean_vec)
            S_W += np.cov(X_c, rowvar=False) * (X_c.shape[0] - 1)

        # 计算总均值
        mean_overall = np.mean(X, axis=0)

        # 计算类间散布矩阵
        S_B = np.zeros((n_features, n_features))
        for i, mean_vec in enumerate(mean_vectors):
            n_c = X[y == class_labels[i]].shape[0]
            mean_diff = (mean_vec - mean_overall).reshape(n_features, 1)
            S_B += n_c * (mean_diff @ mean_diff.T)

        # 解决特征值问题
        A = np.linalg.inv(S_W) @ S_B
        eigenvalues, eigenvectors = np.linalg.eig(A)
        
        # 选择前k个最大的特征值对应的特征向量
        sorted_indices = np.argsort(eigenvalues)[::-1]
        self.W_ = eigenvectors[:, sorted_indices[:self.n_components]].real

    def transform(self, X: np.ndarray) -> np.ndarray:
        """
        将输入数据投影到LDA新空间

        Parameters:
        -----------
        X : np.ndarray, shape = [n_samples, n_features]
            输入向量

        Returns:
        --------
        np.ndarray
            投影后的数据
        """
        return X @ self.W_

def generate_data(n_samples: int = 100) -> Tuple[np.ndarray, np.ndarray]:
    """
    生成二分类数据集

    Parameters:
    -----------
    n_samples : int
        样本数量 (默认值为 100)
        
    Returns:
    --------
    Tuple[np.ndarray, np.ndarray]
        特征矩阵和目标值向量
    """
    np.random.seed(0)
    X1 = np.random.randn(n_samples // 2, 2) + np.array([2, 2])
    X2 = np.random.randn(n_samples // 2, 2) + np.array([-2, -2])
    X = np.vstack((X1, X2))
    y = np.hstack((np.zeros(n_samples // 2), np.ones(n_samples // 2)))
    return X, y

def main() -> None:
    """
    主函数，运行LDA并打印结果
    """
    X, y = generate_data()
    lda = FisherLDA(n_components=1)
    lda.fit(X, y)
    X_projected = lda.transform(X)
    
    print("投影矩阵 W:")
    print(lda.W_)
    print("投影后的数据形状:", X_projected.shape)

if __name__ == "__main__":
    main()
</code></pre>
  </div>
</body>
</html>
  