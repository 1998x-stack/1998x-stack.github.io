
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>3.5.2 Maximizing the evidence function</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>01_3.5.2_Maximizing_the_evidence_function</h1>
<pre><code>Lecture: 3_Linear_Models_for_Regression/3.5_The_Evidence_Approximation
Content: 01_3.5.2_Maximizing_the_evidence_function
</code></pre>
<h3>3.5.2 最大化证据函数</h3>
<p>在贝叶斯线性回归模型中，最大化证据函数可以帮助我们选择超参数 $ \alpha $ 和 $ \beta $，从而避免过拟合和欠拟合问题。证据函数的最大化提供了一种系统的方法来优化模型复杂度和数据拟合质量之间的平衡。</p>
<h3>证据函数的最大化</h3>
<p>证据函数的形式如下：</p>
<p>$$ p(t|\alpha, \beta) = \left( \frac{\beta}{2\pi} \right)^{N/2} \left( \frac{\alpha}{2\pi} \right)^{M/2} \int \exp{-E(\mathbf{w})} d\mathbf{w} $$</p>
<p>其中：</p>
<p>$$ E(\mathbf{w}) = \beta E_D(\mathbf{w}) + \alpha E_W(\mathbf{w}) $$</p>
<p>$$ E_D(\mathbf{w}) = \frac{1}{2} |\mathbf{t} - \Phi \mathbf{w}|^2 $$</p>
<p>$$ E_W(\mathbf{w}) = \frac{1}{2} \mathbf{w}^T \mathbf{w} $$</p>
<p>完成平方后，我们可以将证据函数写成对数形式：</p>
<p>$$ \ln p(t|\alpha, \beta) = \frac{M}{2} \ln \alpha + \frac{N}{2} \ln \beta - E(\mathbf{m}_N) - \frac{1}{2} \ln |\mathbf{A}| - \frac{N}{2} \ln (2\pi) $$</p>
<p>为了最大化证据函数，我们需要对 $ \alpha $ 和 $ \beta $ 进行优化。</p>
<h3>优化超参数 $ \alpha $</h3>
<p>首先考虑最大化 $ p(t|\alpha, \beta) $ 关于 $ \alpha $ 的部分。定义以下特征值方程：</p>
<p>$$ \left( \beta \Phi^T \Phi \right) \mathbf{u}_i = \lambda_i \mathbf{u}_i $$</p>
<p>从公式 (3.81) 可知，矩阵 $ \mathbf{A} $ 的特征值为 $ \alpha + \lambda_i $。现在考虑 $ \ln |\mathbf{A}| $ 关于 $ \alpha $ 的导数：</p>
<p>$$ \frac{d}{d\alpha} \ln |\mathbf{A}| = \frac{d}{d\alpha} \ln \prod_i (\lambda_i + \alpha) = \sum_i \frac{1}{\lambda_i + \alpha} $$</p>
<p>因此，关于 $ \alpha $ 的驻点满足：</p>
<p>$$ 0 = \frac{M}{2\alpha} - \frac{1}{2} \mathbf{m}_N^T \mathbf{m}_N - \frac{1}{2} \sum_i \frac{1}{\lambda_i + \alpha} $$</p>
<p>乘以 $ 2\alpha $ 并重新整理，我们得到：</p>
<p>$$ \alpha \mathbf{m}_N^T \mathbf{m}_N = M - \alpha \sum_i \frac{1}{\lambda_i + \alpha} = \gamma $$</p>
<p>其中 $ \gamma $ 定义为：</p>
<p>$$ \gamma = \sum_i \frac{\lambda_i}{\alpha + \lambda_i} $$</p>
<p>从 (3.90) 可以看出，最大化边缘似然的 $ \alpha $ 满足：</p>
<p>$$ \alpha = \frac{\gamma}{\mathbf{m}_N^T \mathbf{m}_N} $$</p>
<h3>优化超参数 $ \beta $</h3>
<p>类似地，我们考虑最大化 $ p(t|\alpha, \beta) $ 关于 $ \beta $ 的部分。定义误差函数：</p>
<p>$$ E_D(\mathbf{m}_N) = \frac{1}{2} |\mathbf{t} - \Phi \mathbf{m}_N|^2 $$</p>
<p>计算 $ \beta $ 的驻点满足：</p>
<p>$$ 0 = \frac{N}{2\beta} - \frac{1}{2} E_D(\mathbf{m}_N) $$</p>
<p>解得：</p>
<p>$$ \beta = \frac{N - \gamma}{2 E_D(\mathbf{m}_N)} $$</p>
<h3>详细推导与代码实现</h3>
<p>下面是一个示例代码，用于最大化证据函数并计算超参数 $ \alpha $ 和 $ \beta $：</p>
<pre><code class="language-python">import numpy as np
from scipy.linalg import inv

class BayesianLinearRegression:
    def __init__(self, alpha: float, beta: float):
        self.alpha = alpha
        self.beta = beta
        self.m_N = None
        self.S_N = None

    def fit(self, X: np.ndarray, t: np.ndarray):
        X = np.hstack([np.ones((X.shape[0], 1)), X])
        S_0_inv = self.alpha * np.eye(X.shape[1])
        self.S_N = inv(S_0_inv + self.beta * X.T @ X)
        self.m_N = self.beta * self.S_N @ X.T @ t

    def update_hyperparameters(self, X: np.ndarray, t: np.ndarray):
        X = np.hstack([np.ones((X.shape[0], 1)), X])
        eigvals = np.linalg.eigvalsh(self.beta * X.T @ X)
        gamma = np.sum(eigvals / (self.alpha + eigvals))
        
        self.alpha = gamma / np.sum(self.m_N**2)
        self.beta = (X.shape[0] - gamma) / np.sum((t - X @ self.m_N)**2)

    def evidence(self, X: np.ndarray, t: np.ndarray) -&gt; float:
        X = np.hstack([np.ones((X.shape[0], 1)), X])
        A = self.alpha * np.eye(X.shape[1]) + self.beta * X.T @ X
        E_mN = (self.beta / 2) * np.sum((t - X @ self.m_N)**2) + (self.alpha / 2) * np.sum(self.m_N**2)
        log_evidence = (X.shape[1] / 2) * np.log(self.alpha) + (X.shape[0] / 2) * np.log(self.beta) - E_mN - (1 / 2) * np.log(np.linalg.det(A)) - (X.shape[0] / 2) * np.log(2 * np.pi)
        return log_evidence

if __name__ == &quot;__main__&quot;:
    X_train = np.array([[0.1], [0.4], [0.7], [1.0]])
    t_train = np.array([1.1, 1.9, 3.0, 4.2])
    
    model = BayesianLinearRegression(alpha=1.0, beta=25.0)
    model.fit(X_train, t_train)
    evidence = model.evidence(X_train, t_train)
    print(&quot;初始证据函数对数值: &quot;, evidence)
    
    model.update_hyperparameters(X_train, t_train)
    print(&quot;更新后超参数 α: &quot;, model.alpha)
    print(&quot;更新后超参数 β: &quot;, model.beta)
    
    evidence = model.evidence(X_train, t_train)
    print(&quot;更新后证据函数对数值: &quot;, evidence)
</code></pre>
<h3>代码解释</h3>
<ol>
<li><strong>类定义</strong>:
<ul>
<li><code>BayesianLinearRegression</code> 类用于实现贝叶斯线性回归模型，并最大化证据函数。</li>
<li>初始化时需要指定先验分布的方差参数 <code>alpha</code> 和噪声精度参数 <code>beta</code>。</li>
</ul>
</li>
<li><strong>拟合模型</strong>:
<ul>
<li><code>fit</code> 方法用于拟合模型，计算后验分布的均值向量 <code>m_N</code> 和协方差矩阵 <code>S_N</code>。</li>
</ul>
</li>
<li><strong>更新超参数</strong>:
<ul>
<li><code>update_hyperparameters</code> 方法计算并更新超参数 <code>alpha</code> 和 <code>beta</code>。</li>
</ul>
</li>
<li><strong>证据函数</strong>:
<ul>
<li><code>evidence</code> 方法计算证据函数的对数值。</li>
</ul>
</li>
<li><strong>示例</strong>:
<ul>
<li>在 <code>__main__</code> 中，通过示例数据演示了模型的拟合、证据函数的计算和超参数的更新过程。</li>
</ul>
</li>
</ol>
<h3>检查代码逻辑</h3>
<ul>
<li>使用 <code>np.hstack</code> 添加偏置项，确保输入数据包含截距。</li>
<li>使用矩阵运算和线性代数库确保计算的准确性和高效性。</li>
<li>通过打印重要信息（如证据函数的对数值、超参数 α 和 β）来验证模型的正确性。</li>
</ul>

    <h3>Python 文件</h3>
    <pre><code># 01_3.5.2_Maximizing_the_evidence_function

"""
Lecture: 3_Linear_Models_for_Regression/3.5_The_Evidence_Approximation
Content: 01_3.5.2_Maximizing_the_evidence_function
"""

import numpy as np
from scipy.linalg import inv

class BayesianLinearRegression:
    def __init__(self, alpha: float, beta: float):
        self.alpha = alpha
        self.beta = beta
        self.m_N = None
        self.S_N = None

    def fit(self, X: np.ndarray, t: np.ndarray):
        X = np.hstack([np.ones((X.shape[0], 1)), X])
        S_0_inv = self.alpha * np.eye(X.shape[1])
        self.S_N = inv(S_0_inv + self.beta * X.T @ X)
        self.m_N = self.beta * self.S_N @ X.T @ t

    def update_hyperparameters(self, X: np.ndarray, t: np.ndarray):
        X = np.hstack([np.ones((X.shape[0], 1)), X])
        eigvals = np.linalg.eigvalsh(self.beta * X.T @ X)
        gamma = np.sum(eigvals / (self.alpha + eigvals))
        
        self.alpha = gamma / np.sum(self.m_N**2)
        self.beta = (X.shape[0] - gamma) / np.sum((t - X @ self.m_N)**2)

    def evidence(self, X: np.ndarray, t: np.ndarray) -> float:
        X = np.hstack([np.ones((X.shape[0], 1)), X])
        A = self.alpha * np.eye(X.shape[1]) + self.beta * X.T @ X
        E_mN = (self.beta / 2) * np.sum((t - X @ self.m_N)**2) + (self.alpha / 2) * np.sum(self.m_N**2)
        log_evidence = (X.shape[1] / 2) * np.log(self.alpha) + (X.shape[0] / 2) * np.log(self.beta) - E_mN - (1 / 2) * np.log(np.linalg.det(A)) - (X.shape[0] / 2) * np.log(2 * np.pi)
        return log_evidence

if __name__ == "__main__":
    X_train = np.array([[0.1], [0.4], [0.7], [1.0]])
    t_train = np.array([1.1, 1.9, 3.0, 4.2])
    
    model = BayesianLinearRegression(alpha=1.0, beta=25.0)
    model.fit(X_train, t_train)
    evidence = model.evidence(X_train, t_train)
    print("初始证据函数对数值: ", evidence)
    
    model.update_hyperparameters(X_train, t_train)
    print("更新后超参数 α: ", model.alpha)
    print("更新后超参数 β: ", model.beta)
    
    evidence = model.evidence(X_train, t_train)
    print("更新后证据函数对数值: ", evidence)
</code></pre>
  </div>
</body>
</html>
  