
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>2.5.2 Nearest neighbour methods</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>01_2.5.2_Nearest-neighbour_methods</h1>
<pre><code>Lecture: 2_Probability_Distributions/2.5_Nonparametric_Methods
Content: 01_2.5.2_Nearest-neighbour_methods
</code></pre>
<h3>PDF 探索和详细分析</h3>
<h4>最近邻方法（Nearest-Neighbour Methods）</h4>
<p>在2.5.2节中，讨论了最近邻方法的概念及其在概率密度估计和分类中的应用。最近邻方法是一种非参数方法，用于在数据空间中找到给定点的局部密度或进行分类。</p>
<h4>定义与背景</h4>
<ol>
<li>
<p><strong>最近邻方法的概念</strong>：
最近邻方法（Nearest-Neighbour Methods）通过计算数据空间中离目标点最近的K个点来估计局部密度或进行分类。它的基本思想是，通过这些邻近点的分布来推测目标点的特性。</p>
</li>
<li>
<p><strong>密度估计</strong>：
最近邻密度估计的基本公式为：
$$
p(x) = \frac{K}{NV}
$$
其中，$ K $ 是包含在体积 $ V $ 内的数据点数，$ N $ 是总样本数，$ V $ 是包含 $ K $ 个点的球体积。</p>
</li>
<li>
<p><strong>带宽选择</strong>：
与核密度估计中的固定带宽不同，最近邻方法根据数据点的分布动态调整带宽（体积）。在数据密集区域，较小的带宽可以避免过度平滑；在数据稀疏区域，较大的带宽可以减少噪声影响。</p>
</li>
<li>
<p><strong>K值选择</strong>：</p>
<ul>
<li>较小的 $ K $ 值会导致估计结果对噪声敏感，密度函数呈现“尖锐”特性。</li>
<li>较大的 $ K $ 值则会导致过度平滑，可能丢失数据的局部结构。</li>
<li>最优的 $ K $ 值通常介于两者之间，需要通过交叉验证等方法进行选择。</li>
</ul>
</li>
<li>
<p><strong>分类应用</strong>：
在分类任务中，K最近邻分类器（K-Nearest Neighbour, KNN）通过计算目标点的K个最近邻点，并根据这些邻点的类别来预测目标点的类别。具体步骤如下：</p>
<ul>
<li>对于每个测试点，找到训练数据集中离它最近的K个点。</li>
<li>根据这K个点的类别，使用投票法决定测试点的类别。</li>
<li>当K=1时，该方法称为最近邻规则。</li>
</ul>
</li>
</ol>
<h4>应用示例</h4>
<ol>
<li>
<p><strong>密度估计</strong>：
假设我们有一组数据，利用最近邻方法估计其概率密度。首先选择一个K值，然后计算每个点的局部密度，最后求和得到总体的概率密度估计。</p>
</li>
<li>
<p><strong>分类任务</strong>：
对于分类任务，利用KNN算法预测测试点的类别。通过选择合适的K值，可以在不做任何分布假设的情况下，实现对未知样本的分类。</p>
</li>
</ol>
<h4>优缺点</h4>
<ul>
<li><strong>优点</strong>：
<ul>
<li>简单直观，容易实现。</li>
<li>无需对数据进行参数化建模，可以处理各种形状的分布。</li>
</ul>
</li>
<li><strong>缺点</strong>：
<ul>
<li>计算复杂度较高，特别是在高维数据情况下。</li>
<li>需要存储所有的训练数据，对内存要求较大。</li>
</ul>
</li>
</ul>
<h3>结论</h3>
<p>最近邻方法是一种强大的非参数方法，可以在不做任何分布假设的情况下，对数据的概率密度进行估计或进行分类。它通过选择合适的K值，可以灵活地适应各种数据结构，为概率密度估计和分类提供了有效的工具。</p>

    <h3>Python 文件</h3>
    <pre><code># 01_2.5.2_Nearest-neighbour_methods

"""
Lecture: 2_Probability_Distributions/2.5_Nonparametric_Methods
Content: 01_2.5.2_Nearest-neighbour_methods
"""

import numpy as np
from scipy.spatial import distance
from typing import List, Tuple

class NearestNeighbour:
    def __init__(self, k: int = 1):
        """
        初始化最近邻类
        
        参数:
        k (int): 最近邻的数量
        """
        self.k = k

    def fit(self, data: np.ndarray, labels: np.ndarray) -> None:
        """
        拟合最近邻分类器
        
        参数:
        data (np.ndarray): 训练数据集
        labels (np.ndarray): 训练数据标签
        """
        self.data = data
        self.labels = labels

    def _find_neighbours(self, point: np.ndarray) -> List[int]:
        """
        找到给定点的最近邻
        
        参数:
        point (np.ndarray): 给定点
        
        返回:
        List[int]: 最近邻的索引列表
        """
        distances = distance.cdist([point], self.data, metric='euclidean').flatten()
        neighbour_indices = np.argsort(distances)[:self.k]
        return neighbour_indices

    def predict(self, points: np.ndarray) -> np.ndarray:
        """
        预测给定点的标签
        
        参数:
        points (np.ndarray): 测试数据集
        
        返回:
        np.ndarray: 预测标签
        """
        predictions = []
        for point in points:
            neighbour_indices = self._find_neighbours(point)
            neighbour_labels = self.labels[neighbour_indices]
            predicted_label = np.bincount(neighbour_labels).argmax()
            predictions.append(predicted_label)
        return np.array(predictions)

    def predict_proba(self, points: np.ndarray) -> np.ndarray:
        """
        预测给定点的标签概率
        
        参数:
        points (np.ndarray): 测试数据集
        
        返回:
        np.ndarray: 预测标签的概率
        """
        probabilities = []
        for point in points:
            neighbour_indices = self._find_neighbours(point)
            neighbour_labels = self.labels[neighbour_indices]
            counts = np.bincount(neighbour_labels, minlength=np.max(self.labels) + 1)
            probabilities.append(counts / self.k)
        return np.array(probabilities)

# 示例用法
if __name__ == "__main__":
    # 生成示例数据
    np.random.seed(0)
    data = np.random.randn(100, 2)
    labels = np.random.randint(0, 2, 100)
    
    # 创建最近邻分类器
    nn = NearestNeighbour(k=3)
    nn.fit(data, labels)
    
    # 预测测试点的标签
    test_points = np.random.randn(10, 2)
    predictions = nn.predict(test_points)
    probabilities = nn.predict_proba(test_points)
    
    print("预测标签:", predictions)
    print("预测标签的概率:", probabilities)</code></pre>
  </div>
</body>
</html>
  