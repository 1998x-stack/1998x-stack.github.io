
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>2.3.4 Imperfect Real Time Decisions</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>03_2.3.4_Imperfect_Real-Time_Decisions</h1>
<pre><code>
Lecture: 2_Problem-solving/2.3_Adversarial_Search
Content: 03_2.3.4_Imperfect_Real-Time_Decisions

</code></pre>
<h3>2.3.4 不完美实时决策</h3>
<p>在本节中，我们深入探讨了不完美信息环境下的实时决策问题。实时决策通常需要在有限的时间内做出，而不完美信息则意味着决策者无法完全掌握所有相关信息。在这种情况下，如何有效地进行决策成为一个重要的研究课题。以下是对这一章的详细分析：</p>
<h4>1. 引言</h4>
<p>在不完美信息和实时环境下，决策者需要在有限的时间内基于不完全的信息进行决策。这类问题在实际应用中非常常见，如动态市场中的交易决策、实时策略游戏中的指挥决策等。由于时间和信息的限制，传统的完全信息博弈理论在这些情况下显得力不从心。</p>
<h4>2. 实时决策的挑战</h4>
<p>实时决策面临以下主要挑战：</p>
<ul>
<li><strong>时间限制</strong>：决策必须在严格的时间约束内完成，无法进行深度和全面的分析。</li>
<li><strong>信息不完备</strong>：决策者只能基于部分可观察到的信息做出判断，可能会忽略重要的隐性信息。</li>
<li><strong>动态变化</strong>：环境可能随时变化，决策需要快速响应和调整。</li>
</ul>
<h4>3. 剪枝技术与启发式搜索</h4>
<p>为了应对上述挑战，剪枝技术和启发式搜索方法被广泛应用于实时决策中。常用的方法包括：</p>
<ul>
<li><strong>启发式搜索（Heuristic Search）</strong>：利用启发函数快速评估当前状态，指导搜索过程，减少搜索空间。</li>
<li><strong>剪枝（Pruning）</strong>：通过剪去不必要的分支，减少计算量，提高搜索效率。</li>
</ul>
<h4>4. 蒙特卡罗树搜索（Monte Carlo Tree Search, MCTS）</h4>
<p>蒙特卡罗树搜索是一种适用于实时决策的强大算法，特别在不完美信息环境中表现出色。其基本原理是通过随机模拟未来可能的状态，并基于模拟结果进行决策。MCTS 主要包含以下四个步骤：</p>
<ul>
<li><strong>选择（Selection）</strong>：从根节点开始，根据特定的策略选择子节点，直到到达叶节点。</li>
<li><strong>扩展（Expansion）</strong>：如果叶节点不是终局状态，则生成一个或多个子节点。</li>
<li><strong>模拟（Simulation）</strong>：从新扩展的节点开始，进行随机模拟，直至到达终局状态。</li>
<li><strong>回溯（Backpropagation）</strong>：将模拟结果沿着路径回溯，更新所有经过节点的价值评估。</li>
</ul>
<p>通过反复执行上述步骤，MCTS 能够在有限时间内逐渐逼近最优决策。</p>
<h4>5. 实时策略调整</h4>
<p>在实时决策中，策略调整和适应性尤为重要。决策者需要根据最新的观测信息和环境变化，动态调整策略。这种自适应能力可以通过以下方法实现：</p>
<ul>
<li><strong>在线学习（Online Learning）</strong>：通过不断学习和更新决策模型，适应环境变化。</li>
<li><strong>反馈控制（Feedback Control）</strong>：利用反馈机制，实时调整决策参数和策略。</li>
</ul>
<h4>6. 实际应用中的实例</h4>
<p>不完美实时决策在许多实际应用中具有重要意义。例如：</p>
<ul>
<li><strong>自动驾驶</strong>：在不完美信息下，自动驾驶系统需要实时决策，以确保行车安全和效率。</li>
<li><strong>金融交易</strong>：交易者在快速变化的市场环境中，需要基于部分信息做出交易决策，优化收益。</li>
<li><strong>智能机器人</strong>：在复杂和动态的环境中，机器人需要实时调整其行动策略，以完成任务。</li>
</ul>
<h4>7. 总结</h4>
<p>不完美实时决策是一个复杂而充满挑战的研究领域。通过剪枝技术、启发式搜索、蒙特卡罗树搜索等方法，研究者能够在有限时间内找到高质量的决策方案。此外，实时策略调整和在线学习等方法的结合，使得决策系统能够在动态环境中表现出色。尽管面临诸多挑战，不完美实时决策在实际应用中展现出巨大的潜力，为解决复杂的现实问题提供了有效的工具。</p>
<p>通过深入理解和应用这些决策方法，研究者和工程师可以开发出更加智能和高效的系统，以应对不完美信息和实时环境下的各种挑战。</p>

    <h3>Python 文件</h3>
    <pre><code># 03_2.3.4_Imperfect_Real-Time_Decisions

"""

Lecture: 2_Problem-solving/2.3_Adversarial_Search
Content: 03_2.3.4_Imperfect_Real-Time_Decisions

"""

import numpy as np
from typing import Any, List, Tuple, Optional

class GameState:
    """
    游戏状态类，用于表示博弈中的一个状态。
    """
    def __init__(self, board: np.ndarray, player: int):
        """
        初始化游戏状态。

        参数:
        - board (np.ndarray): 当前的棋盘状态。
        - player (int): 当前玩家（1或-1）。
        """
        self.board = board
        self.player = player

    def get_legal_actions(self) -> List[Tuple[int, int]]:
        """
        获取当前状态下的所有合法动作。

        返回:
        - List[Tuple[int, int]]: 合法动作的列表。
        """
        raise NotImplementedError("子类应实现 get_legal_actions 方法。")

    def apply_action(self, action: Tuple[int, int]) -> 'GameState':
        """
        在当前状态下应用一个动作，返回新的游戏状态。

        参数:
        - action (Tuple[int, int]): 要应用的动作。

        返回:
        - GameState: 应用动作后的新游戏状态。
        """
        raise NotImplementedError("子类应实现 apply_action 方法。")

    def is_terminal(self) -> bool:
        """
        判断当前状态是否为终局状态。

        返回:
        - bool: 如果是终局状态，返回 True；否则返回 False。
        """
        raise NotImplementedError("子类应实现 is_terminal 方法。")

    def get_reward(self) -> int:
        """
        获取当前状态的奖励值（仅对终局状态调用）。

        返回:
        - int: 当前状态的奖励值。
        """
        raise NotImplementedError("子类应实现 get_reward 方法。")


class MonteCarloTreeSearchNode:
    """
    蒙特卡罗树搜索节点类。
    """
    def __init__(self, state: GameState, parent: Optional['MonteCarloTreeSearchNode'] = None):
        """
        初始化蒙特卡罗树搜索节点。

        参数:
        - state (GameState): 当前节点的游戏状态。
        - parent (Optional[MonteCarloTreeSearchNode]): 父节点。
        """
        self.state = state
        self.parent = parent
        self.children = []
        self.visits = 0
        self.reward = 0

    def is_fully_expanded(self) -> bool:
        """
        判断节点是否已经完全扩展。

        返回:
        - bool: 如果节点已经完全扩展，返回 True；否则返回 False。
        """
        return len(self.children) == len(self.state.get_legal_actions())

    def best_child(self, exploration_weight: float = 1.0) -> 'MonteCarloTreeSearchNode':
        """
        获取当前节点的最佳子节点。

        参数:
        - exploration_weight (float): 探索权重。

        返回:
        - MonteCarloTreeSearchNode: 最佳子节点。
        """
        choices_weights = [
            (child.reward / child.visits) + exploration_weight * np.sqrt((2 * np.log(self.visits) / child.visits))
            for child in self.children
        ]
        return self.children[np.argmax(choices_weights)]

    def expand(self) -> 'MonteCarloTreeSearchNode':
        """
        扩展当前节点的一个子节点。

        返回:
        - MonteCarloTreeSearchNode: 新扩展的子节点。
        """
        actions = self.state.get_legal_actions()
        for action in actions:
            if action not in [child.state for child in self.children]:
                new_state = self.state.apply_action(action)
                child_node = MonteCarloTreeSearchNode(new_state, self)
                self.children.append(child_node)
                return child_node
        raise Exception("无法扩展节点。")

    def rollout(self) -> int:
        """
        从当前节点进行随机模拟，直到到达终局状态。

        返回:
        - int: 模拟结果的奖励值。
        """
        current_state = self.state
        while not current_state.is_terminal():
            action = np.random.choice(current_state.get_legal_actions())
            current_state = current_state.apply_action(action)
        return current_state.get_reward()

    def backpropagate(self, reward: int):
        """
        进行回溯，更新当前节点及其所有祖先节点的访问次数和奖励值。

        参数:
        - reward (int): 模拟结果的奖励值。
        """
        self.visits += 1
        self.reward += reward
        if self.parent:
            self.parent.backpropagate(reward)


class MonteCarloTreeSearch:
    """
    蒙特卡罗树搜索算法类。
    """
    def __init__(self, state: GameState):
        """
        初始化蒙特卡罗树搜索算法。

        参数:
        - state (GameState): 初始游戏状态。
        """
        self.root = MonteCarloTreeSearchNode(state)

    def best_action(self, simulations_number: int) -> GameState:
        """
        执行给定次数的模拟，返回最佳动作。

        参数:
        - simulations_number (int): 模拟次数。

        返回:
        - GameState: 最佳动作后的新游戏状态。
        """
        for _ in range(simulations_number):
            node = self._tree_policy()
            reward = node.rollout()
            node.backpropagate(reward)
        return self.root.best_child(exploration_weight=0).state

    def _tree_policy(self) -> MonteCarloTreeSearchNode:
        """
        选择树中需要扩展的节点。

        返回:
        - MonteCarloTreeSearchNode: 需要扩展的节点。
        """
        node = self.root
        while not node.state.is_terminal():
            if not node.is_fully_expanded():
                return node.expand()
            else:
                node = node.best_child()
        return node

# 示例游戏状态类（以井字棋为例）
class TicTacToeState(GameState):
    """
    井字棋游戏状态类。
    """
    def get_legal_actions(self) -> List[Tuple[int, int]]:
        return [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == 0]

    def apply_action(self, action: Tuple[int, int]) -> 'TicTacToeState':
        new_board = np.copy(self.board)
        new_board[action] = self.player
        return TicTacToeState(new_board, -self.player)

    def is_terminal(self) -> bool:
        for i in range(3):
            if abs(np.sum(self.board[i, :])) == 3 or abs(np.sum(self.board[:, i])) == 3:
                return True
        if abs(np.sum(self.board.diagonal())) == 3 or abs(np.sum(np.fliplr(self.board).diagonal())) == 3:
            return True
        if not np.any(self.board == 0):
            return True
        return False

    def get_reward(self) -> int:
        for i in range(3):
            if np.sum(self.board[i, :]) == 3 or np.sum(self.board[:, i]) == 3:
                return 1
            if np.sum(self.board[i, :]) == -3 or np.sum(self.board[:, i]) == -3:
                return -1
        if np.sum(self.board.diagonal()) == 3 or np.sum(np.fliplr(self.board).diagonal()) == 3:
            return 1
        if np.sum(self.board.diagonal()) == -3 or np.sum(np.fliplr(self.board).diagonal()) == -3:
            return -1
        return 0

# 示例用法：
if __name__ == "__main__":
    # 初始化井字棋初始状态
    initial_board = np.zeros((3, 3), dtype=int)
    initial_state = TicTacToeState(initial_board, 1)

    # 创建蒙特卡罗树搜索算法实例
    mcts = MonteCarloTreeSearch(initial_state)

    # 执行蒙特卡罗树搜索，找到最佳动作
    best_state = mcts.best_action(1000)
    print(f"最佳状态: \n{best_state.board}")
</code></pre>
  </div>
</body>
</html>
  