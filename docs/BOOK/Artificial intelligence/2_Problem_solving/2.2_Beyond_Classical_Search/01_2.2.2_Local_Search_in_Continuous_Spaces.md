# 01_2.2.2_Local_Search_in_Continuous_Spaces

"""

Lecture: 2_Problem-solving/2.2_Beyond_Classical_Search
Content: 01_2.2.2_Local_Search_in_Continuous_Spaces

"""

### 本地搜索算法在连续空间中的应用

在第4章中，我们探讨了超越经典搜索的方法，特别是本地搜索算法在连续空间中的应用。本节内容详细介绍了这些算法，包括梯度上升、模拟退火、牛顿-拉夫森法等。

#### 梯度上升法

梯度上升法（Steepest-Ascent Hill Climbing）是本地搜索算法中最基础的一种。其基本思想是通过沿着目标函数梯度的方向不断更新当前状态，从而找到函数的局部最大值。

**公式：**
$$ x \leftarrow x + \alpha \nabla f(x) $$
其中，$ \alpha $ 是一个小常数，表示步长。

**调整步长 $ \alpha $ 的方法：**
- **步长太小**：需要太多步骤才能达到最优。
- **步长太大**：可能会越过最优点。

为了克服这个问题，通常使用线搜索（Line Search）技术。线搜索通过不断增加 $ \alpha $ 的值直到目标函数开始减小，此时的 $ \alpha $ 值即为最优步长。

#### 模拟退火

模拟退火（Simulated Annealing）是一种基于统计物理的随机优化算法，通过模拟物理退火过程来避免陷入局部最优。

**模拟退火算法步骤：**
1. 初始化温度 $ T $。
2. 从当前状态出发，随机选择一个邻近状态。
3. 计算能量变化 $ \Delta E $。
4. 如果 $ \Delta E > 0 $，接受新状态；否则，以概率 $ \exp(\Delta E / T) $ 接受新状态。
5. 降低温度 $ T $ 并重复上述步骤，直到系统达到平衡或温度降到零。

模拟退火的优势在于它能够有效地跳出局部最优，通过逐渐降低温度来探索更广阔的解空间。

#### 牛顿-拉夫森法

牛顿-拉夫森法（Newton-Raphson Method）是一种基于导数的优化算法，通过迭代求解方程的根来找到目标函数的最优值。

**牛顿-拉夫森更新公式：**
$$ x \leftarrow x - \frac{g(x)}{g'(x)} $$
其中，$ g(x) $ 是目标函数的导数。

在优化问题中，我们需要找到目标函数的梯度为零的点，即：
$$ \nabla f(x) = 0 $$
更新公式可以写成矩阵形式：
$$ x \leftarrow x - H_f(x)^{-1} \nabla f(x) $$
其中，$ H_f(x) $ 是目标函数的Hessian矩阵，其元素为二阶导数。

牛顿-拉夫森法通过利用梯度和Hessian矩阵的信息，可以快速收敛到目标函数的局部最优。然而，对于高维问题，计算Hessian矩阵的代价较高，因此常常使用近似方法。

#### 连续空间中的优化问题

在连续空间中，本地搜索算法同样面临局部最优、山脊和高原问题。随机重启和模拟退火是常用的克服这些问题的方法。连续高维空间中搜索变得更加困难，因为空间大且容易迷失方向。

**约束优化：**
约束优化问题是指解必须满足某些硬性约束条件，例如机场选址问题中，机场必须位于某个区域内。这类问题的难度取决于约束条件和目标函数的性质。

**线性规划：**
线性规划问题是最常见的约束优化问题之一，其约束必须是线性不等式，目标函数也是线性的。线性规划问题的时间复杂度是多项式级别的，是目前研究最广泛、应用最广的优化问题类型之一。

综上所述，本地搜索算法在连续空间中的应用非常广泛，从梯度上升、模拟退火到牛顿-拉夫森法，每种算法都有其独特的优势和适用场景。在实际应用中，需要根据具体问题选择合适的算法，以达到最优的解决方案 。