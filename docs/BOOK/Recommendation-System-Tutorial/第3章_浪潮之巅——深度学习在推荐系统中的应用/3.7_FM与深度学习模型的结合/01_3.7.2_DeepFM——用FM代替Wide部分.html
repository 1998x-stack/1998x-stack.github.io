
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>3.7.2 DeepFM——用FM代替Wide部分</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>01_3.7.2 DeepFM——用FM代替Wide部分</h1>
<pre><code>Lecture: 第3章 浪潮之巅——深度学习在推荐系统中的应用/3.7 FM与深度学习模型的结合
Content: 01_3.7.2 DeepFM——用FM代替Wide部分
</code></pre>
<h3>3.7.2 DeepFM——用FM代替Wide部分</h3>
<h4>1. 背景和动机</h4>
<p>在推荐系统中，Wide&amp;Deep模型通过结合浅层的线性模型（Wide部分）和深层的非线性模型（Deep部分），在特征组合和泛化能力之间取得了平衡。然而，Wide部分虽然能够有效地处理大规模的稀疏特征，但其缺乏自动特征组合的能力，这在一定程度上限制了模型的表达能力。为了解决这个问题，DeepFM模型应运而生。DeepFM模型将因子分解机（FM）与深度神经网络结合，通过用FM替代Wide部分，增强了特征交互能力。</p>
<h4>2. DeepFM模型概述</h4>
<p>DeepFM模型由哈尔滨工业大学和华为公司联合提出，其模型结构如图3-19所示。DeepFM对Wide&amp;Deep模型的主要改进在于用FM替换了原来的Wide部分，从而加强了浅层网络部分特征组合的能力。具体而言，DeepFM模型的左边部分是FM模型，右边部分是深度神经网络（DNN），两部分共享相同的Embedding层。</p>
<h4>3. FM模型与Deep部分的结合</h4>
<p>在DeepFM模型中，FM部分和Deep部分的输入都是特征的Embedding向量。FM部分通过对不同特征域的Embedding向量进行两两交叉，捕捉到特征之间的二阶交互信息。具体地，FM部分将Embedding向量作为原FM模型中的隐向量，对特征进行交叉计算。然后，将FM的输出与DNN的输出一起输入到最后的输出层，进行目标拟合。</p>
<h4>4. DeepFM模型的创新点</h4>
<p>与Wide&amp;Deep模型相比，DeepFM模型的主要创新点在于以下几个方面：</p>
<ol>
<li><strong>增强的特征交互能力</strong>：通过用FM替代Wide部分，DeepFM模型能够自动进行特征组合，增强了浅层网络的特征交互能力。</li>
<li><strong>共享Embedding层</strong>：FM部分和DNN部分共享相同的Embedding层，这不仅减少了参数数量，还提高了特征表示的一致性。</li>
<li><strong>联合训练</strong>：DeepFM模型通过联合训练FM部分和DNN部分，能够更好地优化特征表示和特征交互。</li>
</ol>
<h4>5. DeepFM模型的优势</h4>
<p>DeepFM模型在推荐系统中展现了以下优势：</p>
<ol>
<li><strong>自动特征组合</strong>：相比于Wide部分需要手动设计特征组合，FM部分能够自动进行特征交互，捕捉高阶特征组合信息。</li>
<li><strong>训练效率高</strong>：共享Embedding层减少了参数数量，同时联合训练的方式提高了模型的训练效率。</li>
<li><strong>泛化能力强</strong>：结合FM和DNN的优势，DeepFM模型在特征表达和泛化能力之间取得了平衡，具有较强的适应性。</li>
</ol>
<h4>6. 实验结果与分析</h4>
<p>DeepFM模型在多个推荐任务中表现优异，通过实验结果验证了其在点击率预测、评分预测等任务中的效果。具体而言，DeepFM模型在特征组合和泛化能力上优于Wide&amp;Deep模型，同时在训练效率和预测性能上也有显著提升。</p>
<h4>7. 总结</h4>
<p>DeepFM模型通过将FM模型与深度神经网络结合，解决了Wide&amp;Deep模型在特征组合上的不足。其创新点在于用FM替代Wide部分，增强了特征交互能力，同时通过共享Embedding层和联合训练，提高了模型的训练效率和泛化能力。在推荐系统中，DeepFM模型提供了一种有效的解决方案，在特征表示和特征交互上展现了强大的性能优势。</p>

    <h3>Python 文件</h3>
    <pre><code># 01_3.7.2 DeepFM——用FM代替Wide部分

"""
Lecture: 第3章 浪潮之巅——深度学习在推荐系统中的应用/3.7 FM与深度学习模型的结合
Content: 01_3.7.2 DeepFM——用FM代替Wide部分
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

class FM(nn.Module):
    def __init__(self, num_features, k):
        super(FM, self).__init__()
        self.num_features = num_features
        self.k = k
        self.linear = nn.Linear(num_features, 1)
        self.v = nn.Parameter(torch.randn(num_features, k))

    def forward(self, x):
        linear_part = self.linear(x)
        interactions_part_1 = torch.pow(torch.matmul(x, self.v), 2)
        interactions_part_2 = torch.matmul(torch.pow(x, 2), torch.pow(self.v, 2))
        interactions_part = 0.5 * torch.sum(interactions_part_1 - interactions_part_2, dim=1, keepdim=True)
        output = linear_part + interactions_part
        return output

class DeepFM(nn.Module):
    def __init__(self, num_features, k, hidden_dims):
        super(DeepFM, self).__init__()
        self.num_features = num_features
        self.k = k

        # FM部分
        self.fm_linear = nn.Linear(num_features, 1)
        self.fm_v = nn.Parameter(torch.randn(num_features, k))

        # DNN部分
        self.embeddings = nn.Embedding(num_features, k)
        self.dnn_input_dim = num_features * k
        layers = []
        input_dim = self.dnn_input_dim
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(input_dim, hidden_dim))
            layers.append(nn.ReLU())
            input_dim = hidden_dim
        layers.append(nn.Linear(input_dim, 1))
        self.dnn = nn.Sequential(*layers)

    def forward(self, x):
        # FM部分
        fm_linear_part = self.fm_linear(x)
        fm_interactions_part_1 = torch.pow(torch.matmul(x, self.fm_v), 2)
        fm_interactions_part_2 = torch.matmul(torch.pow(x, 2), torch.pow(self.fm_v, 2))
        fm_interactions_part = 0.5 * torch.sum(fm_interactions_part_1 - fm_interactions_part_2, dim=1, keepdim=True)
        fm_output = fm_linear_part + fm_interactions_part

        # DNN部分
        x_embed = self.embeddings(x.long()).view(-1, self.dnn_input_dim)
        dnn_output = self.dnn(x_embed)

        # 输出
        output = fm_output + dnn_output
        return output

# 数据准备
num_features = 100000
k = 32
hidden_dims = [64, 32]
batch_size = 64
num_epochs = 10

# 生成示例数据
X = torch.randint(0, num_features, (batch_size, num_features)).float()
y = torch.randn(batch_size, 1)

# 初始化模型
model = DeepFM(num_features, k, hidden_dims)
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

# 训练模型
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X)
    loss = criterion(outputs, y)
    loss.backward()
    optimizer.step()
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")</code></pre>
  </div>
</body>
</html>
  