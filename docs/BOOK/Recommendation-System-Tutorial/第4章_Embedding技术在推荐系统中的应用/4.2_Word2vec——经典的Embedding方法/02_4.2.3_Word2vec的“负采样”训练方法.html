
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>4.2.3 Word2vec的“负采样”训练方法</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>02_4.2.3 Word2vec的“负采样”训练方法</h1>
<pre><code>Lecture: 第4章 Embedding技术在推荐系统中的应用/4.2 Word2vec——经典的Embedding方法
Content: 02_4.2.3 Word2vec的“负采样”训练方法
</code></pre>
<h3>4.2.3 Word2vec的“负采样”训练方法</h3>
<h4>一、背景与基本概念</h4>
<p>Word2vec模型在自然语言处理领域取得了广泛应用，它通过将词语映射到低维稠密向量空间，使得语义相近的词在向量空间中的距离较近。然而，Word2vec模型在实际训练过程中面临着计算复杂度高的问题。为了解决这个问题，负采样（Negative Sampling）技术应运而生。</p>
<h4>二、负采样的基本原理</h4>
<p>在Word2vec模型中，假设语料库中的词的数量为10000，这意味着在每次迭代更新隐层到输出层神经元的权重时，都需要计算所有字典中的所有10000个词的预测误差。这种巨大的计算量在实际训练过程中几乎无法承受。为了减轻训练负担，负采样方法通过只计算采样出的少量负样本的预测误差，减少计算量。</p>
<h4>三、负采样的具体实现</h4>
<ol>
<li>
<p><strong>多分类问题的简化</strong>：</p>
<ul>
<li>在原始Word2vec模型中，计算条件概率时需要对所有词进行softmax计算，这相当于一个多分类问题。负采样通过简化为近似的二分类问题，极大地降低了计算复杂度。</li>
</ul>
</li>
<li>
<p><strong>优化目标函数</strong>：</p>
<ul>
<li>负采样方法将原始的优化目标函数转化为如下形式：
$$
L = \log{\sigma(v_{w_O}^T \cdot h)} + \sum_{i=1}^{k}E_{w_i \sim P_n(w)}[\log{\sigma(-v_{w_i}^T \cdot h)}]
$$
其中，$h$为隐层向量，$v_{w_O}$为正样本的输出词向量，$w_i$为负样本，$P_n(w)$为负样本的分布。</li>
</ul>
</li>
<li>
<p><strong>负样本的选择</strong>：</p>
<ul>
<li>负采样的关键在于如何选择负样本。通常情况下，负样本是从整个词汇表中随机选择的，但选择的概率可以按照词频分布进行调整，以提高训练效果。</li>
</ul>
</li>
<li>
<p><strong>计算复杂度的降低</strong>：</p>
<ul>
<li>通过负采样，每次迭代只需计算少量负样本的预测误差，而不是整个词汇表的预测误差。这样，计算复杂度可以显著降低。例如，在每轮梯度下降迭代中，计算复杂度至少可以缩小为原来的1/1000（假设词汇表大小为10000）。</li>
</ul>
</li>
</ol>
<h4>四、负采样的应用案例</h4>
<ol>
<li>
<p><strong>YouTube推荐系统</strong>：</p>
<ul>
<li>YouTube在视频推荐系统中使用了负采样技术，以减少每次预测的分类数量，从而加快模型的收敛速度。具体方法在Word2vec的基础上进行了改进。</li>
</ul>
</li>
<li>
<p><strong>Airbnb的房源推荐</strong>：</p>
<ul>
<li>Airbnb在房源推荐中通过负采样技术，将用户点击序列中的房源作为正样本，从语料库中随机选取房源作为负样本，生成用户和房源的Embedding向量，推荐符合用户兴趣的房源。</li>
</ul>
</li>
</ol>
<h4>五、负采样的优势和局限性</h4>
<ol>
<li>
<p><strong>优势</strong>：</p>
<ul>
<li><strong>计算效率高</strong>：通过减少需要计算的样本数量，负采样大大提高了训练效率。</li>
<li><strong>实现简单</strong>：相对于层级softmax（Hierarchical Softmax），负采样的实现更为简单，且效果显著。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>采样偏差</strong>：负采样过程中，如果负样本选择不当，可能会引入采样偏差，影响模型的训练效果。</li>
<li><strong>超参数选择</strong>：负采样的效果对负样本数量等超参数较为敏感，需要仔细调试以达到最佳效果。</li>
</ul>
</li>
</ol>
<h4>六、负采样在实际应用中的优化</h4>
<ol>
<li><strong>动态采样策略</strong>：
<ul>
<li>根据词频动态调整负样本的选择概率，提高模型的训练效果。</li>
</ul>
</li>
<li><strong>并行计算</strong>：
<ul>
<li>利用多线程或分布式计算框架，进一步提高负采样训练的效率。</li>
</ul>
</li>
</ol>
<h3>总结</h3>
<p>负采样作为Word2vec模型的一种重要训练方法，通过简化计算过程，大大提高了模型的训练效率。它的引入不仅解决了计算复杂度高的问题，还在多个实际应用中取得了显著效果。掌握负采样的基本原理和实现方法，对于深入理解和应用Word2vec模型具有重要意义。</p>

    <h3>Python 文件</h3>
    <pre><code># 02_4.2.3 Word2vec的“负采样”训练方法

"""
Lecture: 第4章 Embedding技术在推荐系统中的应用/4.2 Word2vec——经典的Embedding方法
Content: 02_4.2.3 Word2vec的“负采样”训练方法
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import Counter
from typing import List, Tuple

class Word2VecDataset(torch.utils.data.Dataset):
    """
    Word2Vec 数据集类，用于生成 Skip-Gram 训练样本。
    
    Attributes:
        data: 语料库列表
        word2idx: 词到索引的映射
        idx2word: 索引到词的映射
        pairs: Skip-Gram 样本对
        vocab_size: 词汇表大小
    """
    def __init__(self, corpus: List[str], window_size: int = 2) -> None:
        self.data = corpus
        self.window_size = window_size
        self.word2idx, self.idx2word = self._build_vocab(self.data)
        self.pairs = self._generate_pairs(self.data, self.window_size)
        self.vocab_size = len(self.word2idx)
        
    def __len__(self) -> int:
        return len(self.pairs)
    
    def __getitem__(self, idx: int) -> Tuple[int, int]:
        return self.pairs[idx]
    
    def _build_vocab(self, corpus: List[str]) -> Tuple[dict, dict]:
        """
        构建词汇表和索引映射。
        
        Args:
            corpus: 语料库列表
        
        Returns:
            word2idx: 词到索引的映射
            idx2word: 索引到词的映射
        """
        word_counts = Counter(corpus)
        idx2word = [word for word, _ in word_counts.items()]
        word2idx = {word: idx for idx, word in enumerate(idx2word)}
        return word2idx, idx2word
    
    def _generate_pairs(self, corpus: List[str], window_size: int) -> List[Tuple[int, int]]:
        """
        生成 Skip-Gram 样本对。
        
        Args:
            corpus: 语料库列表
            window_size: 窗口大小
        
        Returns:
            pairs: Skip-Gram 样本对列表
        """
        pairs = []
        for i, word in enumerate(corpus):
            for j in range(max(0, i - window_size), min(len(corpus), i + window_size + 1)):
                if i != j:
                    pairs.append((self.word2idx[word], self.word2idx[corpus[j]]))
        return pairs


class Word2VecModel(nn.Module):
    """
    Word2Vec 模型类，使用 Skip-Gram 结构和负采样训练方法。
    
    Attributes:
        embedding_dim: 嵌入向量维度
        vocab_size: 词汇表大小
        embeddings: 嵌入层
    """
    def __init__(self, vocab_size: int, embedding_dim: int) -> None:
        super(Word2VecModel, self).__init__()
        self.embedding_dim = embedding_dim
        self.vocab_size = vocab_size
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        
    def forward(self, pos_words: torch.Tensor, neg_words: torch.Tensor) -> torch.Tensor:
        """
        前向传播计算。
        
        Args:
            pos_words: 正样本词索引张量
            neg_words: 负样本词索引张量
        
        Returns:
            loss: 训练损失
        """
        pos_embedding = self.embeddings(pos_words)
        neg_embedding = self.embeddings(neg_words)
        
        pos_loss = -torch.log(torch.sigmoid(torch.sum(pos_embedding, dim=1)))
        neg_loss = -torch.log(torch.sigmoid(-torch.sum(neg_embedding, dim=1)))
        
        loss = torch.mean(pos_loss + neg_loss)
        return loss


def train_word2vec(corpus: List[str], embedding_dim: int = 100, window_size: int = 2, num_epochs: int = 5, batch_size: int = 64, neg_samples: int = 10) -> Word2VecModel:
    """
    训练 Word2Vec 模型。
    
    Args:
        corpus: 语料库列表
        embedding_dim: 嵌入向量维度
        window_size: 窗口大小
        num_epochs: 训练轮数
        batch_size: 批量大小
        neg_samples: 负样本数量
    
    Returns:
        model: 训练好的 Word2Vec 模型
    """
    dataset = Word2VecDataset(corpus, window_size)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    model = Word2VecModel(dataset.vocab_size, embedding_dim)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    for epoch in range(num_epochs):
        total_loss = 0
        for pos_words, context_words in dataloader:
            neg_words = torch.randint(0, dataset.vocab_size, (batch_size, neg_samples))
            
            optimizer.zero_grad()
            loss = model(pos_words, neg_words)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        print(f'Epoch: {epoch+1}, Loss: {total_loss / len(dataloader)}')
    
    return model


if __name__ == "__main__":
    corpus = ["我", "爱", "自然", "语言", "处理", "和", "深度", "学习", "自然", "语言", "处理", "非常", "有趣"]
    model = train_word2vec(corpus, embedding_dim=50, window_size=2, num_epochs=10, batch_size=4, neg_samples=5)

    # 保存模型
    torch.save(model.state_dict(), "word2vec_model.pth")
    
    # 打印一些嵌入向量示例
    word_indices = [0, 1, 2, 3, 4]  # 示例单词索引
    embeddings = model.embeddings(torch.tensor(word_indices))
    for idx, embedding in zip(word_indices, embeddings):
        print(f"Word: {corpus[idx]}, Embedding: {embedding.detach().numpy()}")</code></pre>
  </div>
</body>
</html>
  