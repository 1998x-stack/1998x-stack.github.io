
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>3.2.3 Singular Value Decomposition</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>02_3.2.3_Singular_Value_Decomposition</h1>
<pre><code>Lecture: 3_Linear_Least_Squares_Problems/3.2_Matrix_Factorizations_That_Solve_the_Linear_Least_Squares_Problem
Content: 02_3.2.3_Singular_Value_Decomposition
</code></pre>
<h3>奇异值分解 (Singular Value Decomposition, SVD) 极其详细分析</h3>
<h4>基本概念</h4>
<p>奇异值分解 (SVD) 是矩阵分解中非常重要的一种方法，它可以将一个任意的 $ m \times n $ 矩阵 $ A $ 分解为三个矩阵的乘积，即：</p>
<p>$$ A = U \Sigma V^T $$</p>
<p>其中：</p>
<ul>
<li>$ U $ 是 $ m \times m $ 的正交矩阵，其列称为左奇异向量。</li>
<li>$ \Sigma $ 是 $ m \times n $ 的对角矩阵，其对角线上的元素称为奇异值，按降序排列。</li>
<li>$ V $ 是 $ n \times n $ 的正交矩阵，其列称为右奇异向量。</li>
</ul>
<h4>数学描述</h4>
<p>假设 $ A $ 是一个 $ m \times n $ 的矩阵，我们可以找到 $ U $、$ \Sigma $ 和 $ V $ 满足：</p>
<p>$$ A = U \Sigma V^T $$</p>
<p>其中：</p>
<ul>
<li>$ U $ 的列向量是 $ A A^T $ 的特征向量。</li>
<li>$ V $ 的列向量是 $ A^T A $ 的特征向量。</li>
<li>$ \Sigma $ 的对角线元素是 $ A $ 的非负奇异值，是 $ A A^T $ 或 $ A^T A $ 的非负平方根。</li>
</ul>
<h4>详细推导过程</h4>
<ol>
<li>
<p><strong>特征值与特征向量</strong>：</p>
<ul>
<li>对于矩阵 $ A $，我们首先计算 $ A A^T $ 和 $ A^T A $ 的特征值和特征向量。</li>
<li>设 $ \lambda_i $ 是 $ A^T A $ 的特征值，$ v_i $ 是对应的特征向量，则 $ A A^T $ 的特征值也为 $ \lambda_i $，对应的特征向量为 $ A v_i $。</li>
</ul>
</li>
<li>
<p><strong>构造矩阵 $ U $ 和 $ V $</strong>：</p>
<ul>
<li>令 $ v_i $ 为 $ A^T A $ 的特征向量，且满足 $ A^T A v_i = \lambda_i v_i $。</li>
<li>构造 $ V $ 矩阵：$ V $ 的列向量即为这些特征向量 $ v_i $。</li>
<li>构造 $ U $ 矩阵：$ U $ 的列向量为 $ u_i = \frac{A v_i}{\sqrt{\lambda_i}} $。</li>
</ul>
</li>
<li>
<p><strong>构造矩阵 $ \Sigma $</strong>：</p>
<ul>
<li>$ \Sigma $ 为对角矩阵，其对角线元素为 $ \sigma_i = \sqrt{\lambda_i} $，这些 $ \sigma_i $ 称为奇异值。</li>
</ul>
</li>
<li>
<p><strong>验证分解结果</strong>：</p>
<ul>
<li>将 $ A $ 分解为 $ A = U \Sigma V^T $，验证左右两边是否相等，以保证分解的正确性。</li>
</ul>
</li>
</ol>
<h4>几何解释</h4>
<p>SVD 从几何上将矩阵 $ A $ 视作一个从 $ R^n $ 到 $ R^m $ 的线性映射。SVD 的核心思想是通过选择适当的正交坐标系，将矩阵 $ A $ 映射到一个对角矩阵 $ \Sigma $。在这个新的坐标系下，矩阵 $ A $ 的映射变得更加简洁明了。</p>
<h4>数值稳定性</h4>
<p>SVD 的数值稳定性来源于以下几个方面：</p>
<ol>
<li><strong>正交性</strong>：矩阵 $ U $ 和 $ V $ 都是正交矩阵，计算时不会放大误差。</li>
<li><strong>奇异值排序</strong>：奇异值按照降序排列，计算过程中有利于误差的控制。</li>
<li><strong>算法稳定性</strong>：计算 SVD 的算法（如 Golub-Kahan 双对角化方法）设计上非常稳定，适合处理大规模稠密矩阵。</li>
</ol>
<h4>比较 Householder 反射和 Givens 旋转</h4>
<table>
<thead>
<tr>
<th>方法</th>
<th>优点</th>
<th>缺点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>Householder 反射</td>
<td>数值稳定性好，适用于大规模稠密矩阵</td>
<td>计算复杂度较高，难以并行化</td>
<td>需要高精度的数值计算，特别是稠密矩阵</td>
</tr>
<tr>
<td>Givens 旋转</td>
<td>适用于稀疏矩阵，容易并行化</td>
<td>数值稳定性较差，误差累积问题明显</td>
<td>稀疏矩阵或逐元素处理的情况</td>
</tr>
</tbody>
</table>
<h4>数值稳定性分析</h4>
<ul>
<li>
<p><strong>Householder 反射的数值稳定性</strong>：</p>
<ul>
<li>由于 Householder 反射矩阵 $ P $ 是正交的，进行矩阵乘法时不会放大条件数，误差均匀分布，具有良好的数值稳定性。</li>
<li>适合处理高精度数值计算，特别是大规模稠密矩阵。</li>
</ul>
</li>
<li>
<p><strong>Givens 旋转的数值稳定性</strong>：</p>
<ul>
<li>逐元素消去，局部操作有利于控制误差，但累计误差问题较为明显。</li>
<li>适合处理稀疏矩阵，但在处理密集矩阵时，数值稳定性稍逊于 Householder 反射。</li>
</ul>
</li>
</ul>
<p>总结来看，SVD 通过将矩阵分解为三个正交矩阵的乘积，有效地提高了数值计算的稳定性，适用于多种线性代数问题。Householder 反射和 Givens 旋转各有优劣，选择时应根据具体应用场景和数值稳定性要求进行合理选取。</p>

    <h3>Python 文件</h3>
    <pre><code># 02_3.2.3_Singular_Value_Decomposition

"""
Lecture: 3_Linear_Least_Squares_Problems/3.2_Matrix_Factorizations_That_Solve_the_Linear_Least_Squares_Problem
Content: 02_3.2.3_Singular_Value_Decomposition
"""

import numpy as np

def compute_svd(A):
    """
    从头开始计算矩阵 A 的奇异值分解 (SVD)
    
    输入:
        A - 输入矩阵, 大小为 (m, n)
    输出:
        U - 左奇异向量矩阵, 大小为 (m, m)
        Sigma - 奇异值对角矩阵, 大小为 (m, n)
        Vt - 右奇异向量矩阵的转置, 大小为 (n, n)
    """
    # 计算 A^T A 的特征值和特征向量
    AtA = np.dot(A.T, A)
    eigenvalues, V = np.linalg.eigh(AtA)
    
    # 将特征值按降序排列
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[idx]
    V = V[:, idx]
    
    # 计算奇异值
    singular_values = np.sqrt(eigenvalues)
    
    # 构造奇异值对角矩阵 Sigma
    m, n = A.shape
    Sigma = np.zeros((m, n))
    np.fill_diagonal(Sigma, singular_values)
    
    # 计算左奇异向量矩阵 U
    U = np.zeros((m, m))
    for i in range(n):
        U[:, i] = np.dot(A, V[:, i]) / singular_values[i]
    
    # 如果 m > n，填充 U 的剩余列
    if m > n:
        for i in range(n, m):
            # 选择一个正交于前 n 个列的随机向量
            u = np.random.rand(m)
            for j in range(i):
                u -= np.dot(U[:, j], u) * U[:, j]
            U[:, i] = u / np.linalg.norm(u)
    
    return U, Sigma, V.T

# 测试函数
def test_compute_svd():
    A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=float)  # 示例矩阵
    U, Sigma, Vt = compute_svd(A)

    # 打印结果
    print('U:')
    print(U)
    print('Sigma:')
    print(Sigma)
    print('Vt:')
    print(Vt)
    print('A:')
    print(A)
    print('U @ Sigma @ Vt:')
    print(U @ Sigma @ Vt)

# 运行测试函数
test_compute_svd()
</code></pre>
  </div>
</body>
</html>
  