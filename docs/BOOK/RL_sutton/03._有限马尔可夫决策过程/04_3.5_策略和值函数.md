# 04_3.5_策略和值函数

"""
Lecture: /03._有限马尔可夫决策过程
Content: 04_3.5_策略和值函数
"""

## 3.5 策略和值函数

### 引言

在强化学习中，策略和值函数是两个核心概念。策略定义了代理在每个状态下选择动作的规则，而值函数则用于评估状态或状态-动作对的长期收益。这两个概念在有限马尔可夫决策过程（MDP）中起着至关重要的作用。本节将详细探讨策略和值函数的定义、作用及其在强化学习中的应用。

### 策略（Policy）

#### 定义
策略 $\pi$ 是指代理在每个状态 $s$ 下选择动作 $a$ 的概率分布。具体来说，策略可以表示为：

$$ \pi(a \mid s) = P(A_t = a \mid S_t = s) $$

其中，$\pi(a \mid s)$ 表示在状态 $s$ 下选择动作 $a$ 的概率。

#### 分类
策略可以分为确定性策略和随机性策略：
- **确定性策略**：在每个状态下，总是选择同一个动作，即 $\pi(s) = a$。
- **随机性策略**：在每个状态下，以一定的概率分布选择动作，即 $\pi(a \mid s)$。

#### 作用
策略是强化学习算法的核心，决定了代理在不同状态下的行为选择。通过优化策略，代理可以最大化其长期累积奖励。

### 值函数（Value Function）

#### 定义
值函数用于评估某一状态或状态-动作对的长期收益。值函数主要包括状态值函数 $V(s)$ 和行动值函数 $Q(s, a)$：

- **状态值函数 $V(s)$**：表示在状态 $s$ 下，遵循策略 $\pi$ 时的预期总回报。
  $$ V^{\pi}(s) = \mathbb{E}^{\pi}[G_t \mid S_t = s] $$
  
- **行动值函数 $Q(s, a)$**：表示在状态 $s$ 选择动作 $a$ 后，遵循策略 $\pi$ 时的预期总回报。
  $$ Q^{\pi}(s, a) = \mathbb{E}^{\pi}[G_t \mid S_t = s, A_t = a] $$

#### 计算
值函数的计算依赖于贝尔曼方程。对于状态值函数 $V(s)$，贝尔曼方程为：

$$ V^{\pi}(s) = \sum_{a} \pi(a \mid s) \sum_{s'} P(s' \mid s, a) \left[ R(s, a, s') + \gamma V^{\pi}(s') \right] $$

对于行动值函数 $Q(s, a)$，贝尔曼方程为：

$$ Q^{\pi}(s, a) = \sum_{s'} P(s' \mid s, a) \left[ R(s, a, s') + \gamma \sum_{a'} \pi(a' \mid s') Q^{\pi}(s', a') \right] $$

#### 作用
值函数用于评估代理的长期收益，是策略优化的基础。通过估计值函数，代理可以比较不同状态和动作的收益，从而优化其策略。

### 策略优化

#### 策略迭代
策略迭代是一种通过交替执行策略评估和策略改进来优化策略的方法。具体步骤如下：
1. **策略评估**：根据当前策略 $\pi$ 计算值函数 $V^{\pi}$。
2. **策略改进**：根据值函数 $V^{\pi}$ 更新策略，使其在每个状态下选择的动作最大化预期回报。

#### 值迭代
值迭代是一种直接通过更新值函数来优化策略的方法。具体步骤如下：
1. 初始化值函数 $V(s)$。
2. 迭代更新值函数，直到收敛：
   $$ V(s) \leftarrow \max_{a} \sum_{s'} P(s' \mid s, a) \left[ R(s, a, s') + \gamma V(s') \right] $$
3. 根据最终的值函数 $V(s)$ 确定最优策略。

### 应用实例

#### 游戏
在游戏中，策略和值函数可以用于设计智能AI，使其能够根据游戏状态选择最优动作，提高游戏胜率。

#### 自动驾驶
在自动驾驶任务中，策略和值函数可以用于优化车辆的驾驶策略，提高行驶安全性和效率。

#### 工业控制
在工业控制任务中，策略和值函数可以用于设计优化控制策略，提高生产效率和产品质量。

### 结论

策略和值函数是强化学习中的两个核心概念。策略定义了代理在每个状态下的行为选择规则，而值函数用于评估状态或状态-动作对的长期收益。通过理解和优化策略和值函数，代理可以在复杂环境中实现最优决策。本节提供了对策略和值函数的详细分析，为读者理解和应用强化学习算法提供了理论基础和实践指导。