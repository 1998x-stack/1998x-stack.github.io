
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>5.6 增量实现</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>05_5.6_增量实现</h1>
<pre><code>Lecture: /05._蒙特卡罗方法
Content: 05_5.6_增量实现
</code></pre>
<h3>05_5.6 增量实现</h3>
<h4>增量实现简介</h4>
<p>增量实现（Incremental Implementation）是一种通过逐步更新值函数的方法，适用于蒙特卡罗预测和控制。在每个序列结束后，利用新获得的返回值逐步调整估计值，而不是等到所有数据收集完毕后再进行批量更新。增量方法具有高效、节省内存等优点，尤其适用于大规模数据和在线学习场景。</p>
<h4>增量实现的核心概念</h4>
<ol>
<li>
<p><strong>增量更新</strong>：</p>
<ul>
<li>在每次序列结束后，利用新返回值 $ G $ 更新状态值函数或动作值函数。</li>
<li>采用增量公式逐步调整估计值，而无需存储和重新计算所有历史数据。</li>
</ul>
</li>
<li>
<p><strong>普通重要性采样</strong>：</p>
<ul>
<li>返回值 $ G_t $ 通过重要性采样比率 $ \rho_{t:T} $ 进行调整，然后进行简单平均。</li>
<li>增量公式如下：
$$
Q_{n+1}(s, a) = Q_n(s, a) + \frac{1}{n} \left(G_t - Q_n(s, a)\right)
$$</li>
</ul>
</li>
<li>
<p><strong>加权重要性采样</strong>：</p>
<ul>
<li>加权重要性采样通过对比率进行归一化处理，减少方差。</li>
<li>增量公式如下：
$$
Q_{n+1}(s, a) = Q_n(s, a) + \frac{W_n}{C_n} \left(G_t - Q_n(s, a)\right)
$$</li>
<li>其中，$ W_n $ 为当前返回值的权重，$ C_n $ 为累计权重。</li>
</ul>
</li>
</ol>
<h4>增量实现的步骤</h4>
<ol>
<li>
<p><strong>初始化</strong>：</p>
<ul>
<li>初始化状态值函数 $ V(s) $ 或动作值函数 $ Q(s, a) $。</li>
<li>初始化累计权重和返回值列表。</li>
</ul>
</li>
<li>
<p><strong>生成序列</strong>：</p>
<ul>
<li>使用行为策略生成一个完整的序列，记录状态、动作和相应的奖励。</li>
</ul>
</li>
<li>
<p><strong>计算重要性采样比率</strong>：</p>
<ul>
<li>对于每一个状态-动作对，计算重要性采样比率 $ \rho_{t:T} $，该比率为目标策略和行为策略在当前序列下的概率之比。</li>
</ul>
</li>
<li>
<p><strong>更新值函数</strong>：</p>
<ul>
<li>根据计算得到的比率和返回值，使用普通重要性采样或加权重要性采样的方法更新状态值函数或动作值函数。</li>
</ul>
</li>
<li>
<p><strong>重复</strong>：</p>
<ul>
<li>反复进行生成序列、计算比率和更新值函数，直到值函数收敛。</li>
</ul>
</li>
</ol>
<h4>增量实现的优缺点</h4>
<p><strong>优点</strong>：</p>
<ol>
<li><strong>高效</strong>：增量实现每次只需处理一个新序列，节省内存和计算资源。</li>
<li><strong>适用于在线学习</strong>：能够实时更新估计值，适用于在线学习和大规模数据。</li>
</ol>
<p><strong>缺点</strong>：</p>
<ol>
<li><strong>高方差</strong>：普通重要性采样可能具有高方差，导致收敛速度较慢。</li>
<li><strong>偏差问题</strong>：加权重要性采样在统计意义上是有偏的，但偏差随着样本数量的增加逐渐减小。</li>
</ol>
<h4>实例应用</h4>
<p><strong>示例：二十一点（Blackjack）</strong>：</p>
<ul>
<li>在二十一点游戏中，玩家的目标是在不超过21点的情况下，尽量接近21点。通过增量实现，可以在使用随机策略（行为策略）的同时评估特定策略（目标策略）的效果。实验结果表明，加权重要性采样在估计游戏状态值时，比普通重要性采样具有更低的方差，更加稳定。</li>
</ul>
<h4>结论</h4>
<p>增量实现通过逐步更新估计值的方法，能够有效地处理大规模数据和在线学习问题。普通重要性采样和加权重要性采样分别在无偏性和方差控制上提供了不同的优势。加权重要性采样由于其更低的方差，在实际应用中更受青睐。</p>
<h3>总结</h3>
<p>增量实现通过重要性采样方法，能够在不改变当前行为策略的情况下，有效评估和学习目标策略。通过调整返回值的期望，普通重要性采样提供了无偏的估计，而加权重要性采样通过减少方差提供了更稳定的估计。增量实现的高效性和适应性，使其在强化学习中具有广泛的应用前景。</p>

    <h3>Python 文件</h3>
    <pre><code># 05_5.6_增量实现

"""
Lecture: /05._蒙特卡罗方法
Content: 05_5.6_增量实现
"""

import numpy as np
from typing import Dict, Tuple, List

class IncrementalMonteCarlo:
    """
    增量实现的蒙特卡罗方法类，包含用于求解马尔可夫决策过程（MDP）的策略评估和改进算法。

    Attributes:
        states: 状态集合
        actions: 动作集合
        transition_probabilities: 状态转移概率矩阵
        rewards: 奖励矩阵
        gamma: 折扣因子
        epsilon: 探索率
    """

    def __init__(self, states: List[int], actions: List[int],
                 transition_probabilities: Dict[Tuple[int, int, int], float],
                 rewards: Dict[Tuple[int, int, int], float], gamma: float, epsilon: float):
        """
        初始化增量实现的蒙特卡罗方法类。

        参数:
            states: 状态集合
            actions: 动作集合
            transition_probabilities: 状态转移概率矩阵
            rewards: 奖励矩阵
            gamma: 折扣因子
            epsilon: 探索率
        """
        self.states = states
        self.actions = actions
        self.transition_probabilities = transition_probabilities
        self.rewards = rewards
        self.gamma = gamma
        self.epsilon = epsilon
        self.q_value = {(s, a): 0 for s in states for a in actions}
        self.cumulative_weights = {(s, a): 0 for s in states for a in actions}
        self.policy = {s: np.random.choice(actions) for s in states}

    def generate_episode(self) -> List[Tuple[int, int, float]]:
        """
        生成一个遵循当前策略的完整序列。

        返回:
            List[Tuple[int, int, float]]: 序列列表，包含状态、动作和奖励
        """
        episode = []
        state = np.random.choice(self.states)
        while True:
            action = self.policy[state] if np.random.rand() > self.epsilon else np.random.choice(self.actions)
            next_state = np.random.choice(self.states, p=[self.transition_probabilities[(state, action, s)] for s in self.states])
            reward = self.rewards[(state, action, next_state)]
            episode.append((state, action, reward))
            if next_state == None:  # 假设终止状态用 None 表示
                break
            state = next_state
        return episode

    def update_q_value(self, episode: List[Tuple[int, int, float]]) -> None:
        """
        使用生成的序列更新动作值函数。

        参数:
            episode: 序列列表，包含状态、动作和奖励
        """
        G = 0
        W = 1
        for state, action, reward in reversed(episode):
            G = self.gamma * G + reward
            self.cumulative_weights[(state, action)] += W
            self.q_value[(state, action)] += (W / self.cumulative_weights[(state, action)]) * (G - self.q_value[(state, action)])
            if action != self.policy[state]:
                break
            W *= 1.0 / (1e-10 + self.epsilon / len(self.actions) + (1 - self.epsilon) * int(action == np.argmax([self.q_value[(state, a)] for a in self.actions])))

    def policy_improvement(self) -> None:
        """
        根据当前的动作值函数改进策略。
        """
        for state in self.states:
            action_values = {a: self.q_value[(state, a)] for a in self.actions}
            self.policy[state] = max(action_values, key=action_values.get)

    def incremental_monte_carlo(self, num_episodes: int) -> None:
        """
        增量蒙特卡罗控制算法，通过策略评估和策略改进找到最优策略。

        参数:
            num_episodes: 运行的序列数量
        """
        for _ in range(num_episodes):
            episode = self.generate_episode()
            self.update_q_value(episode)
            self.policy_improvement()

# 测试代码
if __name__ == "__main__":
    # 定义状态、动作、转移概率和奖励
    states = [0, 1, 2, 3]
    actions = [0, 1]
    transition_probabilities = {
        (0, 0, 0): 0.7, (0, 0, 1): 0.3,
        (0, 1, 0): 0.4, (0, 1, 1): 0.6,
        (1, 0, 2): 0.8, (1, 0, 3): 0.2,
        (1, 1, 2): 0.5, (1, 1, 3): 0.5,
        (2, 0, 0): 0.9, (2, 0, 1): 0.1,
        (2, 1, 0): 0.2, (2, 1, 1): 0.8,
        (3, 0, 2): 0.6, (3, 0, 3): 0.4,
        (3, 1, 2): 0.3, (3, 1, 3): 0.7,
    }
    rewards = {
        (0, 0, 0): 5, (0, 0, 1): 10,
        (0, 1, 0): 2, (0, 1, 1): 7,
        (1, 0, 2): 15, (1, 0, 3): 20,
        (1, 1, 2): 9, (1, 1, 3): 13,
        (2, 0, 0): 6, (2, 0, 1): 8,
        (2, 1, 0): 3, (2, 1, 1): 4,
        (3, 0, 2): 12, (3, 0, 3): 18,
        (3, 1, 2): 14, (3, 1, 3): 11,
    }
    gamma = 0.9
    epsilon = 0.1
    num_episodes = 1000

    mc_control = IncrementalMonteCarlo(states, actions, transition_probabilities, rewards, gamma, epsilon)
    mc_control.incremental_monte_carlo(num_episodes)

    print("最终动作值函数:", mc_control.q_value)
    print("最终策略:", mc_control.policy)
</code></pre>
  </div>
</body>
</html>
  