# 01_5.2_动作值的蒙特卡罗估计

"""
Lecture: /05._蒙特卡罗方法
Content: 01_5.2_动作值的蒙特卡罗估计
"""

### 01_5.2 动作值的蒙特卡罗估计

#### 动作值的蒙特卡罗估计简介

在强化学习中，当环境模型不可用时，估计动作值（即状态-动作对的值）特别有用。蒙特卡罗方法通过样本序列对状态值和动作值进行估计，这种方法不需要环境的完全模型，只需要从环境中采样得到的序列数据。对于动作值的估计，我们主要目标是估计 $ q_*(s, a) $，即最优动作值函数。

#### 动作值的蒙特卡罗估计的核心概念

1. **策略评估问题**：
   - 策略评估问题是估计 $ q_\pi(s, a) $，即从状态 $ s $ 开始，采取动作 $ a $，然后遵循策略 $ \pi $ 的预期回报。
   - 蒙特卡罗方法通过访问状态-动作对来估计其值。每次访问蒙特卡罗方法估计状态-动作对的值为所有访问后的回报的平均值。首次访问蒙特卡罗方法仅计算每个序列中首次访问状态-动作对后的回报平均值。这些方法随着访问次数趋于无穷大而收敛到真实的期望值。

2. **保持探索**：
   - 需要确保所有动作都有被选择的可能性。如果策略是确定性的，那么只会观察到从每个状态中选择的一个动作的回报。因此，为了比较不同的动作，需要估计每个状态下所有动作的值。

3. **探索起点假设**：
   - 为了保证所有状态-动作对被访问到，可以通过指定序列从某个状态-动作对开始，并确保每个状态-动作对都有非零概率被选为起点。这被称为探索起点假设。

#### 动作值的蒙特卡罗方法的步骤

1. **策略评估**：
   - 策略评估的目标是计算给定策略 $\pi$ 下的动作值函数 $q_\pi(s, a)$。通过对从状态 $s$ 采取动作 $a$ 开始的回报进行平均，估计其值。
   - 每次访问蒙特卡罗方法和首次访问蒙特卡罗方法均可用于动作值的估计。通过对每次访问或首次访问的回报进行平均，逐渐逼近真实值。

2. **策略改进**：
   - 在策略评估的基础上，利用新的动作值函数改进策略，使其在每个状态选择使期望回报最大的动作。

3. **策略控制**：
   - 动作值的蒙特卡罗控制结合了策略评估和策略改进，通过反复迭代这两个过程，直到找到最优策略。

#### 蒙特卡罗方法的应用案例

**示例: 二十一点（Blackjack）**：
- 在二十一点游戏中，玩家的目标是在不超过21点的情况下，尽量接近21点。蒙特卡罗方法可以用于估计玩家在不同策略下的动作值函数。例如，假设一个策略是玩家在手牌总数为20或21时停牌，否则继续要牌。通过模拟大量的二十一点游戏，可以计算出在该策略下各个状态的期望回报。

#### 蒙特卡罗方法的优缺点

**优点**：
1. **不需要模型**：蒙特卡罗方法只需要从环境中采样得到的序列数据，不需要对环境的完全了解。
2. **简单易行**：通过对回报的简单平均来估计值函数，理论上简单易行。
3. **适用于大规模问题**：特别适合用于估计特定状态-动作对的值函数，而不需要计算所有状态的值函数。

**缺点**：
1. **高方差**：蒙特卡罗方法的估计具有高方差，收敛速度可能较慢。
2. **探索问题**：需要确保所有动作都有被选择的可能性，否则无法估计某些动作的值。

#### 结论

动作值的蒙特卡罗方法通过对从环境中采样得到的序列数据进行平均，能够有效地估计状态-动作对的值函数。其无需环境模型、适用于大规模问题的特点，使其在实践中具有广泛的应用前景。通过结合策略评估和策略改进，蒙特卡罗方法能够逐步逼近最优策略，解决复杂的决策问题。

### 总结

动作值的蒙特卡罗方法通过对采样数据的平均来估计状态-动作对的值，适用于不完全了解环境的情况。首次访问和每次访问蒙特卡罗方法都能收敛到真实值，尽管每次访问方法的实现更复杂但收敛更快。应用案例如二十一点游戏展示了其在实际问题中的有效性。总的来说，动作值的蒙特卡罗方法在解决强化学习问题中具有重要作用和广泛应用前景。