
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>5.5 离策略预测通过重要性采样</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>04_5.5_离策略预测通过重要性采样</h1>
<pre><code>Lecture: /05._蒙特卡罗方法
Content: 04_5.5_离策略预测通过重要性采样
</code></pre>
<h3>04_5.5 离策略预测通过重要性采样</h3>
<h4>离策略预测简介</h4>
<p>离策略预测（Off-policy Prediction）是一种在行为策略（behavior policy）与目标策略（target policy）不相同的情况下估计目标策略值函数的方法。其主要目的是在不改变当前策略的情况下，通过从其他策略生成的数据来学习和评估目标策略。为实现这一点，通常使用重要性采样（Importance Sampling）技术来调整回报的期望值，使其反映目标策略的期望。</p>
<h4>重要性采样的核心概念</h4>
<ol>
<li>
<p><strong>重要性采样</strong>：</p>
<ul>
<li>重要性采样是一种调整样本权重以估计目标分布期望值的技术。具体而言，使用行为策略生成的回报通过乘以重要性采样比率来调整，从而得到目标策略下的期望回报。</li>
</ul>
</li>
<li>
<p><strong>普通重要性采样</strong>：</p>
<ul>
<li>在普通重要性采样中，返回值 $ G_t $ 通过重要性采样比率 $ \rho_{t:T-1} $ 进行调整，然后对所有返回值进行简单平均。公式如下：
$$
V(s) = \frac{1}{|T(s)|} \sum_{t \in T(s)} \rho_{t:T-1} G_t
$$</li>
<li>这种方法在理论上是无偏的，但由于比率的方差可能非常大，实际应用中会导致较高的方差。</li>
</ul>
</li>
<li>
<p><strong>加权重要性采样</strong>：</p>
<ul>
<li>加权重要性采样使用加权平均的方法，通过对比率进行归一化处理来减少方差。公式如下：
$$
V(s) = \frac{\sum_{t \in T(s)} \rho_{t:T-1} G_t}{\sum_{t \in T(s)} \rho_{t:T-1}}
$$</li>
<li>虽然这种方法在统计意义上是有偏的，但其偏差随着样本数量的增加逐渐减小，且在实践中方差明显较低，更加稳定。</li>
</ul>
</li>
</ol>
<h4>离策略预测的重要性采样算法</h4>
<p>离策略预测的重要性采样算法可以分为以下几个步骤：</p>
<ol>
<li>
<p><strong>初始化</strong>：</p>
<ul>
<li>随机初始化状态值函数 $ V(s) $ 或动作值函数 $ Q(s, a) $。</li>
<li>初始化累计权重和返回值列表。</li>
</ul>
</li>
<li>
<p><strong>生成序列</strong>：</p>
<ul>
<li>使用行为策略生成一个完整的序列，记录状态、动作和相应的奖励。</li>
</ul>
</li>
<li>
<p><strong>计算重要性采样比率</strong>：</p>
<ul>
<li>对于每一个状态-动作对，计算重要性采样比率 $ \rho_{t:T-1} $，该比率为目标策略和行为策略在当前序列下的概率之比。</li>
</ul>
</li>
<li>
<p><strong>更新值函数</strong>：</p>
<ul>
<li>根据计算得到的比率和返回值，使用普通重要性采样或加权重要性采样的方法更新状态值函数或动作值函数。</li>
</ul>
</li>
<li>
<p><strong>重复</strong>：</p>
<ul>
<li>反复进行生成序列、计算比率和更新值函数，直到值函数收敛。</li>
</ul>
</li>
</ol>
<h4>重要性采样的实际应用</h4>
<p><strong>示例：二十一点（Blackjack）</strong>：</p>
<ul>
<li>在二十一点游戏中，玩家的目标是在不超过21点的情况下，尽量接近21点。通过离策略预测，可以在使用随机策略（行为策略）的同时评估特定策略（目标策略）的效果。实验结果表明，加权重要性采样在估计游戏状态值时，比普通重要性采样具有更低的方差，更加稳定。</li>
</ul>
<h4>重要性采样的优缺点</h4>
<p><strong>优点</strong>：</p>
<ol>
<li><strong>无偏性</strong>：普通重要性采样在理论上是无偏的，即其期望值等于目标策略的真实值。</li>
<li><strong>减少方差</strong>：加权重要性采样通过归一化处理有效减少了方差，实际应用中更加稳定。</li>
</ol>
<p><strong>缺点</strong>：</p>
<ol>
<li><strong>高方差</strong>：普通重要性采样的方差可能非常高，导致收敛速度较慢。</li>
<li><strong>偏差问题</strong>：加权重要性采样在统计意义上是有偏的，但偏差随着样本数量的增加逐渐减小。</li>
</ol>
<h4>结论</h4>
<p>离策略预测通过重要性采样方法在不改变当前行为策略的情况下，有效评估和学习目标策略。通过调整返回值的期望，普通重要性采样和加权重要性采样分别在无偏性和方差控制上提供了不同的优势。加权重要性采样由于其更低的方差，在实际应用中更受青睐。</p>
<h3>总结</h3>
<p>离策略预测通过重要性采样方法能够有效解决行为策略与目标策略不同时的值函数估计问题。通过调整返回值的期望，普通重要性采样提供了无偏的估计，而加权重要性采样通过减少方差提供了更稳定的估计。应用如二十一点游戏展示了其在实际问题中的有效性。总的来说，离策略预测通过重要性采样在解决强化学习问题中具有重要作用和广泛应用前景。</p>

    <h3>Python 文件</h3>
    <pre><code># 04_5.5_离策略预测通过重要性采样

"""
Lecture: /05._蒙特卡罗方法
Content: 04_5.5_离策略预测通过重要性采样
"""

import numpy as np
from typing import Dict, Tuple, List

class OffPolicyPredictionIS:
    """
    离策略预测通过重要性采样类，包含用于求解马尔可夫决策过程（MDP）的策略评估算法。

    Attributes:
        states: 状态集合
        actions: 动作集合
        transition_probabilities: 状态转移概率矩阵
        rewards: 奖励矩阵
        gamma: 折扣因子
        behavior_policy: 行为策略
        target_policy: 目标策略
    """

    def __init__(self, states: List[int], actions: List[int],
                 transition_probabilities: Dict[Tuple[int, int, int], float],
                 rewards: Dict[Tuple[int, int, int], float], gamma: float,
                 behavior_policy: Dict[int, Dict[int, float]],
                 target_policy: Dict[int, Dict[int, float]]):
        """
        初始化离策略预测通过重要性采样类。

        参数:
            states: 状态集合
            actions: 动作集合
            transition_probabilities: 状态转移概率矩阵
            rewards: 奖励矩阵
            gamma: 折扣因子
            behavior_policy: 行为策略
            target_policy: 目标策略
        """
        self.states = states
        self.actions = actions
        self.transition_probabilities = transition_probabilities
        self.rewards = rewards
        self.gamma = gamma
        self.behavior_policy = behavior_policy
        self.target_policy = target_policy
        self.q_value = {(s, a): 0 for s in states for a in actions}
        self.cumulative_weights = {(s, a): 0 for s in states for a in actions}

    def generate_episode(self) -> List[Tuple[int, int, float]]:
        """
        生成一个遵循行为策略的完整序列。

        返回:
            List[Tuple[int, int, float]]: 序列列表，包含状态、动作和奖励
        """
        episode = []
        state = np.random.choice(self.states)
        while True:
            action = np.random.choice(self.actions, p=[self.behavior_policy[state][a] for a in self.actions])
            next_state = np.random.choice(self.states, p=[self.transition_probabilities[(state, action, s)] for s in self.states])
            reward = self.rewards[(state, action, next_state)]
            episode.append((state, action, reward))
            if next_state == None:  # 假设终止状态用 None 表示
                break
            state = next_state
        return episode

    def update_q_value(self, episode: List[Tuple[int, int, float]]) -> None:
        """
        使用生成的序列更新动作值函数。

        参数:
            episode: 序列列表，包含状态、动作和奖励
        """
        G = 0
        W = 1
        for state, action, reward in reversed(episode):
            G = self.gamma * G + reward
            self.cumulative_weights[(state, action)] += W
            self.q_value[(state, action)] += (W / self.cumulative_weights[(state, action)]) * (G - self.q_value[(state, action)])
            if action != self.target_policy[state]:
                break
            W *= 1.0 / self.behavior_policy[state][action]

    def off_policy_prediction(self, num_episodes: int) -> None:
        """
        离策略预测算法，通过重要性采样评估目标策略的动作值函数。

        参数:
            num_episodes: 运行的序列数量
        """
        for _ in range(num_episodes):
            episode = self.generate_episode()
            self.update_q_value(episode)

# 测试代码
if __name__ == "__main__":
    # 定义状态、动作、转移概率和奖励
    states = [0, 1, 2, 3]
    actions = [0, 1]
    transition_probabilities = {
        (0, 0, 0): 0.7, (0, 0, 1): 0.3,
        (0, 1, 0): 0.4, (0, 1, 1): 0.6,
        (1, 0, 2): 0.8, (1, 0, 3): 0.2,
        (1, 1, 2): 0.5, (1, 1, 3): 0.5,
        (2, 0, 0): 0.9, (2, 0, 1): 0.1,
        (2, 1, 0): 0.2, (2, 1, 1): 0.8,
        (3, 0, 2): 0.6, (3, 0, 3): 0.4,
        (3, 1, 2): 0.3, (3, 1, 3): 0.7,
    }
    rewards = {
        (0, 0, 0): 5, (0, 0, 1): 10,
        (0, 1, 0): 2, (0, 1, 1): 7,
        (1, 0, 2): 15, (1, 0, 3): 20,
        (1, 1, 2): 9, (1, 1, 3): 13,
        (2, 0, 0): 6, (2, 0, 1): 8,
        (2, 1, 0): 3, (2, 1, 1): 4,
        (3, 0, 2): 12, (3, 0, 3): 18,
        (3, 1, 2): 14, (3, 1, 3): 11,
    }
    gamma = 0.9

    # 定义行为策略和目标策略
    behavior_policy = {
        0: {0: 0.5, 1: 0.5},
        1: {0: 0.5, 1: 0.5},
        2: {0: 0.5, 1: 0.5},
        3: {0: 0.5, 1: 0.5},
    }
    target_policy = {
        0: {0: 1.0, 1: 0.0},
        1: {0: 1.0, 1: 0.0},
        2: {0: 1.0, 1: 0.0},
        3: {0: 1.0, 1: 0.0},
    }

    num_episodes = 1000

    off_policy = OffPolicyPredictionIS(states, actions, transition_probabilities, rewards, gamma, behavior_policy, target_policy)
    off_policy.off_policy_prediction(num_episodes)

    print("最终动作值函数:", off_policy.q_value)
</code></pre>
  </div>
</body>
</html>
  