
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>5.2 动作值的蒙特卡罗估计</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h1>01_5.2_动作值的蒙特卡罗估计</h1>
<pre><code>Lecture: /05._蒙特卡罗方法
Content: 01_5.2_动作值的蒙特卡罗估计
</code></pre>
<h3>01_5.2 动作值的蒙特卡罗估计</h3>
<h4>动作值的蒙特卡罗估计简介</h4>
<p>在强化学习中，当环境模型不可用时，估计动作值（即状态-动作对的值）特别有用。蒙特卡罗方法通过样本序列对状态值和动作值进行估计，这种方法不需要环境的完全模型，只需要从环境中采样得到的序列数据。对于动作值的估计，我们主要目标是估计 $ q_*(s, a) $，即最优动作值函数。</p>
<h4>动作值的蒙特卡罗估计的核心概念</h4>
<ol>
<li>
<p><strong>策略评估问题</strong>：</p>
<ul>
<li>策略评估问题是估计 $ q_\pi(s, a) $，即从状态 $ s $ 开始，采取动作 $ a $，然后遵循策略 $ \pi $ 的预期回报。</li>
<li>蒙特卡罗方法通过访问状态-动作对来估计其值。每次访问蒙特卡罗方法估计状态-动作对的值为所有访问后的回报的平均值。首次访问蒙特卡罗方法仅计算每个序列中首次访问状态-动作对后的回报平均值。这些方法随着访问次数趋于无穷大而收敛到真实的期望值。</li>
</ul>
</li>
<li>
<p><strong>保持探索</strong>：</p>
<ul>
<li>需要确保所有动作都有被选择的可能性。如果策略是确定性的，那么只会观察到从每个状态中选择的一个动作的回报。因此，为了比较不同的动作，需要估计每个状态下所有动作的值。</li>
</ul>
</li>
<li>
<p><strong>探索起点假设</strong>：</p>
<ul>
<li>为了保证所有状态-动作对被访问到，可以通过指定序列从某个状态-动作对开始，并确保每个状态-动作对都有非零概率被选为起点。这被称为探索起点假设。</li>
</ul>
</li>
</ol>
<h4>动作值的蒙特卡罗方法的步骤</h4>
<ol>
<li>
<p><strong>策略评估</strong>：</p>
<ul>
<li>策略评估的目标是计算给定策略 $\pi$ 下的动作值函数 $q_\pi(s, a)$。通过对从状态 $s$ 采取动作 $a$ 开始的回报进行平均，估计其值。</li>
<li>每次访问蒙特卡罗方法和首次访问蒙特卡罗方法均可用于动作值的估计。通过对每次访问或首次访问的回报进行平均，逐渐逼近真实值。</li>
</ul>
</li>
<li>
<p><strong>策略改进</strong>：</p>
<ul>
<li>在策略评估的基础上，利用新的动作值函数改进策略，使其在每个状态选择使期望回报最大的动作。</li>
</ul>
</li>
<li>
<p><strong>策略控制</strong>：</p>
<ul>
<li>动作值的蒙特卡罗控制结合了策略评估和策略改进，通过反复迭代这两个过程，直到找到最优策略。</li>
</ul>
</li>
</ol>
<h4>蒙特卡罗方法的应用案例</h4>
<p><strong>示例: 二十一点（Blackjack）</strong>：</p>
<ul>
<li>在二十一点游戏中，玩家的目标是在不超过21点的情况下，尽量接近21点。蒙特卡罗方法可以用于估计玩家在不同策略下的动作值函数。例如，假设一个策略是玩家在手牌总数为20或21时停牌，否则继续要牌。通过模拟大量的二十一点游戏，可以计算出在该策略下各个状态的期望回报。</li>
</ul>
<h4>蒙特卡罗方法的优缺点</h4>
<p><strong>优点</strong>：</p>
<ol>
<li><strong>不需要模型</strong>：蒙特卡罗方法只需要从环境中采样得到的序列数据，不需要对环境的完全了解。</li>
<li><strong>简单易行</strong>：通过对回报的简单平均来估计值函数，理论上简单易行。</li>
<li><strong>适用于大规模问题</strong>：特别适合用于估计特定状态-动作对的值函数，而不需要计算所有状态的值函数。</li>
</ol>
<p><strong>缺点</strong>：</p>
<ol>
<li><strong>高方差</strong>：蒙特卡罗方法的估计具有高方差，收敛速度可能较慢。</li>
<li><strong>探索问题</strong>：需要确保所有动作都有被选择的可能性，否则无法估计某些动作的值。</li>
</ol>
<h4>结论</h4>
<p>动作值的蒙特卡罗方法通过对从环境中采样得到的序列数据进行平均，能够有效地估计状态-动作对的值函数。其无需环境模型、适用于大规模问题的特点，使其在实践中具有广泛的应用前景。通过结合策略评估和策略改进，蒙特卡罗方法能够逐步逼近最优策略，解决复杂的决策问题。</p>
<h3>总结</h3>
<p>动作值的蒙特卡罗方法通过对采样数据的平均来估计状态-动作对的值，适用于不完全了解环境的情况。首次访问和每次访问蒙特卡罗方法都能收敛到真实值，尽管每次访问方法的实现更复杂但收敛更快。应用案例如二十一点游戏展示了其在实际问题中的有效性。总的来说，动作值的蒙特卡罗方法在解决强化学习问题中具有重要作用和广泛应用前景。</p>

    <h3>Python 文件</h3>
    <pre><code># 01_5.2_动作值的蒙特卡罗估计

"""
Lecture: /05._蒙特卡罗方法
Content: 01_5.2_动作值的蒙特卡罗估计
"""

import numpy as np
from typing import Dict, Tuple, List

class MonteCarloActionValue:
    """
    动作值蒙特卡罗估计类，包含用于求解马尔可夫决策过程（MDP）的策略评估和改进算法。

    Attributes:
        states: 状态集合
        actions: 动作集合
        transition_probabilities: 状态转移概率矩阵
        rewards: 奖励矩阵
        gamma: 折扣因子
        epsilon: 探索率
    """

    def __init__(self, states: List[int], actions: List[int],
                 transition_probabilities: Dict[Tuple[int, int, int], float],
                 rewards: Dict[Tuple[int, int, int], float], gamma: float, epsilon: float):
        """
        初始化动作值蒙特卡罗估计类。

        参数:
            states: 状态集合
            actions: 动作集合
            transition_probabilities: 状态转移概率矩阵
            rewards: 奖励矩阵
            gamma: 折扣因子
            epsilon: 探索率
        """
        self.states = states
        self.actions = actions
        self.transition_probabilities = transition_probabilities
        self.rewards = rewards
        self.gamma = gamma
        self.epsilon = epsilon
        self.q_value = {(s, a): 0 for s in states for a in actions}
        self.returns = {(s, a): [] for s in states for a in actions}
        self.policy = {s: np.random.choice(actions) for s in states}

    def generate_episode(self, start_state: int) -> List[Tuple[int, int, float]]:
        """
        生成一个遵循当前策略的完整序列。

        参数:
            start_state: 序列的初始状态

        返回:
            List[Tuple[int, int, float]]: 序列列表，包含状态、动作和奖励
        """
        episode = []
        state = start_state
        while True:
            action = self.policy[state] if np.random.rand() > self.epsilon else np.random.choice(self.actions)
            next_state = np.random.choice(self.states, p=[self.transition_probabilities[(state, action, s)] for s in self.states])
            reward = self.rewards[(state, action, next_state)]
            episode.append((state, action, reward))
            if next_state == None:  # 假设终止状态用 None 表示
                break
            state = next_state
        return episode

    def update_q_value(self, episode: List[Tuple[int, int, float]]) -> None:
        """
        使用生成的序列更新动作值函数。

        参数:
            episode: 序列列表，包含状态、动作和奖励
        """
        G = 0
        episode.reverse()
        visited = set()
        for state, action, reward in episode:
            G = self.gamma * G + reward
            if (state, action) not in visited:
                self.returns[(state, action)].append(G)
                self.q_value[(state, action)] = np.mean(self.returns[(state, action)])
                visited.add((state, action))

    def policy_improvement(self) -> None:
        """
        根据当前的动作值函数改进策略。
        """
        for state in self.states:
            action_values = {a: self.q_value[(state, a)] for a in self.actions}
            self.policy[state] = max(action_values, key=action_values.get)

    def monte_carlo_control(self, num_episodes: int) -> None:
        """
        蒙特卡罗控制算法，通过策略评估和策略改进找到最优策略。

        参数:
            num_episodes: 运行的序列数量
        """
        for _ in range(num_episodes):
            start_state = np.random.choice(self.states)
            episode = self.generate_episode(start_state)
            self.update_q_value(episode)
            self.policy_improvement()

# 测试代码
if __name__ == "__main__":
    # 定义状态、动作、转移概率和奖励
    states = [0, 1, 2, 3]
    actions = [0, 1]
    transition_probabilities = {
        (0, 0, 0): 0.7, (0, 0, 1): 0.3,
        (0, 1, 0): 0.4, (0, 1, 1): 0.6,
        (1, 0, 2): 0.8, (1, 0, 3): 0.2,
        (1, 1, 2): 0.5, (1, 1, 3): 0.5,
        (2, 0, 0): 0.9, (2, 0, 1): 0.1,
        (2, 1, 0): 0.2, (2, 1, 1): 0.8,
        (3, 0, 2): 0.6, (3, 0, 3): 0.4,
        (3, 1, 2): 0.3, (3, 1, 3): 0.7,
    }
    rewards = {
        (0, 0, 0): 5, (0, 0, 1): 10,
        (0, 1, 0): 2, (0, 1, 1): 7,
        (1, 0, 2): 15, (1, 0, 3): 20,
        (1, 1, 2): 9, (1, 1, 3): 13,
        (2, 0, 0): 6, (2, 0, 1): 8,
        (2, 1, 0): 3, (2, 1, 1): 4,
        (3, 0, 2): 12, (3, 0, 3): 18,
        (3, 1, 2): 14, (3, 1, 3): 11,
    }
    gamma = 0.9
    epsilon = 0.1
    num_episodes = 1000

    mc_av = MonteCarloActionValue(states, actions, transition_probabilities, rewards, gamma, epsilon)
    mc_av.monte_carlo_control(num_episodes)

    print("最终动作值函数:", mc_av.q_value)
    print("最终策略:", mc_av.policy)</code></pre>
  </div>
</body>
</html>
  