
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>06-activation derivative</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <blockquote>
<p>常用的激活函数，以及对应的导数？</p>
</blockquote>
<p>在神经网络中，激活函数起着将线性输入转换为非线性输出的重要作用，从而使神经网络能够学习和表示复杂的模式。以下是一些常用的激活函数及其对应的导数：</p>
<h3>1. Sigmoid 函数</h3>
<p><strong>定义</strong>：
$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$</p>
<p><strong>导数</strong>：
$$ \sigma'(x) = \sigma(x) (1 - \sigma(x)) $$</p>
<p><strong>特点</strong>：</p>
<ul>
<li><strong>范围</strong>：输出值在 (0, 1) 之间。</li>
<li><strong>优点</strong>：平滑且连续，广泛应用于二分类问题的输出层。</li>
<li><strong>缺点</strong>：在极端值附近梯度接近于0，导致梯度消失问题。</li>
</ul>
<h3>2. Tanh 函数</h3>
<p><strong>定义</strong>：
$$ \text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$</p>
<p><strong>导数</strong>：
$$ \text{tanh}'(x) = 1 - \text{tanh}^2(x) $$</p>
<p><strong>特点</strong>：</p>
<ul>
<li><strong>范围</strong>：输出值在 (-1, 1) 之间。</li>
<li><strong>优点</strong>：比Sigmoid函数中心化（输出均值为0），梯度更大，训练更快。</li>
<li><strong>缺点</strong>：同样存在梯度消失问题。</li>
</ul>
<h3>3. ReLU (Rectified Linear Unit) 函数</h3>
<p><strong>定义</strong>：
$$ \text{ReLU}(x) = \max(0, x) $$</p>
<p><strong>导数</strong>：
$$ \text{ReLU}'(x) = \begin{cases}
1 &amp; \text{if } x &gt; 0 \
0 &amp; \text{if } x \leq 0
\end{cases} $$</p>
<p><strong>特点</strong>：</p>
<ul>
<li><strong>范围</strong>：输出值在 [0, ∞) 之间。</li>
<li><strong>优点</strong>：简单高效，计算速度快，能有效缓解梯度消失问题。</li>
<li><strong>缺点</strong>：在负区间梯度为0，导致神经元可能会“死掉”。</li>
</ul>
<h3>4. Leaky ReLU 函数</h3>
<p><strong>定义</strong>：
$$ \text{Leaky ReLU}(x) = \begin{cases}
x &amp; \text{if } x &gt; 0 \
\alpha x &amp; \text{if } x \leq 0
\end{cases} $$
其中，$\alpha$ 是一个小的常数，一般取值如 0.01。</p>
<p><strong>导数</strong>：
$$ \text{Leaky ReLU}'(x) = \begin{cases}
1 &amp; \text{if } x &gt; 0 \
\alpha &amp; \text{if } x \leq 0
\end{cases} $$</p>
<p><strong>特点</strong>：</p>
<ul>
<li><strong>范围</strong>：输出值在 (-∞, ∞) 之间。</li>
<li><strong>优点</strong>：解决了ReLU的“死亡”问题，允许负值通过。</li>
<li><strong>缺点</strong>：相比ReLU，计算稍复杂。</li>
</ul>
<h3>5. ELU (Exponential Linear Unit) 函数</h3>
<p><strong>定义</strong>：
$$ \text{ELU}(x) = \begin{cases}
x &amp; \text{if } x &gt; 0 \
\alpha (e^x - 1) &amp; \text{if } x \leq 0
\end{cases} $$
其中，$\alpha$ 是一个大于0的常数。</p>
<p><strong>导数</strong>：
$$ \text{ELU}'(x) = \begin{cases}
1 &amp; \text{if } x &gt; 0 \
\alpha e^x &amp; \text{if } x \leq 0
\end{cases} $$</p>
<p><strong>特点</strong>：</p>
<ul>
<li><strong>范围</strong>：输出值在 (-α, ∞) 之间。</li>
<li><strong>优点</strong>：结合了ReLU和Leaky ReLU的优点，在负区间有非零梯度，输出值均值接近于零。</li>
<li><strong>缺点</strong>：计算复杂度比ReLU高。</li>
</ul>
<h3>6. Swish 函数</h3>
<p><strong>定义</strong>：
$$ \text{Swish}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}} $$</p>
<p><strong>导数</strong>：
$$ \text{Swish}'(x) = \text{Swish}(x) + \sigma(x) (1 - \text{Swish}(x)) $$</p>
<p><strong>特点</strong>：</p>
<ul>
<li><strong>范围</strong>：输出值在 (-∞, ∞) 之间。</li>
<li><strong>优点</strong>：平滑且无界，能自适应调整输出，表现优于ReLU。</li>
<li><strong>缺点</strong>：计算复杂度较高。</li>
</ul>
<h3>参考资料</h3>
<ol>
<li><a href="https://www.deeplearningbook.org/">Activation Functions</a>: &quot;Deep Learning&quot; by Ian Goodfellow, Yoshua Bengio, and Aaron Courville</li>
<li><a href="https://towardsdatascience.com/activation-functions-and-its-derivatives-6d9c94fd2d53">Understanding the Gradient of Activation Functions</a></li>
<li><a href="https://www.analyticsvidhya.com/blog/2021/05/complete-guide-on-activation-functions/">Activation Functions in Neural Networks</a></li>
</ol>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  