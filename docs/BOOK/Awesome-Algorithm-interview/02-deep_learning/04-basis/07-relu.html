
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>07-relu</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <blockquote>
<p>relu的有优点？又有什么局限性？他们的系列改进方法是啥？</p>
</blockquote>
<h3>ReLU的优点与局限性及其改进方法</h3>
<h4>ReLU的优点</h4>
<ol>
<li>
<p><strong>简单高效</strong>：</p>
<ul>
<li>计算简单，只需进行一次比较操作 $\max(0, x)$，计算速度快。</li>
</ul>
</li>
<li>
<p><strong>加速收敛</strong>：</p>
<ul>
<li>相较于Sigmoid和Tanh，ReLU能加速神经网络的训练过程。因为ReLU的导数恒为1，不会出现梯度消失的问题，从而加快梯度下降算法的收敛速度。</li>
</ul>
</li>
<li>
<p><strong>稀疏激活</strong>：</p>
<ul>
<li>ReLU会使一部分神经元输出为零，导致网络在某些层的稀疏性，从而提高模型的计算效率和内存利用率。</li>
</ul>
</li>
</ol>
<h4>ReLU的局限性</h4>
<ol>
<li>
<p><strong>死亡ReLU问题</strong>：</p>
<ul>
<li>当神经元的输入总是负值时，该神经元将永远不会被激活（输出始终为零），导致神经元“死亡”。这种现象通常发生在使用较大学习率时。</li>
</ul>
</li>
<li>
<p><strong>不平衡的梯度</strong>：</p>
<ul>
<li>ReLU的输出不平衡（输出值在[0, ∞)），可能导致梯度不平衡问题，进而影响训练稳定性。</li>
</ul>
</li>
</ol>
<h4>ReLU的改进方法</h4>
<ol>
<li>
<p><strong>Leaky ReLU</strong></p>
<p><strong>定义</strong>：
$$
\text{Leaky ReLU}(x) = \begin{cases}
x &amp; \text{if } x &gt; 0 \
\alpha x &amp; \text{if } x \leq 0
\end{cases}
$$
其中，$\alpha$ 是一个小常数，通常取值如0.01。</p>
<p><strong>优点</strong>：</p>
<ul>
<li>解决了死亡ReLU问题，在负值区域有小的非零梯度，允许一些负值通过，从而使得神经元有可能被重新激活。</li>
</ul>
</li>
<li>
<p><strong>Parametric ReLU (PReLU)</strong></p>
<p><strong>定义</strong>：
$$
\text{PReLU}(x) = \begin{cases}
x &amp; \text{if } x &gt; 0 \
\alpha x &amp; \text{if } x \leq 0
\end{cases}
$$
其中，$\alpha$ 是一个可学习的参数。</p>
<p><strong>优点</strong>：</p>
<ul>
<li>提供了更大的灵活性，因为 $\alpha$ 是通过训练数据学习得到的，能更好地适应不同的任务。</li>
</ul>
</li>
<li>
<p><strong>Exponential Linear Unit (ELU)</strong></p>
<p><strong>定义</strong>：
$$
\text{ELU}(x) = \begin{cases}
x &amp; \text{if } x &gt; 0 \
\alpha (e^x - 1) &amp; \text{if } x \leq 0
\end{cases}
$$
其中，$\alpha$ 是一个大于0的常数。</p>
<p><strong>优点</strong>：</p>
<ul>
<li>在负值区域有非零输出，使得输出均值接近于零，从而提高训练速度。</li>
<li>能够缓解死亡ReLU问题，并且输出更平滑，具有更好的梯度特性。</li>
</ul>
</li>
<li>
<p><strong>Scaled Exponential Linear Unit (SELU)</strong></p>
<p><strong>定义</strong>：
$$
\text{SELU}(x) = \lambda \begin{cases}
x &amp; \text{if } x &gt; 0 \
\alpha (e^x - 1) &amp; \text{if } x \leq 0
\end{cases}
$$
其中，$\lambda$ 和 $\alpha$ 是固定的参数。</p>
<p><strong>优点</strong>：</p>
<ul>
<li>SELU在深度网络中具有自正则化的特性，能够自动保持激活值的均值和方差稳定。</li>
</ul>
</li>
<li>
<p><strong>Swish</strong></p>
<p><strong>定义</strong>：
$$
\text{Swish}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}
$$</p>
<p><strong>优点</strong>：</p>
<ul>
<li>平滑且无界，能自适应调整输出，表现优于ReLU。</li>
</ul>
</li>
</ol>
<h3>参考资料</h3>
<ol>
<li><a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">Rectified Linear Unit (ReLU)</a></li>
<li><a href="https://towardsdatascience.com/understanding-leaky-relu-and-why-you-should-use-it-a77b4c3f3c5d">Understanding Leaky ReLU</a></li>
<li><a href="https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6">Exponential Linear Unit (ELU) and Scaled ELU (SELU)</a></li>
<li><a href="https://arxiv.org/abs/1710.05941">Swish: A Self-Gated Activation Function</a></li>
</ol>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  