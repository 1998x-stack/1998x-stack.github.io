
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>05-vanishing exploding gradients</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <blockquote>
<p>什么是梯度消失和梯度爆炸？</p>
</blockquote>
<h3>梯度消失和梯度爆炸</h3>
<h4>梯度消失（Vanishing Gradient）</h4>
<p><strong>定义</strong>：
梯度消失是指在深度神经网络训练过程中，随着反向传播的进行，梯度在逐层传递时逐渐变小，最终接近于零，导致前几层的权重几乎不更新。这种现象使得网络难以学习和调整前几层的参数，从而影响模型性能。</p>
<p><strong>原因</strong>：</p>
<ol>
<li>
<p><strong>激活函数的选择</strong>：</p>
<ul>
<li>Sigmoid和Tanh等激活函数的导数在输入较大或较小时会趋近于零，导致梯度消失。</li>
<li>例如，Sigmoid函数的导数是：
$$
\sigma'(z) = \sigma(z)(1 - \sigma(z))
$$
当 $ \sigma(z) $ 接近0或1时，导数接近于零。</li>
</ul>
</li>
<li>
<p><strong>权重初始化不当</strong>：</p>
<ul>
<li>如果权重初始化过小，前向传播过程中会导致激活值趋近于零，从而在反向传播时梯度也趋近于零。</li>
</ul>
</li>
</ol>
<p><strong>解决方法</strong>：</p>
<ol>
<li>
<p><strong>使用ReLU激活函数</strong>：</p>
<ul>
<li>ReLU（Rectified Linear Unit）及其变种（如Leaky ReLU、ELU）能够有效缓解梯度消失问题，因为它们在正区间的导数恒为1。</li>
</ul>
</li>
<li>
<p><strong>适当的权重初始化</strong>：</p>
<ul>
<li>使用如Xavier初始化或He初始化等方法，根据网络层数和激活函数特点合理初始化权重。</li>
</ul>
</li>
<li>
<p><strong>批归一化（Batch Normalization）</strong>：</p>
<ul>
<li>对每层的输出进行归一化，减小内部协变量偏移，保持激活值在合理范围内。</li>
</ul>
</li>
</ol>
<h4>梯度爆炸（Exploding Gradient）</h4>
<p><strong>定义</strong>：
梯度爆炸是指在深度神经网络训练过程中，随着反向传播的进行，梯度在逐层传递时逐渐增大，最终导致梯度值非常大，使得模型参数更新时出现数值不稳定甚至溢出的问题。</p>
<p><strong>原因</strong>：</p>
<ol>
<li>
<p><strong>权重初始化不当</strong>：</p>
<ul>
<li>如果权重初始化过大，前向传播过程中会导致激活值和梯度变得非常大。</li>
</ul>
</li>
<li>
<p><strong>深层网络结构</strong>：</p>
<ul>
<li>在非常深的网络中，误差的累积会导致梯度在反向传播过程中不断放大。</li>
</ul>
</li>
</ol>
<p><strong>解决方法</strong>：</p>
<ol>
<li>
<p><strong>梯度剪裁（Gradient Clipping）</strong>：</p>
<ul>
<li>对梯度值进行裁剪，将其限制在某个范围内，以避免梯度值过大。</li>
<li>公式：当梯度 $ g $ 超过设定阈值 $ \theta $ 时，进行剪裁：
$$
g = \frac{\theta}{| g |} g
$$</li>
</ul>
</li>
<li>
<p><strong>适当的权重初始化</strong>：</p>
<ul>
<li>使用较小的权重初始值，避免初始阶段就出现梯度爆炸问题。</li>
</ul>
</li>
<li>
<p><strong>使用合适的优化算法</strong>：</p>
<ul>
<li>使用如Adam、RMSprop等自适应学习率优化算法，能动态调整学习率，避免梯度爆炸。</li>
</ul>
</li>
</ol>
<h3>参考资料</h3>
<ol>
<li>
<p><strong>Understanding the vanishing gradient problem</strong>:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">Wikipedia</a></li>
</ul>
</li>
<li>
<p><strong>Exploding and Vanishing Gradients</strong>:</p>
<ul>
<li><a href="https://www.deeplearning.ai/ai-notes/initialization/">Deep Learning AI Notes</a></li>
</ul>
</li>
<li>
<p><strong>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</strong>:</p>
<ul>
<li><a href="https://arxiv.org/abs/1502.03167">Arxiv</a></li>
</ul>
</li>
<li>
<p><strong>Gradient Clipping</strong>:</p>
<ul>
<li><a href="https://towardsdatascience.com/gradient-clipping-82a5e00b0ebc">Towards Data Science</a></li>
</ul>
</li>
</ol>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  