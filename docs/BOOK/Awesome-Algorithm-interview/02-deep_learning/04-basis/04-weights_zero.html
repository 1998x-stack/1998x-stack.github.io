
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>04-weights zero</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <blockquote>
<p>NN的权重参数能否初始化为0？</p>
</blockquote>
<p>在神经网络（NN）中，权重参数的初始化对模型的训练和性能具有重要影响。下面将详细解释为什么权重参数不能全部初始化为0。</p>
<h3>原因</h3>
<h4>1. 对称性破坏</h4>
<p><strong>对称性问题</strong>：</p>
<ul>
<li>如果所有权重参数都初始化为0，那么在前向传播时，每个神经元的输出将是相同的。</li>
<li>由于反向传播算法依赖于梯度更新，如果所有权重初始化为0，那么所有神经元的梯度也是相同的。这样，每个神经元的权重在每次更新时都相同，导致它们无法学习到不同的特征。</li>
<li>这种情况称为对称性问题，会导致网络中的每个神经元在训练过程中保持对称，从而无法发挥神经网络的强大功能。</li>
</ul>
<h4>2. 梯度为零</h4>
<p><strong>梯度消失</strong>：</p>
<ul>
<li>在梯度下降过程中，权重参数是通过反向传播计算得到的梯度进行更新的。如果所有权重初始化为0，网络的激活值也将导致梯度为0，从而无法更新权重。</li>
<li>这种情况下，神经网络无法学习，训练过程将无法进行。</li>
</ul>
<h3>权重初始化的方法</h3>
<p>为了解决以上问题，有几种常用的权重初始化方法：</p>
<h4>1. 随机初始化</h4>
<p><strong>随机初始化</strong>：</p>
<ul>
<li>将权重初始化为随机小值。常用的方法是从标准正态分布或均匀分布中随机采样。</li>
<li>例如，使用均匀分布的随机初始化：
$$
W \sim \mathcal{U}(-a, a)
$$
其中，$a$ 是一个小正数，通常根据输入和输出的节点数量进行选择。</li>
</ul>
<h4>2. Xavier初始化（Glorot初始化）</h4>
<p><strong>Xavier初始化</strong>：</p>
<ul>
<li>这种方法针对Sigmoid和Tanh激活函数进行优化，确保输入和输出的方差一致。</li>
<li>权重初始化公式：
$$
W \sim \mathcal{U}\left(-\sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}, \sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}\right)
$$
其中，$n_{\text{in}}$ 和 $n_{\text{out}}$ 分别是前一层和后一层的节点数量。</li>
</ul>
<h4>3. He初始化</h4>
<p><strong>He初始化</strong>：</p>
<ul>
<li>这种方法针对ReLU和其变种激活函数进行优化。</li>
<li>权重初始化公式：
$$
W \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}}}\right)
$$
其中，$n_{\text{in}}$ 是前一层的节点数量。</li>
</ul>
<h3>参考资料</h3>
<ol>
<li>
<p><strong>Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville</strong>: Comprehensive explanation on neural networks and their initialization methods.</p>
<ul>
<li><a href="http://www.deeplearningbook.org/">Book link</a></li>
</ul>
</li>
<li>
<p><strong>Understanding Xavier Initialization</strong>: Article explaining the benefits and applications of Xavier initialization.</p>
<ul>
<li><a href="https://towardsdatascience.com/understanding-xavier-initialization-in-deep-learning-3b43e5c8a0ed">Towards Data Science article</a></li>
</ul>
</li>
<li>
<p><strong>He Initialization Explained</strong>: Detailed explanation of He initialization and its applications in neural networks.</p>
<ul>
<li><a href="https://deepai.org/machine-learning-glossary-and-terms/he-initialization">DeepAI article</a></li>
</ul>
</li>
</ol>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  