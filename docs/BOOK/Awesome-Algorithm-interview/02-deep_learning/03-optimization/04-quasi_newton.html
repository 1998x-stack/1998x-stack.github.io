
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>04-quasi newton</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <blockquote>
<p>梯度下降与拟牛顿法的异同？</p>
</blockquote>
<h3>梯度下降法与拟牛顿法的异同</h3>
<p>梯度下降法和拟牛顿法都是用于优化的算法，但它们在计算效率、收敛速度、内存使用等方面有很大的不同。以下是它们的详细比较：</p>
<h3>梯度下降法（Gradient Descent）</h3>
<h4>基本原理</h4>
<ul>
<li><strong>定义</strong>：梯度下降法是一种基于梯度的优化算法，通过迭代地调整参数以最小化目标函数。</li>
<li><strong>更新规则</strong>：
$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta_t)
$$
其中，$\theta_t$ 是第 $t$ 次迭代的参数，$\eta$ 是学习率，$\nabla_\theta J(\theta_t)$ 是目标函数 $J(\theta)$ 对参数 $\theta_t$ 的梯度。</li>
</ul>
<h4>优点</h4>
<ul>
<li><strong>实现简单</strong>：梯度下降算法易于实现，计算要求较低。</li>
<li><strong>适用广泛</strong>：适用于各种类型的优化问题，包括大规模数据集。</li>
</ul>
<h4>缺点</h4>
<ul>
<li><strong>收敛速度慢</strong>：在高维空间中，梯度下降可能收敛速度较慢。</li>
<li><strong>依赖学习率</strong>：需要选择合适的学习率，学习率过大或过小都会影响收敛效果。</li>
<li><strong>局部最优</strong>：可能会陷入局部最优解。</li>
</ul>
<h3>拟牛顿法（Quasi-Newton Methods）</h3>
<h4>基本原理</h4>
<ul>
<li><strong>定义</strong>：拟牛顿法是一类优化算法，通过近似牛顿法中所需的Hessian矩阵来进行参数更新。</li>
<li><strong>常见算法</strong>：BFGS（Broyden-Fletcher-Goldfarb-Shanno）和L-BFGS（Limited-memory BFGS）是最常见的拟牛顿法。</li>
<li><strong>更新规则</strong>：
$$
\theta_{t+1} = \theta_t - H_t^{-1} \nabla_\theta J(\theta_t)
$$
其中，$H_t$ 是Hessian矩阵的近似，通常通过迭代过程来更新。</li>
</ul>
<h4>优点</h4>
<ul>
<li><strong>收敛速度快</strong>：拟牛顿法通常比梯度下降法收敛更快，尤其是在二次可微目标函数中。</li>
<li><strong>无需选择学习率</strong>：拟牛顿法通过Hessian矩阵的近似自动调整步长，无需手动选择学习率。</li>
</ul>
<h4>缺点</h4>
<ul>
<li><strong>计算复杂度高</strong>：需要计算和存储Hessian矩阵的近似，计算复杂度较高。</li>
<li><strong>内存消耗大</strong>：对于大规模问题，Hessian矩阵的存储和更新需要大量内存。</li>
</ul>
<h3>异同点总结</h3>
<h4>相同点</h4>
<ul>
<li><strong>目标</strong>：两者都用于最小化目标函数，通过迭代更新参数来找到最优解。</li>
<li><strong>基于梯度</strong>：都使用梯度信息来指导参数更新方向。</li>
</ul>
<h4>不同点</h4>
<ul>
<li><strong>更新方式</strong>：
<ul>
<li><strong>梯度下降法</strong>：使用固定学习率和梯度进行更新。</li>
<li><strong>拟牛顿法</strong>：使用Hessian矩阵的近似进行更新，自适应调整步长。</li>
</ul>
</li>
<li><strong>计算复杂度</strong>：
<ul>
<li><strong>梯度下降法</strong>：计算简单，适合大规模问题。</li>
<li><strong>拟牛顿法</strong>：计算复杂，适合中小规模问题，需要更多内存。</li>
</ul>
</li>
<li><strong>收敛速度</strong>：
<ul>
<li><strong>梯度下降法</strong>：可能收敛较慢，尤其在高维空间。</li>
<li><strong>拟牛顿法</strong>：通常收敛更快，但计算成本更高。</li>
</ul>
</li>
</ul>
<h3>参考资料</h3>
<ol>
<li>
<p><strong>Gradient Descent</strong>:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent</a></li>
<li><a href="https://towardsdatascience.com/understanding-gradient-descent-algorithm-81b0232f7f25">Understanding Gradient Descent Algorithm</a></li>
</ul>
</li>
<li>
<p><strong>Quasi-Newton Methods</strong>:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Quasi-Newton_method">Quasi-Newton Methods</a></li>
<li><a href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm">BFGS Algorithm</a></li>
<li><a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS Algorithm</a></li>
</ul>
</li>
</ol>
<p>通过对这两种优化算法的了解，可以更好地选择和应用它们来解决具体的优化问题。</p>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  