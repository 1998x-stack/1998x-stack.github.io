
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>05-regularization</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <blockquote>
<p>L1和L2正则分别有什么特点？为何L1稀疏？</p>
</blockquote>
<h3>L1 和 L2 正则化的特点</h3>
<h4>L1 正则化（Lasso）</h4>
<p><strong>定义</strong>：
L1 正则化在损失函数中加入权重绝对值的和，其公式如下：
$$ L_{\text{L1}} = \sum_{i=1}^n |w_i| $$</p>
<p><strong>特点</strong>：</p>
<ol>
<li><strong>稀疏性</strong>：L1 正则化会产生稀疏解，即一些权重会被缩减为零。这样可以进行特征选择，使模型更加简洁。</li>
<li><strong>鲁棒性</strong>：对异常值有较好的鲁棒性，因为它对大误差进行较少的惩罚。</li>
<li><strong>适用场景</strong>：在需要特征选择的高维数据集中，L1 正则化非常有效，如文本分类、基因数据分析等。</li>
</ol>
<p><strong>为何 L1 稀疏</strong>：</p>
<ul>
<li>L1 正则化的目标是最小化权重的绝对值和，这导致了一些权重被压缩到零。</li>
<li>当梯度下降时，L1 正则化对权重的更新方向是固定的，这意味着它更倾向于使较小的权重迅速衰减到零，产生稀疏的权重向量。</li>
</ul>
<h4>L2 正则化（Ridge）</h4>
<p><strong>定义</strong>：
L2 正则化在损失函数中加入权重平方和，其公式如下：
$$ L_{\text{L2}} = \sum_{i=1}^n w_i^2 $$</p>
<p><strong>特点</strong>：</p>
<ol>
<li><strong>平滑性</strong>：L2 正则化会使权重趋向于更小且均匀的值，而不会使它们变为零。</li>
<li><strong>防止过拟合</strong>：通过惩罚大权重，L2 正则化有效地防止模型过拟合。</li>
<li><strong>适用场景</strong>：在大部分机器学习任务中，L2 正则化都非常有效，尤其是当所有特征都可能有贡献时，如回归分析、图像分类等。</li>
</ol>
<h3>L1 和 L2 正则化的比较</h3>
<ol>
<li>
<p><strong>稀疏性</strong>：</p>
<ul>
<li><strong>L1 正则化</strong>：能够产生稀疏解，适合特征选择。</li>
<li><strong>L2 正则化</strong>：不会产生稀疏解，权重趋向于更小且均匀的值。</li>
</ul>
</li>
<li>
<p><strong>解决问题的类型</strong>：</p>
<ul>
<li><strong>L1 正则化</strong>：适用于高维数据和需要特征选择的情况。</li>
<li><strong>L2 正则化</strong>：适用于防止过拟合，并保持所有特征的贡献。</li>
</ul>
</li>
<li>
<p><strong>优化方式</strong>：</p>
<ul>
<li><strong>L1 正则化</strong>：可能导致优化问题变得非平滑，梯度下降法可能不如 L2 高效。</li>
<li><strong>L2 正则化</strong>：优化问题是平滑的，梯度下降法收敛速度较快。</li>
</ul>
</li>
</ol>
<h3>公式总结</h3>
<ul>
<li>
<p><strong>L1 正则化</strong>：
$$ J(\theta) = J_{\text{original}}(\theta) + \lambda \sum_{i=1}^n |\theta_i| $$</p>
</li>
<li>
<p><strong>L2 正则化</strong>：
$$ J(\theta) = J_{\text{original}}(\theta) + \lambda \sum_{i=1}^n \theta_i^2 $$</p>
</li>
</ul>
<h3>参考资料</h3>
<ol>
<li>
<p><strong>&quot;Deep Learning&quot; by Ian Goodfellow, Yoshua Bengio, and Aaron Courville</strong>: This book provides a comprehensive explanation of regularization techniques including L1 and L2 regularization.</p>
<ul>
<li><a href="http://www.deeplearningbook.org/">Book link</a></li>
</ul>
</li>
<li>
<p><strong>&quot;Elements of Statistical Learning&quot; by Trevor Hastie, Robert Tibshirani, and Jerome Friedman</strong>: This book offers an in-depth discussion on L1 (Lasso) and L2 (Ridge) regularization.</p>
<ul>
<li><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">Book link</a></li>
</ul>
</li>
<li>
<p><strong>Understanding L1 and L2 Regularization</strong>:</p>
<ul>
<li><a href="https://towardsdatascience.com/understanding-l1-and-l2-regularization-7f1b4c21fb7e">Towards Data Science article</a></li>
<li><a href="https://www.analyticsvidhya.com/blog/2021/01/understanding-the-difference-between-l1-and-l2-regularization/">Analytics Vidhya article</a></li>
</ul>
</li>
</ol>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  