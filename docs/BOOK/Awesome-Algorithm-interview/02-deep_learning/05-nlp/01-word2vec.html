
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>01-word2vec</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <blockquote>
<p>详细展开 word2vec的原理</p>
</blockquote>
<h3>Word2Vec的原理</h3>
<p>Word2Vec是一种用于学习词向量表示的技术，由Tomas Mikolov及其团队在2013年提出。它通过将词映射到低维连续向量空间中，使得词与词之间的语义关系得以保留。Word2Vec主要有两种模型架构：Skip-gram和CBOW（Continuous Bag of Words）。以下是对这两种模型的详细解释：</p>
<h4>1. Skip-gram模型</h4>
<p>Skip-gram模型的目标是根据中心词（输入词）预测其上下文词（目标词）。换句话说，对于给定的中心词，模型尝试预测在其窗口范围内出现的所有上下文词。</p>
<p><strong>模型架构</strong>：</p>
<ul>
<li>输入层：每个输入词表示为一个独热编码向量（one-hot vector）。</li>
<li>投影层：通过嵌入矩阵将高维独热向量映射到低维稠密向量（词向量）。</li>
<li>输出层：计算每个上下文词的概率分布，通常使用Softmax函数。</li>
</ul>
<p><strong>目标函数</strong>：</p>
<ul>
<li>最大化在给定中心词的情况下生成上下文词的概率。</li>
<li>使用负采样（Negative Sampling）或分层Softmax（Hierarchical Softmax）来提高计算效率。</li>
</ul>
<p><strong>数学表达</strong>：</p>
<ul>
<li>目标函数：$
\frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log P(w_{t+j} | w_t)
$
其中，$T$是词序列的总长度，$c$是窗口大小，$w_t$是中心词，$w_{t+j}$是上下文词。</li>
<li>条件概率：$
P(w_O | w_I) = \frac{\exp(\mathbf{v}<em w_I="">{w_O} \cdot \mathbf{v}</em>)}{\sum_{w \in V} \exp(\mathbf{v}<em w_I="">w \cdot \mathbf{v}</em>)}
$
其中，$\mathbf{v}<em w_I="">{w_O}$和$\mathbf{v}</em>$分别是上下文词和中心词的词向量，$V$是词汇表。</li>
</ul>
<h4>2. CBOW模型</h4>
<p>CBOW模型的目标是根据上下文词预测中心词。与Skip-gram相反，CBOW通过输入窗口内所有上下文词，预测位于窗口中心的词。</p>
<p><strong>模型架构</strong>：</p>
<ul>
<li>输入层：窗口内所有上下文词的独热编码向量。</li>
<li>投影层：将上下文词向量求平均，得到一个上下文向量。</li>
<li>输出层：计算中心词的概率分布，通常使用Softmax函数。</li>
</ul>
<p><strong>目标函数</strong>：</p>
<ul>
<li>最大化在给定上下文词的情况下生成中心词的概率。</li>
</ul>
<p><strong>数学表达</strong>：</p>
<ul>
<li>目标函数：$
\frac{1}{T} \sum_{t=1}^{T} \log P(w_t | w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c})
$
其中，$T$是词序列的总长度，$c$是窗口大小。</li>
<li>条件概率：与Skip-gram相同，使用Softmax函数计算。</li>
</ul>
<h4>3. 负采样（Negative Sampling）</h4>
<p>负采样是一种改进的训练方法，通过简化目标函数来提高训练效率。它通过在训练过程中仅更新少量的负样本，从而加速模型训练。</p>
<p><strong>负采样的目标函数</strong>：
$$
\log \sigma(\mathbf{v}<em w_I="">{w_O} \cdot \mathbf{v}</em>) + \sum_{i=1}^{k} \mathbb{E}<em w_i="">{w_i \sim P_n(w)} [\log \sigma(-\mathbf{v}</em> \cdot \mathbf{v}_{w_I})]
$$
其中，$\sigma(x)$是Sigmoid函数，$k$是负样本数量，$P_n(w)$是负样本的概率分布。</p>
<h3>实现示例（使用Python的gensim库）</h3>
<p>以下是使用gensim库实现Word2Vec的示例代码：</p>
<pre><code class="language-python">from gensim.models import Word2Vec

# 示例数据
sentences = [
    ['i', 'love', 'machine', 'learning'],
    ['word2vec', 'is', 'a', 'great', 'tool'],
    ['i', 'enjoy', 'learning', 'new', 'things']
]

# 训练Word2Vec模型
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)  # sg=1表示使用Skip-gram模型

# 获取词向量
vector = model.wv['machine']
print(&quot;词向量：\n&quot;, vector)
</code></pre>
<h3>参考文献</h3>
<ol>
<li>
<p><strong>Distributed Representations of Words and Phrases and their Compositionality by Tomas Mikolov et al.</strong>：</p>
<ul>
<li>提供了Word2Vec的详细理论和实现。</li>
<li><a href="https://arxiv.org/abs/1310.4546">论文链接</a></li>
</ul>
</li>
<li>
<p><strong>Efficient Estimation of Word Representations in Vector Space by Tomas Mikolov et al.</strong>：</p>
<ul>
<li>介绍了Skip-gram和CBOW模型，以及负采样方法。</li>
<li><a href="https://arxiv.org/abs/1301.3781">论文链接</a></li>
</ul>
</li>
<li>
<p><strong>Gensim文档</strong>：</p>
<ul>
<li>提供了使用Gensim库进行Word2Vec训练的实际示例和教程。</li>
<li><a href="https://radimrehurek.com/gensim/models/word2vec.html">Gensim文档</a></li>
</ul>
</li>
</ol>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  