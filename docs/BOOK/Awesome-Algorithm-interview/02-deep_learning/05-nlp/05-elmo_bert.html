
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>05-elmo bert</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <blockquote>
<p>了解elmo和bert吗？简述与word embedding的联系和区别</p>
</blockquote>
<h3>ELMo 和 BERT 简述</h3>
<p>ELMo（Embeddings from Language Models）和 BERT（Bidirectional Encoder Representations from Transformers）是两种先进的自然语言处理（NLP）模型，它们基于上下文生成动态词嵌入，与传统的静态词嵌入（如Word2Vec和GloVe）有显著不同。</p>
<h4>ELMo</h4>
<p><strong>原理</strong>：
ELMo 是一种基于双向语言模型（biLM）的词嵌入方法，它通过在给定上下文的基础上生成词的向量表示。ELMo 的关键特点是它能够生成动态词嵌入，即同一个词在不同的上下文中可以有不同的向量表示。</p>
<p><strong>模型结构</strong>：</p>
<ul>
<li>ELMo 使用两层双向 LSTM（Long Short-Term Memory）来捕捉上下文信息。</li>
<li>每个词的嵌入表示是通过结合双向 LSTM 输出的加权和。</li>
</ul>
<p><strong>优点</strong>：</p>
<ul>
<li>上下文感知：同一个词在不同的上下文中有不同的嵌入表示。</li>
<li>改进了下游任务的表现，如命名实体识别（NER）、情感分析等。</li>
</ul>
<p><strong>引用</strong>：
Peters, Matthew E., et al. &quot;Deep contextualized word representations.&quot; arXiv preprint arXiv:1802.05365 (2018).
<a href="https://arxiv.org/abs/1802.05365">论文链接</a></p>
<h4>BERT</h4>
<p><strong>原理</strong>：
BERT 是一种基于 Transformer 的模型，通过掩码语言模型（Masked Language Model）和下一个句子预测（Next Sentence Prediction）进行训练。BERT 能够在给定上下文的基础上生成词的向量表示，支持双向编码，即同时考虑词的左侧和右侧上下文。</p>
<p><strong>模型结构</strong>：</p>
<ul>
<li>BERT 使用 Transformer 编码器的堆叠层，典型配置有 BERT-base（12层）和 BERT-large（24层）。</li>
<li>掩码语言模型：在输入序列中随机掩盖一些词，模型通过预测这些掩盖的词进行训练。</li>
<li>下一个句子预测：通过给定的句子对来预测第二个句子是否是第一个句子的下一个句子。</li>
</ul>
<p><strong>优点</strong>：</p>
<ul>
<li>双向编码：同时考虑词的左右上下文，提高了嵌入表示的质量。</li>
<li>对下游任务的适应性强：通过微调，可以很好地适应各种NLP任务，如问答系统、文本分类等。</li>
</ul>
<p><strong>引用</strong>：
Devlin, Jacob, et al. &quot;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.&quot; arXiv preprint arXiv:1810.04805 (2018).
<a href="https://arxiv.org/abs/1810.04805">论文链接</a></p>
<h3>ELMo 和 BERT 与传统词嵌入的联系和区别</h3>
<h4>传统词嵌入（如Word2Vec和GloVe）</h4>
<p><strong>原理</strong>：</p>
<ul>
<li><strong>Word2Vec</strong>：通过 Skip-gram 或 CBOW 模型，根据词的上下文词来生成词嵌入。</li>
<li><strong>GloVe</strong>：通过全局词共现矩阵，生成词嵌入。</li>
</ul>
<p><strong>特点</strong>：</p>
<ul>
<li><strong>静态嵌入</strong>：每个词的向量表示是固定的，不随上下文变化。</li>
<li><strong>训练方式</strong>：基于局部上下文窗口或全局共现信息。</li>
</ul>
<h4>联系</h4>
<ol>
<li><strong>目标相似</strong>：都旨在将词表示为低维向量，使得相似词在向量空间中距离较近。</li>
<li><strong>下游任务</strong>：所有这些嵌入表示都可以用于各种NLP任务，如分类、聚类、情感分析等。</li>
</ol>
<h4>区别</h4>
<ol>
<li>
<p><strong>上下文感知</strong>：</p>
<ul>
<li><strong>传统词嵌入</strong>：静态词嵌入，同一个词在所有上下文中有相同的向量表示。</li>
<li><strong>ELMo 和 BERT</strong>：动态词嵌入，同一个词在不同上下文中有不同的向量表示。</li>
</ul>
</li>
<li>
<p><strong>模型结构</strong>：</p>
<ul>
<li><strong>传统词嵌入</strong>：基于简单的神经网络结构（如Word2Vec的Skip-gram和CBOW）。</li>
<li><strong>ELMo</strong>：基于双向LSTM。</li>
<li><strong>BERT</strong>：基于Transformer的编码器。</li>
</ul>
</li>
<li>
<p><strong>训练方法</strong>：</p>
<ul>
<li><strong>传统词嵌入</strong>：使用局部上下文窗口或全局共现矩阵。</li>
<li><strong>ELMo</strong>：使用双向语言模型。</li>
<li><strong>BERT</strong>：使用掩码语言模型和下一个句子预测。</li>
</ul>
</li>
</ol>
<h3>实现示例（使用Python的Transformers库）</h3>
<p>以下是使用Transformers库加载BERT和ELMo嵌入的示例代码：</p>
<p><strong>BERT嵌入</strong>：</p>
<pre><code class="language-python">from transformers import BertTokenizer, BertModel
import torch

# 加载预训练的BERT模型和tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# 输入文本
text = &quot;Here is some text to encode.&quot;
inputs = tokenizer(text, return_tensors='pt')

# 获取词嵌入
with torch.no_grad():
    outputs = model(**inputs)
    last_hidden_states = outputs.last_hidden_state

print(&quot;BERT嵌入：&quot;, last_hidden_states)
</code></pre>
<p><strong>ELMo嵌入</strong>（使用TensorFlow Hub）：</p>
<pre><code class="language-python">import tensorflow as tf
import tensorflow_hub as hub

# 加载预训练的ELMo模型
elmo = hub.load(&quot;https://tfhub.dev/google/elmo/3&quot;)

# 输入文本
texts = [&quot;Here is some text to encode.&quot;]

# 获取词嵌入
embeddings = elmo(texts, signature=&quot;default&quot;, as_dict=True)[&quot;elmo&quot;]

print(&quot;ELMo嵌入：&quot;, embeddings)
</code></pre>
<h3>参考文献</h3>
<ol>
<li>
<p><strong>Peters, Matthew E., et al. &quot;Deep contextualized word representations.&quot; arXiv preprint arXiv:1802.05365 (2018).</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/1802.05365">论文链接</a></li>
</ul>
</li>
<li>
<p><strong>Devlin, Jacob, et al. &quot;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.&quot; arXiv preprint arXiv:1810.04805 (2018).</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/1810.04805">论文链接</a></li>
</ul>
</li>
<li>
<p><strong>TensorFlow Hub: ELMo</strong></p>
<ul>
<li><a href="https://tfhub.dev/google/elmo/3">ELMo on TensorFlow Hub</a></li>
</ul>
</li>
<li>
<p><strong>Hugging Face Transformers Documentation</strong></p>
<ul>
<li><a href="https://huggingface.co/transformers/">Transformers Documentation</a></li>
</ul>
</li>
</ol>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  