
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>09-metric in decision tree</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <blockquote>
<p>决策树有哪些划分指标？区别与联系？</p>
</blockquote>
<p>决策树是一种广泛应用于分类和回归任务的非参数监督学习算法。决策树的构建过程中，选择最优特征进行数据划分是至关重要的步骤。以下是几种常用的决策树划分指标，它们在选择最优特征时有不同的准则和计算方法。</p>
<h3>1. 信息增益（Information Gain）</h3>
<p><strong>定义</strong>：
信息增益衡量的是通过一个特征划分数据集后，信息熵的减少量。信息熵衡量的是数据集的不确定性。</p>
<p><strong>计算方法</strong>：</p>
<ul>
<li>
<p>信息熵（Entropy）：
$$
H(D) = -\sum_{i=1}^{n} p_i \log_2 p_i
$$
其中，$p_i$ 是类别 $i$ 在数据集 $D$ 中的概率。</p>
</li>
<li>
<p>信息增益（Information Gain）：
$$
IG(D, A) = H(D) - \sum_{v \in \text{Values}(A)} \frac{|D_v|}{|D|} H(D_v)
$$
其中，$D_v$ 是特征 $A$ 取值为 $v$ 的子集。</p>
</li>
</ul>
<p><strong>优点</strong>：</p>
<ul>
<li>计算简单，易于理解。</li>
<li>常用于分类任务（如ID3算法）。</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>偏向于选择取值较多的特征，容易导致过拟合。</li>
</ul>
<h3>2. 信息增益率（Information Gain Ratio）</h3>
<p><strong>定义</strong>：
信息增益率是信息增益的改进，旨在解决信息增益偏向选择取值较多特征的问题。</p>
<p><strong>计算方法</strong>：</p>
<ul>
<li>
<p>基于信息增益的基础，计算分裂信息（Split Information）：
$$
SI(D, A) = -\sum_{v \in \text{Values}(A)} \frac{|D_v|}{|D|} \log_2 \frac{|D_v|}{|D|}
$$</p>
</li>
<li>
<p>信息增益率（Gain Ratio）：
$$
GR(D, A) = \frac{IG(D, A)}{SI(D, A)}
$$</p>
</li>
</ul>
<p><strong>优点</strong>：</p>
<ul>
<li>解决了信息增益偏向选择取值较多特征的问题。</li>
<li>常用于分类任务（如C4.5算法）。</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>计算较为复杂。</li>
</ul>
<h3>3. 基尼系数（Gini Index）</h3>
<p><strong>定义</strong>：
基尼系数用于衡量数据集的不纯度。基尼系数越小，数据集越纯。</p>
<p><strong>计算方法</strong>：</p>
<ul>
<li>
<p>基尼系数（Gini Index）：
$$
Gini(D) = 1 - \sum_{i=1}^{n} p_i^2
$$
其中，$p_i$ 是类别 $i$ 在数据集 $D$ 中的概率。</p>
</li>
<li>
<p>基于特征 $A$ 的基尼系数：
$$
Gini(D, A) = \sum_{v \in \text{Values}(A)} \frac{|D_v|}{|D|} Gini(D_v)
$$</p>
</li>
</ul>
<p><strong>优点</strong>：</p>
<ul>
<li>计算简单，适用于大多数分类任务。</li>
<li>常用于分类和回归任务（如CART算法）。</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>对样本不平衡敏感。</li>
</ul>
<h3>4. 方差（Variance）</h3>
<p><strong>定义</strong>：
方差用于衡量数值型目标变量的离散程度。常用于回归任务。</p>
<p><strong>计算方法</strong>：</p>
<ul>
<li>方差（Variance）：
$$
\text{Var}(D) = \frac{1}{|D|} \sum_{i=1}^{|D|} (y_i - \bar{y})^2
$$
其中，$y_i$ 是样本的目标值，$\bar{y}$ 是目标值的均值。</li>
</ul>
<p><strong>优点</strong>：</p>
<ul>
<li>适用于回归任务。</li>
<li>简单易计算。</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>对异常值敏感。</li>
</ul>
<h3>区别与联系</h3>
<p><strong>区别</strong>：</p>
<ol>
<li><strong>应用领域</strong>：信息增益和信息增益率主要用于分类任务，而方差主要用于回归任务。基尼系数则可以用于分类和回归任务。</li>
<li><strong>偏向性</strong>：信息增益偏向于选择取值较多的特征，信息增益率通过分裂信息调整了这一偏向性。基尼系数通过度量不纯度来选择特征。</li>
<li><strong>计算复杂度</strong>：信息增益和基尼系数的计算相对简单，而信息增益率的计算较为复杂。</li>
</ol>
<p><strong>联系</strong>：</p>
<ol>
<li><strong>目标</strong>：所有这些指标的目标都是选择最优特征，以最大程度地分割数据，使得分割后的子集更加纯净或更好地拟合目标变量。</li>
<li><strong>基础</strong>：它们都基于某种度量不纯度或不确定性的方式来选择特征。</li>
</ol>
<h3>参考文献</h3>
<ol>
<li>
<p><strong>&quot;Pattern Recognition and Machine Learning&quot; by Christopher M. Bishop</strong>：</p>
<ul>
<li>提供了决策树和划分指标的详细理论和推导。</li>
<li><a href="https://www.springer.com/gp/book/9780387310732">书籍链接</a></li>
</ul>
</li>
<li>
<p><strong>&quot;The Elements of Statistical Learning&quot; by Trevor Hastie, Robert Tibshirani, and Jerome Friedman</strong>：</p>
<ul>
<li>介绍了决策树、划分指标及其他机器学习算法。</li>
<li><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">书籍链接</a></li>
</ul>
</li>
<li>
<p><strong>scikit-learn文档</strong>：</p>
<ul>
<li>提供了决策树算法的实际实现和案例。</li>
<li><a href="https://scikit-learn.org/stable/modules/tree.html">scikit-learn文档</a></li>
</ul>
</li>
</ol>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  