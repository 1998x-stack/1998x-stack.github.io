
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <title>05-graph attention</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=STIX+Two+Math&display=swap">
  <link rel="stylesheet" href="../../markdown.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../../markdown.js"></script>
</head>
<body>
  <div class="container">
    <h2>图像中的注意力机制</h2>
<p>在图像处理中，除了常见的Self-Attention机制外，还有空间注意力和通道注意力两种重要的注意力机制。下面将详细介绍这两种注意力机制及其实现方法。</p>
<h3>一、空间注意力机制</h3>
<p>空间注意力机制（Spatial Attention Mechanism）主要关注图像中不同空间位置的重要性，通过对空间位置的加权来突出显著区域。</p>
<h4>1. 实现步骤</h4>
<ol>
<li>
<p><strong>计算空间注意力图</strong>：</p>
<ul>
<li>将输入特征图沿通道维度进行全局平均池化和全局最大池化，得到两个二维特征图。</li>
<li>将这两个特征图沿通道维度进行拼接，并通过一个卷积层生成空间注意力图。</li>
<li>使用Sigmoid激活函数将空间注意力图的值映射到0到1之间。</li>
</ul>
</li>
<li>
<p><strong>加权输入特征图</strong>：</p>
<ul>
<li>将空间注意力图与原始输入特征图逐元素相乘，得到加权后的特征图。</li>
</ul>
</li>
</ol>
<h4>2. 公式表示</h4>
<p>设输入特征图为 $ \mathbf{F} $，其空间注意力图为 $ \mathbf{M}_s $，则空间注意力图的计算公式为：</p>
<p>$ \mathbf{M}_s = \sigma(f^{7 \times 7}([\text{AvgPool}(\mathbf{F}); \text{MaxPool}(\mathbf{F})])) $</p>
<p>其中，$\sigma$ 表示Sigmoid激活函数，$f^{7 \times 7}$ 表示卷积核大小为 $7 \times 7$ 的卷积操作，$[\cdot; \cdot]$ 表示在通道维度上的拼接操作。</p>
<p>最终的加权特征图为：</p>
<p>$ \mathbf{F}' = \mathbf{M}_s \odot \mathbf{F} $</p>
<h3>二、通道注意力机制</h3>
<p>通道注意力机制（Channel Attention Mechanism）主要关注图像特征图中不同通道的重要性，通过对通道进行加权来突出重要通道。</p>
<h4>1. 实现步骤</h4>
<ol>
<li>
<p><strong>计算通道注意力图</strong>：</p>
<ul>
<li>将输入特征图沿空间维度进行全局平均池化和全局最大池化，得到两个一维特征图。</li>
<li>将这两个一维特征图通过一个共享的全连接层，再通过ReLU激活函数，最后通过另一个全连接层生成通道注意力图。</li>
<li>使用Sigmoid激活函数将通道注意力图的值映射到0到1之间。</li>
</ul>
</li>
<li>
<p><strong>加权输入特征图</strong>：</p>
<ul>
<li>将通道注意力图与原始输入特征图逐通道相乘，得到加权后的特征图。</li>
</ul>
</li>
</ol>
<h4>2. 公式表示</h4>
<p>设输入特征图为 $ \mathbf{F} $，其通道注意力图为 $ \mathbf{M}_c $，则通道注意力图的计算公式为：</p>
<p>$ \mathbf{M}_c = \sigma(W_1(\text{ReLU}(W_0(\text{AvgPool}(\mathbf{F}))) + W_1(\text{ReLU}(W_0(\text{MaxPool}(\mathbf{F}))))) $</p>
<p>其中，$\text{AvgPool}$ 和 $\text{MaxPool}$ 分别表示全局平均池化和全局最大池化，$W_0$ 和 $W_1$ 是全连接层的权重矩阵。</p>
<p>最终的加权特征图为：</p>
<p>$ \mathbf{F}' = \mathbf{M}_c \odot \mathbf{F} $</p>
<h3>三、总结</h3>
<p>空间注意力和通道注意力是图像处理中常用的两种注意力机制，它们通过对输入特征图进行加权处理，能够有效地提升特征表示的质量。空间注意力关注图像的显著区域，而通道注意力关注特征图中的重要通道。两者结合使用能够进一步提高图像处理任务的性能。</p>
<h3>参考资料</h3>
<ul>
<li><a href="https://towardsdatascience.com/attention-mechanism-4489a3e5d8a7">Attention Mechanism in Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1807.06521">CBAM: Convolutional Block Attention Module</a></li>
<li><a href="https://arxiv.org/abs/1709.01507">SENet: Squeeze-and-Excitation Networks</a></li>
</ul>

    <h3>Python 文件</h3>
    <pre><code>对应的 Python 文件不存在。</code></pre>
  </div>
</body>
</html>
  