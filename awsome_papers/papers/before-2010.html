<h2>
 before 2010
</h2>
<ul>
 <li>
  A fast learning algorithm for deep belief nets.  [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjmxcXKjtDQAhVEjpQKHU5uAVkQFgggMAA&amp;url=https%3A%2F%2Fwww.cs.toronto.edu%2F~hinton%2Fabsps%2Ffastnc.pdf&amp;usg=AFQjCNELT7wVLLgpvARk6bCARpfzwWUOLg">
   url
  </a>
  ] :star:
 </li>
 <li>
  A Tutorial on Energy-Based Learning.
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiBk8i0jtDQAhVFJJQKHffxCp0QFggdMAA&amp;url=http%3A%2F%2Fyann.lecun.com%2Fexdb%2Fpublis%2Fpdf%2Flecun-06.pdf&amp;usg=AFQjCNHSCWGMFSXY4CSmXrAC4UpJD6izOw">
   [url]
  </a>
 </li>
 <li>
  <b>
   [LeNet]
  </b>
  Gradient-based learning applied to document recognition. [
  <a href="http://yann.lecun.com/exdb/lenet/">
   pdf
  </a>
  ] :star:
 </li>
 <li>
  Constructing Informative Priors using Transfer Learning. [
  <a href="http://ai.stanford.edu/~ang/papers/icml06-transferinformativepriors.pdf">
   url
  </a>
  ]
 </li>
 <li>
  Connectionist Temporal Classification: Labelling unsegmented Sequence Data with Recurrent Neural Networks. [
  <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjf9J2yoo7RAhUoxFQKHZFPD_wQFggfMAA&amp;url=http%3A%2F%2Fmachinelearning.wustl.edu%2Fmlpapers%2Fpaper_files%2Ficml2006_GravesFGS06.pdf&amp;usg=AFQjCNFrqG2eQSvESxvp7EhHYfe9y-gH_Q">
   url
  </a>
  ]
 </li>
 <li>
  Deep Boltzmann Machines.  [
  <a href="http://www.utstat.toronto.edu/~rsalakhu/papers/dbm.pdf">
   url
  </a>
  ] :star:
 </li>
 <li>
  Exploring Strategies for Training Deep Neural Networks.
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiawMrP7NDQAhXJTLwKHeZzBxgQFgggMAA&amp;url=http%3A%2F%2Fdeeplearning.cs.cmu.edu%2Fpdfs%2F1111%2Fjmlr10_larochelle.pdf&amp;usg=AFQjCNE9A4CWIZpcCM4FFVcB5lWL-49mlw">
   [url]
  </a>
 </li>
 <li>
  Efficient Learning of Sparse Representations with an Energy-Based Model.
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjSloOhjtDQAhVBRJQKHaWRAicQFgghMAA&amp;url=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F3112-efficient-learning-of-sparse-representations-with-an-energy-based-model.pdf&amp;usg=AFQjCNFZs1ap9T-WHpdAUtFgX2aFs-38sg">
   [url]
  </a>
  :star:
 </li>
 <li>
  Efficient sparse coding algorithms.
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwipv5r1kNDQAhVDGZQKHXXjC-cQFggiMAA&amp;url=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F2979-efficient-sparse-coding-algorithms.pdf&amp;usg=AFQjCNEZEP5SxMogeVfZA0mmECXQzQXfqQ">
   [url]
  </a>
  :star:
 </li>
 <li>
  Energy-Based Models in Document Recognition and Computer Vision.
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwipucXYkNDQAhUDj5QKHcTrCKMQFggjMAA&amp;url=http%3A%2F%2Fyann.lecun.com%2Fexdb%2Fpublis%2Fpdf%2Flecun-icdar-keynote-07.pdf&amp;usg=AFQjCNFXXuq-tKKteAowMiWkRLhhBl89nA">
   [url]
  </a>
 </li>
 <li>
  Extracting and Composing Robust Features with Denoising Autoencoders.
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiLicmC7NDQAhXFXrwKHY0ADcUQFgggMAA&amp;url=http%3A%2F%2Fmachinelearning.org%2Farchive%2Ficml2008%2Fpapers%2F592.pdf&amp;usg=AFQjCNHhfwA6PKI3gKjnBc36z7Jqs7d0mw">
   [url]
  </a>
  :star:
 </li>
 <li>
  Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition.
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjByO3069DQAhVET7wKHcwSDFQQFggvMAE&amp;url=https%3A%2F%2Farxiv.org%2Fpdf%2F1010.3467&amp;usg=AFQjCNHcNp5zQf6YypllW96kWFpXXMCB7g">
   [url]
  </a>
 </li>
 <li>
  Gaussian Process Models for Link Analysis and Transfer Learning. [
  <a href="http://papers.nips.cc/paper/3284-gaussian-process-models-for-link-analysis-and-transfer-learning.pdf">
   url
  </a>
  ]
 </li>
 <li>
  Greedy Layer-Wise Training of Deep Networks.
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwic_va3kNDQAhWDF5QKHdw7A-YQFgggMAA&amp;url=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F3048-greedy-layer-wise-training-of-deep-networks.pdf&amp;usg=AFQjCNEKqhptR9m0CF7Ygu6UhJD3teRXnQ">
   [url]
  </a>
  :star:
 </li>
 <li>
  Learning Invariant Features through Topographic Filter Maps.
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjPn5m07NDQAhUIvrwKHY0WCKoQFgggMAA&amp;url=http%3A%2F%2Fwww.cs.toronto.edu%2F~ranzato%2Fpublications%2Fkavukcuoglu-cvpr09.pdf&amp;usg=AFQjCNFsBjrEyT8Ct8hlBz2h82ngVcp_wA">
   [url]
  </a>
 </li>
 <li>
  Linear Spatial Pyramid Matching Using Sparse Coding for Image Classification.
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiGkqSn7NDQAhWCa7wKHWtwBSUQFggjMAA&amp;url=http%3A%2F%2Fwww.ifp.illinois.edu%2F~jyang29%2Fpapers%2FCVPR09-ScSPM.pdf&amp;usg=AFQjCNFJtIxHQ6evWkvnfax-9HVg4G8SdQ">
   [url]
  </a>
  :star:
 </li>
 <li>
  Mapping and Revising Markov Logic Networks for Transfer Learning. [
  <a href="http://www.aaai.org/Papers/AAAI/2007/AAAI07-096.pdf">
   url
  </a>
  ]
 </li>
 <li>
  Nonlinear Learning using Local Coordinate Coding.  [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiyobCZ7NDQAhVFgrwKHQ_EC1kQFgggMAA&amp;url=http%3A%2F%2Fece.duke.edu%2F~lcarin%2Fnips09_lcc.pdf&amp;usg=AFQjCNFWboAUuKb5D50fnZFVsx4TWuCwTw">
   url
  </a>
  ] :star:
 </li>
 <li>
  Notes on Convolutional Neural Networks.
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiuh-SFjtDQAhUMjZQKHQ8xAxAQFggiMAA&amp;url=http%3A%2F%2Fcogprints.org%2F5869%2F1%2Fcnn_tutorial.pdf&amp;usg=AFQjCNGqmw7vLOJXSwyHyS6SPTDD5VOiGg">
   [url]
  </a>
 </li>
 <li>
  Reducing the Dimensionality of Data with Neural Networks. [
  <a href="http://science.sciencemag.org/content/313/5786/504">
   science
  </a>
  ] :star:
 </li>
 <li>
  To Recognize Shapes, First Learn to Generate Images.
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjNk4jKjNDQAhUMppQKHROED1cQFgggMAA&amp;url=http%3A%2F%2Fwww.cs.toronto.edu%2F~fritz%2Fabsps%2FmontrealTR.pdf&amp;usg=AFQjCNGmWlKfMB2j-3PWensTW6Q6k9A1uA">
   [url]
  </a>
 </li>
 <li>
  Scaling Learning Algorithms towards AI.  [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjmk-SjkNDQAhVFipQKHapJAKQQFggiMAA&amp;url=http%3A%2F%2Fyann.lecun.com%2Fexdb%2Fpublis%2Fpdf%2Fbengio-lecun-07.pdf&amp;usg=AFQjCNGsg3RffgzLebvpoqnCMK7BFEA-3A">
   url
  </a>
  ] :star:
 </li>
 <li>
  Sparse deep belief net model for visual area V2.  [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjTr5b4j9DQAhUTv5QKHROTAHMQFgggMAA&amp;url=http%3A%2F%2Fai.stanford.edu%2F~ang%2Fpapers%2Fnips07-sparsedeepbeliefnetworkv2.pdf&amp;usg=AFQjCNHRZL9gavkOrCmx0OdMzD9blaUC8Q">
   url
  </a>
  ] :star:
 </li>
 <li>
  Sparse Feature Learning for Deep Belief Networks.
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiNiK3Wj9DQAhUBt5QKHVOsDKIQFgggMAA&amp;url=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F3363-sparse-feature-learning-for-deep-belief-networks.pdf&amp;usg=AFQjCNFNpWYDTG49fBdogmG-7L4tDgM7kQ">
   [url]
  </a>
 </li>
 <li>
  Training restricted Boltzmann machines using approximations to the likelihood gradient.
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjH1Kvp69DQAhVEVbwKHVo8A70QFgggMAA&amp;url=http%3A%2F%2Fwww.machinelearning.org%2Farchive%2Ficml2008%2Fpapers%2F638.pdf&amp;usg=AFQjCNF2KMp3ZrdqkeUwe0v_jYoEGmuPDg">
   [url]
  </a>
 </li>
 <li>
  Training Products of experts by minimizing contrastive divergence
  .
  <a href="Training Products of Experts by Minimizing Contrastive Divergence">
   [url]
  </a>
  ] :star:
 </li>
 <li>
  Using Fast Weights to Improve Persistent Contrastive Divergence.  [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiNt4WO7NDQAhVGybwKHR5yC6sQFgggMAA&amp;url=http%3A%2F%2Fwww.cs.toronto.edu%2F~tijmen%2Ffpcd%2Ffpcd.ps.gz&amp;usg=AFQjCNEfcQVHseNmdUyK1q6nVcyYM-9-dQ">
   url
  </a>
  ] :star:
 </li>
 <li>
  Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition.  [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjjrqmLj9DQAhUEI5QKHWe2CCcQFgggMAA&amp;url=http%3A%2F%2Fyann.lecun.com%2Fexdb%2Fpublis%2Fpdf%2Franzato-cvpr-07.pdf&amp;usg=AFQjCNFwvKjLRcBMth7fYufJcCZzmirlOw">
   url
  </a>
  ]
 </li>
 <li>
  What is the Best Multi-Stage Architecture for Object Recognition?.  [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwi5pMD46tDQAhUBxrwKHcVNBSUQFggdMAA&amp;url=http%3A%2F%2Fyann.lecun.com%2Fexdb%2Fpublis%2Fpdf%2Fjarrett-iccv-09.pdf&amp;usg=AFQjCNFNWVpfiBL6O_xYc_FzAf2RF2VyTw">
   url
  </a>
  ] :star:
 </li>
</ul>
<h3>
 Transfer learning
</h3>
<ul>
 <li>
  A Survey on Transfer Learning.
  <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwj2zKuZr47RAhVqqVQKHWk0BtYQFgg5MAI&amp;url=%68%74%74%70%3a%2f%2f%63%73%2e%67%6d%75%2e%65%64%75%2f%7e%63%61%72%6c%6f%74%74%61%2f%74%65%61%63%68%69%6e%67%2f%43%53%37%37%35%2d%73%31%30%2f%72%65%61%64%69%6e%67%73%2f%74%72%61%6e%73%66%65%72%6c%65%61%72%6e%69%6e%67%2e%70%64%66&amp;usg=AFQjCNFwymAVN8XeRhulhDnWBSc_ErtzYA">
   [url]
  </a>
  ] :star:
 </li>
 <li>
  Modeling Transfer Relationships Between Learning Tasks for Improved Inductive Transfer. [
  <a href="http://maple.cs.umbc.edu/papers/ModelingTransferRelationships.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  To Transfer or Not To Transfer.[
  <a href="http://web.engr.oregonstate.edu/~tgd/publications/rosenstein-marx-kaelbling-dietterich-hnb-nips2005-transfer-workshop.pdf">
   url
  </a>
  ]
 </li>
 <li>
  Transfer learning for text classification. [
  <a href="http://robotics.stanford.edu/~ang/papers/nips05-transfer.pdf">
   url
  </a>
  ]
 </li>
 <li>
  Transfer learning for collaborative filtering via a rating-matrix generative model.[
  <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=4&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjNs6n5so7RAhVJrVQKHbKsDFEQFghEMAM&amp;url=http%3A%2F%2Fvideolectures.net%2Fsite%2Fnormal_dl%2Ftag%3D47942%2Ficml09_li_tlcfvrmgm_01.pdf&amp;usg=AFQjCNGUCLWwmiR1zhCb6L_BzmMHyB6z5w">
   url
  </a>
  ]
 </li>
 <li>
  Transfer learning from multiple source domains via consensus regularization. [
  <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiloLWpuo7RAhWqrFQKHbyMD-kQFggwMAI&amp;url=http%3A%2F%2Fwww3.ntu.edu.sg%2Fhome%2Fsinnopan%2Fpublications%2F%5BEMCL14%5DTransfer%2520Learning%2520with%2520Multiple%2520Sources%2520via%2520Consensus%2520Regularized%2520Autoencoders.pdf&amp;usg=AFQjCNFi6ftA2gms7vi71-ktkt_WTum46Q">
   url
  </a>
  ]
 </li>
 <li>
  Transfer Learning for Reinforcement Learning Domains: A Survey. [
  <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiu9KmlsY7RAhXJiVQKHTs_B34QFggfMAA&amp;url=http%3A%2F%2Fwww.cs.utexas.edu%2F~ai-lab%2Fpubs%2FJMLR09-taylor.pdf&amp;usg=AFQjCNH-Irnjnro_TgQx2T9D8mAz4KwHhw">
   url
  </a>
  ] :star:
 </li>
 <li>
  <b>
   [Zero-Shot]
  </b>
  Zero-Shot Learning with Semantic Output Codes.
  <a href="http://www.cs.cmu.edu/afs/cs/project/theo-73/www/papers/zero-shot-learning.pdf">
   <code>
    pdf
   </code>
  </a>
  :star:
 </li>
</ul>
<h4>
 Instance transfer
</h4>
<ul>
 <li>
  An improved categorization of classifierâ€™s sensitivity on sample selection bias. [
  <a href="https://pdfs.semanticscholar.org/c43c/f8d7fbb41bb9c415f83574ea251b0273acbe.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Boosting for transfer learning. [
  <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjb4qT6r47RAhVDl1QKHfYYAfIQFgghMAA&amp;url=http%3A%2F%2Fftp.cse.ust.hk%2F~qyang%2FDocs%2F2007%2Ftradaboost.pdf&amp;usg=AFQjCNHrEGRLIvusHTEIwYmIGRUsUqeuPw">
   url
  </a>
  ] :star:
  <ul>
   <li>
    A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting. [
    <a href="http://www.face-rec.org/algorithms/Boosting-Ensemble/decision-theoretic_generalization.pdf">
     pdf
    </a>
    ] :star:
   </li>
  </ul>
 </li>
 <li>
  Correcting sample selection bias by unlabeled data. [
  <a href="https://papers.nips.cc/paper/3075-correcting-sample-selection-bias-by-unlabeled-data.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Cross domain distribution adaptation via kernel mapping. [
  <a href="http://xueshu.baidu.com/s?wd=paperuri%3A%28561d2ced4ef800e5c475d510a9b9cb2b%29&amp;filter=sc_long_sign&amp;tn=SE_xueshusource_2kduw22v&amp;sc_vurl=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Bjsessionid%3DD2DFDC65A233E29C69866A058656020A%3Fdoi%3D10.1.1.157.9773%26rep%3Drep1%26type%3Dpdf&amp;ie=utf-8&amp;sc_us=6113889194888417404">
   pdf
  </a>
  ]
 </li>
 <li>
  Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation.[
  <a href="https://papers.nips.cc/paper/3248-direct-importance-estimation-with-model-selection-and-its-application-to-covariate-shift-adaptation.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Discriminative learning for differing training and test distributions. [
  <a href="http://machinelearning.org/proceedings/icml2007/papers/303.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Domain Adaptation via Transfer Component Analysis. [
  <a href="http://home.cse.ust.hk/~qyang/Docs/2009/TCA.pdf">
   pdf
  </a>
  ] :star:
 </li>
 <li>
  Instance Weighting for Domain Adaptation in NLP. [
  <a href="http://sifaka.cs.uiuc.edu/czhai/pub/acl07.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Logistic regression with an auxiliary data source. [
  <a href="http://people.ee.duke.edu/~xjliao/paper/ICML05_MigLogit.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Transferring Naive Bayes Classifiers for Text Classification. [
  <a href="https://www.cse.ust.hk/~qyang/Docs/2007/daiaaai07.pdf">
   pdf
  </a>
  ]
 </li>
</ul>
<h4>
 Feature representation transfer
</h4>
<ul>
 <li>
  A Spectral Regularization Framework for Multi-Task Structure Learning. [
  <a href="https://papers.nips.cc/paper/3187-a-spectral-regularization-framework-for-multi-task-structure-learning.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Biographies, bollywood, boom- boxes and blenders: Domain adaptation for sentiment classification. [
  <a href="https://www.cs.jhu.edu/~mdredze/publications/sentiment_acl07.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Co-clustering based Classification for Out-of-domain Documents. [
  <a href="https://er2004.cse.ust.hk/~qyang/Docs/2007/daikdd.pdf">
   pdf
  </a>
  ] :star:
 </li>
 <li>
  Domain adaptation with structural correspondence learning. [
  <a href="http://john.blitzer.com/papers/emnlp06.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Frustratingly easy domain adaptation. [
  <a href="http://www.umiacs.umd.edu/~hal/docs/daume07easyadapt.pdf">
   pdf
  </a>
  ] :star:
 </li>
 <li>
  Kernel-based inductive transfer. [
  <a href="https://pdfs.semanticscholar.org/45b1/32687d62da38ca2ce0a05e4b52bcf51f1f6f.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Learning a meta-level prior for feature relevance from multiple related tasks. [
  <a href="http://ai.stanford.edu/~koller/Papers/Lee+al:ICML07.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Multi-task feature and kernel selection for svms. [
  <a href="http://www.cs.columbia.edu/~jebara/papers/metalearn.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Multi-task feature learning. [
  <a href="https://papers.nips.cc/paper/3143-multi-task-feature-learning.pdf">
   pdf
  </a>
  ] :star:
 </li>
 <li>
  Self-taught Clustering. [
  <a href="http://www.machinelearning.org/archive/icml2008/papers/432.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Self-taught Learning-Transfer Learning from Unlabeled Data. [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjfz7qOkNDQAhWJoZQKHSVzDOgQFggdMAA&amp;url=http%3A%2F%2Fai.stanford.edu%2F~hllee%2Ficml07-selftaughtlearning.pdf&amp;usg=AFQjCNG71_SrmGXuzHiE5Qo2ugmF96NKgw">
   url
  </a>
  ] :star:
 </li>
 <li>
  Spectral domain-transfer learning. [
  <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjg5ofSs47RAhWGjVQKHbtwB5gQFggsMAE&amp;url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.141.1551&amp;usg=AFQjCNEnGKdMTf64btUV6QsVSn9RgphWLA">
   url
  </a>
  ] :star:
 </li>
 <li>
  Transfer learning via dimensionality reduction. [
  <a href="http://home.cse.ust.hk/~qyang/Docs/2008/AAAIsinnoA.pdf">
   pdf
  </a>
  ]
 </li>
</ul>
<h4>
 Parameter transfer
</h4>
<ul>
 <li>
  Knowledge transfer via multiple model local structure mapping. [
  <a href="http://aisl.umbc.edu/resources/1145.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Learning Gaussian Process Kernels via Hierarchical Bayes. [
  <a href="https://papers.nips.cc/paper/2595-learning-gaussian-process-kernels-via-hierarchical-bayes.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Learning to learn with the informative vector machine. [
  <a href="ftp://ftp.dcs.shef.ac.uk/home/neil/mtivm.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Multi-task Gaussian Process Prediction. [
  <a href="https://papers.nips.cc/paper/3189-multi-task-gaussian-process-prediction.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Regularized multi-task learning. [
  <a href="http://www0.cs.ucl.ac.uk/staff/M.Pontil/reading/mt-kdd.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  The more you know, the less you learn: from knowledge transfer to one-shot learning of object categories.[
  <a href="http://ftp.idiap.ch/pub/courses/EE-700/material/28-11-2012/Tommasi_BMVC_2009.pdf">
   pdf
  </a>
  ]
 </li>
</ul>
<h4>
 Relational knowledge transfer
</h4>
<ul>
 <li>
  Deep transfer via second-order markov logic. [
  <a href="http://homes.cs.washington.edu/~pedrod/papers/mlc09a.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Mapping and revising markov logic networks for transfer learning. [
  <a href="http://dl.acm.org/citation.cfm?id=1619743">
   pdf
  </a>
  ]
 </li>
 <li>
  Transfer learning by mapping with minimal target data. [
  <a href="http://www.cs.utexas.edu/~ml/papers/lily-ws-aaai-08.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Translated learning: Transfer learning across different feature spaces.[
  <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2008_0098.pdf">
   url
  </a>
  ] :star:
 </li>
</ul>
