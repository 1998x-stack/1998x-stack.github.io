<h2>
 Nature language process
</h2>
<ul>
 <li>
  500+ Times Faster Than Deep Learning (A Case Study Exploring Faster Methods for Text Mining StackOverflow).
  <a href="https://arxiv.org/pdf/1802.05319.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation.
  <a href="https://arxiv.org/pdf/1805.06553.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Analysis Methods in Neural Language Processing: A Survey.
  <a href="https://arxiv.org/pdf/1812.08951.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  An end-to-end TextSpotter with Explicit Alignment and Attention.
  <a href="https://arxiv.org/pdf/1803.03474.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/tonghe90/textspotter">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  An Introductory Survey on Attention Mechanisms in NLP Problems.
  <a href="https://arxiv.org/pdf/1811.05544.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Annotation Artifacts in Natural Language Inference Data.
  <a href="https://arxiv.org/pdf/1803.02324.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  A Universal Music Translation Network.
  <a href="https://arxiv.org/pdf/1805.07848.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension.
  <a href="https://arxiv.org/pdf/1810.05682.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Calculating the similarity between words and sentences using a lexical database and corpus statistics.
  <a href="https://arxiv.org/pdf/1802.05667.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Chinese Text in the Wild.
  <a href="https://arxiv.org/pdf/1803.00085.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Content Selection in Deep Learning Models of Summarization.
  <a href="https://arxiv.org/pdf/1810.12343.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Constituency Parsing with a Self-Attentive Encoder.
  <a href="https://arxiv.org/pdf/1805.01052.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/nikitakit/self-attentive-parser">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Embedding Logical Queries on Knowledge Graphs.
  <a href="https://arxiv.org/pdf/1806.01445.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/williamleif/graphqembed">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Extracting Action Sequences from Texts Based on Deep Reinforcement Learning.
  <a href="https://arxiv.org/pdf/1803.02632.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Generating Wikipedia by Summarizing Long Sequences.
  <a href="https://arxiv.org/pdf/1801.10198.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  How Images Inspire Poems: Generating Classical Chinese Poetry from Images with Memory Networks.
  <a href="https://arxiv.org/pdf/1803.02994.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  IncepText: A New Inception-Text Module with Deformable PSROI Pooling for Multi-Oriented Scene Text Detection.
  <a href="https://arxiv.org/pdf/1805.01167.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Linguistically-Informed Self-Attention for Semantic Role Labeling.
  <a href="https://arxiv.org/pdf/1804.08199.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Neural Relational Inference for Interacting Systems.
  <a href="https://arxiv.org/pdf/1802.04687.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ethanfetaya/nri">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Neural Text Generation: Past, Present and Beyond.
  <a href="https://arxiv.org/pdf/1803.07133.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  PixelLink: Detecting Scene Text via Instance Segmentation.
  <a href="https://arxiv.org/pdf/1801.01315.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ZJULearning/pixel_link">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Shape Robust Text Detection with Progressive Scale Expansion Network.
  <a href="https://arxiv.org/pdf/1806.02559.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/whai362/PSENet">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings.
  <a href="https://arxiv.org/pdf/1803.08495.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  TextTopicNet - Self-Supervised Learning of Visual Features Through Embedding Images on Semantic Text Spaces.
  <a href="https://arxiv.org/pdf/1807.02110.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/lluisgomez/TextTopicNet">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Using J-K fold Cross Validation to Reduce Variance When Tuning NLP Models.
  <a href="https://arxiv.org/pdf/1806.07139.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/henrymoss/COLING2018">
   <code>
    code
   </code>
  </a>
 </li>
</ul>
<h3>
 Chatbot
</h3>
<ul>
 <li>
  A Deep Reinforcement Learning Chatbot (Short Version).
  <a href="https://arxiv.org/pdf/1801.06700.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Dialogue Generation: From Imitation Learning to Inverse Reinforcement Learning.
  <a href="https://arxiv.org/pdf/1812.03509.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Goal-Oriented Chatbot Dialog Management Bootstrapping with Transfer Learning.
  <a href="https://arxiv.org/pdf/1802.00500.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems.
  <a href="https://arxiv.org/pdf/1804.08217.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/HLTCHKUST/Mem2Seq">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  The Design and Implementation of XiaoIce, an Empathetic Social Chatbot.
  <a href="https://arxiv.org/pdf/1812.08989.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h3>
 Embeddings
</h3>
<ul>
 <li>
  An efficient framework for learning sentence representations.
  <a href="https://arxiv.org/pdf/1803.02893.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks.
  <a href="https://arxiv.org/pdf/1811.06031.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/huggingface/hmtl">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings.
  <a href="https://arxiv.org/pdf/1805.06297.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/artetxem/vecmap">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Beyond Word Embeddings: Learning Entity and Concept Representations from Large Scale Knowledge Bases.
  <a href="https://arxiv.org/pdf/1801.00388.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Concept2vec: Metrics for Evaluating Quality of Embeddings for Ontological Concepts.
  <a href="https://arxiv.org/pdf/1803.04488.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deep contextualized word representations.
  <a href="https://arxiv.org/pdf/1802.05365.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Evaluating Compositionality in Sentence Embeddings.
  <a href="https://arxiv.org/pdf/1802.04302.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ishita-dg/ScrambleTests">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  From Word to Sense Embeddings: A Survey on Vector Representations of Meaning.
  <a href="https://arxiv.org/pdf/1805.04032.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning Domain-Sensitive and Sentiment-Aware Word Embeddings.
  <a href="https://arxiv.org/pdf/1805.03801.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning Role-based Graph Embeddings.
  <a href="https://arxiv.org/pdf/1802.02896.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/benedekrozemberczki/role2vec">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning Word Embeddings for Low-resource Languages by PU Learning.
  <a href="https://arxiv.org/pdf/1805.03366.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  On the Dimensionality of Word Embedding.
  <a href="https://arxiv.org/pdf/1812.04224.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ziyin-dl/word-embedding-dimensionality-selection">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Poincar√© GloVe: Hyperbolic Word Embeddings.
  <a href="https://arxiv.org/pdf/1810.06546.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Query2Vec: NLP Meets Databases for Generalized Workload Analytics.
  <a href="https://arxiv.org/pdf/1801.05613.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.
  <a href="https://arxiv.org/pdf/1802.01241.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Speech2Vec: A Sequence-to-Sequence Framework for Learning Word Embeddings from Speech.
  <a href="https://arxiv.org/pdf/1803.08976.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Word Embedding Attention Network: Generating Words by Querying Distributed Word Representations for Paraphrase Generation.
  <a href="https://arxiv.org/pdf/1803.01465.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/lancopku/WEAN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Word2Bits - Quantized Word Vectors.
  <a href="https://arxiv.org/pdf/1803.05651.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/agnusmaximus/Word2Bits">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
</ul>
<h3>
 Keyphrase Extraction
</h3>
<ul>
 <li>
  EmbedRank: Unsupervised Keyphrase Extraction using Sentence Embeddings.
  <a href="https://arxiv.org/pdf/1801.04470.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h3>
 Knowledge Graphs
</h3>
<ul>
 <li>
  Variational Knowledge Graph Reasoning.
  <a href="https://arxiv.org/pdf/1803.06581.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h3>
 Language Model
</h3>
<ul>
 <li>
  An Analysis of Neural Language Modeling at Multiple Scales.
  <a href="https://arxiv.org/pdf/1803.08240.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/salesforce/awd-lstm-lm">
   <code>
    code
   </code>
  </a>
 </li>
</ul>
<h3>
 Pos-tagging
</h3>
<ul>
 <li>
  Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks.
  <a href="https://arxiv.org/pdf/1801.07772.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/boknilev/nmt-repr-analysis">
   <code>
    code
   </code>
  </a>
 </li>
</ul>
<h3>
 QA
</h3>
<ul>
 <li>
  A Corpus for Modeling Word Importance in Spoken Dialogue Transcripts.
  <a href="https://arxiv.org/pdf/1801.09746.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Question-Focused Multi-Factor Attention Network for Question Answering.
  <a href="https://arxiv.org/pdf/1801.08290.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/nusnlp/amanda">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Being curious about the answers to questions: novelty search with learned attention.
  <a href="https://arxiv.org/pdf/1806.00201.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/arayabrain/QuestionDrivenNovelty">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Bilinear Attention Networks.
  <a href="https://arxiv.org/pdf/1805.07932.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/jnhwkim/ban-vqa">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  EARL: Joint Entity and Relation Linking for Question Answering over Knowledge Graphs.
  <a href="https://arxiv.org/pdf/1801.03825.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//AskNowQA/EARL">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Finding ReMO (Related Memory Object): A Simple Neural Architecture for Text based Reasoning.
  <a href="https://arxiv.org/pdf/1801.08459.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/juung/RMN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  The Natural Language Decathlon: Multitask Learning as Question Answering.
  <a href="https://arxiv.org/pdf/1806.08730.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge.
  <a href="https://arxiv.org/pdf/1803.05457.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/allenai/arc-solvers">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  TVQA: Localized, Compositional Video Question Answering.
  <a href="https://arxiv.org/pdf/1809.01696.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Visual Question Generation as Dual Task of Visual Question Answering.
  <a href="http://cvboy.com/publication/cvpr2018_iqan/">
   <code>
    url
   </code>
  </a>
  <a href="https://github.com/yikang-li/iQAN">
   <code>
    code
   </code>
  </a>
 </li>
</ul>
<h3>
 NER
</h3>
<ul>
 <li>
  Chinese NER Using Lattice LSTM.
  <a href="https://arxiv.org/pdf/1805.02023.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/jiesutd/LatticeLSTM">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Named Entity Disambiguation using Deep Learning on Graphs.
  <a href="https://arxiv.org/pdf/1810.09164.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/contextscout/ned-graphs">
   <code>
    code
   </code>
  </a>
 </li>
</ul>
<h3>
 NMT
</h3>
<ul>
 <li>
  Achieving Human Parity on Automatic Chinese to English News Translation.
  <a href="https://arxiv.org/pdf/1803.05567.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.
  <a href="https://arxiv.org/pdf/1805.01565.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Phrase-Based &amp; Neural Unsupervised Machine Translation.
  <a href="https://arxiv.org/pdf/1804.07755.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Self-Attention with Relative Position Representations.
  <a href="https://arxiv.org/pdf/1803.02155.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h3>
 Recommender Systems
</h3>
<ul>
 <li>
  Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba.
  <a href="https://arxiv.org/pdf/1803.02349.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  DKN: Deep Knowledge-Aware Network for News Recommendation.
  <a href="https://arxiv.org/pdf/1801.08284.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Explainable Recommendation: A Survey and New Perspectives.
  <a href="https://arxiv.org/pdf/1804.11192.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  FashionNet: Personalized Outfit Recommendation with Deep Neural Network.
  <a href="https://arxiv.org/pdf/1810.02443.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Graph Convolutional Neural Networks for Web-Scale Recommender Systems.
  <a href="https://arxiv.org/pdf/1806.01973.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning Tree-based Deep Model for Recommender Systems.
  <a href="https://arxiv.org/pdf/1801.02294.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  MARS: Memory Attention-Aware Recommender System.
  <a href="https://arxiv.org/pdf/1805.07037.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Offline A/B testing for Recommender Systems.
  <a href="https://arxiv.org/pdf/1801.07030.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System.
  <a href="https://arxiv.org/pdf/1809.07428.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/graytowne/rank_distill">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Sequence-Aware Recommender Systems.
  <a href="https://arxiv.org/pdf/1802.08452.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Top-K Off-Policy Correction for a REINFORCE Recommender System.
  <a href="https://arxiv.org/pdf/1812.02353.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h3>
 Sentiment Analysis
</h3>
<ul>
 <li>
  Combination of Domain Knowledge and Deep Learning for Sentiment Analysis.
  <a href="https://arxiv.org/pdf/1806.08760.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Word2Vec and Doc2Vec in Unsupervised Sentiment Analysis of Clinical Discharge Summaries.
  <a href="https://arxiv.org/pdf/1805.00352.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h3>
 Speech Recognition
</h3>
<ul>
 <li>
  Automatic Detection of Online Jihadist Hate Speech.
  <a href="https://arxiv.org/pdf/1803.04596.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Do WaveNets Dream of Acoustic Waves?
  <a href="https://arxiv.org/pdf/1802.08370.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  End-to-End Speech Recognition From the Raw Waveform.
  <a href="https://arxiv.org/pdf/1806.07098.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Large-Scale Visual Speech Recognition.
  <a href="https://arxiv.org/pdf/1807.05162.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Speech Emotion Recognition with Data Augmentation and Layer-wise Learning Rate Adjustment.
  <a href="https://arxiv.org/pdf/1802.05630.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Stochastic WaveNet: A Generative Latent Variable Model for Sequential Data.
  <a href="https://arxiv.org/pdf/1806.06116.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  VoxCeleb2: Deep Speaker Recognition.
  <a href="https://arxiv.org/pdf/1806.05622.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h3>
 Text Classification
</h3>
<ul>
 <li>
  Clinical Text Classification with Rule-based Features and Knowledge-guided Convolutional Neural Networks.
  <a href="https://arxiv.org/pdf/1807.07425.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Fake News Identification on Twitter with Hybrid CNN and RNN Models.
  <a href="https://arxiv.org/pdf/1806.11316.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Fine-tuned Language Models for Text Classification.
  <a href="https://arxiv.org/pdf/1801.06146.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Joint Embedding of Words and Labels for Text Classification.
  <a href="https://arxiv.org/pdf/1805.04174.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/guoyinwang/LEAM">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Online Embedding Compression for Text Classification using Low Rank Matrix Factorization.
  <a href="https://arxiv.org/pdf/1811.00641.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines.
  <a href="https://arxiv.org/pdf/1805.06061.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/Noahs-ARK/soft_patterns">
   <code>
    code
   </code>
  </a>
 </li>
</ul>
