<h1>
 Deep learning
</h1>
<ul>
 <li>
  Accelerating CNN inference on FPGAs: A Survey.
  <a href="https://arxiv.org/pdf/1806.01683.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Adaptive Neural Trees.
  <a href="https://arxiv.org/pdf/1807.06699.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Adding One Neuron Can Eliminate All Bad Local Minima.
  <a href="https://arxiv.org/pdf/1805.08671.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Dual Approach to Scalable Verification of Deep Networks.
  <a href="https://arxiv.org/pdf/1803.06567.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Geometric Theory of Higher-Order Automatic Differentiation.
  <a href="https://arxiv.org/pdf/1812.11592.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A graph-embedded deep feedforward network for disease outcome classification and feature selection using gene expression data.
  <a href="https://arxiv.org/pdf/1801.06202.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/yunchuankong/NetworkNeuralNetwork">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution.
  <a href="https://eng.uber.com/coordconv/">
   <code>
    url
   </code>
  </a>
  :star:
 </li>
 <li>
  A Survey on Neural Network-Based Summarization Methods.
  <a href="https://arxiv.org/pdf/1804.04589.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Tutorial on Network Embeddings.
  <a href="https://arxiv.org/pdf/1808.02590.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Unified Probabilistic Model for Learning Latent Factors and Their Connectivities from High-Dimensional Data.
  <a href="https://arxiv.org/pdf/1805.09567.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Backdrop: Stochastic Backpropagation.
  <a href="https://arxiv.org/pdf/1806.01337.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/dexgen/backdrop">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Batch Kalman Normalization: Towards Training Deep Neural Networks with Micro-Batches.
  <a href="https://arxiv.org/pdf/1802.03133.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Bayesian Convolutional Neural Networks.
  <a href="https://arxiv.org/pdf/1806.05978.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/felix-laumann/Bayesian_CNN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Bayesian Deep Convolutional Encoder-Decoder Networks for Surrogate Modeling and Uncertainty Quantification.
  <a href="https://arxiv.org/pdf/1801.06879.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Bayesian Layers: A Module for Neural Network Uncertainty.
  <a href="https://arxiv.org/pdf/1812.03973.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Bayesian Neural Networks.
  <a href="https://arxiv.org/pdf/1801.07710.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/mullachv/MLExp">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Bayesian Optimization in AlphaGo.
  <a href="https://arxiv.org/pdf/1812.06855.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs.
  <a href="https://arxiv.org/pdf/1801.05453.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  BindsNET: A machine learning-oriented spiking neural networks library in Python.
  <a href="https://arxiv.org/pdf/1806.01423.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/Hananel-Hazan/bindsnet">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Capturing Structure Implicitly from Time-Series having Limited Data.
  <a href="https://arxiv.org/pdf/1803.05867.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/emaasit/long-range-extrapolation">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Class label autoencoder for zero-shot learning.
  <a href="https://arxiv.org/pdf/1801.08301.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Closing the AI Knowledge Gap.
  <a href="https://arxiv.org/pdf/1803.07233.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Clustering with Deep Learning: Taxonomy and New Methods.
  <a href="https://arxiv.org/pdf/1801.07648.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/elieJalbout/Clustering-with-Deep-learning">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Collaborative Multi-modal deep learning for the personalized product retrieval in Facebook Marketplace.
  <a href="https://arxiv.org/pdf/1805.12312.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Conditional Neural Processes.
  <a href="https://arxiv.org/pdf/1807.01613.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Convolutional Neural Networks with Recurrent Neural Filters.
  <a href="https://arxiv.org/pdf/1808.09315.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/bloomberg/cnn-rnf">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Data Dropout: Optimizing Training Data for Convolutional Neural Networks.
  <a href="https://arxiv.org/pdf/1809.00193.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Decorrelated Batch Normalization.
  <a href="https://arxiv.org/pdf/1804.08450.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/umich-vl/DecorrelatedBN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Decoupled Networks.
  <a href="https://arxiv.org/pdf/1804.08071.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/wy1iu/DCNets">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Deep Convolutional Networks as shallow Gaussian Processes.
  <a href="https://arxiv.org/pdf/1808.05587.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/rhaps0dy/convnets-as-gps">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Deep Embedding Kernel.
  <a href="https://arxiv.org/pdf/1804.05806.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deep Graph Infomax.
  <a href="https://arxiv.org/pdf/1809.10341.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations.
  <a href="https://arxiv.org/pdf/1801.06637.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/maziarraissi/DeepHPMs">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions.
  <a href="https://arxiv.org/pdf/1806.09228.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/Sandbox3aster/Deep-K-Means-pytorch">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning.
  <a href="https://arxiv.org/pdf/1803.04765.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deep Learning.
  <a href="https://arxiv.org/pdf/1807.07987.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deep Learning using Rectified Linear Units (ReLU).
  <a href="https://arxiv.org/pdf/1803.08375.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/AFAgarap/relu-classifier">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Deep Learning with the Random Neural Network and its Applications.
  <a href="https://arxiv.org/pdf/1810.08653.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deep Multimodal Subspace Clustering Networks.
  <a href="https://arxiv.org/pdf/1804.06498.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deep Neural Decision Trees.
  <a href="https://arxiv.org/pdf/1806.06988.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/wOOL/DNDT">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Deep Self-Organization: Interpretable Discrete Representation Learning on Time Series.
  <a href="https://arxiv.org/pdf/1806.02199.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deep Super Learner: A Deep Ensemble for Classification Problems.
  <a href="https://arxiv.org/pdf/1803.02323.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/levyben/DeepSuperLearner">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Detail-Preserving Pooling in Deep Networks.
  <a href="https://arxiv.org/pdf/1804.04076.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Detecting Dead Weights and Units in Neural Networks.
  <a href="https://arxiv.org/pdf/1806.06068.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Digging Into Self-Supervised Monocular Depth Estimation.
  <a href="https://arxiv.org/pdf/1806.01260.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Don't Use Large Mini-Batches, Use Local SGD.
  <a href="https://arxiv.org/pdf/1808.07217.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Dropout is a special case of the stochastic delta rule: faster and more accurate deep learning.
  <a href="https://arxiv.org/pdf/1808.03578.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  DroNet: Learning to Fly by Driving.
  <a href="http://rpg.ifi.uzh.ch/docs/RAL18_Loquercio.pdf">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/uzh-rpg/rpg_public_dronet">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Dynamic Graph Neural Networks.
  <a href="https://arxiv.org/pdf/1810.10627.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Dynamic Planning Networks.
  <a href="https://arxiv.org/pdf/1812.11240.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++.
  <a href="https://arxiv.org/pdf/1803.09693.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/davidjesusacu/polyrnn-pp">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Efficient Neural Architecture Search ia Parameter Sharing.
  <a href="https://arxiv.org/pdf/1802.03268.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/carpedm20/ENAS-pytorch">
   <code>
    pytorch
   </code>
  </a>
  <a href="https://github.com/melodyguan/enas">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Entropy and mutual information in models of deep neural networks.
  <a href="https://arxiv.org/pdf/1805.09785.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  EcoRNN: Fused LSTM RNN Implementation with Data Layout Optimization.
  <a href="https://arxiv.org/pdf/1805.08899.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  E-swish: Adjusting Activations to Different Network Depths.
  <a href="https://arxiv.org/pdf/1801.07145.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Etymo: A New Discovery Engine for AI Research.
  <a href="https://arxiv.org/pdf/1801.08573.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Evaluating Feature Importance Estimates.
  <a href="https://arxiv.org/pdf/1806.10758.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Extremely Fast Decision Tree.
  <a href="https://arxiv.org/pdf/1802.08780.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/chaitanya-m/kdd2018">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  Eyeriss v2: A Flexible and High-Performance Accelerator for Emerging Deep Neural Networks.
  <a href="https://arxiv.org/pdf/1807.07928.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Fast Decoding in Sequence Models using Discrete Latent Variables.
  <a href="https://arxiv.org/pdf/1803.03382.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling.
  <a href="https://arxiv.org/pdf/1801.10247.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/matenure/FastGCN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Fast Neural Architecture Search of Compact Semantic Segmentation Models via Auxiliary Cells.
  <a href="https://arxiv.org/pdf/1810.10804.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Foundations of Sequence-to-Sequence Modeling for Time Series.
  <a href="https://arxiv.org/pdf/1805.03714.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  From Nodes to Networks: Evolving Recurrent Neural Networks.
  <a href="https://arxiv.org/pdf/1803.04439.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network.
  <a href="https://arxiv.org/pdf/1808.03314.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Gaussian Process Behaviour in Wide Deep Neural Networks.
  <a href="https://arxiv.org/pdf/1804.11271.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/widedeepnetworks/widedeepnetworks">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels.
  <a href="https://arxiv.org/pdf/1805.07836.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Geometric Understanding of Deep Learning.
  <a href="https://arxiv.org/pdf/1805.10451.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  GossipGraD: Scalable Deep Learning using Gossip Communication based Asynchronous Gradient Descent.
  <a href="https://arxiv.org/pdf/1803.05880.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Gradient Acceleration in Activation Functions.
  <a href="https://arxiv.org/pdf/1806.09783.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Graph Capsule Convolutional Neural Networks.
  <a href="https://arxiv.org/pdf/1805.08090.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/vermaMachineLearning/Graph-Capsule-CNN-Networks/">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Graph Partition Neural Networks for Semi-Supervised Classification.
  <a href="https://arxiv.org/pdf/1803.06272.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Model.
  <a href="https://arxiv.org/pdf/1802.08773.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/JiaxuanYou/graph-generation">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Group Normalization.
  <a href="https://arxiv.org/pdf/1803.08494.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Hierarchical Graph Representation Learning with Differentiable Pooling.
  <a href="https://arxiv.org/pdf/1806.08804.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  High-Accuracy Low-Precision Training.
  <a href="https://arxiv.org/pdf/1803.03383.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  High Dimensional Bayesian Optimization Using Dropout.
  <a href="https://arxiv.org/pdf/1802.05400.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  How Does Batch Normalization Help Optimization.
  <a href="https://arxiv.org/pdf/1805.11604.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  How Powerful are Graph Neural Networks?
  <a href="https://openreview.net/pdf?id=ryGs6iA5Km">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Hybrid Decision Making: When Interpretable Models Collaborate With Black-Box Models.
  <a href="https://arxiv.org/pdf/1802.04346.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/wangtongada/CoBRUSH">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Hybrid Gradient Boosting Trees and Neural Networks for Forecasting Operating Room Data.
  <a href="https://arxiv.org/pdf/1801.07384.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Hyperbolic Neural Networks.
  <a href="https://arxiv.org/pdf/1805.09112.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/dalab/hyperbolic_nn">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  IcoRating: A Deep-Learning System for Scam ICO Identification.
  <a href="https://arxiv.org/pdf/1803.03670.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Impacts of Dirty Data: and Experimental Evaluation.
  <a href="https://arxiv.org/pdf/1803.06071.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/qizhixinhit/Dirty-dataImpacts">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Implicit Autoencoders.
  <a href="https://arxiv.org/pdf/1805.09804.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Improving Multi-Person Pose Estimation using Label Correction.
  <a href="https://arxiv.org/pdf/1811.03331.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Incremental Training of Deep Convolutional Neural Networks.
  <a href="https://arxiv.org/pdf/1803.10232.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Introducing Neuromodulation in Deep Neural Networks to Learn Adaptive Behaviours.
  <a href="https://arxiv.org/pdf/1812.09113.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Labelling as an unsupervised learning problem.
  <a href="https://arxiv.org/pdf/1805.03911.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Large Data and Zero Noise Limits of Graph-Based Semi-Supervised Learning Algorithms.
  <a href="https://arxiv.org/pdf/1805.09450.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Large-Margin Classification in Hyperbolic Space.
  <a href="https://arxiv.org/pdf/1806.00437.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/hhcho/hyplinear">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  LARNN: Linear Attention Recurrent Neural Network.
  <a href="https://arxiv.org/pdf/1808.05578.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning Classifiers with Fenchel-Young Losses: Generalized Entropies, Margins, and Algorithms.
  <a href="https://arxiv.org/pdf/1805.09717.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning Latent Representations in Neural Networks for Clustering through Pseudo Supervision and Graph-based Activity Regularization.
  <a href="https://arxiv.org/pdf/1802.03063.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning Longer-term Dependencies in RNNs with Auxiliary Losses.
  <a href="https://arxiv.org/pdf/1803.00144.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning Networks from Random Walk-Based Node Similarities.
  <a href="https://arxiv.org/pdf/1801.07386.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/cnmusco/graph-similarity-learning">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning to generate classifiers.
  <a href="https://arxiv.org/pdf/1803.11373.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/arayabrain/ClassifierGenerators">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning to Learn Without Labels.
  <a href="https://openreview.net/forum?id=ByoT9Fkvz">
   <code>
    pdf
   </code>
  </a>
  :star:
 </li>
 <li>
  Learning to Make Predictions on Graphs with Autoencoders.
  <a href="https://arxiv.org/pdf/1802.08352.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/vuptran/graph-representation-learning">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning to Reweight Examples for Robust Deep Learning.
  <a href="https://arxiv.org/pdf/1803.09050.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/danieltan07/learning-to-reweight-examples">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning to Zoom: a Saliency-Based Sampling Layer for Neural Networks.
  <a href="https://arxiv.org/pdf/1809.03355.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning Unsupervised Learning Rules.
  <a href="https://arxiv.org/pdf/1804.00222.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/tensorflow/models/tree/master/research/learning_unsupervised_learning">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data.
  <a href="https://arxiv.org/pdf/1812.04227.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Linear Backprop in non-linear networks.
  <a href="https://openreview.net/forum?id=ByfPDyrYim">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Links: A High-Dimensional Online Clustering Method.
  <a href="https://arxiv.org/pdf/1801.10123.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation.
  <a href="https://arxiv.org/pdf/1805.07036.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  LSTM stack-based Neural Multi-sequence Alignment TeCHnique.
  <a href="https://arxiv.org/pdf/1803.00057.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  MemGEN: Memory is All You Need.
  <a href="https://arxiv.org/pdf/1803.11203.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Measuring the Effects of Data Parallelism on Neural Network Training.
  <a href="https://arxiv.org/pdf/1811.03600.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Modeling Attention Flow on Graphs.
  <a href="https://arxiv.org/pdf/1811.00497.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Modeling Dynamics with Deep Transition-Learning Networks.
  <a href="https://arxiv.org/pdf/1802.03497.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Model Selection Techniques -- An Overview.
  <a href="https://arxiv.org/pdf/1810.09583.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Multi-Layered Gradient Boosting Decision Trees.
  <a href="https://arxiv.org/pdf/1806.00007.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Multivariate LSTM-FCNs for Time Series Classification.
  <a href="https://arxiv.org/pdf/1801.04503.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/houshd/MLSTM-FCN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Network Distance Based on Laplacian Flows on Graphs.
  <a href="https://arxiv.org/pdf/1810.02906.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Neural Arithmetic Logic Units.
  <a href="https://arxiv.org/pdf/1808.00508.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Neural Architecture Optimization.
  <a href="https://arxiv.org/pdf/1808.07233.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/renqianluo/NAO">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Neural Architecture Search: A Survey.
  <a href="https://arxiv.org/pdf/1808.05377.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Neural Nearest Neighbors Networks.
  <a href="https://arxiv.org/pdf/1810.12575.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/visinf/n3net/">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Neural Networks Regularization Through Representation Learning.
  <a href="https://arxiv.org/pdf/1807.05292.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Not All Samples Are Created Equal: Deep Learning with Importance Sampling.
  <a href="https://arxiv.org/pdf/1803.00942.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Online normalizer calculation for softmax.
  <a href="https://arxiv.org/pdf/1805.02867.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  [Best Paper] On the Convergence of Adam and Beyond.
  <a href="https://openreview.net/forum?id=ryQu7f-RZ">
   <code>
    pdf
   </code>
  </a>
  :star:
 </li>
 <li>
  On the Theory of Variance Reduction for Stochastic Gradient Monte Carlo.
  <a href="https://arxiv.org/pdf/1802.05431.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Parallel Grid Pooling for Data Augmentation.
  <a href="https://arxiv.org/pdf/1803.11370.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/akitotakeki/pgp-chainer">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding.
  <a href="https://arxiv.org/pdf/1809.07426.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights.
  <a href="https://arxiv.org/pdf/1801.06519.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/arunmallya/piggyback">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Pitfalls of Graph Neural Network Evaluation.
  <a href="https://arxiv.org/pdf/1811.05868.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Pooling is neither necessary nor sufficient for appropriate deformation stability in CNNs.
  <a href="https://arxiv.org/pdf/1804.04438.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Probabilistic Recurrent State-Space Models.
  <a href="https://arxiv.org/pdf/1801.10395.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/andreasdoerr/PR-SSM">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Progress &amp; Compress: A scalable framework for continual learning.
  <a href="https://arxiv.org/pdf/1805.06370.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume.
  <a href="https://arxiv.org/pdf/1709.02371.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/NVlabs/PWC-Net">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Pyramid Stereo Matching Network.
  <a href="https://arxiv.org/pdf/1803.08669.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/JiaRenChang/PSMNet">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Random depthwise signed convolutional neural networks.
  <a href="https://arxiv.org/pdf/1806.05789.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Random Fourier Features for Kernel Ridge Regression: Approximation Bounds and Statistical Guarantees.
  <a href="https://arxiv.org/pdf/1804.09893.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Random Warping Series: A Random Features Method for Time-Series Embedding.
  <a href="https://arxiv.org/pdf/1809.05259.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/IBM/RandomWarpingSeries">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Relational recurrent neural networks.
  <a href="https://arxiv.org/pdf/1806.01822.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Representation Learning with Contrastive Predictive Coding.
  <a href="https://arxiv.org/pdf/1807.03748.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  ResNet with one-neuron hidden layers is a Universal Approximator.
  <a href="https://arxiv.org/pdf/1806.10909.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Revisiting Small Batch Training for Deep Neural Networks.
  <a href="https://arxiv.org/pdf/1804.07612.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  R3-Net: A Deep Network for Multi-oriented Vehicle Detection in Aerial Images and Videos.
  <a href="https://arxiv.org/pdf/1808.05560.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Rotation Equivariance and Invariance in Convolutional Neural Networks.
  <a href="https://arxiv.org/pdf/1805.12301.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Scale-free network clustering in hyperbolic and other random graphs.
  <a href="https://arxiv.org/pdf/1812.03002.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  SlimNets: An Exploration of Deep Model Compression and Acceleration.
  <a href="https://arxiv.org/pdf/1808.00496.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ChristopherSweeney/SlimNets">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Smallify: Learning Network Size while Training.
  <a href="https://arxiv.org/pdf/1806.03723.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Sparsely Connected Convolutional Networks.
  <a href="https://arxiv.org/pdf/1801.05895.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  SparseMAP: Differentiable Sparse Structured Inference.
  <a href="https://arxiv.org/pdf/1802.04223.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  SpectralNet: Spectral Clustering using Deep Neural Networks.
  <a href="https://arxiv.org/pdf/1801.01587.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//kstant0725/SpectralNet">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Spiking Deep Residual Network.
  <a href="https://arxiv.org/pdf/1805.01352.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Spherical CNNs.
  <a href="https://arxiv.org/pdf/1801.10130.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/jonas-koehler/s2cnn">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Step Size Matters in Deep Learning.
  <a href="https://arxiv.org/pdf/1805.08890.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Supervised classification of Dermatological diseases by Deep neural networks.
  <a href="https://arxiv.org/pdf/1802.03752.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://www.dropbox.com/sh/pwe3tqrb2zijexq/AADpQ9WKOdSfTdvHkVtT_GHKa?dl=0">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Supervising Unsupervised Learning with Evolutionary Algorithm in Deep Neural Network.
  <a href="https://arxiv.org/pdf/1803.10397.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Syntax-Aware Language Modeling with Recurrent Neural Networks.
  <a href="https://arxiv.org/pdf/1803.03665.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Realistic Evaluation of Deep Semi-Supervised Learning Algorithms.
  <a href="https://arxiv.org/pdf/1804.09170.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/brain-research/realistic-ssl-evaluation">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Testing Deep Neural Networks.
  <a href="https://arxiv.org/pdf/1803.04792.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/theyoucheng/deepcover">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  The Lottery Ticket Hypothesis: Training Pruned Neural Networks.
  <a href="https://arxiv.org/pdf/1803.03635.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  The Matrix Calculus You Need For Deep Learning.
  <a href="https://arxiv.org/pdf/1802.01528.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Theory and Algorithms for Forecasting Time Series.
  <a href="https://arxiv.org/pdf/1803.05814.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  The Singular Values of Convolutional Layers.
  <a href="https://arxiv.org/pdf/1805.10408.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  The unreasonable effectiveness of the forget gate.
  <a href="https://arxiv.org/pdf/1804.04849.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Time is of the Essence: Machine Learning-based Intrusion Detection in Industrial Time Series Data.
  <a href="https://arxiv.org/pdf/1809.07500.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Time Series Segmentation through Automatic Feature Learning.
  <a href="https://arxiv.org/pdf/1801.05394.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Towards a Theoretical Understanding of Batch Normalization.
  <a href="https://arxiv.org/pdf/1805.10694.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Towards Efficient Large-Scale Graph Neural Network Computing.
  <a href="https://arxiv.org/pdf/1810.08403.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Tracking Network Dynamics: a review of distances and similarity metrics.
  <a href="https://arxiv.org/pdf/1801.07351.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Training convolutional neural networks with megapixel images.
  <a href="https://openreview.net/forum?id=HJ7lIcjoM">
   <code>
    url
   </code>
  </a>
  <a href="https://github.com/DIAGNijmegen/StreamingSGD">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Tree-CNN: A Deep Convolutional Neural Network for Lifelong Learning.
  <a href="https://arxiv.org/pdf/1802.05800.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  t-SNE-CUDA: GPU-Accelerated t-SNE and its Applications to Modern Data.
  <a href="https://arxiv.org/pdf/1807.11824.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/CannyLab/tsne-cuda">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring.
  <a href="https://arxiv.org/pdf/1802.04633.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  TVM: End-to-End Optimization Stack for Deep Learning.
  <a href="https://arxiv.org/pdf/1802.04799.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction.
  <a href="https://arxiv.org/pdf/1802.03426.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/lmcinnes/umap">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Understanding Convolutional Neural Network Training with Information Theory.
  <a href="https://arxiv.org/pdf/1804.06537.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Understanding Individual Decisions of CNNs via Contrastive Backpropagation.
  <a href="https://arxiv.org/pdf/1812.02100.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift.
  <a href="https://arxiv.org/pdf/1801.05134.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Understanding the Loss Surface of Neural Networks for Binary Classification.
  <a href="https://arxiv.org/pdf/1803.00909.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Universal Deep Neural Network Compression.
  <a href="https://arxiv.org/pdf/1802.02271.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  URLNet: Learning a URL Representation with Deep Learning for Malicious URL Detection.
  <a href="https://arxiv.org/pdf/1802.03162.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/Antimalweb/URLNet">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Wasserstein is all you need.
  <a href="https://arxiv.org/pdf/1808.09663.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Weighted Risk Minimization &amp; Deep Learning.
  <a href="https://arxiv.org/pdf/1812.03372.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  What Do We Understand About Convolutional Networks?
  <a href="https://arxiv.org/pdf/1803.08834.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h2>
 Attention
</h2>
<ul>
 <li>
  Attention-based Graph Neural Network for Semi-supervised Learning.
  <a href="https://arxiv.org/pdf/1803.03735.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Attention Solves Your TSP.
  <a href="https://arxiv.org/pdf/1803.08475.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/wouterkool/attention-tsp">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Automatic Instrument Segmentation in Robot-Assisted Surgery Using Deep Learning.
  <a href="https://www.biorxiv.org/content/early/2018/03/03/275867">
   <code>
    url
   </code>
  </a>
 </li>
 <li>
  Compositional Attention Networks for Machine Reasoning.
  <a href="https://arxiv.org/pdf/1803.03067.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/stanfordnlp/mac-network">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Hyperbolic Attention Networks.
  <a href="https://arxiv.org/pdf/1805.09786.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Inference, Learning and Attention Mechanisms that Exploit and Preserve Sparsity in Convolutional Networks.
  <a href="https://arxiv.org/pdf/1801.10585.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/TimoHackel/ILA-SCNN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  MAttNet: Modular Attention Network for Referring Expression Comprehension.
  <a href="https://arxiv.org/pdf/1801.08186.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Reinforced Self-Attention Network: a Hybrid of Hard and Soft Attention for Sequence Modeling.
  <a href="https://arxiv.org/pdf/1801.10296.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Tell Me Where to Look: Guided Attention Inference Network.
  <a href="https://arxiv.org/pdf/1802.10171.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/alokwhitewolf/Guided-Attention-Inference-Network">
   <code>
    CODE
   </code>
  </a>
 </li>
</ul>
<h2>
 Generative learning
</h2>
<ul>
 <li>
  Adversarial Attack on Graph Structured Data.
  <a href="https://arxiv.org/pdf/1806.02371.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Adversarial Attacks Against Medical Deep Learning Systems.
  <a href="https://arxiv.org/pdf/1804.05296.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/sgfin/adversarial-medicine">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Adversarial Classification on Social Networks.
  <a href="https://arxiv.org/pdf/1801.08159.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Adversarial Logit Pairing.
  <a href="https://arxiv.org/pdf/1803.06373.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Adversarial Reprogramming of Neural Networks.
  <a href="https://arxiv.org/pdf/1806.11146.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Adversarial Spheres.
  <a href="https://arxiv.org/pdf/1801.02774.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  AmbientGAN: Generative models from lossy measurements.
  <a href="https://openreview.net/forum?id=Hy7fDog0b">
   <code>
    url
   </code>
  </a>
  <a href="https://github.com/shinseung428/ambientGAN_TF">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  An empirical study on evaluation metrics of generative adversarial networks.
  <a href="https://arxiv.org/pdf/1806.07755.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/xuqiantong/GAN-Metrics">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Anime Style Space Exploration Using Metric Learning and Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1805.07997.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Autoencoding topology.
  <a href="https://arxiv.org/pdf/1803.00156.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  CariGANs: Unpaired Photo-to-Caricature Translation.
  <a href="https://arxiv.org/pdf/1811.00222.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  CartoonGAN: Generative Adversarial Networks for Photo Cartoonization.
  <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2205.pdf">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/Yijunmaverick/CartoonGAN-Test-Pytorch-Torch">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  cGANs with Projection Discriminator.
  <a href="https://openreview.net/pdf?id=ByS1VpgRZ">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/pfnet-research/sngan_projection">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  ClusterGAN : Latent Space Clustering in Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1809.03627.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Compositional GAN: Learning Conditional Image Composition.
  <a href="https://arxiv.org/pdf/1807.07560.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/azadis/CompositionalGAN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Counterfactuals uncover the modular structure of deep generative models.
  <a href="https://arxiv.org/pdf/1812.03253.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  CR-GAN: Learning Complete Representations for Multi-view Generation.
  <a href="https://arxiv.org/pdf/1806.11191.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/bluer555/CR-GAN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Deep Generative Markov State Models.
  <a href="https://arxiv.org/pdf/1805.07601.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/amardt/DeepGenMSM">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Deep Learning for Imbalance Data Classification using Class Expert Generative Adversarial Network.
  <a href="https://arxiv.org/pdf/1807.04585.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  eCommerceGAN : A Generative Adversarial Network for E-commerce.
  <a href="https://arxiv.org/pdf/1801.03244.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Evolving Mario Levels in the Latent Space of a Deep Convolutional Generative Adversarial Network.
  <a href="https://arxiv.org/pdf/1805.00728.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Geometry Score: A Method For Comparing Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1802.02664.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/geom-score/geometry-score">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Generating Handwritten Chinese Characters using CycleGAN.
  <a href="https://arxiv.org/pdf/1801.08624.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/changebo/HCCG-CycleGAN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Generative Adversarial Networks using Adaptive Convolution.
  <a href="https://arxiv.org/pdf/1802.02226.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Improving GANs Using Optimal Transport.
  <a href="https://arxiv.org/pdf/1803.05573.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Inverting The Generator Of A Generative Adversarial Network (II).
  <a href="https://arxiv.org/pdf/1802.05701.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ToniCreswell/InvertingGAN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning Dynamics of Linear Denoising Autoencoders.
  <a href="https://arxiv.org/pdf/1806.05413.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/arnupretorius/lindaedynamics_icml2018">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning Inverse Mappings with Adversarial Criterion.
  <a href="https://arxiv.org/pdf/1802.04504.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/zhangjiyi/FAAE">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  New Losses for Generative Adversarial Learning.
  <a href="https://arxiv.org/pdf/1807.01290.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  On Generation of Adversarial Examples using Convex Programming.
  <a href="https://arxiv.org/pdf/1803.03607.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ebalda/adversarialconvex">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  On the Latent Space of Wasserstein Auto-Encoders.
  <a href="https://arxiv.org/pdf/1802.03761.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Recurrent Neural Network-Based Semantic Variational Autoencoder for Sequence-to-Sequence Learning.
  <a href="https://arxiv.org/pdf/1802.03238.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Scalable Factorized Hierarchical Variational Autoencoder Training.
  <a href="https://arxiv.org/pdf/1804.03201.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/wnhsu/ScalableFHVAE">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Semi-Amortized Variational Autoencoders.
  <a href="https://arxiv.org/pdf/1802.02550.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/harvardnlp/sa-vae">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Semi-supervised Learning on Graphs with Generative Adversarial Nets.
  <a href="https://arxiv.org/pdf/1809.00130.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Siamese networks for generating adversarial examples.
  <a href="https://arxiv.org/pdf/1805.01431.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1803.10892.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/agrimgupta92/sgan">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Sylvester Normalizing Flows for Variational Inference.
  <a href="https://arxiv.org/pdf/1803.05649.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Synthesizing Audio with Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1802.04208.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  tempoGAN: A Temporally Coherent, Volumetric GAN for Super-resolution Fluid Flow.
  <a href="https://arxiv.org/pdf/1801.09710.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  The relativistic discriminator: a key element missing from standard GAN.
  <a href="https://arxiv.org/pdf/1807.00734.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/AlexiaJM/RelativisticGAN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer.
  <a href="https://arxiv.org/pdf/1807.07543.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/brain-research/acai">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Unsupervised Cipher Cracking Using Discrete GANs.
  <a href="https://arxiv.org/pdf/1801.04883.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//for-ai/CipherGAN">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Variational Bayesian Monte Carlo.
  <a href="https://arxiv.org/pdf/1810.05558.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/lacerbi/vbmc">
   <code>
    code
   </code>
  </a>
 </li>
</ul>
<h2>
 Meta Learning
</h2>
<ul>
 <li>
  Bayesian Model-Agnostic Meta-Learning.
  <a href="https://arxiv.org/pdf/1806.03836.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Meta-Learning: A Survey.
  <a href="https://arxiv.org/pdf/1810.03548.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Meta-Learning for Semi-Supervised Few-Shot Classification.
  <a href="https://arxiv.org/pdf/1803.00676.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/renmengye/few-shot-ssl-public">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Reptile: a Scalable Metalearning Algorithm.
  <a href="https://arxiv.org/pdf/1803.02999.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h2>
 Optimization
</h2>
<ul>
 <li>
  Averaging Weights Leads to Wider Optima and Better Generalization.
  <a href="https://arxiv.org/pdf/1803.05407.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Computational Optimal Transport.
  <a href="https://arxiv.org/pdf/1803.00567.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Energy-entropy competition and the effectiveness of stochastic gradient descent in machine learning.
  <a href="https://arxiv.org/pdf/1803.01927.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Gradient Descent Quantizes ReLU Network Features.
  <a href="https://arxiv.org/pdf/1803.08367.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  L4: Practical loss-based stepsize adaptation for deep learning.
  <a href="https://arxiv.org/pdf/1802.05074.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/martius-lab/l4-optimizer">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Sequential Preference-Based Optimization.
  <a href="https://arxiv.org/pdf/1801.02788.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/prefopt/prefopt">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Sever: A Robust Meta-Algorithm for Stochastic Optimization.
  <a href="https://arxiv.org/pdf/1803.02815.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Shampoo: Preconditioned Stochastic Tensor Optimization.
  <a href="https://arxiv.org/pdf/1802.09568.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/moskomule/shampoo.pytorch">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  Optimizing for Generalization in Machine Learning with Cross-Validation Gradients.
  <a href="https://arxiv.org/pdf/1805.07072.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/sbarratt/crossval">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  WNGrad: Learn the Learning Rate in Gradient Descent.
  <a href="https://arxiv.org/pdf/1803.02865.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h2>
 Transfer Learning
</h2>
<ul>
 <li>
  3D Convolutional Encoder-Decoder Network for Low-Dose CT via Transfer Learning from a 2D Trained Network.
  <a href="https://arxiv.org/pdf/1802.05656.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Adversarial Transfer Learning.
  <a href="https://arxiv.org/pdf/1812.02849.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Arbitrary Style Transfer with Style-Attentional Networks.
  <a href="https://arxiv.org/pdf/1812.02342.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Survey on Deep Transfer Learning.
  <a href="https://arxiv.org/pdf/1808.01974.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Avatar-Net: Multi-scale Zero-shot Style Transfer by Feature Decoration.
  <a href="https://arxiv.org/pdf/1805.03857.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Capsule networks for low-data transfer learning.
  <a href="https://arxiv.org/pdf/1804.10172.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer.
  <a href="https://arxiv.org/pdf/1804.06437.pdfv1">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/lijuncen/Sentiment-and-Style-Transfer">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learn from Your Neighbor: Learning Multi-modal Mappings from Sparse Annotations.
  <a href="https://arxiv.org/pdf/1806.02934.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  [Best Paper] Taskonomy: Disentangling Task Transfer Learning.
  <a href="https://arxiv.org/pdf/1804.08328.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
</ul>
<h2>
 Zero/One Shot Learning
</h2>
<ul>
 <li>
  A Large-scale Attribute Dataset for Zero-shot Learning.
  <a href="https://arxiv.org/pdf/1804.04314.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deep Triplet Ranking Networks for One-Shot Recognition.
  <a href="https://arxiv.org/pdf/1804.07275.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  One-Shot Learning using Mixture of Variational Autoencoders: a Generalization Learning approach.
  <a href="https://arxiv.org/pdf/1804.07645.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  One-Shot Unsupervised Cross Domain Translation.
  <a href="https://arxiv.org/pdf/1806.06029.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/sagiebenaim/OneShotTranslation">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Preserving Semantic Relations for Zero-Shot Learning.
  <a href="https://arxiv.org/pdf/1803.03049.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
