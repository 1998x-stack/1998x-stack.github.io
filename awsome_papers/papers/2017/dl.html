<h2>
 Deep Learning
</h2>
<h3>
 Deep learning
</h3>
<ul>
 <li>
  A Bayesian Perspective on Generalization and Stochastic Gradient Descent.
  <a href="https://arxiv.org/pdf/1710.06451.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Bridge Between Hyperparameter Optimization and Larning-to-learn.
  <a href="https://arxiv.org/pdf/1712.06283.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//lucfra/FAR-HO">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Accelerating Stochastic Gradient Descent.
  <a href="https://arxiv.org/pdf/1704.08227.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour.
  <a href="https://research.fb.com/publications/ImageNet1kIn1h/">
   <code>
    url
   </code>
  </a>
 </li>
 <li>
  Activation Ensembles for Deep Neural Networks.
  <a href="https://arxiv.org/pdf/1702.07790.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks.
  <a href="https://arxiv.org/pdf/1712.02029.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Deep Convolutional Auto-Encoder with Pooling - Unpooling Layers in Caffe.
  <a href="https://arxiv.org/pdf/1701.04949.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <a href="https://einstein.ai/research/domain-specific-language-for-automated-rnn-architecture-search">
   A Flexible Approach to Automated RNN Architecture Generation.
  </a>
  <a href="https://arxiv.org/pdf/1712.07316.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Generalization of Convolutional Neural Networks to Graph-Structured Data.
  <a href="https://arxiv.org/pdf/1704.08165.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/hechtlinger/graph_cnn">
   <code>
    keras
   </code>
  </a>
 </li>
 <li>
  A GPU-Based Solution to Fast Calculation of Betweenness Centrality on Large Weighted Networks.
  <a href="https://arxiv.org/pdf/1701.05975.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics.
  <a href="https://arxiv.org/pdf/1702.05575.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  AI Programmer: Autonomously Creating Software Programs Using Genetic Algorithms.
  <a href="https://arxiv.org/pdf/1709.05703.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Alignment of dynamic networks.
  <a href="https://arxiv.org/pdf/1701.08842.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Linear-Time Kernel Goodness-of-Fit Test.
  <a href="https://arxiv.org/pdf/1705.07673.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//wittawatj/kernel-gof">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  A Matrix Factorization Approach for Learning Semidefinite-Representable Regularizers.
  <a href="https://arxiv.org/pdf/1701.01207.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks.
  <a href="https://arxiv.org/pdf/1705.09786.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  An Effective Training Method For Deep Convolutional Neural Network.
  <a href="https://arxiv.org/pdf/1708.01666.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  A neural algorithm for a fundamental computing problem.
  <a href="http://science.sciencemag.org/content/358/6364/793">
   <code>
    url
   </code>
  </a>
  :star:
 </li>
 <li>
  A Probabilistic Framework for Location Inference from Social Media.
  <a href="https://arxiv.org/pdf/1702.07281.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Saddle Point Approach to Structured Low-rank Matrix Learning in Large-scale Applications.
  <a href="https://arxiv.org/pdf/1704.07352.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650727591&amp;idx=1&amp;sn=176d4a7ab8e06451b0deb2d74c3a5794">
   A simple neural network module for relational reasoning.
  </a>
  <a href="https://arxiv.org/pdf/1706.01427.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  AutoBagging: Learning to Rank Bagging Workflows with Metalearning.
  <a href="https://arxiv.org/pdf/1706.09367.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/fhpinto/autoBagging">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Automated Curriculum Learning for Neural Networks.
  <a href="https://arxiv.org/pdf/1704.03003.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Autoencoder Regularized NetworkAn Effective Training Method For Deep Convolutional Neural Network. For Driving Style Representation Learning.
  <a href="https://arxiv.org/pdf/1701.01272.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Axiomatic Attribution for Deep Networks.
  <a href="https://arxiv.org/pdf/1703.01365.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/hiranumn/IntegratedGradients">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Backpropagation through the Void: Optimizing control variates for black-box gradient estimation.
  <a href="https://arxiv.org/pdf/1711.00123.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//duvenaud/relax">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Backprop without Learning Rates Through Coin Betting.
  <a href="https://arxiv.org/pdf/1705.07795.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/bremen79/cocob">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models.
  <a href="https://arxiv.org/pdf/1702.03275.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Bayesian Compression for Deep Learning.
  <a href="https://arxiv.org/pdf/1705.08665.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//KarenUllrich/Tutorial_BayesianCompressionForDL">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Bayesian Recurrent Neural Networks.
  <a href="https://arxiv.org/pdf/1704.02798.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/DeNeutoy/bayesian-rnn">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Beyond Sparsity: Tree Regularization of Deep Models for Interpretability.
  <a href="https://arxiv.org/pdf/1711.06178.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//dtak/tree-regularization-public">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  <a href="https://arxiv.org/pdf/1707.04476.pdf">
   Big Data vs. complex physical models: a scalable inference algorithm
  </a>
  <a href="https://github.com/JohannesBuchner/massivedatans/">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  <a href="http://weibo.com/1402400261/ECqPJziqk?type=comment#_rnd1491216627188">
   Billion-scale similarity search with GPUs.
  </a>
  <a href="https://arxiv.org/pdf/1702.08734.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/facebookresearch/faiss">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Bolt: Accelerated Data Mining with Fast Vector Compression.
  <a href="https://arxiv.org/pdf/1706.10283.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/dblalock/bolt">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  CASED: Curriculum Adaptive Sampling for Extreme Data Imbalance.
  <a href="https://www.researchgate.net/publication/319461093_CASED_Curriculum_Adaptive_Sampling_for_Extreme_Data_Imbalance">
   <code>
    url
   </code>
  </a>
  <a href="https://github.com/taki0112/CASED-Tensorflow">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  CNN Is All You Need.
  <a href="https://arxiv.org/pdf/1712.09662.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Combining Machine Learning and Physics to Understand Glassy Systems.
  <a href="https://arxiv.org/pdf/1709.08015.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Complex Networks: from Classical to Quantum.
  <a href="https://arxiv.org/pdf/1702.08459.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Concrete Dropout.
  <a href="https://arxiv.org/pdf/1705.07832.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  [Best Paper] Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments.
  <a href="https://arxiv.org/pdf/1710.03641.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Convolutional Gaussian Processes.
  <a href="https://arxiv.org/pdf/1709.01894.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/markvdw/convgp">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Conversation Modeling on Reddit using a Graph-Structured LSTM.
  <a href="https://arxiv.org/pdf/1704.02080.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s/HOR-0VicK__H3rrJFO19bA">
   Convolutional Sequence to Sequence Learning.
  </a>
  <a href="https://s3.amazonaws.com/fairseq/papers/convolutional-sequence-to-sequence-learning.pdf">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/facebookresearch/fairseq">
   <code>
    torch
   </code>
  </a>
  <a href="https://github.com/tobyyouup/conv_seq2seq">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Coordinating Filters for Faster Deep Neural Networks.
  <a href="https://arxiv.org/pdf/1703.09746.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/wenwei202/caffe/tree/sfm">
   <code>
    caffle
   </code>
  </a>
 </li>
 <li>
  Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks.
  <a href="https://arxiv.org/pdf/1702.05870.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  DAGER: Deep Age, Gender and Emotion Recognition Using Convolutional Neural Network.
  <a href="https://arxiv.org/pdf/1702.04280.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  DAGGER: A sequential algorithm for FDR control on DAGs.
  <a href="https://arxiv.org/pdf/1709.10250.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Dance Dance Convolution.
  <a href="https://arxiv.org/pdf/1703.06891.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/chrisdonahue/ddc">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Data Distillation: Towards Omni-Supervised Learning.
  <a href="https://arxiv.org/pdf/1712.04440.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  <a href="http://dustintran.com/blog/deep-and-hierarchical-implicit-models">
   Deep and Hierarchical Implicit Models.
  </a>
  <a href="https://arxiv.org/pdf/1702.08896.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  DeepArchitect: Automatically Designing and Training Deep Architectures.
  <a href="https://arxiv.org/pdf/1704.08792.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/negrinho/deep_architect">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Deep CNN ensembles and suggestive annotations for infant brain MRI segmentation.
  <a href="https://arxiv.org/pdf/1712.05319.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//josedolz/SemiDenseNet">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Deep Complex Networks.
  <a href="https://arxiv.org/pdf/1705.09792.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ChihebTrabelsi/deep_complex_networks">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Deep Depth From Focus.
  <a href="https://arxiv.org/pdf/1704.01085.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/gameover27/ddff-pytorch">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  Deep Echo State Network (DeepESN): A Brief Survey.
  <a href="https://arxiv.org/pdf/1712.04323.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  DeepFM: A Factorization-Machine based Neural Network for CTR Prediction.
  <a href="https://arxiv.org/pdf/1703.04247.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deep &amp; Cross Network for Ad Click Predictions.
  <a href="https://arxiv.org/pdf/1708.05123.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  <b>
   [GcForest]
  </b>
  <a href="https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&amp;mid=2651994082&amp;idx=1&amp;sn=3a1f21ab37ea8322c6700f660b71648a">
   Towards An Alternative to Deep Neural Networks.
  </a>
  <a href="https://arxiv.org/pdf/1702.08835.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/kingfengji/gcForest">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  DeepPicar: A Low-cost Deep Neural Network-based Autonomous Car.
  <a href="https://arxiv.org/pdf/1712.08644.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//heechul/picar">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Deep Gaussian Mixture Models.
  <a href="https://arxiv.org/pdf/1711.06929.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deep Hashing Network for Unsupervised Domain Adaptation.
  <a href="https://arxiv.org/pdf/1706.07522.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/hemanthdv/da-hash">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Deep-HiTS: Rotation Invariant Convolutional Neural Network for Transient Detection.
  <a href="https://arxiv.org/pdf/1701.00458.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/guille-c/Deep-HiTS">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Deep Learning: A Bayesian Perspective.
  <a href="https://arxiv.org/pdf/1706.00473.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s/8U3vFaf3SDCYnWy4lQv6uw">
   Deep Learning as a Mixed Convex-Combinatorial Optimization Problem.
  </a>
  <a href="https://arxiv.org/pdf/1710.11573.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Deep Learning for Time-Series Analysis.
  <a href="https://arxiv.org/pdf/1701.01887.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deep learning for universal linear embeddings of nonlinear dynamics.
  <a href="https://arxiv.org/pdf/1712.09707.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deep Learning With Dynamic Computation Graphs.
  <a href="https://openreview.net/pdf?id=ryrGawqex">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  DeepNAT: Deep Convolutional Neural Network for Segmenting Neuroanatomy.
  <a href="https://arxiv.org/pdf/1702.08192.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/TJKlein/DeepNAT">
   <code>
    caffe
   </code>
  </a>
 </li>
 <li>
  Deep Network Guided Proof Search.
  <a href="https://arxiv.org/pdf/1701.06972.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deep Value Networks Learn to Evaluate and Iteratively Refine Structured Outputs.
  <a href="https://arxiv.org/pdf/1703.04363.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/gyglim/dvn">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650726926&amp;idx=5&amp;sn=9e053e9864dc789f5b393a4c49242486">
   DeepXplore: Automated Whitebox Testing of Deep Learning Systems.
  </a>
  <a href="https://arxiv.org/pdf/1705.06640.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  DeepZip: Lossless Compression using Recurrent Networks.
  <a href="https://web.stanford.edu/class/cs224n/reports/2761006.pdf">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com//kedartatwawadi/NN_compression">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s?__biz=MzA3Mjk0OTgyMg==&amp;mid=2651123298&amp;idx=1&amp;sn=9f1159e14a3dba122f122aed1b51dbb7">
   Deformable Convolutional Networks
  </a>
  .
  <a href="https://arxiv.org/pdf/1703.06211.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/1zb/deformable-convolution-pytorch">
   <code>
    pytorch
   </code>
  </a>
  :star:
 </li>
 <li>
  <b>
   [Edward]
  </b>
  Deep Probabilistic Programming.
  <a href="https://arxiv.org/pdf/1701.03757.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="http://edwardlib.org/">
   <code>
    Web
   </code>
  </a>
  :star:
 </li>
 <li>
  <a href="https://arxiv.org/pdf/1703.05605.pdf">
   Deep Sketch Hashing: Fast Free-hand Sketch-Based Image Retrieval.
  </a>
  <a href="https://github.com/ymcidence/DeepSketchHashing">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction.
  <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/ST-ResNet-AAAI17-Zhang.pdf">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/lucktroy/DeepST/tree/master/scripts/papers/AAAI17">
   <code>
    code
   </code>
  </a>
  ] :star:
 </li>
 <li>
  Deep Unsupervised Clustering Using Mixture of Autoencoders.
  <a href="https://arxiv.org/pdf/1712.07788.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <b>
   [DeepStack]
  </b>
  <a href="http://www.jiqizhixin.com/article/2395">
   Expert-Level Artificial Intelligence in No-Limit Poker.
  </a>
  <a href="https://arxiv.org/pdf/1701.01724.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Dense Transformer Networks.
  <a href="https://arxiv.org/pdf/1705.08881.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/divelab/dtn">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Diabetic Retinopathy Detection via Deep Convolutional Networks for Discriminative Localization and Visual Explanation.
  <a href="https://arxiv.org/pdf/1703.10757.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/cauchyturing/kaggle_diabetic_RAM">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Diagonal RNNs in Symbolic Music Modeling.
  <a href="https://arxiv.org/pdf/1704.05420.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ycemsubakan/diagonal_rnns">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Different approaches to community detection.
  <a href="https://arxiv.org/pdf/1712.06468.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Dilated Recurrent Neural Networks.
  <a href="https://arxiv.org/pdf/1710.02224.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/zalandoresearch/pt-dilate-rnn">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  Dilated Residual Networks.
  <a href="https://arxiv.org/pdf/1705.09914.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients.
  <a href="https://arxiv.org/pdf/1705.07774.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/lballes/msvag">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Distilling a Neural Network Into a Soft Decision Tree.
  <a href="https://arxiv.org/pdf/1711.09784.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//kimhc6028/soft-decision-tree">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  Distributed Online Learning of Event Definitions.
  <a href="https://arxiv.org/pdf/1705.02175.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  dna2vec: Consistent vector representations of variable-length k-mers.
  <a href="https://arxiv.org/pdf/1701.06279.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//pnpnpn/dna2vec">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Dockerface: an easy to install and use Faster R-CNN face detector in a Docker container.
  <a href="https://arxiv.org/pdf/1708.04370.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/natanielruiz/dockerface">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Domain-adaptive deep network compression.
  <a href="https://arxiv.org/pdf/1709.01041.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/mmasana/DALR">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Doubly Stochastic Variational Inference for Deep Gaussian Processes.
  <a href="https://arxiv.org/pdf/1705.08933.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ICL-SML/Doubly-Stochastic-DGP">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Dropout Feature Ranking for Deep Learning Models.
  <a href="https://arxiv.org/pdf/1712.08645.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  DSOD: Learning Deeply Supervised Object Detectors from Scratch.
  <a href="https://arxiv.org/pdf/1708.01241.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/szq0214/DSOD">
   <code>
    caffe
   </code>
  </a>
 </li>
 <li>
  Dual Path Networks.
  <a href="https://arxiv.org/pdf/1707.01629.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/oyam/pytorch-DPNs">
   <code>
    pytorch
   </code>
  </a>
  :star:
 </li>
 <li>
  Dynamic Routing Between Capsules.
  <a href="https://arxiv.org/pdf/1710.09829.pdf">
   <code>
    code
   </code>
  </a>
  <a href="https://github.com/danielhavir/capsule-network">
   <code>
    pytorch
   </code>
  </a>
  :star:
 </li>
 <li>
  <b>
   [DyNet]
  </b>
  The Dynamic Neural Network Toolkit.
  <a href="https://arxiv.org/pdf/1701.03980.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/clab/dynet">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  EC3: Combining Clustering and Classification for Ensemble Learning.
  <a href="https://arxiv.org/pdf/1708.08591.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Efficient Algorithms for t-distributed Stochastic Neighborhood Embedding.
  <a href="https://arxiv.org/pdf/1712.09005.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Efficient Information Flow Maximization in Probabilistic Graphs.
  <a href="https://arxiv.org/pdf/1701.05395.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Efficient probabilistic inference in generic neural networks trained with non-probabilistic feedback.
  <a href="https://www.nature.com/articles/s41467-017-00181-8.pdf">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/eminorhan/inevitable-probability">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Encouraging LSTMs to Anticipate Actions Very Early.
  <a href="https://arxiv.org/pdf/1703.07023.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  End-to-end Network for Twitter Geolocation Prediction and Hashing.
  <a href="https://arxiv.org/pdf/1710.04802.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/jhlau/twitter-deepgeo">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures.
  <a href="http://www.aclweb.org/anthology/P/P16/P16-1105.pdf">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/tticoin/LSTM-ER">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Ensemble Sales Forecasting Study in Semiconductor Industry.
  <a href="https://arxiv.org/pdf/1705.00003.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/qx0731/ensemble_forecast_methods">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Evolving Deep Neural Networks.
  <a href="https://arxiv.org/pdf/1703.00548.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Fast Detection of Community Structures using Graph Traversal in Social Networks.
  <a href="https://arxiv.org/pdf/1707.04459.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/sna-lincom/LINCOM">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Fast Landmark Localization with 3D Component Reconstruction and CNN for Cross-Pose Recognition.
  <a href="https://arxiv.org/pdf/1708.09580.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Fast-Slow Recurrent Neural Networks.
  <a href="https://arxiv.org/pdf/1705.08639.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/amujika/Fast-Slow-LSTM">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Forward Thinking: Building Deep Random Forests.
  <a href="https://arxiv.org/pdf/1705.07366.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/tkchris93/ForwardThinking">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Fraternal Dropout.
  <a href="https://arxiv.org/pdf/1711.00066.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//kondiz/fraternal-dropout">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  Fully Convolutional Architectures for Multi-Class Segmentation in Chest Radiographs.
  <a href="https://arxiv.org/pdf/1701.08816.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Gated Orthogonal Recurrent Units: On Learning to Forget.
  <a href="https://arxiv.org/pdf/1706.02761.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks.
  <a href="https://arxiv.org/pdf/1701.05923.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Generalized Orderless Pooling Performs Implicit Salient Matching.
  <a href="http://openaccess.thecvf.com/content_iccv_2017/html/Simon_Generalized_Orderless_Pooling_ICCV_2017_paper.html">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//cvjena/alpha_pooling">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  gkm-DNN: efficient prediction using gapped k-mer features and deep neural networks.
  <a href="http://www.biorxiv.org/content/biorxiv/early/2017/07/31/170761.full.pdf">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Good Semi-supervised Learning that Requires a Bad GAN.
  <a href="https://arxiv.org/pdf/1705.09783.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Gradual Learning of Deep Recurrent Neural Networks.
  <a href="https://arxiv.org/pdf/1708.08863.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Graffinity: Visualizing Connectivity In Large Graphs.
  <a href="https://arxiv.org/pdf/1703.07729.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/visdesignlab/graffinity">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Graph Convolutional Matrix Completion.
  <a href="https://arxiv.org/pdf/1706.02263.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/riannevdberg/gc-mc">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Graph Structure Learning from Unlabeled Data for Event Detection.
  <a href="https://arxiv.org/pdf/1701.01470.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Hidden Community Detection in Social Networks.
  <a href="https://arxiv.org/pdf/1702.07462.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Hierarchical loss for classification.
  <a href="https://arxiv.org/pdf/1709.01062.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Holistic Interstitial Lung Disease Detection using Deep Convolutional Neural Networks: Multi-label Learning and Unordered Pooling.
  <a href="https://arxiv.org/pdf/1701.05616.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <a href="https://zhuanlan.zhihu.com/p/27555858">
   Hyperparameter Optimization: A Spectral Approach.
  </a>
  <a href="https://arxiv.org/pdf/1706.00764.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Hyperparameters Optimization in Deep Convolutional Neural Network / Bayesian Approach with Gaussian Process Prior.
  <a href="https://arxiv.org/pdf/1712.07233.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Improving Generalization Performance by Switching from Adam to SGD.
  <a href="https://arxiv.org/pdf/1712.07628.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Information Perspective to Probabilistic Modeling: Boltzmann Machines versus Born Machines.
  <a href="https://arxiv.org/pdf/1712.04144.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Interpretable Explanations of Black Boxes by Meaningful Perturbation.
  <a href="https://arxiv.org/pdf/1704.03296.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Label Efficient Learning of Transferable Representations across Domains and Tasks.
  <a href="https://arxiv.org/pdf/1712.00123.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning Accurate Low-Bit Deep Neural Networks with Stochastic Quantization.
  <a href="https://arxiv.org/pdf/1708.01001.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/dongyp13/Stochastic-Quantization">
   <code>
    caffe
   </code>
  </a>
 </li>
 <li>
  Learning Bag-of-Features Pooling for Deep Convolutional Neural Networks.
  <a href="https://arxiv.org/pdf/1707.08105.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/passalis/cbof">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning by Association - A versatile semi-supervised training method for neural networks.
  <a href="https://arxiv.org/pdf/1706.00909.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/haeusser/learning_by_association">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning Chained Deep Features and Classifiers for Cascade in Object Detection.
  <a href="https://arxiv.org/pdf/1702.07054.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees.
  <a href="https://arxiv.org/pdf/1702.08833.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Learning Efficient Convolutional Networks through Network Slimming.
  <a href="https://arxiv.org/pdf/1708.06519.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/foolwood/pytorch-slimming">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning Gradient Descent: Better Generalization and Longer Horizons.
  <a href="https://arxiv.org/pdf/1703.03633.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/vfleaking/rnnprop">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning Hierarchical Information Flow with Recurrent Neural Modules.
  <a href="https://arxiv.org/pdf/1706.05744.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning to Acquire Information.
  <a href="https://arxiv.org/pdf/1704.06131.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning to Compose Domain-Specific Transformations for Data Augmentation.
  <a href="https://arxiv.org/pdf/1709.01643.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/HazyResearch/tanda">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning to Compose Skills.
  <a href="https://arxiv.org/pdf/1711.11289.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//himanshusahni/ComposeNet">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning to Invert: Signal Recovery via Deep Convolutional Networks.
  <a href="https://arxiv.org/pdf/1701.03891.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning to Learn from Weak Supervision by Full Supervision.
  <a href="http://mostafadehghani.com/2017/12/01/learning-to-learn-from-weak-supervision-by-full-supervision/">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning to Remember Rare Events.
  <a href="https://openreview.net/pdf?id=SJTQLdqlg">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/tensorflow/models/tree/master/learning_to_remember_rare_events">
   <code>
    tensorflow
   </code>
  </a>
  ] :star:
 </li>
 <li>
  Learning to Segment Instances in Videos with Spatial Propagation Network.
  <a href="http://davischallenge.org/challenge2017/papers/DAVIS-Challenge-6th-Team.pdf">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/JingchunCheng/Seg-with-SPN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning Wasserstein Embeddings.
  <a href="https://arxiv.org/pdf/1710.07457.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  LDMNet: Low Dimensional Manifold Regularized Neural Networks.
  <a href="https://arxiv.org/pdf/1711.06246.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  LipNet: Sentence-level Lipreading.
  <a href="https://openreview.net/pdf?id=BkjLkSqxg">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/bshillingford/LipNet">
   <code>
    torch
   </code>
  </a>
 </li>
 <li>
  LSTM Fully Convolutional Networks for Time Series Classification.
  <a href="https://arxiv.org/pdf/1709.05206.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/houshd/LSTM-FCN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Lucid Data Dreaming for Object Tracking.
  <a href="https://arxiv.org/pdf/1703.09554.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Machine learning of neural representations of suicide and emotion concepts identifies suicidal youth.
  <a href="https://www.nature.com/articles/s41562-017-0234-y">
   <code>
    url
   </code>
  </a>
 </li>
 <li>
  Machine Learning on Sequential Data Using a Recurrent Weighted Average.
  <a href="https://arxiv.org/pdf/1703.01253.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/jostmey/rwa">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Markov Chain Monte Carlo Methods for Bayesian Data Analysis in Astronomy.
  <a href="https://arxiv.org/pdf/1706.01629.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/sanjibs/bmcmc/">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s?__biz=MzA4NzE1NzYyMw==&amp;mid=2247488392&amp;idx=2&amp;sn=7c8e41aef37c370d6155607283d776ef">
   Mask R-CNN.
  </a>
  <a href="https://arxiv.org/pdf/1703.06870.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Mastering the game of Go without human knowledge.
  <a href="https://www.nature.com/nature/journal/v550/n7676/full/nature24270.html">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  MEBoost: Mixing Estimators with Boosting for Imbalanced Data Classification.
  <a href="https://arxiv.org/pdf/1712.06658.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Memory-Efficient Implementation of DenseNets.
  <a href="https://arxiv.org/pdf/1707.06990.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/gpleiss/efficient_densenet_pytorch">
   <code>
    pytorch
   </code>
  </a>
  :star:
 </li>
 <li>
  Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.
  <a href="https://arxiv.org/pdf/1703.01780.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//CuriousAI/mean-teacher">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural Networks.
  <a href="https://arxiv.org/pdf/1711.06788.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.
  <a href="https://arxiv.org/pdf/1703.03400.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/cbfinn/maml">
   <code>
    code
   </code>
  </a>
  <a href="https://github.com/katerakelly/pytorch-maml">
   <code>
    pytorch
   </code>
  </a>
  :star:
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650725078&amp;idx=4&amp;sn=f23476a1abf7017686f2b18ca5dd83d8">
   Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games.
  </a>
  <a href="https://arxiv.org/pdf/1703.10069.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Multi-Scale Dense Convolutional Networks for Efficient Prediction.
  <a href="https://arxiv.org/pdf/1703.09844.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/gaohuang/MSDNet">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Multiscale Hierarchical Convolutional Networks.
  <a href="https://arxiv.org/pdf/1703.04140.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/jhjacobsen/HierarchicalCNN">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Natasha: Faster Stochastic Non-Convex Optimization via Strongly Non-Convex Parameter.
  <a href="https://arxiv.org/pdf/1702.00763.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  NDT: Neual Decision Tree Towards Fully Functioned Neural Graph.
  <a href="https://arxiv.org/pdf/1712.05934.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Network Backboning with Noisy Data.
  <a href="https://arxiv.org/pdf/1701.07336.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks .
  <a href="https://arxiv.org/pdf/1712.03298.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Neural Decision Trees.
  <a href="https://arxiv.org/pdf/1702.07360.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Neural Message Passing for Quantum Chemistry.
  <a href="https://arxiv.org/pdf/1704.01212.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/priba/nmp_qc">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Neural networks for topology optimization.
  <a href="https://arxiv.org/pdf/1709.09578.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ISosnovik/top">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Neural Networks Regularization Through Invariant Features Learning.
  <a href="https://arxiv.org/pdf/1709.01867.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Neural Ranking Models with Weak Supervision.
  <a href="https://arxiv.org/pdf/1704.08803.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Neural Task Programming: Learning to Generalize Across Hierarchical Tasks.
  <a href="https://arxiv.org/pdf/1710.01813.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ntp-project/ntp">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s/xI9JjRKT1K-CmRgRBNwAFA">
   Neural Variational Inference and Learning in Undirected Graphical Models.
  </a>
  <a href="https://arxiv.org/pdf/1711.02679.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s/9rmj9_9TNBMS--yrpTIzcA">
   Non-local Neural Networks.
  </a>
  <a href="https://arxiv.org/pdf/1711.07971.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//titu1994/keras-non-local-nets">
   <code>
    keras
   </code>
  </a>
  :star:
 </li>
 <li>
  One Model To Learn Them All.
  <a href="https://arxiv.org/pdf/1706.05137.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/tensorflow/tensor2tensor">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  One Network to Solve Them All --- Solving Linear Inverse Problems using Deep Projection Models.
  <a href="https://arxiv.org/pdf/1703.09912.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//rick-chang/OneNet">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Online Convolutional Dictionary Learning.
  <a href="https://arxiv.org/pdf/1706.09563.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Online Learning with Gated Linear Networks.
  <a href="https://arxiv.org/pdf/1712.01897.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  On the Information Bottleneck Theory of Deep Learning.
  <a href="https://openreview.net/forum?id=ry_WPG-A-&amp;noteId=ry_WPG-A-">
   <code>
    pdf
   </code>
  </a>
  :star:
 </li>
 <li>
  On weight initialization in deep neural networks.
  <a href="https://arxiv.org/pdf/1704.08863.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/sidkk86/weight_initialization">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Opening the Black Box of Deep Neural Networks via Information.
  <a href="https://arxiv.org/pdf/1703.00810.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Optimization as a Model for Few-Shot Learning.
  <a href="https://openreview.net/pdf?id=rJY0-Kcll">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/twitter/meta-learning-lstm">
   <code>
    torch
   </code>
  </a>
  :star:
 </li>
 <li>
  OptNet: Differentiable Optimization as a Layer in Neural Networks.
  <a href="https://arxiv.org/pdf/1703.00443.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/locuslab/optnet">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  <a href="http://mp.weixin.qq.com/s/qkYHcDpIMM5W7D_NWoa5ww">
   Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.
  </a>
  <a href="https://arxiv.org/pdf/1701.06538.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <b>
   [PathNet]
  </b>
  Evolution Channels Gradient Descent in Super Neural Networks.
  <a href="https://arxiv.org/pdf/1701.08734.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/jaesik817/pathnet">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Peephole: Predicting Network Performance Before Training.
  <a href="https://arxiv.org/pdf/1712.03351.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Performance Evaluation of Container-based Virtualization for High Performance Computing Environments.
  <a href="https://arxiv.org/pdf/1709.10140.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  pix2code: Generating Code from a Graphical User Interface Screenshot.
  <a href="https://arxiv.org/pdf/1705.07962.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/tonybeltramelli/pix2code">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  PixelNet: Representation of the pixels, by the pixels, and for the pixels.
  <a href="https://arxiv.org/pdf/1702.06506.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/aayushbansal/PixelNet">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space.
  <a href="https://arxiv.org/pdf/1706.02413.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/charlesq34/pointnet2">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Privacy-Preserving Deep Inference for Rich User Data on The Cloud.
  <a href="https://arxiv.org/pdf/1710.01727.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/aliosia/DeepPrivInf2017">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Privileged Multi-label Learning.
  <a href="https://arxiv.org/pdf/1701.07194.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Prochlo: Strong Privacy for Analytics in the Crowd.
  <a href="https://arxiv.org/pdf/1710.00901.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Progressive Neural Architecture Search.
  <a href="https://arxiv.org/pdf/1712.00559.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/chenxi116/PNASNet.TF">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  PRUNE: Preserving Proximity and Global Ranking for Network Embedding.
  <a href="https://papers.nips.cc/paper/7110-prune-preserving-proximity-and-global-ranking-for-network-embedding">
   <code>
    url
   </code>
  </a>
  <a href="https://github.com/ntumslab/PRUNE">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  QCD-Aware Recursive Neural Networks for Jet Physics.
  <a href="https://arxiv.org/pdf/1702.00748.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/glouppe/recnn">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  QuickNet: Maximizing Efficiency and Efficacy in Deep Architectures.
  <a href="https://arxiv.org/pdf/1701.02291.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Recurrent Additive Networks.
  <a href="https://arxiv.org/pdf/1705.07393.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/bheinzerling/ran">
   <code>
    pytorch
   </code>
  </a>
  :star:
 </li>
 <li>
  Recurrent Pixel Embedding for Instance Grouping.
  <a href="https://arxiv.org/pdf/1712.08273.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//aimerykong/Recurrent-Pixel-Embedding-for-Instance-Grouping">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Reducing Deep Network Complexity with Fourier Transform Methods.
  <a href="https://arxiv.org/pdf/1801.01451.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/andrew-jeremy/Reducing-Deep-Network-Complexity-with-Fourier-Transform-Methods">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Reducing Reparameterization Gradient Variance.
  <a href="https://arxiv.org/pdf/1705.07880.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/andymiller/ReducedVarianceReparamGradients">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  RelNN: A Deep Neural Model for Relational Learning.
  <a href="https://arxiv.org/pdf/1712.02831.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//Mehran-k/RelNN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Resting state fMRI functional connectivity-based classification using a convolutional neural network architecture.
  <a href="https://arxiv.org/pdf/1707.06682.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/MRegina/connectome_conv_net">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Robust Loss Functions under Label Noise for Deep Neural Networks.
  <a href="https://arxiv.org/pdf/1712.09482.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  [best paper] Safe and Nested Endgame Solving for Imperfect-Information Games.
  <a href="http://www.cs.cmu.edu/~noamb/papers/17-AAAI-Refinement.pdf">
   <code>
    pdf
   </code>
  </a>
  :star:
 </li>
 <li>
  Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition.
  <a href="https://arxiv.org/pdf/1710.07324.pdf">
   <code>
    arxiv
   </code>
  </a>
  [
  <code>
   https://github.com//izmailovpavel/TTGP
  </code>
  ]
 </li>
 <li>
  Scaling the Scattering Transform: Deep Hybrid Networks.
  <a href="https://arxiv.org/pdf/1703.08961.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/edouardoyallon/pyscatwave">
   <code>
    torch
   </code>
  </a>
 </li>
 <li>
  <a href="https://deepmind.com/blog/imagine-creating-new-visual-concepts-recombining-familiar-ones/">
   SCAN: Learning Abstract Hierarchical Compositional Visual Concepts.
  </a>
  <a href="https://arxiv.org/pdf/1707.03389.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <a href="http://people.csail.mit.edu/yusuf/see-hear-read/">
   See, Hear, and Read: Deep Aligned Representations.
  </a>
  <a href="https://arxiv.org/pdf/1706.00932.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650727793&amp;idx=1&amp;sn=3a86e5b9ce5bfc1d37c51dc0f09a84bc">
   Self-Normalizing Neural Networks.
  </a>
  <a href="https://arxiv.org/pdf/1706.02515.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/kevinzakka/research-paper-notes/blob/master/snn.md">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Self-supervised Learning of Motion Capture.
  <a href="https://arxiv.org/pdf/1712.01337.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Semi-Supervised Deep Learning for Monocular Depth Map Prediction.
  <a href="https://arxiv.org/pdf/1702.02706.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Semi-Supervised Endmember Identification In Nonlinear Spectral Mixtures Via Semantic Representation.
  <a href="https://arxiv.org/pdf/1701.00804.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Sentiment Predictability for Stocks.
  <a href="https://arxiv.org/pdf/1712.05785.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//jorpro/DeepTimeSeries">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Separable Fully Connected Layers Improve Deep Learning Models For Genomics.
  <a href="http://www.biorxiv.org/content/early/2017/07/07/146431">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/kundajelab/keras/tree/keras_1">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Shake-Shake regularization.
  <a href="https://arxiv.org/pdf/1705.07485.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/xgastaldi/shake-shake">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  ShaResNet: reducing residual network parameter number by sharing weights.
  <a href="https://arxiv.org/pdf/1702.08782.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/aboulch/sharesnet">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  ShortFuse: Biomedical Time Series Representations in the Presence of Structured Information.
  <a href="https://arxiv.org/pdf/1705.04790.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices.
  <a href="https://arxiv.org/pdf/1707.01083.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/camel007/Caffe-ShuffleNet">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s/DS1qVBUt24HWlWyvT_YDKQ">
   Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions.
  </a>
  <a href="https://arxiv.org/pdf/1711.08141.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Simple And Efficient Architecture Search for Convolutional Neural Networks.
  <a href="https://arxiv.org/pdf/1711.04528.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Simplified Gating in Long Short-term Memory (LSTM) Recurrent Neural Networks.
  <a href="https://arxiv.org/pdf/1701.03441.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/jingweimo/Modified-LSTM">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  SmoothGrad: removing noise by adding noise.
  <a href="https://arxiv.org/pdf/1706.03825.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/PAIR-code/saliency">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Sparse canonical correlation analysis.
  <a href="https://arxiv.org/pdf/1705.10865.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Sparse-Input Neural Networks for High-dimensional Nonparametric Regression and Classification.
  <a href="https://arxiv.org/pdf/1711.07592.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//jjfeng/spinn">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Sparse Neural Networks Topologies.
  <a href="https://arxiv.org/pdf/1706.05683.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Squeeze-and-Excitation Networks.
  <a href="https://arxiv.org/pdf/1709.01507.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/hujie-frank/SENet">
   <code>
    code
   </code>
  </a>
  <a href="https://github.com/taki0112/SENet-Tensorflow">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Statistical inference for network samples using subgraph counts.
  <a href="https://arxiv.org/pdf/1701.00505.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Steerable CNNs.
  <a href="https://openreview.net/pdf?id=rJQKYt5ll">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Stochastic L-BFGS Revisited: Improved Convergence Rates and Practical Acceleration Strategies.
  <a href="https://arxiv.org/pdf/1704.00116.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Stochastic Subsampling for Factorizing Huge Matrices.
  <a href="https://arxiv.org/pdf/1701.05363.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/arthurmensch/modl">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Structured Embedding Models for Grouped Data.
  <a href="https://arxiv.org/pdf/1709.10367.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/mariru/structured_embeddings">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  struc2vec: Learning Node Representations from Structural Identity.
  <a href="https://arxiv.org/pdf/1704.03165.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/leoribeiro/struc2vec">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Submanifold Sparse Convolutional Networks.
  <a href="https://arxiv.org/pdf/1706.01307.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/facebookresearch/SparseConvNet">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates.
  <a href="https://arxiv.org/pdf/1708.07120.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/lnsmith54/super-convergence">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  SurfaceNet: An End-to-end 3D Neural Network for Multiview Stereopsis.
  <a href="https://arxiv.org/pdf/1708.01749.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Tensor Regression Networks with various Low-Rank Tensor Approximations.
  <a href="https://arxiv.org/pdf/1712.09520.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//Vixaer/LowRankTRN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  TF Boosted Trees: A scalable TensorFlow based framework for gradient boosting.
  <a href="https://arxiv.org/pdf/1710.11555.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  The Case for Learned Index Structures.
  <a href="https://arxiv.org/pdf/1712.01208.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  <a href="https://medium.com/towards-data-science/notes-on-the-cramer-gan-752abd505c00">
   The Cramer Distance as a Solution to Biased Wasserstein Gradients.
  </a>
  <a href="https://arxiv.org/pdf/1705.10743.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ypxie/pytorch-cramer-Gan">
   <code>
    pytorch
   </code>
  </a>
  :star:
 </li>
 <li>
  The Future of Ad Blocking: An Analytical Framework and New Techniques.
  <a href="https://arxiv.org/pdf/1705.08568.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/citp/ad-blocking">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  The Power of Sparsity in Convolutional Neural Networks.
  <a href="https://arxiv.org/pdf/1702.06257.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Time-Contrastive Networks: Self-Supervised Learning from Multi-View Observation.
  <a href="https://arxiv.org/pdf/1704.06888.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Time Series Anomaly Detection; Detection of anomalous drops with limited features and sparse examples in noisy highly periodic data.
  <a href="https://arxiv.org/pdf/1708.03665.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  To Boost or Not to Boost? On the Limits of Boosted Trees for Object Detection.
  <a href="https://arxiv.org/pdf/1701.01692.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Towards "AlphaChem": Chemical Synthesis Planning with Tree Search and Deep Neural Network Policies.
  <a href="https://arxiv.org/pdf/1702.00020.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Towards a New Interpretation of Separable Convolutions.
  <a href="https://arxiv.org/pdf/1701.04489.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Towards Deeper Understanding of Variational Autoencoding Models.
  <a href="https://arxiv.org/pdf/1702.08658.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ShengjiaZhao/Generalized-PixelVAE">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes.
  <a href="https://arxiv.org/pdf/1706.10239.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Training a Fully Convolutional Neural Network to Route Integrated Circuits.
  <a href="https://arxiv.org/pdf/1706.08948.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/sjain-stanford/deep-route">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  Training Quantized Nets: A Deeper Understanding.
  <a href="https://arxiv.org/pdf/1706.02379.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <a href="https://www.zhihu.com/question/65244705">
   Training RNNs as Fast as CNNs.
  </a>
  <a href="https://arxiv.org/pdf/1709.02755.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/taolei87/sru">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Train longer, generalize better: closing the generalization gap in large batch training of neural networks.
  <a href="https://arxiv.org/pdf/1705.08741.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/eladhoffer/bigBatch">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  Trust Region Policy Optimization.
  <a href="https://arxiv.org/pdf/1502.05477.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ikostrikov/pytorch-trpo">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices.
  <a href="https://arxiv.org/pdf/1701.00485.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Understanding Black-box Predictions via Influence Functions.
  <a href="https://arxiv.org/pdf/1703.04730.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  <a href="http://weibo.com/ttarticle/p/show?id=2309351000224100795007132875&amp;u=1402400261&amp;m=4100900743750148&amp;cu=3655689037">
   Understanding deep learning requires rethinking generalization.
  </a>
  <a href="https://arxiv.org/pdf/1611.03530.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Understanding Hidden Memories of Recurrent Neural Networks.
  <a href="https://arxiv.org/pdf/1710.10777.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Understanding the Effective Receptive Field in Deep Convolutional Neural Networks.
  <a href="https://arxiv.org/pdf/1701.04128.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Understanding trained CNNs by indexing neuron selectivity.
  <a href="https://arxiv.org/pdf/1702.00382.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Unsupervised feature learning with discriminative encoder.
  <a href="https://arxiv.org/pdf/1709.00672.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data.
  <a href="https://arxiv.org/pdf/1709.07902.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//wnhsu/FactorizedHierarchicalVAE">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Unsupervised Representation Learning by Sorting Sequences.
  <a href="https://arxiv.org/pdf/1708.01246.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/HsinYingLee/OPN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  [best paper] Variance-based regularization with convex objectives.
  <a href="https://arxiv.org/pdf/1610.02581.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Variational Inference using Implicit Distributions.
  <a href="https://arxiv.org/pdf/1702.08235.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Visual Explanations for Convolutional Neural Networks via Input Resampling.
  <a href="https://arxiv.org/pdf/1707.09641.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/blengerich/explainable-cnn">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Visualizing the Loss Landscape of Neural Nets.
  <a href="https://arxiv.org/pdf/1712.09913.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Who Said What: Modeling Individual Labelers Improves Classification.
  <a href="https://arxiv.org/pdf/1703.08774.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s/T-v9OTcJa5OQ71QmYrFtbg">
   YellowFin and the Art of Momentum Tuning.
  </a>
  <a href="https://arxiv.org/pdf/1706.03471.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/JianGoForIt/YellowFin">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
</ul>
<h3>
 Attention and memory
</h3>
<ul>
 <li>
  A Structured Self-attentive Sentence Embedding.
  <a href="https://arxiv.org/pdf/1703.03130.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/Diego999/SelfSent">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Attention-based Extraction of Structured Information from Street View Imagery.
  <a href="https://arxiv.org/pdf/1704.03549.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Attention-Based Multimodal Fusion for Video Description.
  <a href="https://arxiv.org/pdf/1701.03126.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Attention Is All You Need.
  <a href="https://arxiv.org/pdf/1706.03762.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/Kyubyong/transformer">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Attention networks for image-to-text.
  <a href="https://arxiv.org/pdf/1712.04046.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//jvpoulos/Attention-OCR/">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Attentional Pooling for Action Recognition.
  <a href="https://rohitgirdhar.github.io/AttentionalPoolingAction/">
   <code>
    url
   </code>
  </a>
  <a href="https://github.com//rohitgirdhar/AttentionalPoolingAction">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Deep Memory Networks for Attitude Identification.
  <a href="https://arxiv.org/pdf/1701.04189.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Dynamic Computational Time for Visual Attention(DT-RAM).
  <a href="https://arxiv.org/pdf/1703.10332.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/baidu-research/DT-RAM">
   <code>
    torch
   </code>
  </a>
  :star:
 </li>
 <li>
  Efficient Attention using a Fixed-Size Memory Representation.
  <a href="https://arxiv.org/pdf/1707.00110.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  End-to-End Attention based Text-Dependent Speaker Verification.
  <a href="https://arxiv.org/pdf/1701.00562.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Graph Attention Networks.
  <a href="https://arxiv.org/pdf/1710.10903.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/PetarV-/GAT">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  <b>
   [HRAN]
  </b>
  Hierarchical Recurrent Attention Network for Response Generation.
  <a href="https://arxiv.org/pdf/1701.07149.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/LynetteXing1991/HRAN">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Online and Linear-Time Attention by Enforcing Monotonic Alignments.
  <a href="https://arxiv.org/pdf/1704.00784.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/craffel/mad">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Residual Attention Network for Image Classification.
  <a href="https://arxiv.org/pdf/1704.06904.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/buptwangfei/residual-attention-network">
   <code>
    caffle
   </code>
  </a>
 </li>
 <li>
  Segmentation-Aware Convolutional Networks Using Local Attention Masks.
  <a href="https://arxiv.org/pdf/1708.04607.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/aharley/segaware">
   <code>
    caffe
   </code>
  </a>
 </li>
 <li>
  Sequential Attention.
  <a href="https://arxiv.org/pdf/1705.02269.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Structural Attention Neural Networks for improved sentiment analysis.
  <a href="https://arxiv.org/pdf/1701.01811.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Structured Attention Networks.
  <a href="https://arxiv.org/pdf/1702.00887.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/harvardnlp/struct-attn">
   <code>
    github
   </code>
  </a>
 </li>
</ul>
<h3>
 Generative learning
</h3>
<ul>
 <li>
  <b>
   [AdaGAN]
  </b>
  Boosting Generative Models.
  <a href="https://arxiv.org/pdf/1701.02386.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/tolstikhin/adagan">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Adversarial Discriminative Domain Adaptation.
  <a href="https://arxiv.org/pdf/1702.05464.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//corenel/pytorch-adda">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods.
  <a href="https://arxiv.org/pdf/1705.07263.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="http://nicholas.carlini.com/code/">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s?__biz=MzI5NTIxNTg0OA==&amp;mid=2247486092&amp;idx=2&amp;sn=c1dd3dbe70a765600f72ea4f45d642ed">
   Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong.
  </a>
  <a href="https://arxiv.org/pdf/1706.04701.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Adversarial Examples that Fool Detectors.
  <a href="https://arxiv.org/pdf/1712.02494.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Adversarial Generator-Encoder Networks.
  <a href="http://sites.skoltech.ru/app/data/uploads/sites/25/2017/04/AGE.pdf">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/DmitryUlyanov/AGE">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1701.04722.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/LMescheder/AdversarialVariationalBayes">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Age Progression/Regression by Conditional Adversarial Autoencoder.
  <a href="https://arxiv.org/pdf/1702.08423.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ZZUTK/Face-Aging-CAAE">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  A-NICE-MC: Adversarial Training for MCMC.
  <a href="https://arxiv.org/pdf/1706.07561.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ermongroup/a-nice-mc">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  An Online Learning Approach to Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1706.03269.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Are GANs Created Equal? A Large-Scale Study.
  <a href="https://arxiv.org/pdf/1711.10337.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  ArtGAN: Artwork Synthesis with Conditional Categorial GANs.
  <a href="https://arxiv.org/pdf/1702.03410.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&amp;mid=2651993385&amp;idx=2&amp;sn=b381743378a7cfb04abc807f4a5b2f40">
   Adversarial Attacks on Neural Network Policies.
  </a>
  <a href="http://rll.berkeley.edu/adversarial/arXiv2017_AdversarialAttacks.pdf">
   <code>
    pdf
   </code>
  </a>
  :star:
 </li>
 <li>
  Adversarial Discriminative Domain Adaptation.
  <a href="https://arxiv.org/pdf/1702.05464.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//corenel/pytorch-adda">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  Adversarial Networks for the Detection of Aggressive Prostate Cancer.
  <a href="https://arxiv.org/pdf/1702.08014.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Aspect-augmented Adversarial Networks for Domain Adaptation.
  <a href="https://arxiv.org/pdf/1701.00188.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Automatic Generation of Typographic Font from a Small Font Subset.
  <a href="https://arxiv.org/pdf/1701.05703.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Bayesian GAN.
  <a href="https://arxiv.org/pdf/1705.09558.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//andrewgordonwilson/bayesgan/">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  <b>
   [BEGAN]
  </b>
  Boundary Equilibrium Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1703.10717.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/carpedm20/BEGAN-pytorch">
   <code>
    pytorch
   </code>
  </a>
  <a href="https://github.com/carpedm20/BEGAN-tensorflow">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Boosted Generative Models.
  <a href="https://arxiv.org/pdf/1702.08484.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training.
  <a href="https://arxiv.org/pdf/1709.02023.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/mkocaoglu/CausalGAN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Class-Splitting Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1709.07359.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/CIFASIS/splitting_gan">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  ConvNets and ImageNet Beyond Accuracy: Explanations, Bias Detection, Adversarial Examples and Model Criticism.
  <a href="https://arxiv.org/pdf/1711.11443.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields.
  <a href="https://arxiv.org/pdf/1708.08819.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/bioinf-jku/coulomb_gan">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Counterfactual Control for Free from Generative Models.
  <a href="https://arxiv.org/pdf/1702.06676.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/arayabrain/GenerativeControl">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks.
  <a href="https://arxiv.org/pdf/1711.07064.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//KupynOrest/DeblurGAN">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  Deep adversarial neural decoding.
  <a href="https://arxiv.org/pdf/1705.07109.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning.
  <a href="https://arxiv.org/pdf/1702.07464.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deformable GANs for Pose-based Human Image Generation.
  <a href="https://arxiv.org/pdf/1801.00055.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//AliaksandrSiarohin/pose-gan">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  DeLiGAN : Generative Adversarial Networks for Diverse and Limited Data.
  <a href="https://arxiv.org/pdf/1706.02071.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/val-iisc/deligan">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Dense Associative Memory is Robust to Adversarial Inputs.
  <a href="https://arxiv.org/pdf/1701.00939.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Denoising Adversarial Autoencoders.
  <a href="https://arxiv.org/pdf/1703.01220.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ToniCreswell/DAAE_">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models.
  <a href="https://arxiv.org/pdf/1712.04248.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//bethgelab/foolbox">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Distributional Adversarial Networks.
  <a href="https://arxiv.org/pdf/1706.09549.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ChengtaoLi/dan">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  End-to-End Differentiable Adversarial Imitation Learning.
  <a href="http://proceedings.mlr.press/v70/baram17a.html">
   <code>
    url
   </code>
  </a>
  <a href="https://github.com/itaicaspi/mgail">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Expecting the Unexpected: Training Detectors for Unusual Pedestrians with Adversarial Imposters.
  <a href="https://arxiv.org/pdf/1703.06283.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/huangshiyu13/RPNplus">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Face Aging With Conditional Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1702.01983.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Face Super-Resolution Through Wasserstein GANs.
  <a href="https://arxiv.org/pdf/1705.02438.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/MandyZChen/srez">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Face Synthesis from Facial Identity Features.
  <a href="https://arxiv.org/pdf/1701.04851.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Face Synthesis from Visual Attributes via Sketch using Conditional VAEs and GANs.
  <a href="https://arxiv.org/pdf/1801.00077.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//DetionDX/Attribute2Sketch2Face">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  f-GANs in an Information Geometric Nutshell.
  <a href="https://arxiv.org/pdf/1707.04385.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/qulizhen/fgan_info_geometric">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Fisher GAN.
  <a href="https://arxiv.org/pdf/1705.09675.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Gang of GANs: Generative Adversarial Networks with Maximum Margin Ranking.
  <a href="https://arxiv.org/pdf/1704.04865.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Geometric GAN.
  <a href="https://arxiv.org/pdf/1705.02894.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  GeneGAN: Learning Object Transfiguration and Attribute Subspace from Unpaired Data.
  <a href="https://arxiv.org/pdf/1705.04932.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/Prinsphield/GeneGAN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Generative Adversarial Active Learning.
  <a href="https://arxiv.org/pdf/1702.07956.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Generative Adversarial Networks recover features in astrophysical images of galaxies beyond the deconvolution limit.
  <a href="https://arxiv.org/pdf/1702.00403.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/SpaceML/GalaxyGAN/">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Generative Adversarial Residual Pairwise Networks for One Shot Learning.
  <a href="https://arxiv.org/pdf/1703.08033.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Generative Adversarial Training for Markov Chains.
  <a href="https://openreview.net/pdf?id=S1L-hCNtl">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/ermongroup/markov-chain-gan">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Generative Cooperative Net for Image Generation and Data Augmentation.
  <a href="https://arxiv.org/pdf/1705.02887.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Generative Face Completion.
  <a href="https://arxiv.org/pdf/1704.05838.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Generative Temporal Models with Memory.
  <a href="https://arxiv.org/pdf/1702.04649.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Generalization and Equilibrium in Generative Adversarial Nets (GANs).
  <a href="https://arxiv.org/pdf/1703.00573.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  GibbsNet: Iterative Adversarial Inference for Deep Graphical Models.
  <a href="https://arxiv.org/pdf/1712.04120.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/wlwkgus/GibbsNet">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  GP-GAN: Towards Realistic High-Resolution Image Blending.
  <a href="https://arxiv.org/pdf/1703.07195.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/wuhuikai/GP-GAN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Gradient descent GAN optimization is locally stable.
  <a href="https://arxiv.org/pdf/1706.04156.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Guiding InfoGAN with Semi-Supervision.
  <a href="https://arxiv.org/pdf/1707.04487.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/spurra/ss-infogan">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs.
  <a href="https://arxiv.org/pdf/1711.11585.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/NVIDIA/pix2pixHD">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  How to Train Your DRAGAN.
  <a href="https://arxiv.org/pdf/1705.07215.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/kodalinaveen3/DRAGAN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Hybrid VAE: Improving Deep Generative Models using Partial Observations.
  <a href="https://arxiv.org/pdf/1711.11566.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Image De-raining Using a Conditional Generative Adversarial Network.
  <a href="https://arxiv.org/pdf/1701.05957.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ruimashita/caffe-train/blob/master/vgg.train_val.prototxt">
   <code>
    caffe
   </code>
  </a>
 </li>
 <li>
  Image Generation and Editing with Variational Info Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1701.04568.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Improved Training of Wasserstein GANs.
  <a href="https://arxiv.org/pdf/1704.00028.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/igul222/improved_wgan_training">
   <code>
    tensorflow
   </code>
  </a>
  <a href="https://github.com/caogang/wgan-gp">
   <code>
    pytorch
   </code>
  </a>
  :star:
 </li>
 <li>
  Improved Semi-supervised Learning with GANs using Manifold Invariances.
  <a href="https://arxiv.org/pdf/1705.08850.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  InfoVAE: Information Maximizing Variational Autoencoders.
  <a href="https://arxiv.org/pdf/1706.02262.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  It Takes (Only) Two: Adversarial Generator-Encoder Networks.
  <a href="https://arxiv.org/pdf/1704.02304.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/DmitryUlyanov/AGE">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models.
  <a href="https://arxiv.org/pdf/1711.05772.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="http://t.cn/RjYZgvA?u=1402400261&amp;m=4175302026003996&amp;cu=undefined">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  LatentPoison - Adversarial Attacks On The Latent Space.
  <a href="https://arxiv.org/pdf/1711.02879.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//ToniCreswell/Adversarial-Attack-On-Latent-Space">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning Hierarchical Features from Generative Models.
  <a href="https://arxiv.org/pdf/1702.08396.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ShengjiaZhao/Variational-Ladder-Autoencoder">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Learning Texture Manifolds with the Periodic Spatial GAN.
  <a href="https://arxiv.org/pdf/1705.06566.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ubergmann/psgan">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Lifelong Generative M odeling.
  <a href="https://arxiv.org/pdf/1705.09847.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/jramapuram/LifelongVAE">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning Disentangled Representations with Semi-Supervised Deep Generative Models.
  <a href="https://arxiv.org/pdf/1706.00400.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation.
  <a href="https://arxiv.org/pdf/1703.01560.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/jwyang/lr-gan.pytorch">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  <b>
   [DiscoGAN]
  </b>
  Learning to Discover Cross-Domain Relations with Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1703.05192.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/dhgrs/chainer-DiscoGAN">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Learning to Draw Dynamic Agent Goals with Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1702.07319.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <a href="https://zhuanlan.zhihu.com/p/25204020">
   Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities.
  </a>
  <a href="https://arxiv.org/pdf/1701.06264.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/guojunq/lsgan">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Mastering Sketching: Adversarial Augmentation for Structured Prediction.
  <a href="https://arxiv.org/pdf/1703.08966.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/bobbens/sketch_simplification">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  <b>
   [MADGAN]
  </b>
  Multi-Agent Diverse Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1704.02906.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/arnabgho/MADGAN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction.
  <a href="https://arxiv.org/pdf/1704.01691.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/violet-zct/MSVED-morph-reinflection">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders.
  <a href="https://arxiv.org/pdf/1704.01279.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/tensorflow/magenta/tree/master/magenta/models/nsynth">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  On the Effects of Batch and Weight Normalization in Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1704.03971.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/stormraiser/GAN-weight-norm">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Parseval Networks: Improving Robustness to Adversarial Examples.
  <a href="https://arxiv.org/pdf/1704.08847.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  PixelSNAIL: An Improved Autoregressive Generative Model.
  <a href="https://arxiv.org/pdf/1712.09763.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//neocxi/pixelsnail-public">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Progressive Growing of GANs for Improved Quality, Stability, and Variation.
  <a href="https://arxiv.org/pdf/1710.10196.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//nashory/pggan-pytorch">
   <code>
    pytorch
   </code>
  </a>
  :star:
 </li>
 <li>
  Recent Advances in Recurrent Neural Networks.
  <a href="https://arxiv.org/pdf/1801.01078.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Reconstruction of three-dimensional porous media using generative adversarial neural networks.
  <a href="https://arxiv.org/pdf/1704.03225.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/LukasMosser/PorousMediaGan">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Replacement AutoEncoder: A Privacy-Preserving Algorithm for Sensory Data Analysis.
  <a href="https://arxiv.org/pdf/1710.06564.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/mmalekzadeh/replacement-autoencoder">
   <code>
    keras
   </code>
  </a>
 </li>
 <li>
  <b>
   [SalGAN]
  </b>
  Visual Saliency Prediction with Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1701.01081.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/imatge-upc/saliency-salgan-2017">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Scene Graph Generation by Iterative Message Passing.
  <a href="https://arxiv.org/pdf/1701.02426.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/danfeiX/scene-graph-TF-release">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  SEGAN: Speech Enhancement Generative Adversarial Network.
  <a href="https://arxiv.org/pdf/1703.09452.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/santi-pdp/segan">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Softmax GAN.
  <a href="https://arxiv.org/pdf/1704.06191.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Spectral Normalization for Generative Adversarial Networks.
  <a href="https://openreview.net/pdf?id=B1QRgziT-">
   <code>
    pdf
   </code>
  </a>
  :star:
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s/bIziwKBB2sXHxIDRpkYlHw">
   StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation.
  </a>
  <a href="https://arxiv.org/pdf/1711.09020.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//yunjey/StarGAN">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Steganographic Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1703.05502.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/dvolkhonskiy/adversarial-steganography">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  TextureGAN: Controlling Deep Image Synthesis with Texture Patches.
  <a href="https://arxiv.org/pdf/1706.02823.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Towards an Understanding of Our World by GANing Videos in the Wild.
  <a href="https://arxiv.org/pdf/1711.11453.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//bernhard2202/improved-video-gan">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Towards Deep Learning Models Resistant to Adversarial Attacks.
  <a href="https://arxiv.org/pdf/1706.06083.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/tensorflow/cleverhans/blob/master/cleverhans/attacks.py">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Towards Diverse and Natural Image Descriptions via a Conditional GAN.
  <a href="http://weibo.com/1402400261/EBP7EgRkQ?type=comment#_rnd1490711748139">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Towards Principled Methods for Training Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1701.04862.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Training GANs with Optimism.
  <a href="https://arxiv.org/pdf/1711.00141.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//vsyrgkanis/optimistic_GAN_training">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.
  <a href="https://arxiv.org/pdf/1703.10593.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/junyanz/CycleGAN">
   <code>
    torch
   </code>
  </a>
  :star:
 </li>
 <li>
  Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery.
  <a href="http://weibo.com/1402400261/EAKUStV9i?type=comment#_rnd1490105990159">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Unsupervised Diverse Colorization via Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1702.06674.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Unsupervised Image-to-Image Translation with Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1701.02676.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1511.06434.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/carpedm20/DCGAN-tensorflow">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s?__biz=MzI5NTIxNTg0OA==&amp;mid=2247486161&amp;idx=2&amp;sn=e11c6bb9839a1fde85e96d26ac2d0b75">
   Variational Approaches for Auto-Encoding Generative Adversarial Networks.
  </a>
  <a href="https://arxiv.org/pdf/1706.04987.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  VAE with a VampPrior.
  <a href="https://arxiv.org/pdf/1705.07120.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/jmtomczak/vae_vampprior">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  VIGAN: Missing View Imputation with Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1708.06724.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/chaoshangcs/VIGAN">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  Visual Feature Attribution using Wasserstein GANs.
  <a href="https://arxiv.org/pdf/1711.08998.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/orobix/Visual-Feature-Attribution-Using-Wasserstein-GANs-Pytorch">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  Wasserstein Auto-Encoders.
  <a href="https://arxiv.org/pdf/1711.01558.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/tolstikhin/wae">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  <b>
   [WGAN]
  </b>
  <a href="https://zhuanlan.zhihu.com/p/25071913">
   Wasserstein GAN.
  </a>
  <a href="https://arxiv.org/pdf/1701.07875.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/martinarjovsky/WassersteinGAN">
   <code>
    pytorch
   </code>
  </a>
  <a href="https://github.com/Zardinality/WGAN-tensorflow">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Wasserstein Learning of Deep Generative Point Process Models.
  <a href="https://arxiv.org/pdf/1705.08051.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  WaterGAN: Unsupervised Generative Network to Enable Real-time Color Correction of Monocular Underwater Images.
  <a href="https://arxiv.org/pdf/1702.07392.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/kskin/WaterGAN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings.
  <a href="https://arxiv.org/pdf/1711.05139.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h3>
 Transfer learning
</h3>
<ul>
 <li>
  Aligned Image-Word Representations Improve Inductive Transfer Across Vision-Language Tasks.
  <a href="https://arxiv.org/pdf/1704.00260.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  An Interpretable Knowledge Transfer Model for Knowledge Base Completion.
  <a href="https://arxiv.org/pdf/1704.05908.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Arbitrary Style Transfer In Real-Time With Adaptive Instance Normalization.
  <a href="https://openreview.net/pdf?id=B1fUVMzKg">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/xunhuang1995/AdaIN-style">
   <code>
    code
   </code>
  </a>
  <a href="https://github.com/elleryqueenhomels/arbitrary_style_transfer">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model.
  <a href="https://arxiv.org/pdf/1706.01554.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/jiasenlu/visDial.pytorch">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  Borrowing Treasures from the Wealthy: Deep Transfer Learning through Selective Joint Fine-tuning.
  <a href="https://arxiv.org/pdf/1702.08690.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Bringing Impressionism to Life with Neural Style Transfer in Come Swim.
  <a href="https://arxiv.org/pdf/1701.04928.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Characterizing and Improving Stability in Neural Style Transfer.
  <a href="https://arxiv.org/pdf/1705.02092.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  DSSD : Deconvolutional Single Shot Detector.
  <a href="https://arxiv.org/pdf/1701.06659.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deep Photo Style Transfer.
  <a href="https://arxiv.org/pdf/1703.07511.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/luanfujun/deep-photo-styletransfer">
   <code>
    torch
   </code>
  </a>
 </li>
 <li>
  Demystifying Neural Style Transfer.
  <a href="https://arxiv.org/pdf/1701.01036.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/lyttonhao/Neural-Style-MMD">
   <code>
    mxnet
   </code>
  </a>
 </li>
 <li>
  Fashioning with Networks: Neural Style Transfer to Design Clothes.
  <a href="https://arxiv.org/pdf/1707.09899.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning More Universal Representations for Transfer-Learning.
  <a href="https://arxiv.org/pdf/1712.09708.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning Transferable Architectures for Scalable Image Recognition.
  <a href="https://arxiv.org/pdf/1707.07012.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//titu1994/Keras-NASNet">
   <code>
    keras
   </code>
  </a>
 </li>
 <li>
  Massive Exploration of Neural Machine Translation Architectures.
  <a href="https://arxiv.org/pdf/1703.03906.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/google/seq2seq/">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  <a href="https://zhuanlan.zhihu.com/p/25892708">
   Multi-style Generative Network for Real-time Transfer.
  </a>
  <a href="https://arxiv.org/pdf/1703.06953.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/zhanghang1989/MSG-Net">
   <code>
    torch
   </code>
  </a>
  :star:
 </li>
 <li>
  Mutual Alignment Transfer Learning.
  <a href="https://arxiv.org/pdf/1707.07907.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  One-Shot Imitation Learning.
  <a href="https://arxiv.org/pdf/1703.07326.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  One-Shot Visual Imitation Learning via Meta-Learning.
  <a href="https://arxiv.org/pdf/1709.04905.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650723351&amp;idx=5&amp;sn=704f34894da5166701baa9af39684b3e">
   PathNet: Evolution Channels Gradient Descent in Super Neural Networks.
  </a>
  <a href="https://arxiv.org/pdf/1701.08734.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Question Answering through Transfer Learning from Large Fine-grained Supervision Data.
  <a href="https://arxiv.org/pdf/1702.02171.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Real-Time Neural Style Transfer for Videos.
  <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Real-Time_Neural_Style_CVPR_2017_paper.pdf">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Son of Zorn's Lemma: Targeted Style Transfer Using Instance-aware Semantic Segmentation.
  <a href="https://arxiv.org/pdf/1701.02357.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  StyleBank: An Explicit Representation for Neural Image Style Transfer.
  <a href="https://arxiv.org/pdf/1703.09210.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Style Transfer for Anime Sketches with Enhanced Residual U-net and Auxiliary Classifier GAN.
  <a href="https://arxiv.org/pdf/1706.03319.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Style Transfer Generative Adversarial Networks: Learning To Play Chess Differently.
  <a href="https://openreview.net/pdf?id=HkpbnufYe">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  The Space of Transferable Adversarial Examples.
  <a href="https://arxiv.org/pdf/1704.03453.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Transferring Face Verification Nets To Pain and Expression Regression.
  <a href="https://arxiv.org/pdf/1702.06925.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/happynear/PainRegression">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Transfer Learning for Low-Resource Chinese Word Segmentation with a Novel Neural Network.
  <a href="https://arxiv.org/pdf/1702.04488.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/jincy520/Low-Resource-CWS-">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Transfer learning for music classification and regression tasks.
  <a href="https://arxiv.org/pdf/1703.09179.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/keunwoochoi/transfer_learning_music">
   <code>
    keras
   </code>
  </a>
 </li>
 <li>
  Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks.
  <a href="https://arxiv.org/pdf/1703.06345.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Transfer Learning with Label Noise.
  <a href="https://arxiv.org/pdf/1707.09724.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Unsupervised Image-to-Image Translation Networks.
  <a href="https://arxiv.org/pdf/1703.00848.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Universal Style Transfer via Feature Transforms.
  <a href="https://arxiv.org/pdf/1705.08086.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/Yijunmaverick/UniversalStyleTransfer">
   <code>
    torch
   </code>
  </a>
 </li>
 <li>
  Visual Attribute Transfer through Deep Image Analogy.
  <a href="https://arxiv.org/pdf/1705.01088.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/msracver/Deep-Image-Analogy">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Zero-Shot Transfer Learning for Event Extraction.
  <a href="https://arxiv.org/pdf/1707.01066.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
