<h2>
 Nature language process
</h2>
<h3>
 deep learning
</h3>
<ul>
 <li>
  A Comparative Study of Word Embeddings for Reading Comprehension.
  <a href="https://arxiv.org/pdf/1703.00993.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Advances in Pre-Training Distributed Word Representations.
  <a href="https://arxiv.org/pdf/1712.09405.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  An Automated Text Categorization Framework based on Hyperparameter Optimization.
  <a href="https://arxiv.org/pdf/1704.01975.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/INGEOTEC/microTC">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  An Embedded Deep Learning based Word Prediction.
  <a href="https://arxiv.org/pdf/1707.01662.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/meinwerk/WordPrediction">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  An End-to-End Architecture for Keyword Spotting and Voice Activity Detection.
  <a href="https://arxiv.org/pdf/1611.09405.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/mindorii/kws">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  A Neural Framework for Generalized Topic Models.
  <a href="https://arxiv.org/pdf/1705.09296.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/dallascard/neural_topic_models">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  An online sequence-to-sequence model for noisy speech recognition.
  <a href="https://arxiv.org/pdf/1706.06428.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  A Read-Write Memory Network for Movie Story Understanding.
  <a href="https://arxiv.org/pdf/1709.09345.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/seilna/RWMN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning.
  <a href="https://arxiv.org/pdf/1711.07613.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Review on Deep Learning Techniques Applied to Semantic Segmentation.
  <a href="https://arxiv.org/pdf/1704.06857.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Structured Self-attentive Sentence Embedding.
  <a href="https://arxiv.org/pdf/1703.03130.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/kaushalshetty/Structured-Self-Attention">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  A Tidy Data Model for Natural Language Processing using cleanNLP.
  <a href="https://arxiv.org/pdf/1703.09570.pdf">
   <code>
    arXiv
   </code>
  </a>
 </li>
 <li>
  Attention-based Mixture Density Recurrent Networks for History-based Recommendation.
  <a href="https://arxiv.org/pdf/1709.07545.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Attention-based Natural Language Person Retrieval.
  <a href="https://arxiv.org/pdf/1705.08923.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Autoencoding Variational Inference For Topic Models.
  <a href="https://arxiv.org/pdf/1703.01488.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/akashgit/autoencoding_vi_for_topic_models">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Automated Crowdturfing Attacks and Defenses in Online Review Systems.
  <a href="https://arxiv.org/pdf/1708.08151.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Automatic Rule Extraction from Long Short Term Memory Networks.
  <a href="https://arxiv.org/pdf/1702.02540.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Wavenet for Speech Denoising.
  <a href="https://arxiv.org/pdf/1706.07162.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/drethage/speech-denoising-wavenet">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Big Picture Machine Learning: Classifying Text with Neural Networks and TensorFlow.
  <a href="https://medium.freecodecamp.com/big-picture-machine-learning-classifying-text-with-neural-networks-and-tensorflow-d94036ac2274">
   <code>
    url
   </code>
  </a>
  <a href="https://github.com/dmesquita/understanding_tensorflow_nn">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Billion-scale similarity search with GPUs.
  <a href="https://arxiv.org/pdf/1702.08734.pdf">
   <code>
    arXiv
   </code>
  </a>
  <a href="https://github.com/facebookresearch/faiss">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Collaborative Recurrent Neural Networks for Dynamic Recommender Systems.
  <a href="https://infoscience.epfl.ch/record/222477/files/ko101.pdf">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/lca4/collaborative-rnn">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Context Aware Document Embedding.
  <a href="https://arxiv.org/pdf/1707.01521.pdf">
   <code>
    arXiv
   </code>
  </a>
 </li>
 <li>
  DANCin SEQ2SEQ: Fooling Text Classifiers with Adversarial Text Example Generation.
  <a href="https://arxiv.org/pdf/1712.05419.pdf">
   <code>
    arXiv
   </code>
  </a>
  <a href="https://github.com//CatherineWong/dancin_seq2seq">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Deep Collaborative Autoencoder for Recommender Systems: A Unified Framework for Explicit and Implicit Feedback.
  <a href="https://arxiv.org/pdf/1712.09043.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deep Multitask Learning for Semantic Dependency Parsing.
  <a href="https://arxiv.org/pdf/1704.06855.pdf">
   <code>
    arXiv
   </code>
  </a>
 </li>
 <li>
  Deep Recurrent Neural Network for Protein Function Prediction from Sequence.
  <a href="https://arxiv.org/pdf/1701.08318.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deep Voice: Real-time Neural Text-to-Speech.
  <a href="https://arxiv.org/pdf/1702.07825.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Depthwise Separable Convolutions for Neural Machine Translation.
  <a href="https://arxiv.org/pdf/1706.03059.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Detecting Oriented Text in Natural Images by Linking Segments.
  <a href="https://arxiv.org/pdf/1703.06520.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/dengdan/seglink">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Dialog Context Language Modeling with Recurrent Neural Networks.
  <a href="https://arxiv.org/pdf/1701.04056.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  dna2vec: Consistent vector representations of variable-length k-mers.
  <a href="https://arxiv.org/pdf/1701.06279.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://pnpnpn.github.io/dna2vec/">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  DropoutNet: Addressing Cold Start in Recommender Systems.
  <a href="http://www.cs.toronto.edu/~mvolkovs/nips2017_deepcf.pdf">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com//layer6ai-labs/DropoutNet">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Efficient Natural Language Response Suggestion for Smart Reply.
  <a href="https://arxiv.org/pdf/1705.00652.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Efficient Vector Representation For Documents Through Corruption.
  <a href="https://openreview.net/pdf?id=B1Igu2ogg">
   <code>
    pdf
   </code>
  </a>
  ]
  <a href="https://github.com/mchen24/iclr2017">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  End-to-End MAP Training of a Hybrid HMM-DNN Model.
  <a href="https://arxiv.org/pdf/1703.10356.pdf">
   <code>
    arXiv
   </code>
  </a>
 </li>
 <li>
  End-to-end semantic face segmentation with conditional random fields as convolutional, recurrent and adversarial networks.
  <a href="https://arxiv.org/pdf/1703.03305.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  End-to-End Task-Completion Neural Dialogue Systems.
  <a href="https://arxiv.org/pdf/1703.01008.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/MiuLab/TC-Bot">
   <code>
    code
   </code>
  </a>
  ] :star:
 </li>
 <li>
  Explaining Recurrent Neural Network Predictions in Sentiment Analysis.
  <a href="https://arxiv.org/pdf/1706.07206.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ArrasL/LRP_for_LSTM">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Fast and Accurate Entity Recognition with Iterated Dilated Convolutions.
  <a href="https://arxiv.org/pdf/1702.02098.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/iesl/dilated-cnn-ner">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Focusing Attention: Towards Accurate Text Recognition in Natural Images.
  <a href="https://arxiv.org/pdf/1709.02054.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learned in Translation: Contextualized Word Vectors.
  <a href="https://arxiv.org/pdf/1708.00107.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/salesforce/cove">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Letter-Based Speech Recognition with Gated ConvNets.
  <a href="https://arxiv.org/pdf/1712.09444.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//facebookresearch/wav2letter">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Generating Sentences by Editing Prototypes.
  <a href="https://arxiv.org/pdf/1709.08878.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Get To The Point: Summarization with Pointer-Generator Networks.
  <a href="https://arxiv.org/pdf/1704.04368.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/becxer/pointer-generator">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  <a href="http://research.baidu.com/gnr/">
   Globally Normalized Reader.
  </a>
  <a href="https://www.aclweb.org/anthology/D17-1112">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/baidu-research/GloballyNormalizedReader">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Global Relation Embedding for Relation Extraction.
  <a href="https://arxiv.org/pdf/1704.05958.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ppuliu/GloRE">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Graph Convolutional Networks for Named Entity Recognition.
  <a href="https://arxiv.org/pdf/1709.10053.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ContextScout/gcn_ner">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Graph Embedding Techniques, Applications, and Performance: A Survey.
  <a href="https://arxiv.org/pdf/1705.02801.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/palash1992/GEM">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Joint Semantic Synthesis and Morphological Analysis of the Derived Word.
  <a href="https://arxiv.org/pdf/1701.00946.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Jointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs.
  <a href="https://arxiv.org/pdf/1705.09189.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Jointly Trained Sequential Labeling and Classification by Sparse Attention Neural Networks.
  <a href="https://arxiv.org/pdf/1709.10191.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  HDLTex: Hierarchical Deep Learning for Text Classification.
  <a href="https://arxiv.org/pdf/1709.08267.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/kk7nc/HDLTex">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  High-Throughput and Language-Agnostic Entity Disambiguation and Linking on User Generated Data.
  <a href="https://arxiv.org/pdf/1703.04498.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Improving End-to-End Speech Recognition with Policy Learning.
  <a href="https://arxiv.org/pdf/1712.07101.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Improving speech recognition by revising gated recurrent units.
  <a href="https://arxiv.org/pdf/1710.00641.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Label Refinement Network for Coarse-to-Fine Semantic Segmentation.
  <a href="https://arxiv.org/pdf/1703.00551.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  LangPro: Natural Language Theorem Prover.
  <a href="https://arxiv.org/pdf/1708.09417.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/kovvalsky/LangPro">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Language Generation with Recurrent Generative Adversarial Networks without Pre-training.
  <a href="https://arxiv.org/pdf/1706.01399.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/amirbar/rnn.wgan">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  LanideNN: Multilingual Language Identification on Character Window.
  <a href="https://arxiv.org/pdf/1701.03338.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/tomkocmi/LanideNN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Latent Intention Dialogue Models.
  <a href="https://arxiv.org/pdf/1705.10229.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning a Natural Language Interface with Neural Programmer.
  <a href="https://arxiv.org/pdf/1611.08945.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/tensorflow/models/tree/master/neural_programmer">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Learning Arbitrary Potentials in CRFs with Gradient Descent.
  <a href="https://arxiv.org/pdf/1701.06805.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning Convolutional Text Representations for Visual Question Answering.
  <a href="https://arxiv.org/pdf/1705.06824.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/divelab/vqa-text">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders.
  <a href="https://arxiv.org/pdf/1703.10960.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/snakeztc/NeuralDialog-CVAE">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Learning Important Features Through Propagating Activation Differences.
  <a href="https://arxiv.org/pdf/1704.02685.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/kundajelab/deeplift">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Learning Structured Text Representations.
  <a href="https://arxiv.org/pdf/1705.09207.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//nlpyang/structured">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Network.
  <a href="https://arxiv.org/pdf/1706.02337.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning to Generate Reviews and Discovering Sentiment.
  <a href="https://arxiv.org/pdf/1704.01444.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/openai/generating-reviews-discovering-sentiment">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Learning to Reason: End-to-End Module Networks for Visual Question Answering.
  <a href="https://arxiv.org/pdf/1704.05526.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ronghanghu/n2nmn">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Learning to Skim Text.
  <a href="https://arxiv.org/pdf/1704.06877.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning with Confident Examples: Rank Pruning for Robust Classification with Noisy Labels.
  <a href="https://arxiv.org/pdf/1705.01936.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/cgnorthcutt/rankpruning">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Machine Comprehension by Text-to-Text Neural Question Generation.
  <a href="https://arxiv.org/pdf/1705.02012.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Multi-Label Classification of Patient Notes a Case Study on ICD Code Assignment.
  <a href="https://arxiv.org/pdf/1709.09587.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/talbaumel/MIMIC">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities.
  <a href="https://arxiv.org/pdf/1701.02025.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Named Entity Recognition with stack residual LSTM and trainable bias decoding.
  <a href="https://arxiv.org/pdf/1706.07598.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Natural Language Processing with Small Feed-Forward Networks.
  <a href="https://arxiv.org/pdf/1708.00214.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Neural Probabilistic Model for Non-projective MST Parsing.
  <a href="https://arxiv.org/pdf/1701.00874.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics.
  <a href="http://www.aclweb.org/anthology/D17-1023">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/zhezhaoa/ngram2vec">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  N-gram Language Modeling using Recurrent Neural Network Estimation.
  <a href="https://arxiv.org/pdf/1703.10724.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s/ckxFVBZtKYd-EBOkglnFPA">
   Non-Autoregressive Neural Machine Translation.
  </a>
  <a href="https://einstein.ai/static/images/pages/research/non-autoregressive-neural-mt.pdf">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/salesforce/nonauto-nmt">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  One Representation per Word - Does it make Sense for Composition?.
  <a href="https://arxiv.org/pdf/1702.06696.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Optimal Hyperparameters for Deep LSTM-Networks for Sequence Labeling Tasks.
  <a href="https://arxiv.org/pdf/1707.06799.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Outlier Detection for Text Data : An Extended Version.
  <a href="https://128.84.21.199/abs/1701.01325v1">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Person Search with Natural Language Description.
  <a href="https://arxiv.org/pdf/1702.05729.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Poincaré Embeddings for Learning Hierarchical Representations.
  <a href="https://arxiv.org/pdf/1705.08039.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/TatsuyaShirakawa/poincare-embedding">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Question Answering from Unstructured Text by Retrieval and Comprehension.
  <a href="https://arxiv.org/pdf/1703.08885.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations.
  <a href="https://arxiv.org/pdf/1702.07826.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Recurrent and Contextual Models for Visual Question Answering.
  <a href="https://arxiv.org/pdf/1703.08120.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Recurrent neural networks with specialized word embeddings for health-domain named-entity recognition.
  <a href="https://arxiv.org/pdf/1706.09569.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ijauregiCMCRC/healthNER">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Recurrent Recommender Networks.
  <a href="http://alexbeutel.com/papers/rrn_wsdm2017.pdf">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Recurrent Relational Networks for Complex Relational Reasoning.
  <a href="https://arxiv.org/pdf/1711.08028.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  R-NET: Machine Reading Comprehension With Self-Matching Networks.
  <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/HKUST-KnowComp/R-Net">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  SearchQA: A New Q&amp;A Dataset Augmented with Context from a Search Engine.
  <a href="https://arxiv.org/pdf/1704.05179.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/nyu-dl/SearchQA">
   <code>
    pytorch
   </code>
  </a>
  :star:
 </li>
 <li>
  Sentiment Analysis by Joint Learning of Word Embeddings and Classifier.
  <a href="https://arxiv.org/pdf/1708.03995.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <a href="http://weibo.com/ttarticle/p/show?id=2309351000224096147458277147&amp;u=1402400261&amp;m=4096554547383656&amp;cu=3291965747&amp;ru=1402400261&amp;rm=4091597186263507">
   Semantic Instance Segmentation via Deep Metric Learning.
  </a>
  <a href="https://arxiv.org/pdf/1703.10277.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Semantic keyword spotting by learning from images and speech.
  <a href="https://arxiv.org/pdf/1710.01949.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/kamperh/semantic_flickraudio">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  State-of-the-art Speech Recognition With Sequence-to-Sequence Models.
  <a href="https://arxiv.org/pdf/1712.01769.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  struc2vec: Learning Node Representations from Structural Identity.
  <a href="https://arxiv.org/pdf/1704.03165.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/leoribeiro/struc2vec">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Symbolic, Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning: a Survey.
  <a href="https://arxiv.org/pdf/1702.00764.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <b>
   [Tacotron]
  </b>
  A Fully End-to-End Text-To-Speech Synthesis Model.
  <a href="https://arxiv.org/pdf/1703.10135.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/google/tacotron">
   <code>
    code
   </code>
  </a>
  <a href="https://github.com/Kyubyong/tacotron">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Text Summarization using Abstract Meaning Representation.
  <a href="https://arxiv.org/pdf/1706.01678.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering.
  <a href="https://arxiv.org/pdf/1704.04497.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  The NarrativeQA Reading Comprehension Challenge.
  <a href="https://arxiv.org/pdf/1712.07040.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Topic Compositional Neural Language Model.
  <a href="https://arxiv.org/pdf/1712.09783.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Topics and Label Propagation: Best of Both Worlds for Weakly Supervised Text Classification.
  <a href="https://arxiv.org/pdf/1712.02767.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Twitter100k: A Real-world Dataset for Weakly Supervised Cross-Media Retrieval.
  <a href="https://arxiv.org/pdf/1703.06618.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/huyt16/Twitter100k/">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Understanding Convolution for Semantic Segmentation.
  <a href="https://arxiv.org/pdf/1702.08502.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/TuSimple/TuSimple-DUC/">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Unsupervised Neural Machine Translation.
  <a href="https://arxiv.org/pdf/1710.11041.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/artetxem/undreamt">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  Vector Embedding of Wikipedia Concepts and Entities.
  <a href="https://arxiv.org/pdf/1702.03470.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/ehsansherkat/ConVec">
   <code>
    code
   </code>
  </a>
  ]
 </li>
 <li>
  VINet: Visual-Inertial Odometry as a Sequence-to-Sequence Learning Problem.
  <a href="https://arxiv.org/pdf/1701.08376.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  What's in a Question: Using Visual Questions as a Form of Supervision.
  <a href="https://arxiv.org/pdf/1704.03895.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/sidgan/whats_in_a_question">
   <code>
    code
   </code>
  </a>
 </li>
</ul>
<h3>
 Attention and memory
</h3>
<ul>
 <li>
  Attentive Memory Networks: Efficient Machine Reading for Conversational Search.
  <a href="https://arxiv.org/pdf/1712.07229.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering.
  <a href="http://www.panderson.me/images/1707.07998-up-down.pdf">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com//peteanderson80/bottom-up-attention">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Gated-Attention Architectures for Task-Oriented Language Grounding.
  <a href="https://arxiv.org/pdf/1706.07230.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/devendrachaplot/DeepRL-Grounding">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  End-to-End Attention based Text-Dependent Speaker Verification.
  <a href="https://arxiv.org/pdf/1701.00562.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Frustratingly Short Attention Spans in Neural Language Modeling.
  <a href="https://arxiv.org/pdf/1702.04521.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Recurrent Neural Network-Based Sentence Encoder with Gated Attention for Natural Language Inference.
  <a href="https://arxiv.org/pdf/1708.01353.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/lukecq1231/enc_nli">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Single Shot Text Detector with Regional Attention.
  <a href="https://arxiv.org/pdf/1709.00138.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//BestSonny/SSTD">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Structural Attention Neural Networks for improved sentiment analysis.
  <a href="https://arxiv.org/pdf/1701.01811.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h3>
 Generative learning
</h3>
<ul>
 <li>
  Adversarial Generation of Natural Language.
  <a href="https://arxiv.org/pdf/1705.10929.pdf">
   <code>
    arXiv
   </code>
  </a>
 </li>
 <li>
  <a href="https://zhuanlan.zhihu.com/p/25027693">
   Adversarial Learning for Neural Dialogue Generation.
  </a>
  <a href="https://arxiv.org/pdf/1701.06547.pdf">
   <code>
    arXiv
   </code>
  </a>
  <a href="https://github.com/BigPlay/Adversarial-Learning-for-Neural-Dialogue-Generation-in-Tensorflow">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Adversarial Multi-Criteria Learning for Chinese Word Segmentation.
  <a href="https://arxiv.org/pdf/1704.07556.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/FudanNLP/adversarial-multi-criteria-learning-for-CWS">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Adversarial Neural Machine Translation.
  <a href="https://arxiv.org/pdf/1704.06933.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Data-driven Natural Language Generation: Paving the Road to Success.
  <a href="https://arxiv.org/pdf/1706.09433.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Get To The Point: Summarization with Pointer-Generator Networks.
  <a href="https://arxiv.org/pdf/1704.04368.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Improved Variational Autoencoders for Text Modeling using Dilated Convolutions.
  <a href="https://arxiv.org/pdf/1702.08139.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models.
  <a href="https://arxiv.org/pdf/1705.10513.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/geek-ai/irgan">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  KATE: K-Competitive Autoencoder for Text.
  <a href="https://arxiv.org/pdf/1705.02033.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/hugochan/K-Competitive-Autoencoder-for-Text">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Long Text Generation via Adversarial Training with Leaked Information.
  <a href="https://arxiv.org/pdf/1709.08624.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/CR-Gjx/LeakGAN">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models.
  <a href="https://arxiv.org/pdf/1705.10843.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/gablg1/ORGAN">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Training Deep AutoEncoders for Collaborative Filtering.
  <a href="https://arxiv.org/pdf/1708.01715.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/okuchaiev/DeepRecoEncoders">
   <code>
    code
   </code>
  </a>
 </li>
</ul>
<h3>
 Transfer learning
</h3>
<ul>
 <li>
  Better Text Understanding Through Image-To-Text Transfer.
  <a href="https://arxiv.org/pdf/1705.08386.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Domain Adaptation in Question Answering.
  <a href="https://arxiv.org/pdf/1702.02171.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Neural Machine Translation and Sequence-to-sequence Models: A Tutorial.
  <a href="https://arxiv.org/pdf/1703.01619.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Semantic Autoencoder for Zero-Shot Learning.
  <a href="https://arxiv.org/pdf/1704.08345.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/Elyorcv/SAE">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Transfer Learning for Low-Resource Chinese Word Segmentation with a Novel Neural Network.
  <a href="https://arxiv.org/pdf/1702.04488.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/jincy520/Low-Resource-CWS-">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Transfer Learning for Named-Entity Recognition with Neural Networks.
  <a href="https://arxiv.org/pdf/1705.06273.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/Franck-Dernoncourt/NeuroNER">
   <code>
    code
   </code>
  </a>
 </li>
</ul>
