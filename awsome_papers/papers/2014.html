<h2>
 2014
</h2>
<h3>
 Deep Learning
</h3>
<ul>
 <li>
  A survey of multiple classifier systems as hybrid systems.
  <a href="http://www.sciencedirect.com/science/article/pii/S156625351300047X">
   <code>
    science
   </code>
  </a>
  :star:
 </li>
 <li>
  A survey on feature selection methods.
  <a href="http://www.sciencedirect.com/science/article/pii/S0045790613003066">
   <code>
    science
   </code>
  </a>
  :star:
 </li>
 <li>
  <b>
   [DeepFaceVariant]
  </b>
  <a href="http://www.ifight.me/197/">
   Deep Learning Face Representation from Predicting 10,000 Classes.
  </a>
  [
  <a href="http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf">
   pdf
  </a>
  ] [
  <a href="https://github.com/joyhuang9473/deepid-implementation">
   code
  </a>
  ] :star:
 </li>
 <li>
  Dropout: A Simple Way to Prevent Neural Networks from Overfitting. [
  <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Generative Moment Matching Networks. [
  <a href="https://arxiv.org/pdf/1502.02761.pdf">
   arxiv
  </a>
  ] [
  <a href="https://github.com/yujiali/gmmn">
   code
  </a>
  ]
 </li>
 <li>
  <b>
   [Inception V1]
  </b>
  <a href="http://blog.csdn.net/u014114990/article/details/50370446">
   Going Deeper with Convolutions
  </a>
  . [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjHpOvi5NDQAhUCxLwKHU4BBM8QFgguMAE&amp;url=https%3A%2F%2Fwww.cs.unc.edu%2F~wliu%2Fpapers%2FGoogLeNet.pdf&amp;usg=AFQjCNHSEJVb0PWLBIG-Y-zWh9gRv9ehBQ">
   url
  </a>
  ] :star:
 </li>
 <li>
  Learning Longer Memory in Recurrent Neural Networks. [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiOkqOu5dDQAhVFa7wKHc7pCdgQFggsMAE&amp;url=https%3A%2F%2Farxiv.org%2Fpdf%2F1412.7753&amp;usg=AFQjCNEz4_vREocEuriflTVFg0GrMmaqfw">
   url
  </a>
  ]
 </li>
 <li>
  Learning to Execute. [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiVoZuO5tDQAhWJwLwKHVouD40QFggdMAA&amp;url=https%3A%2F%2Farxiv.org%2Fabs%2F1410.4615&amp;usg=AFQjCNEXYyZHLwwTzovP3pHsWa_jxvWvEQ">
   url
  </a>
  ]
 </li>
 <li>
  <a href="http://blog.csdn.net/happyer88/article/details/51418059">
   Multi-scale Orderless Pooling of Deep Convolutional Activation Features.
  </a>
  <a href="https://arxiv.org/pdf/1403.1840.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  <a href="http://blog.csdn.net/hjimce/article/details/50458190">
   Network In Network
  </a>
  .
  <a href="https://arxiv.org/pdf/1312.4400.pdf">
   <code>
    arxiv
   </code>
  </a>
  ] :star:
 </li>
 <li>
  <b>
   [OverFeat]
  </b>
  <a href="http://blog.csdn.net/whiteinblue/article/details/43374195">
   OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks.
  </a>
  <a href="https://arxiv.org/pdf/1312.6229.pdf">
   <code>
    arxiv
   </code>
  </a>
  ]
  <a href="https://github.com/sermanet/OverFeat">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Recurrent Neural Network Regularization.
  <a href="https://arxiv.org/pdf/1409.2329.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Show and Tell: A Neural Image Caption Generator.[
  <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjL6s6Xn47RAhVlqVQKHaynDI4QFggnMAE&amp;url=%68%74%74%70%73%3a%2f%2f%61%72%78%69%76%2e%6f%72%67%2f%70%64%66%2f%31%34%31%31%2e%34%35%35%35&amp;usg=AFQjCNEawcm4ZOK9ZVIgCjylPb2HY1UOug">
   url
  </a>
  ] :star:
 </li>
 <li>
  <b>
   [SPPNet]
  </b>
  <a href="http://blog.csdn.net/whiteinblue/article/details/43415035">
   Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition.
  </a>
  <a href="https://arxiv.org/pdf/1406.4729.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/yhenon/keras-spp">
   <code>
    keras
   </code>
  </a>
  :star:
 </li>
 <li>
  Striving for Simplicity: The All Convolutional Net. [
  <a href="https://arxiv.org/pdf/1412.6806.pdf">
   arxiv
  </a>
  ] :star:
 </li>
 <li>
  Towards end-to-end speech recognition with recurrent neural networks.[
  <a href="http://jmlr.org/proceedings/papers/v32/graves14.pdf">
   pdf
  </a>
  ] :star:
 </li>
 <li>
  <b>
   [VGGNet]
  </b>
  <a href="http://www.cnblogs.com/xuanyuyt/p/5743758.html">
   Very Deep Convolutional Networks for Large-Scale Image Recognition
  </a>
  .
  <a href="https://arxiv.org/pdf/1409.1556.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://gist.github.com/ksimonyan/211839e770f7b538e2d8">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  What Regularized Auto-Encoders Learn from the Data.
  <a href="https://arxiv.org/pdf/1211.4246.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h3>
 Generative learning
</h3>
<ul>
 <li>
  <b>
   [GAN]
  </b>
  <a href="http://blog.csdn.net/solomon1558/article/details/52549409">
   Generative Adversarial Nets.
  </a>
  <a href="https://arxiv.org/pdf/1406.2661.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/goodfeli/adversarial">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  <b>
   [CGNA]
  </b>
  <a href="http://blog.csdn.net/solomon1558/article/details/52555083">
   Conditional Generative Adversarial Nets.
  </a>
  <a href="https://arxiv.org/pdf/1411.1784.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/zhangqianhui/Conditional-Gans">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Deep Visual-Semantic Alignments for Generating Image Descriptions.
  <a href="https://arxiv.org/pdf/1412.2306.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Explaining and Harnessing Adversarial Examples.
  <a href="https://arxiv.org/pdf/1412.6572.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  On distinguishability criteria for estimating generative models.
  <a href="https://arxiv.org/pdf/1412.6515.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <b>
   [VAE]
  </b>
  Semi-Supervised Learning with Deep Generative Models.
  <a href="https://arxiv.org/pdf/1406.5298.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/dpkingma/nips14-ssl">
   <code>
    code
   </code>
  </a>
  <a href="https://github.com/hwalsuklee/tensorflow-mnist-CVAE">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
</ul>
<h3>
 Attention and memory
</h3>
<ul>
 <li>
  End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results. [
  <a href="https://arxiv.org/pdf/1412.1602.pdf">
   arxiv
  </a>
  ]
 </li>
 <li>
  Memory Networks. [
  <a href="https://arxiv.org/pdf/1410.3916.pdf">
   arxiv
  </a>
  ] :star:
 </li>
 <li>
  <a href="http://www.cnblogs.com/wangxiaocvpr/p/5559961.html">
   Multiple Object Recognition with Visual Attention.
  </a>
  [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjW5KK95tDQAhVEbbwKHU3yC40QFgguMAE&amp;url=https%3A%2F%2Farxiv.org%2Fpdf%2F1412.7755&amp;usg=AFQjCNEdl2iMZSeK_mYsIKs8HXm4yI6zKQ">
   url
  </a>
  ]
 </li>
 <li>
  <b>
   [Attention In NLP First]
  </b>
  <a href="http://blog.csdn.net/u011414416/article/details/51057789">
   Neural Machine Translation by Jointly Learning to Align and Translate.
  </a>
  [
  <a href="https://arxiv.org/pdf/1409.0473.pdf">
   arxiv
  </a>
  ] [
  <a href="https://github.com/spro/torch-seq2seq-attention">
   code
  </a>
  ] :star:
 </li>
 <li>
  <b>
   [First Memory Paper]
  </b>
  Neural Turing Machines. [
  <a href="https://arxiv.org/pdf/1410.5401.pdf">
   arxiv
  </a>
  ] :star:
 </li>
 <li>
  <b>
   [RAM]
  </b>
  <a href="http://www.cnblogs.com/wangxiaocvpr/p/5537454.html">
   Recurrent Models of Visual Attention.
  </a>
  [
  <a href="https://arxiv.org/pdf/1406.6247.pdf">
   arxiv
  </a>
  ] [
  <a href="https://github.com/jlindsey15/RAM">
   tensorflow
  </a>
  ] :star:
 </li>
 <li>
  <b>
   [Seq2Seq]
  </b>
  Sequence to Sequence Learning with Neural Networks.  [
  <a href="https://arxiv.org/pdf/1409.3215.pdf">
   arxiv
  </a>
  ] [
  <a href="https://github.com/farizrahman4u/seq2seq">
   code
  </a>
  ] :star:
 </li>
</ul>
<h3>
 Deep Reinforcement Learning
</h3>
<ul>
 <li>
  Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning.[
  <a href="http://papers.nips.cc/paper/5421-deep-learning-for-real-time-atari-game-play-using-offline-monte-carlo-tree-search-planning.pdf">
   url
  </a>
  ]
 </li>
</ul>
<h3>
 Transfer learning
</h3>
<ul>
 <li>
  Adaptation regularization: a general framework for transfer learning. [
  <a href="http://www3.ntu.edu.sg/home/sinnopan/publications/[TKDE14]Adaptation%20Regularization%20A%20General%20Framework%20for%20Transfer%20Learning.pdf">
   pdf
  </a>
  ] :star:
 </li>
 <li>
  Heterogeneous Domain Adaptation for Multiple Classes. [
  <a href="http://jmlr.org/proceedings/papers/v33/zhou14.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  How transferable are features in deep neural networks?
  <a href="https://arxiv.org/pdf/1411.1792.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Hybrid heterogeneous transfer learning through deep learning. [
  <a href="http://www.ntu.edu.sg/home/sinnopan/publications/[AAAI14]Hybrid%20Heterogeneous%20Transfer%20Learning%20through%20Deep%20Learning.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Learning with Augmented Features for Supervised and Semi-supervised Heterogeneous Domain Adaptation. [
  <a href="http://lxduan.info/papers/LiTPAMI2014.pdf">
   pdf
  </a>
  ]
 </li>
 <li>
  Machine learning for targeted display advertising: transfer learning in action. [
  <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiZr8H5uY7RAhXJq1QKHUocC64QFggfMAA&amp;url=http%3A%2F%2Fdstillery.com%2Fwp-content%2Fuploads%2F2014%2F05%2FMachine-learning_target-display.pdf&amp;usg=AFQjCNGDcM3pAUJ9-ZL7i0ujCUIWHenABQ">
   url
  </a>
  ]
 </li>
 <li>
  Source Free Transfer Learning for Text Classification. [
  <a href="http://www.cse.ust.hk/~yinz/SourceFreeTransferLearningforTextClassification.pdf">
   pdf
  </a>
  ]
 </li>
</ul>
<h3>
 Natural language process
</h3>
<ul>
 <li>
  <a href="http://www.jeyzhang.com/cnn-apply-on-modelling-sentence.html">
   A Convolutional Neural Network for Modelling Sentences.
  </a>
  <a href="https://arxiv.org/pdf/1404.2188.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/FredericGodin/DynamicCNN">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Automatic Construction and Natural-Language Description of Nonparametric Regression Models. [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwj3xcWb49DQAhWIw7wKHSXFCfEQFggrMAE&amp;url=http%3A%2F%2Fwww.aaai.org%2Focs%2Findex.php%2FAAAI%2FAAAI14%2Fpaper%2FviewFile%2F8240%2F8564&amp;usg=AFQjCNFyni0wwo38CsLRVtSPMm6BlL7QpA">
   url
  </a>
  ]
 </li>
 <li>
  <a href="https://arxiv.org/pdf/1408.5882.pdf">
   Convolutional Neural Networks for Sentence Classification.
  </a>
  <a href="https://arxiv.org/pdf/1408.5882.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/abhaikollara/CNN-Sentence-Classification">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Distributed Representations of Sentences and Documents Generating Distribution. [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiulvPb49DQAhUGbrwKHeFRAlsQFggiMAA&amp;url=http%3A%2F%2Fcs.stanford.edu%2F~quocle%2Fparagraph_vector.pdf&amp;usg=AFQjCNESECVF_9eXAkAjfSqqHrqlxkVQgg">
   url
  </a>
  ] :star:
 </li>
 <li>
  Effective Use of Word Order for Text Categorization with Convolutional Neural Networks. [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiIl7qS5NDQAhVD2LwKHct_CVYQFggpMAE&amp;url=https%3A%2F%2Farxiv.org%2Fpdf%2F1412.1058&amp;usg=AFQjCNHDPOYHMKWIhirkznqnLq_mw4CqMQ">
   url
  </a>
  ]
 </li>
 <li>
  Grammar as a Foreign Language. [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwitq9CI5dDQAhXCu7wKHTUIBiAQFggpMAE&amp;url=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5635-grammar-as-a-foreign-language.pdf&amp;usg=AFQjCNELENZf9OsnZ6q0LexQYcbjCHBv0w">
   url
  </a>
  ]
 </li>
 <li>
  <b>
   [GRU]
  </b>
  <a href="http://www.zmonster.me/notes/phrase_representation_using_rnn_encoder_decoder.html">
   Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation
  </a>
  .
  <a href="https://arxiv.org/pdf/1406.1078.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  On the Properties of Neural Machine Translation- Encoder-Decoder Approaches. [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjqtZDc59DQAhUGyrwKHbhDBLUQFggsMAE&amp;url=https%3A%2F%2Farxiv.org%2Fpdf%2F1409.1259&amp;usg=AFQjCNG6_CJ8ZYMv5sx4K59mRIPpHlL-Yg">
   url
  </a>
  ]
 </li>
 <li>
  On Using Very Large Target Vocabulary for Neural Machine Translation.  [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiSle7659DQAhULTbwKHfaiBsoQFggsMAE&amp;url=http%3A%2F%2Fwww.aclweb.org%2Fanthology%2FP15-1001&amp;usg=AFQjCNFUabHMFw5X9gjg26vjoDljEd4s_g">
   url
  </a>
  ]
 </li>
 <li>
  Reading Text in the Wild with Convolutional Neural Networks.  [
  <a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiBlcrmn9PQAhXGW7wKHa6VAEwQFggsMAE&amp;url=https%3A%2F%2Fwww.robots.ox.ac.uk%2F~vgg%2Fpublications%2F2016%2FJaderberg16%2Fjaderberg16.pdf&amp;usg=AFQjCNG2V55rN1HOyhtSMLcHAyiuAYFl3A">
   url
  </a>
  ]
 </li>
 <li>
  <b>
   [Seq2Seq]
  </b>
  Sequence to Sequence Learning with Neural Networks.
  <a href="https://arxiv.org/pdf/1409.3215.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/farizrahman4u/seq2seq">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
</ul>
