<h2>
 deep reinforcement learning
</h2>
<ul>
 <li>
  <a href="../../README.md">
   return to home
  </a>
 </li>
</ul>
<h3>
 papers
</h3>
<ul>
 <li>
  Asynchronous Methods for Deep Reinforcement Learning. [
  <a href="http://arxiv.org/abs/1602.01783">
   arxiv
  </a>
  ] :star:
 </li>
 <li>
  Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning, E. Parisotto, et al.,
  <em>
   ICLR
  </em>
  . [
  <a href="http://arxiv.org/abs/1511.06342">
   arxiv
  </a>
  ]
 </li>
 <li>
  A New Softmax Operator for Reinforcement Learning.[
  <a href="https://128.84.21.199/abs/1612.05628?context=cs">
   url
  </a>
  ]
 </li>
 <li>
  Benchmarking Deep Reinforcement Learning for Continuous Control, Y. Duan et al.,
  <em>
   ICML
  </em>
  . [
  <a href="https://arxiv.org/pdf/1604.06778.pdf">
   arxiv
  </a>
  ]
 </li>
 <li>
  Better Computer Go Player with Neural Network and Long-term Prediction, Y. Tian et al.,
  <em>
   ICLR
  </em>
  . [
  <a href="http://arxiv.org/abs/1511.06410">
   arxiv
  </a>
  ]
 </li>
 <li>
  Deep Reinforcement Learning in Parameterized Action Space, M. Hausknecht et al.,
  <em>
   ICLR
  </em>
  . [
  <a href="http://arxiv.org/abs/1511.04143">
   arxiv
  </a>
  ]
 </li>
 <li>
  Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks, R. Houthooft et al.,
  <em>
   arXiv
  </em>
  . [
  <a href="http://arxiv.org/abs/1605.09674">
   url
  </a>
  ]
 </li>
 <li>
  Control of Memory, Active Perception, and Action in Minecraft, J. Oh et al.,
  <em>
   ICML
  </em>
  . [
  <a href="http://arxiv.org/abs/1605.09128">
   arxiv
  </a>
  ]
 </li>
 <li>
  Continuous Deep Q-Learning with Model-based Acceleration, S. Gu et al.,
  <em>
   ICML
  </em>
  . [
  <a href="http://arxiv.org/abs/1603.00748">
   arxiv
  </a>
  ]
 </li>
 <li>
  Continuous control with deep reinforcement learning. [
  <a href="http://arxiv.org/abs/1509.02971">
   arxiv
  </a>
  ] :star:
 </li>
 <li>
  Deep Successor Reinforcement Learning. [
  <a href="http://arxiv.org/abs/1606.02396">
   arxiv
  </a>
  ]
 </li>
 <li>
  Dynamic Frame skip Deep Q Network, A. S. Lakshminarayanan et al.,
  <em>
   IJCAI Deep RL Workshop
  </em>
  . [
  <a href="http://arxiv.org/abs/1605.05365">
   arxiv
  </a>
  ]
 </li>
 <li>
  Deep Exploration via Bootstrapped DQN. [
  <a href="http://arxiv.org/abs/1602.04621">
   arxiv
  </a>
  ] :star:
 </li>
 <li>
  Deep Reinforcement Learning for Dialogue Generation. [
  <a href="https://arxiv.org/pdf/1606.01541.pdf">
   arxiv
  </a>
  ]
  <a href="https://github.com/BigPlay/Deep-Reinforcement-Learning-for-Dialogue-Generation-in-tensorflow">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Deep Reinforcement Learning in Parameterized Action Space. [
  <a href="http://arxiv.org/abs/1511.04143">
   arxiv
  </a>
  ] :star:
 </li>
 <li>
  Deep Reinforcement Learning with Successor Features for Navigation across Similar Environments.[
  <a href="https://scirate.com/arxiv/1612.05533">
   url
  </a>
  ]
 </li>
 <li>
  Designing Neural Network Architectures using Reinforcement Learning.
  <a href="https://arxiv.org/pdf/1611.02167.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://bowenbaker.github.io/metaqnn/">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Dialogue manager domain adaptation using Gaussian process reinforcement learning. [
  <a href="https://arxiv.org/pdf/1609.02846.pdf">
   arxiv
  </a>
  ]
 </li>
 <li>
  End-to-End Reinforcement Learning of Dialogue Agents for Information Access. [
  <a href="https://arxiv.org/pdf/1609.00777.pdf">
   arxiv
  </a>
  ]
 </li>
 <li>
  Generating Text with Deep Reinforcement Learning. [
  <a href="https://arxiv.org/pdf/1510.09202.pdf">
   arxiv
  </a>
  ]
 </li>
 <li>
  Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization, C. Finn et al.,
  <em>
   arXiv
  </em>
  . [
  <a href="http://arxiv.org/abs/1603.00448">
   arxiv
  </a>
  ]
 </li>
 <li>
  Hierarchical Reinforcement Learning using Spatio-Temporal Abstractions and Deep Neural Networks, R. Krishnamurthy et al.,
  <em>
   arXiv
  </em>
  . [
  <a href="https://arxiv.org/pdf/1605.05359.pdf">
   arxiv
  </a>
  ]
 </li>
 <li>
  Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation, T. D. Kulkarni et al.,
  <em>
   arXiv
  </em>
  . [
  <a href="https://arxiv.org/pdf/1604.06057.pdf">
   arxiv
  </a>
  ]
 </li>
 <li>
  Hierarchical Object Detection with Deep Reinforcement Learning. [
  <a href="https://arxiv.org/pdf/1611.03718.pdf">
   arxiv
  </a>
  ]
 </li>
 <li>
  High-Dimensional Continuous Control Using Generalized Advantage Estimation, J. Schulman et al.,
  <em>
   ICLR
  </em>
  . [
  <a href="http://arxiv.org/abs/1506.02438">
   arxiv
  </a>
  ]
 </li>
 <li>
  Increasing the Action Gap: New Operators for Reinforcement Learning, M. G. Bellemare et al.,
  <em>
   AAAI
  </em>
  . [
  <a href="http://arxiv.org/abs/1512.04860">
   arxiv
  </a>
  ]
 </li>
 <li>
  Interactive Spoken Content Retrieval by Deep Reinforcement Learning. [
  <a href="https://arxiv.org/pdf/1609.05234.pdf">
   arxiv
  </a>
  ]
 </li>
 <li>
  Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection, S. Levine et al.,
  <em>
   arXiv
  </em>
  . [
  <a href="http://arxiv.org/abs/1603.02199">
   url
  </a>
  ]
 </li>
 <li>
  Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks, J. N. Foerster et al.,
  <em>
   arXiv
  </em>
  . [
  <a href="http://arxiv.org/abs/1602.02672">
   url
  </a>
  ]
 </li>
 <li>
  Learning to compose words into sentences with reinforcement learning. [
  <a href="https://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwim56OJ1I_RAhUJi1QKHcDRAEYQFgg2MAI&amp;url=https%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F5b373g%2Fr_learning_to_compose_words_into_sentences_with%2F&amp;usg=AFQjCNFBoour5fTqAiKQF1NXNon2e-j9pA">
   url
  </a>
  ]
 </li>
 <li>
  Loss is its own Reward: Self-Supervision for Reinforcement Learning.[
  <a href="https://arxiv.org/pdf/1612.07307.pdf">
   arxiv
  </a>
  ]
 </li>
 <li>
  Model-Free Episodic Control. [
  <a href="http://arxiv.org/abs/1606.04460">
   arxiv
  </a>
  ]
 </li>
 <li>
  Mastering the game of Go with deep neural networks and tree search. [
  <a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">
   nature
  </a>
  ] :star:
 </li>
 <li>
  MazeBase: A Sandbox for Learning from Games .[
  <a href="http://arxiv.org/abs/1511.07401">
   arxiv
  </a>
  ]
 </li>
 <li>
  <a href="https://mp.weixin.qq.com/s?__biz=MzI0ODcxODk5OA==&amp;mid=2247483966&amp;idx=1&amp;sn=e3fde0461e10e220aca322ca9395958c">
   Neural Architecture Search with Reinforcement Learning.
  </a>
  [
  <a href="https://openreview.net/pdf?id=r1Ue8Hcxg">
   pdf
  </a>
  ]
 </li>
 <li>
  Neural Combinatorial Optimization with Reinforcement Learning. [
  <a href="https://arxiv.org/pdf/1611.09940.pdf">
   arxiv
  </a>
  ]
 </li>
 <li>
  Non-Deterministic Policy Improvement Stabilizes Approximated Reinforcement Learning. [
  <a href="https://ewrl.files.wordpress.com/2016/11/ewrl13-2016-submission_2.pdf">
   url
  </a>
  ]
 </li>
 <li>
  Online Sequence-to-Sequence Active Learning for Open-Domain Dialogue Generation.
  <em>
   arXiv
  </em>
  . [
  <a href="https://arxiv.org/pdf/1612.03929.pdf">
   arxiv
  </a>
  ]
 </li>
 <li>
  Policy Distillation, A. A. Rusu et at.,
  <em>
   ICLR
  </em>
  . [
  <a href="http://arxiv.org/abs/1511.06295">
   arxiv
  </a>
  ]
 </li>
 <li>
  Prioritized Experience Replay. [
  <a href="http://arxiv.org/abs/1511.05952">
   arxiv
  </a>
  ] :star:
 </li>
 <li>
  Reinforcement Learning Using Quantum Boltzmann Machines. [
  <a href="https://arxiv.org/pdf/1612.05695.pdf">
   arxiv
  </a>
  ]
 </li>
 <li>
  Safe and Efficient Off-Policy Reinforcement Learning, R. Munos et al.[
  <a href="https://arxiv.org/pdf/1606.02647.pdf">
   arxiv
  </a>
  ]
 </li>
 <li>
  <a href="https://zhuanlan.zhihu.com/p/25673276">
   Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving.
  </a>
  [
  <a href="https://arxiv.org/pdf/1610.03295.pdf">
   arxiv
  </a>
  ]
 </li>
 <li>
  Sample-efficient Deep Reinforcement Learning for Dialog Control. [
  <a href="https://scirate.com/arxiv/1612.06000">
   url
  </a>
  ]
 </li>
 <li>
  Self-Correcting Models for Model-Based Reinforcement Learning.[
  <a href="https://scirate.com/arxiv/1612.06018">
   url
  </a>
  ]
 </li>
 <li>
  Unifying Count-Based Exploration and Intrinsic Motivation. [
  <a href="https://arxiv.org/pdf/1606.01868.pdf">
   arxiv
  </a>
  ]
 </li>
 <li>
  Value Iteration Networks. [
  <a href="http://arxiv.org/abs/1602.02867">
   arxiv
  </a>
  ]
 </li>
</ul>
