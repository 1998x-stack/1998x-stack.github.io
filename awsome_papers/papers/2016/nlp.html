<h2>
 natural language process
</h2>
<ul>
 <li>
  <a href="../../README.md">
   return to home
  </a>
 </li>
</ul>
<h3>
 NLP
</h3>
<ul>
 <li>
  Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.
  <a href="https://aclweb.org/anthology/K/K16/K16-1028.pdf">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text Classification.
  <a href="https://arxiv.org/pdf/1611.01884.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Achieving Human Parity in Conversational Speech Recognition.
  <a href="https://arxiv.org/pdf/1610.05256.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A General Framework for Content-enhanced Network Representation Learning.
  <a href="https://arxiv.org/pdf/1610.02906.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Hybrid Geometric Approach for Measuring Similarity Level Among Documents and Document Clustering.
  <a href="http://ieeexplore.ieee.org/document/7474366/?reload=true">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/taki0112/Vector_Similarity">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  A Joint Many-Task Model- Growing a Neural Network for Multiple NLP Tasks.
  <a href="https://arxiv.org/pdf/1611.01587.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  A Semisupervised Approach for Language Identification based on Ladder Networks.
  <a href="http://www.eng.biu.ac.il/goldbej/files/2012/05/Odyssey_2016_paper.pdf">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  A Simple, Fast Diverse Decoding Algorithm for Neural Generation.
  <a href="https://arxiv.org/pdf/1611.08562.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Aspect Level Sentiment Classification with Deep Memory Network.
  <a href="https://arxiv.org/pdf/1605.08900.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/endymecy/transwarp-nlp">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  A Strong Baseline for Learning Cross-Lingual Word Embeddings from Sentence Alignments.
  <a href="https://arxiv.org/pdf/1608.05426.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification.
  <a href="https://arxiv.org/pdf/1610.04989.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Character-level and Multi-channel Convolutional Neural Networks for Large-scale Authorship Attribution.
  <a href="https://arxiv.org/pdf/1609.06686.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  COCO-Text-Dataset and Benchmark for Text Detection and Recognition in Natural Images.
  <a href="http://sunw.csail.mit.edu/papers/01_Veit_SUNw.pdf">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks.
  <a href="https://arxiv.org/pdf/1611.00454.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Collaborative Recurrent Neural Networks for Dynamic Recommender Systems.
  <a href="https://infoscience.epfl.ch/record/222477/files/ko101.pdf">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/lca4/collaborative-rnn">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  Context-aware Natural Language Generation with Recurrent Neural Networks.
  <a href="https://arxiv.org/pdf/1611.09900.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <b>
   [CLSTM]
  </b>
  Contextual LSTM models for Large scale NLP tasks.
  <a href="http://www.csl.sri.com/users/shalini/clstm_dlkdd16.pdf">
   <code>
    pdf
   </code>
  </a>
  :star:
 </li>
 <li>
  Deep Biaffine Attention for Neural Dependency Parsing.
  <a href="https://openreview.net/pdf?id=Hk95PK9le">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/tdozat/Parser">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Deep Semi-Supervised Learning with Linguistically Motivated Sequence Labeling Task Hierarchies.
  <a href="https://arxiv.org/pdf/1612.09113.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Deep Neural Networks for YouTube Recommendations.
  <a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45530.pdf">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Detecting Text in Natural Image with Connectionist Text Proposal Network.
  <a href="https://arxiv.org/pdf/1609.03605.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/qingswu/CTPN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models.
  <a href="https://arxiv.org/pdf/1610.02424.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Efficient Character-level Document Classification by Combining Convolution and Recurrent Layers.
  <a href="https://arxiv.org/pdf/1602.00367.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension.
  <a href="https://arxiv.org/pdf/1610.09996.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  End-to-End Multi-View Networks for Text Classification.
  <a href="https://arxiv.org/pdf/1704.05907.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF.
  <a href="https://arxiv.org/pdf/1603.01354.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference.
  <a href="https://arxiv.org/pdf/1609.06038.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Fully Convolutional Instance-aware Semantic Segmentation.
  <a href="https://arxiv.org/pdf/1611.07709.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/msracver/FCIS">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Generative Deep Neural Networks for Dialogue: A Short Review.
  <a href="https://arxiv.org/pdf/1611.06216.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Generating Factoid Questions With Recurrent Neural Networks- The 30M Factoid Question-Answer Corpus.
  <a href="https://aclweb.org/anthology/P/P16/P16-1056.pdf">
   <code>
    pdf
   </code>
  </a>
  :star:
 </li>
 <li>
  Globally Normalized Transition-Based Neural Networks.
  <a href="https://arxiv.org/pdf/1603.06042.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/tensorflow/models/tree/master/syntaxnet">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  GraphNet: Recommendation system based on language and network structure.
  <a href="https://web.stanford.edu/class/cs224n/reports/2758630.pdf">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  How NOT To Evaluate Your Dialogue System.
  <a href="https://arxiv.org/pdf/1603.08023.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Inducing Multilingual Text Analysis Tools Using Bidirectional Recurrent Neural Networks.
  <a href="https://hal.archives-ouvertes.fr/hal-01374205/document">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Key-Value Memory Networks for Directly Reading Documents.
  <a href="https://arxiv.org/pdf/1606.03126.pdf">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Learning Distributed Representations of Sentences from Unlabelled Data.
  <a href="https://arxiv.org/pdf/1602.03483.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Learning End-to-End Goal-Oriented Dialog.
  <a href="https://arxiv.org/pdf/1605.07683.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/vyraun/chatbot-MemN2N-tensorflow">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Learning Recurrent Span Representations for Extractive Question Answering.
  <a href="https://arxiv.org/pdf/1611.01436.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Learning to Compose Neural Networks for Question Answering.
  <a href="https://arxiv.org/pdf/1601.01705.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Learning to Translate in Real-time with Neural Machine Translation.
  <a href="https://arxiv.org/pdf/1610.00388.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Linguistically Regularized LSTMs for Sentiment Classification.
  <a href="https://arxiv.org/pdf/1611.03949.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Long Short-Term Memory-Networks for Machine Reading.
  <a href="https://aclweb.org/anthology/D16-1053">
   <code>
    url
   </code>
  </a>
  :star:
 </li>
 <li>
  <b>
   [lda2vec]
  </b>
  Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec.
  <a href="https://arxiv.org/pdf/1605.02019.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/cemoody/lda2vec">
   <code>
    code
   </code>
  </a>
  <a href="https://github.com/meereeum/lda2vec-tf">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Modeling Coverage for Neural Machine Translation.
  <a href="http://www.hangli-hl.com/uploads/3/4/4/6/34465961/tu_et_al_2016.pdf">
   <code>
    pdf
   </code>
  </a>
  :star:
 </li>
 <li>
  Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss.
  <a href="https://www.aclweb.org/anthology/P/P16/P16-2067.pdf">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/bplank/bilstm-aux">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  MultiNet: Real-time Joint Semantic Reasoning for Autonomous Driving.
  <a href="https://arxiv.org/pdf/1612.07695.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Neural Architectures for Fine-grained Entity Type Classification.
  <a href="https://arxiv.org/pdf/1606.01341.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Neural Architectures for Named Entity Recognition.
  <a href="https://arxiv.org/pdf/1603.01360.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Neural Emoji Recommendation in Dialogue Systems.
  <a href="https://arxiv.org/pdf/1612.04609.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Neural Paraphrase Generation with Stacked Residual LSTM Networks.
  <a href="https://arxiv.org/pdf/1610.03098.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Neural Semantic Encoders.
  <a href="https://arxiv.org/pdf/1607.04315.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Neural Variational Inference for Text Processing.
  <a href="https://arxiv.org/pdf/1511.06038">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Online Segment to Segment Neural Transduction.
  <a href="https://arxiv.org/pdf/1609.08194.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  On Random Weights for Texture Generation in One Layer Neural Networks.
  <a href="https://arxiv.org/pdf/1612.06070.pdf?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Parallelizing Word2Vec in Shared and Distributed Memory.
  <a href="https://arxiv.org/pdf/1604.04661.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/IntelLabs/pWord2Vec">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences.
  <a href="https://arxiv.org/pdf/1610.09513.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/dannyneil/public_plstm">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Recurrent Neural Network Grammars.
  <a href="https://arxiv.org/pdf/1602.07776.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Reading Wikipedia to Answer Open-Domain Questions.
  <a href="https://arxiv.org/pdf/1704.00051.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/facebookresearch/DrQA">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  ReasoNet: Learning to Stop Reading in Machine Comprehension.
  <a href="https://arxiv.org/pdf/1609.05284.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Sentence Level Recurrent Topic Model- Letting Topics Speak for Themselves.
  <a href="https://arxiv.org/pdf/1604.02038.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Sentence-Level Grammatical Error Identification as Sequence-to-Sequence Correction.
  <a href="https://aclweb.org/anthology/W/W16/W16-0528.pdf">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Sentence Ordering using Recurrent Neural Networks.
  <a href="https://arxiv.org/pdf/1611.02654.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation.
  <a href="https://arxiv.org/pdf/1607.00970.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Sequential Match Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots.
  <a href="https://arxiv.org/pdf/1612.01627.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Structured Sequence Modeling with Graph Convolutional Recurrent Networks.
  <a href="https://arxiv.org/pdf/1612.07659.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <a href="http://weibo.com/ttarticle/p/show?id=2309404086416278721142">
   TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency.
  </a>
  <a href="https://arxiv.org/pdf/1611.01702.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Tracking the World State with Recurrent Entity Networks .
  <a href="https://arxiv.org/pdf/1612.03969.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling.
  <a href="https://arxiv.org/pdf/1611.01462.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/icoxfog417/tying-wv-and-wc">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder.
  <a href="https://arxiv.org/pdf/1607.07514.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/soroushv/Tweet2Vec">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Unsupervised Learning of Sentence Representations using Convolutional Neural Networks.
  <a href="https://arxiv.org/pdf/1611.07897.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Unsupervised neural and Bayesian models for zero-resource speech processing.
  <a href="https://arxiv.org/pdf/1701.00851.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Unsupervised Pretraining for Sequence to Sequence Learning.
  <a href="https://arxiv.org/pdf/1611.02683.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text.
  <a href="https://arxiv.org/pdf/1611.03599.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Very Deep Convolutional Networks for Natural Language Processing.
  <a href="https://arxiv.org/pdf/1606.01781.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Visual Dialog.
  <a href="https://arxiv.org/pdf/1611.08669.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/Cloud-CV/visual-chatbot">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Wav2Letter: an End-to-End ConvNet-based Speech Recognition System.
  <a href="https://arxiv.org/pdf/1609.03193.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <a href="http://blog.csdn.net/dinosoft/article/details/52581368">
   Wide &amp; Deep Learning for Recommender Systems.
  </a>
  <a href="https://arxiv.org/pdf/1606.07792.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://www.tensorflow.org/tutorials/wide_and_deep/">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
</ul>
<h3>
 Generative learning
</h3>
<ul>
 <li>
  Adversarial Training Methods for Semi-Supervised Text Classification.
  <a href="https://arxiv.org/pdf/1605.07725.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Aspect Level Sentiment Classification with Deep Memory Network.
  <a href="https://arxiv.org/pdf/1605.08900.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Generative Adversarial Text to Image Synthesis.
  <a href="https://arxiv.org/pdf/1605.05396.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Modeling documents with Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1612.09122.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  <b>
   [StackGAN]
  </b>
  StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks.
  <a href="https://arxiv.org/pdf/1612.03242.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/hanzhanggit/StackGAN">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
</ul>
<h3>
 Attention and memory
</h3>
<ul>
 <li>
  ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs.
  <a href="https://arxiv.org/pdf/1512.05193.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/galsang/ABCNN">
   <code>
    tensorflow
   </code>
  </a>
 </li>
 <li>
  A Context-aware Attention Network for Interactive Question Answering.
  <a href="https://openreview.net/pdf?id=SkyQWDcex">
   <code>
    url
   </code>
  </a>
 </li>
 <li>
  A Decomposable Attention Model for Natural Language Inference.
  <a href="https://arxiv.org/pdf/1606.01933.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/harvardnlp/decomp-attn">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  A self-attentive sentence embedding.
  <a href="https://openreview.net/pdf?id=BJC_jUqxe">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Aspect Level Sentiment Classification with Deep Memory Network.
  <a href="https://arxiv.org/pdf/1605.08900.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="http://nlp.stanford.edu/sentiment/code.html">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  AttSum: Joint Learning of Focusing and Summarization with Neural Attention.
  <a href="https://arxiv.org/pdf/1604.00125.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Attention-over-Attention Neural Networks for Reading Comprehension.
  <a href="https://arxiv.org/pdf/1607.04423.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/OlavHN/attention-over-attention">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Collective Entity Resolution with Multi-Focal Attention.
  <a href="https://www.aclweb.org/anthology/P/P16/P16-1059.pdf">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  <a href="https://theneuralperspective.com/2017/01/19/gated-attention-readers-for-text-comprehension/">
   Gated-Attention Readers for Text Comprehension.
  </a>
  <a href="https://openreview.net/pdf?id=HkcdHtqlx">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Hierarchical Attention Networks for Document Classification.
  <a href="https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/ematvey/deep-text-classifier">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Hierarchical Memory Networks for Answer Selection on Unknown Words.
  <a href="https://arxiv.org/pdf/1609.08843.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model.
  <a href="https://arxiv.org/pdf/1601.03317.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Improving Attention Modeling with Implicit Distortion and Fertility for Machine Translation.
  <a href="https://www.aclweb.org/anthology/C/C16/C16-1290.pdf">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Iterative Alternating Neural Attention for Machine Reading.
  <a href="https://arxiv.org/pdf/1606.02245.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning.
  <a href="https://arxiv.org/pdf/1609.06773.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Key-Value Memory Networks for Directly Reading Documents.
  <a href="https://arxiv.org/pdf/1606.03126.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks.
  <a href="https://arxiv.org/pdf/1609.03286.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Language to Logical Form with Neural Attention.
  <a href="http://homepages.inf.ed.ac.uk/s1478528/acl16-lang2logic-slides.pdf">
   <code>
    pdf
   </code>
  </a>
  :star:
 </li>
 <li>
  Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention.
  <a href="https://arxiv.org/pdf/1605.09090.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Lexicon Integrated CNN Models with Attention for Sentiment Analysis.
  <a href="https://arxiv.org/pdf/1610.06272.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Memory-enhanced Decoder for Neural Machine Translation.
  <a href="https://arxiv.org/pdf/1606.02003.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Neural Language Correction with Character-Based Attention.
  <a href="https://arxiv.org/pdf/1603.09727.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks.
  <a href="https://arxiv.org/pdf/1611.06204.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h3>
 Neural Machine Translation
</h3>
<ul>
 <li>
  Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models.
  <a href="https://arxiv.org/pdf/1604.00788.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  A Character-level Decoder without Explicit Segmentation for Neural Machine Translation.
  <a href="https://www.aclweb.org/anthology/P/P16/P16-1160.pdf">
   <code>
    pdf
   </code>
  </a>
  :star:
 </li>
 <li>
  A Convolutional Encoder Model for Neural Machine Translation.
  <a href="https://arxiv.org/pdf/1611.02344.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com//pravarmahajan/cnn-encoder-nmt">
   <code>
    pytorch
   </code>
  </a>
 </li>
 <li>
  Character-based Neural Machine Translation.
  <a href="https://arxiv.org/pdf/1511.04586.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Context-Dependent Word Representation for Neural Machine Translation.
  <a href="https://arxiv.org/pdf/1607.00578.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Convolutional Encoders for Neural Machine Translation.
  <a href="https://cs224d.stanford.edu/reports/LambAndrew.pdf">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translatin.
  <a href="https://arxiv.org/pdf/1606.04199.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Dual Learning for Machine Translation.
  <a href="https://papers.nips.cc/paper/6469-dual-learning-for-machine-translation.pdf">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Fast Domain Adaptation for Neural Machine Translation.
  <a href="https://arxiv.org/pdf/1612.06897.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Fully Character-Level Neural Machine Translation without Explicit Segmentation.
  <a href="https://arxiv.org/pdf/1610.03017.pdf">
   <code>
    pdf
   </code>
  </a>
  :star:
 </li>
 <li>
  Google's Multilingual Neural Machine Translation System- Enabling Zero-Shot Translation.
  <a href="https://arxiv.org/pdf/1611.04558.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Google's Neural Machine Translation System- Bridging the Gap between Human and Machine Translation.
  <a href="https://arxiv.org/pdf/1609.08144.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  How Grammatical is Character-level Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs.
  <a href="https://arxiv.org/pdf/1612.04629.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Interactive Attention for Neural Machine Translation.
  <a href="https://arxiv.org/pdf/1610.05011.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Multimodal Attention for Neural Machine Translation.
  <a href="https://arxiv.org/pdf/1609.03976.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism.
  <a href="https://arxiv.org/pdf/1601.01073.pdf">
   <code>
    arxiv
   </code>
  </a>
  :star:
 </li>
 <li>
  Modeling Coverage for Neural Machine Translation.
  <a href="http://www.hangli-hl.com/uploads/3/4/4/6/34465961/tu_et_al_2016.pdf">
   <code>
    pdf
   </code>
  </a>
  :star:
 </li>
 <li>
  Neural Machine Translation in Linear Time.
  <a href="https://arxiv.org/pdf/1610.10099.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Neural Network Translation Models for Grammatical Error Correction.
  <a href="https://arxiv.org/pdf/1606.00189.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Neural Machine Translation with Latent Semantic of Image and Text.
  <a href="https://arxiv.org/pdf/1611.08459.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Neural Machine Translation with Pivot Languages.
  <a href="https://arxiv.org/pdf/1611.04928.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Neural Machine Translation with Recurrent Attention Modeling.
  <a href="https://arxiv.org/pdf/1607.05108.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Neural Machine Translation with Supervised Attention.
  <a href="https://www.aclweb.org/anthology/C/C16/C16-1291.pdf">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Recurrent Neural Machine Translation.
  <a href="https://arxiv.org/pdf/1607.08725.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Semi-Supervised Learning for Neural Machine Translation.
  <a href="http://iiis.tsinghua.edu.cn/~weixu/files/acl2016_chengyong.pdf">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Temporal Attention Model for Neural Machine Translation.
  <a href="https://arxiv.org/pdf/1608.02927.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Zero-Resource Translation with Multi-Lingual Neural Machine Translation.
  <a href="https://arxiv.org/pdf/1606.04164.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h3>
 Neural Language Model
</h3>
<ul>
 <li>
  Character-Aware Neural Language Models.
  <a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12489/12017">
   <code>
    pdf
   </code>
  </a>
  :star:
 </li>
 <li>
  Character-Level Language Modeling with Hierarchical Recurrent Neural Networks.
  <a href="https://arxiv.org/pdf/1609.03777.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Coherent Dialogue with Attention-based Language Models.
  <a href="https://arxiv.org/pdf/1611.06997.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Joint Online Spoken Language Understanding and Language Modeling with Recurrent Neural Networks.
  <a href="https://arxiv.org/pdf/1609.01462.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Improving neural language models with a continuous cache.
  <a href="https://openreview.net/pdf?id=B184E5qee">
   <code>
    pdf
   </code>
  </a>
 </li>
 <li>
  Language Modeling with Gated Convolutional Networks.
  <a href="https://arxiv.org/pdf/1612.08083.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/anantzoid/Language-Modeling-GatedCNN">
   <code>
    tensorflow
   </code>
  </a>
  :star:
 </li>
 <li>
  Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling.
  <a href="https://arxiv.org/pdf/1611.08034.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Recurrent Memory Networks for Language Modeling.
  <a href="https://arxiv.org/pdf/1601.01272.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
