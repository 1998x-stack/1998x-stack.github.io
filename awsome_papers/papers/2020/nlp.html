<h1>
 Natural Language Model
</h1>
<ul>
 <li>
  Energy-Based Models for Text.
  <a href="https://arxiv.org/pdf/2004.10188.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h2>
 Dialog
</h2>
<ul>
 <li>
  Recipes for building an open-domain chatbot.
  <a href="https://arxiv.org/pdf/2004.13637.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h2>
 Generate Adversarial Model
</h2>
<ul>
 <li>
  FastWordBug: A Fast Method To Generate Adversarial Text Against NLP Applications.
  <a href="https://arxiv.org/pdf/2002.00760.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h2>
 Information Extraction
</h2>
<ul>
 <li>
  Rapid Adaptation of BERT for Information Extraction on Domain-Specific Business Documents.
  <a href="https://arxiv.org/pdf/2002.01861.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h2>
 Language Model
</h2>
<ul>
 <li>
  BERT-of-Theseus: Compressing BERT by Progressive Module Replacing.
  <a href="https://arxiv.org/pdf/2002.02925.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/JetRunner/BERT-of-Theseus">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  BERTweet: A pre-trained language model for English Tweets.
  <a href="https://arxiv.org/pdf/2005.10200.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/VinAIResearch/BERTweet">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Blank Language Models.
  <a href="https://arxiv.org/pdf/2002.03079.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Controlling Computation versus Quality for Neural Sequence Models.
  <a href="https://arxiv.org/pdf/2002.07106.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.
  <a href="https://openreview.net/pdf?id=r1xMH1BtvB">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/google-research/electra">
   <code>
    code
   </code>
  </a>
  :star:
 </li>
 <li>
  Extending Multilingual BERT to Low-Resource Languages.
  <a href="https://arxiv.org/pdf/2004.13640.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Limits of Detecting Text Generated by Large-Scale Language Models.
  <a href="https://arxiv.org/pdf/2002.03438.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  PALM: Pre-training an Autoencoding&amp;Autoregressive Language Model for Context-conditioned Generation.
  <a href="https://arxiv.org/pdf/2004.07159.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Pretrained Transformers Improve Out-of-Distribution Robustness.
  <a href="https://arxiv.org/pdf/2004.06100.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Semantics-aware BERT for Language Understanding.
  <a href="https://arxiv.org/pdf/1909.02209.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/cooelf/SemBERT">
   <code>
    code
   </code>
  </a>
 </li>
</ul>
<h2>
 Pos-tagging
</h2>
<ul>
 <li>
  Joint Embedding in Named Entity Linking on Sentence Level.
  <a href="https://arxiv.org/pdf/2002.04936.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h2>
 QA
</h2>
<ul>
 <li>
  AmbigQA: Answering Ambiguous Open-domain Questions.
  <a href="https://arxiv.org/pdf/2004.10645.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Asking and Answering Questions to Evaluate the Factual Consistency of Summaries.
  <a href="https://arxiv.org/pdf/2004.04228.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Conversational Question Answering over Passages by Leveraging Word Proximity Networks.
  <a href="https://arxiv.org/pdf/2004.13117.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/magkai/CROWN">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Probing Emergent Semantics in Predictive Agents via Question Answering.
  <a href="https://arxiv.org/pdf/2006.01016.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Unsupervised Commonsense Question Answering with Self-Talk.
  <a href="https://arxiv.org/pdf/2004.05483.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/vered1986/self_talk">
   <code>
    code
   </code>
  </a>
 </li>
</ul>
<h2>
 Text Classification
</h2>
<ul>
 <li>
  Light-Weighted CNN for Text Classification.
  <a href="https://arxiv.org/pdf/2004.07922.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Multi-Label Text Classification using Attention-based Graph Neural Network.
  <a href="https://arxiv.org/pdf/2003.11644.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h2>
 Text Generation
</h2>
<ul>
 <li>
  NUBIA: NeUral Based Interchangeability Assessor for Text Generation.
  <a href="https://arxiv.org/pdf/2004.14667.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Polarized-VAE: Proximity Based Disentangled Representation Learning for Text Generation.
  <a href="https://arxiv.org/pdf/2004.10809.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Reverse Engineering Configurations of Neural Text Generation Models.
  <a href="https://arxiv.org/pdf/2004.06201.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
