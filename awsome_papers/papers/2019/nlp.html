<h1>
 Nature language process
</h1>
<ul>
 <li>
  Aspect Specific Opinion Expression Extraction using Attention based LSTM-CRF Network.
  <a href="https://arxiv.org/pdf/1902.02709.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning.
  <a href="https://arxiv.org/pdf/1902.02671.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  BioBERT: pre-trained biomedical language representation model for biomedical text mining.
  <a href="https://arxiv.org/pdf/1901.08746.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Cross-lingual Language Model Pretraining.
  <a href="https://arxiv.org/pdf/1901.07291.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings.
  <a href="https://arxiv.org/pdf/1909.10430.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/uhh-lt/bert-sense">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  GILT: Generating Images from Long Text.
  <a href="https://arxiv.org/pdf/1901.02404.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Open Research Knowledge Graph: Towards Machine Actionability in Scholarly Communication.
  <a href="https://arxiv.org/pdf/1901.10816.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  SumQE: a BERT-based Summary Quality Estimation Model.
  <a href="https://arxiv.org/pdf/1909.00578.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/nlpaueb/SumQE">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context.
  <a href="https://arxiv.org/pdf/1901.02860.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/kimiyoung/transformer-xl">
   <code>
    code
   </code>
  </a>
 </li>
</ul>
<h2>
 Embedding
</h2>
<ul>
 <li>
  A Multi-Resolution Word Embedding for Document Retrieval from Large Unstructured Knowledge Bases.
  <a href="https://arxiv.org/pdf/1902.00663.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey.
  <a href="https://arxiv.org/pdf/1902.00753.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Fast Transformer Decoding: One Write-Head is All You Need.
  <a href="https://arxiv.org/pdf/1911.02150.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h2>
 NMT
</h2>
<ul>
 <li>
  Modeling Latent Sentence Structure in Neural Machine Translation.
  <a href="https://arxiv.org/pdf/1901.06436.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Training on Synthetic Noise Improves Robustness to Natural Noise in Machine Translation.
  <a href="https://arxiv.org/pdf/1902.01509.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h2>
 Reading Comprehension
</h2>
<ul>
 <li>
  DREAM: A Challenge Dataset and Models for Dialogue-Based Reading Comprehension.
  <a href="https://arxiv.org/pdf/1902.00164.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  Review Conversational Reading Comprehension.
  <a href="https://arxiv.org/pdf/1902.00821.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h2>
 Recommender Systems
</h2>
<ul>
 <li>
  Behavior Sequence Transformer for E-commerce Recommendation in Alibaba.
  <a href="https://arxiv.org/pdf/1905.06874.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
</ul>
<h2>
 Text Classification
</h2>
<ul>
 <li>
  Delta-training: Simple Semi-Supervised Text Classification using Pretrained Word Embeddings.
  <a href="https://arxiv.org/pdf/1901.07651.pdf">
   <code>
    arxiv
   </code>
  </a>
 </li>
 <li>
  EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks.
  <a href="https://arxiv.org/pdf/1901.11196.pdf">
   <code>
    arxiv
   </code>
  </a>
  <a href="https://github.com/jasonwei20/eda_nlp">
   <code>
    code
   </code>
  </a>
 </li>
 <li>
  Hierarchical Multi-label Text Classification: An Attention-based Recurrent Network Approach.
  <a href="https://dl.acm.org/doi/pdf/10.1145/3357384.3357885?download=true">
   <code>
    pdf
   </code>
  </a>
  <a href="https://github.com/RandolphVI/Hierarchical-Multi-Label-Text-Classification">
   <code>
    code
   </code>
  </a>
 </li>
</ul>
