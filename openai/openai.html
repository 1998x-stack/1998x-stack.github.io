<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en"> <!--<![endif]-->

<head>
    <meta charset="utf-8">

    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>Spinning Up documentation</title>




    <link rel="shortcut icon" href="_static/openai_icon.ico" />












    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/css/modify.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />


    <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">


    <div class="wy-grid-for-nav">


        <nav data-toggle="wy-nav-shift" class="wy-nav-side">
            <div class="wy-side-scroll">
                <div class="wy-side-nav-search">



                    <a href="index.html#document-index">




                        <img src="_static/spinning-up-logo2.png" class="logo" alt="Logo" />

                    </a>





                    <div class="version">
                        latest
                    </div>




                    <div role="search">
                        <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
                            <input type="text" name="q" placeholder="Search docs" />
                            <input type="hidden" name="check_keywords" value="yes" />
                            <input type="hidden" name="area" value="default" />
                        </form>
                    </div>


                </div>

                <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">




                    <!-- Local TOC -->
                    <div class="local-toc">
                        <p class="caption"><span class="caption-text">User Documentation</span></p>
                        <ul>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-user/introduction">Introduction</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#what-this-is">What This Is</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#why-we-built-this">Why We Built This</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#how-this-serves-our-mission">How This Serves Our
                                            Mission</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#code-design-philosophy">Code Design Philosophy</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#long-term-support-and-support-history">Long-Term Support
                                            and Support History</a></li>
                                </ul>
                            </li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-user/installation">Installation</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#installing-python">Installing Python</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#installing-openmpi">Installing OpenMPI</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#installing-spinning-up">Installing Spinning Up</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#check-your-install">Check Your Install</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#installing-mujoco-optional">Installing MuJoCo
                                            (Optional)</a></li>
                                </ul>
                            </li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-user/algorithms">Algorithms</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#what-s-included">What&#8217;s Included</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#why-these-algorithms">Why These Algorithms?</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#code-format">Code Format</a></li>
                                </ul>
                            </li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-user/running">Running Experiments</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#launching-from-the-command-line">Launching from the Command
                                            Line</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#launching-from-scripts">Launching from Scripts</a></li>
                                </ul>
                            </li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-user/saving_and_loading">Experiment Outputs</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#algorithm-outputs">Algorithm Outputs</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#save-directory-location">Save Directory Location</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#loading-and-running-trained-policies">Loading and Running
                                            Trained Policies</a></li>
                                </ul>
                            </li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-user/plotting">Plotting Results</a></li>
                        </ul>
                        <p class="caption"><span class="caption-text">Introduction to RL</span></p>
                        <ul>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-spinningup/rl_intro">Part 1: Key Concepts in RL</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#what-can-rl-do">What Can RL Do?</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#key-concepts-and-terminology">Key Concepts and
                                            Terminology</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#optional-formalism">(Optional) Formalism</a></li>
                                </ul>
                            </li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-spinningup/rl_intro2">Part 2: Kinds of RL Algorithms</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#a-taxonomy-of-rl-algorithms">A Taxonomy of RL
                                            Algorithms</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#links-to-algorithms-in-taxonomy">Links to Algorithms in
                                            Taxonomy</a></li>
                                </ul>
                            </li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-spinningup/rl_intro3">Part 3: Intro to Policy
                                    Optimization</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#deriving-the-simplest-policy-gradient">Deriving the
                                            Simplest Policy Gradient</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#implementing-the-simplest-policy-gradient">Implementing the
                                            Simplest Policy Gradient</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#expected-grad-log-prob-lemma">Expected Grad-Log-Prob
                                            Lemma</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#don-t-let-the-past-distract-you">Don&#8217;t Let the Past
                                            Distract You</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#implementing-reward-to-go-policy-gradient">Implementing
                                            Reward-to-Go Policy Gradient</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#baselines-in-policy-gradients">Baselines in Policy
                                            Gradients</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#other-forms-of-the-policy-gradient">Other Forms of the
                                            Policy Gradient</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#recap">Recap</a></li>
                                </ul>
                            </li>
                        </ul>
                        <p class="caption"><span class="caption-text">Resources</span></p>
                        <ul>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-spinningup/spinningup">Spinning Up as a Deep RL
                                    Researcher</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#the-right-background">The Right Background</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#learn-by-doing">Learn by Doing</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#developing-a-research-project">Developing a Research
                                            Project</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#doing-rigorous-research-in-rl">Doing Rigorous Research in
                                            RL</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#closing-thoughts">Closing Thoughts</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#ps-other-resources">PS: Other Resources</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#references">References</a></li>
                                </ul>
                            </li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-spinningup/keypapers">Key Papers in Deep RL</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#model-free-rl">1. Model-Free RL</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#exploration">2. Exploration</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#transfer-and-multitask-rl">3. Transfer and Multitask RL</a>
                                    </li>
                                    <li class="toctree-l2"><a class="reference internal" href="index.html#hierarchy">4.
                                            Hierarchy</a></li>
                                    <li class="toctree-l2"><a class="reference internal" href="index.html#memory">5.
                                            Memory</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#model-based-rl">6. Model-Based RL</a></li>
                                    <li class="toctree-l2"><a class="reference internal" href="index.html#meta-rl">7.
                                            Meta-RL</a></li>
                                    <li class="toctree-l2"><a class="reference internal" href="index.html#scaling-rl">8.
                                            Scaling RL</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#rl-in-the-real-world">9. RL in the Real World</a></li>
                                    <li class="toctree-l2"><a class="reference internal" href="index.html#safety">10.
                                            Safety</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#imitation-learning-and-inverse-reinforcement-learning">11.
                                            Imitation Learning and Inverse Reinforcement Learning</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#reproducibility-analysis-and-critique">12. Reproducibility,
                                            Analysis, and Critique</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#bonus-classic-papers-in-rl-theory-or-review">13. Bonus:
                                            Classic Papers in RL Theory or Review</a></li>
                                </ul>
                            </li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-spinningup/exercises">Exercises</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#problem-set-1-basics-of-implementation">Problem Set 1:
                                            Basics of Implementation</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#problem-set-2-algorithm-failure-modes">Problem Set 2:
                                            Algorithm Failure Modes</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#challenges">Challenges</a></li>
                                </ul>
                            </li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-spinningup/bench">Benchmarks for Spinning Up
                                    Implementations</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#performance-in-each-environment">Performance in Each
                                            Environment</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#experiment-details">Experiment Details</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#pytorch-vs-tensorflow">PyTorch vs Tensorflow</a></li>
                                </ul>
                            </li>
                        </ul>
                        <p class="caption"><span class="caption-text">Algorithms Docs</span></p>
                        <ul>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-algorithms/vpg">Vanilla Policy Gradient</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#background">Background</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#documentation">Documentation</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#references">References</a></li>
                                </ul>
                            </li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-algorithms/trpo">Trust Region Policy Optimization</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#background">Background</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#documentation">Documentation</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#references">References</a></li>
                                </ul>
                            </li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-algorithms/ppo">Proximal Policy Optimization</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#background">Background</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#documentation">Documentation</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#references">References</a></li>
                                </ul>
                            </li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-algorithms/ddpg">Deep Deterministic Policy Gradient</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#background">Background</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#documentation">Documentation</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#references">References</a></li>
                                </ul>
                            </li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-algorithms/td3">Twin Delayed DDPG</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#background">Background</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#documentation">Documentation</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#references">References</a></li>
                                </ul>
                            </li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-algorithms/sac">Soft Actor-Critic</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#background">Background</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#documentation">Documentation</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#references">References</a></li>
                                </ul>
                            </li>
                        </ul>
                        <p class="caption"><span class="caption-text">Utilities Docs</span></p>
                        <ul>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-utils/logger">Logger</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#using-a-logger">Using a Logger</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#logger-classes">Logger Classes</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#loading-saved-models-pytorch-only">Loading Saved Models
                                            (PyTorch Only)</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#loading-saved-graphs-tensorflow-only">Loading Saved Graphs
                                            (Tensorflow Only)</a></li>
                                </ul>
                            </li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-utils/plotter">Plotter</a></li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-utils/mpi">MPI Tools</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#module-spinup.utils.mpi_tools">Core MPI Utilities</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#mpi-pytorch-utilities">MPI + PyTorch Utilities</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#mpi-tensorflow-utilities">MPI + Tensorflow Utilities</a>
                                    </li>
                                </ul>
                            </li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-utils/run_utils">Run Utils</a>
                                <ul>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#experimentgrid">ExperimentGrid</a></li>
                                    <li class="toctree-l2"><a class="reference internal"
                                            href="index.html#calling-experiments">Calling Experiments</a></li>
                                </ul>
                            </li>
                        </ul>
                        <p class="caption"><span class="caption-text">Etc.</span></p>
                        <ul>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-etc/acknowledgements">Acknowledgements</a></li>
                            <li class="toctree-l1"><a class="reference internal"
                                    href="index.html#document-etc/author">About the Author</a></li>
                        </ul>
                    </div>


                </div>
            </div>
        </nav>

        <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">


            <nav class="wy-nav-top" aria-label="top navigation">

                <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
                <a href="index.html#document-index">Spinning Up</a>

            </nav>


            <div class="wy-nav-content">

                <div class="rst-content">

















                    <div role="navigation" aria-label="breadcrumbs navigation">

                        <ul class="wy-breadcrumbs">

                            <li><a href="index.html#document-index">Docs</a> &raquo;</li>

                            <li>Spinning Up documentation</li>


                            <li class="wy-breadcrumbs-aside">



                                <a href="https://github.com/openai/spinningup/blob/master/docs/index.rst"
                                    class="fa fa-github"> Edit on GitHub</a>



                            </li>

                        </ul>


                        <hr />
                    </div>
                    <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
                        <div itemprop="articleBody">

                            <div class="section" id="welcome-to-spinning-up-in-deep-rl">
                                <h1>Welcome to Spinning Up in Deep RL!<a class="headerlink"
                                        href="#welcome-to-spinning-up-in-deep-rl"
                                        title="Permalink to this headline">¶</a></h1>
                                <img alt="_images/spinning-up-in-rl.png" src="_images/spinning-up-in-rl.png" />
                                <div class="toctree-wrapper compound">
                                    <span id="document-user/introduction"></span>
                                    <div class="section" id="introduction">
                                        <h2><a class="toc-backref" href="#id2">Introduction</a><a class="headerlink"
                                                href="#introduction" title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal" href="#introduction"
                                                        id="id2">Introduction</a>
                                                    <ul>
                                                        <li><a class="reference internal" href="#what-this-is"
                                                                id="id3">What This Is</a></li>
                                                        <li><a class="reference internal" href="#why-we-built-this"
                                                                id="id4">Why We Built This</a></li>
                                                        <li><a class="reference internal"
                                                                href="#how-this-serves-our-mission" id="id5">How This
                                                                Serves Our Mission</a></li>
                                                        <li><a class="reference internal" href="#code-design-philosophy"
                                                                id="id6">Code Design Philosophy</a></li>
                                                        <li><a class="reference internal"
                                                                href="#long-term-support-and-support-history"
                                                                id="id7">Long-Term Support and Support History</a></li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <div class="section" id="what-this-is">
                                            <h3><a class="toc-backref" href="#id3">What This Is</a><a class="headerlink"
                                                    href="#what-this-is" title="Permalink to this headline">¶</a></h3>
                                            <p>Welcome to Spinning Up in Deep RL! This is an educational resource
                                                produced by OpenAI that makes it easier to learn about deep
                                                reinforcement learning (deep RL).</p>
                                            <p>For the unfamiliar: <a class="reference external"
                                                    href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement
                                                    learning</a> (RL) is a machine learning approach for teaching agents
                                                how to solve tasks by trial and error. Deep RL refers to the combination
                                                of RL with <a class="reference external"
                                                    href="http://ufldl.stanford.edu/tutorial/">deep learning</a>.</p>
                                            <p>This module contains a variety of helpful resources, including:</p>
                                            <ul class="simple">
                                                <li>a short <a class="reference external"
                                                        href="../spinningup/rl_intro.html">introduction</a> to RL
                                                    terminology, kinds of algorithms, and basic theory,</li>
                                                <li>an <a class="reference external"
                                                        href="../spinningup/spinningup.html">essay</a> about how to grow
                                                    into an RL research role,</li>
                                                <li>a <a class="reference external"
                                                        href="../spinningup/keypapers.html">curated list</a> of
                                                    important papers organized by topic,</li>
                                                <li>a well-documented <a class="reference external"
                                                        href="https://github.com/openai/spinningup">code repo</a> of
                                                    short, standalone implementations of key algorithms,</li>
                                                <li>and a few <a class="reference external"
                                                        href="../spinningup/exercises.html">exercises</a> to serve as
                                                    warm-ups.</li>
                                            </ul>
                                        </div>
                                        <div class="section" id="why-we-built-this">
                                            <h3><a class="toc-backref" href="#id4">Why We Built This</a><a
                                                    class="headerlink" href="#why-we-built-this"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>One of the single most common questions that we hear is</p>
                                            <blockquote>
                                                <div>
                                                    <div class="line-block">
                                                        <div class="line">If I want to contribute to AI safety, how do I
                                                            get started?</div>
                                                    </div>
                                                </div>
                                            </blockquote>
                                            <p>At OpenAI, we believe that deep learning generally&#8212;and deep
                                                reinforcement learning specifically&#8212;will play central roles in the
                                                development of powerful AI technology. To ensure that AI is safe, we
                                                have to come up with safety strategies and algorithms that are
                                                compatible with this paradigm. As a result, we encourage everyone who
                                                asks this question to study these fields.</p>
                                            <p>However, while there are many resources to help people quickly ramp up on
                                                deep learning, deep reinforcement learning is more challenging to break
                                                into. To begin with, a student of deep RL needs to have some background
                                                in math, coding, and regular deep learning. Beyond that, they need both
                                                a high-level view of the field&#8212;an awareness of what topics are
                                                studied in it, why they matter, and what&#8217;s been done
                                                already&#8212;and careful instruction on how to connect algorithm theory
                                                to algorithm code.</p>
                                            <p>The high-level view is hard to come by because of how new the field is.
                                                There is not yet a standard deep RL textbook, so most of the knowledge
                                                is locked up in either papers or lecture series, which can take a long
                                                time to parse and digest. And learning to implement deep RL algorithms
                                                is typically painful, because either</p>
                                            <ul class="simple">
                                                <li>the paper that publishes an algorithm omits or inadvertently
                                                    obscures key design details,</li>
                                                <li>or widely-public implementations of an algorithm are hard to read,
                                                    hiding how the code lines up with the algorithm.</li>
                                            </ul>
                                            <p>While fantastic repos like <a class="reference external"
                                                    href="https://github.com/rlworkgroup/garage">garage</a>, <a
                                                    class="reference external"
                                                    href="https://github.com/openai/baselines">Baselines</a>, and <a
                                                    class="reference external"
                                                    href="https://github.com/ray-project/ray/tree/master/python/ray/rllib">rllib</a>
                                                make it easier for researchers who are already in the field to make
                                                progress, they build algorithms into frameworks in ways that involve
                                                many non-obvious choices and trade-offs, which makes them hard to learn
                                                from. Consequently, the field of deep RL has a pretty high barrier to
                                                entry&#8212;for new researchers as well as practitioners and hobbyists.
                                            </p>
                                            <p>So our package here is designed to serve as the missing middle step for
                                                people who are excited by deep RL, and would like to learn how to use it
                                                or make a contribution, but don&#8217;t have a clear sense of what to
                                                study or how to transmute algorithms into code. We&#8217;ve tried to
                                                make this as helpful a launching point as possible.</p>
                                            <p>That said, practitioners aren&#8217;t the only people who can (or should)
                                                benefit from these materials. Solving AI safety will require people with
                                                a wide range of expertise and perspectives, and many relevant
                                                professions have no connection to engineering or computer science at
                                                all. Nonetheless, everyone involved will need to learn enough about the
                                                technology to make informed decisions, and several pieces of Spinning Up
                                                address that need.</p>
                                        </div>
                                        <div class="section" id="how-this-serves-our-mission">
                                            <h3><a class="toc-backref" href="#id5">How This Serves Our Mission</a><a
                                                    class="headerlink" href="#how-this-serves-our-mission"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>OpenAI&#8217;s <a class="reference external"
                                                    href="https://blog.openai.com/openai-charter/">mission</a> is to
                                                ensure the safe development of AGI and the broad distribution of
                                                benefits from AI more generally. Teaching tools like Spinning Up help us
                                                make progress on both of these objectives.</p>
                                            <p>To begin with, we move closer to broad distribution of benefits any time
                                                we help people understand what AI is and how it works. This empowers
                                                people to think critically about the many issues we anticipate will
                                                arise as AI becomes more sophisticated and important in our lives.</p>
                                            <p>Also, critically, <a class="reference external"
                                                    href="https://jobs.lever.co/openai">we need people to help</a> us
                                                work on making sure that AGI is safe. This requires a skill set which is
                                                currently in short supply because of how new the field is. We know that
                                                many people are interested in helping us, but don&#8217;t know
                                                how&#8212;here is what you should study! If you can become an expert on
                                                this material, you can make a difference on AI safety.</p>
                                        </div>
                                        <div class="section" id="code-design-philosophy">
                                            <h3><a class="toc-backref" href="#id6">Code Design Philosophy</a><a
                                                    class="headerlink" href="#code-design-philosophy"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>The algorithm implementations in the Spinning Up repo are designed to be
                                            </p>
                                            <blockquote>
                                                <div>
                                                    <ul class="simple">
                                                        <li>as simple as possible while still being reasonably good,
                                                        </li>
                                                        <li>and highly-consistent with each other to expose fundamental
                                                            similarities between algorithms.</li>
                                                    </ul>
                                                </div>
                                            </blockquote>
                                            <p>They are almost completely self-contained, with virtually no common code
                                                shared between them (except for logging, saving, loading, and <a
                                                    class="reference external"
                                                    href="https://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a>
                                                utilities), so that an interested person can study each algorithm
                                                separately without having to dig through an endless chain of
                                                dependencies to see how something is done. The implementations are
                                                patterned so that they come as close to pseudocode as possible, to
                                                minimize the gap between theory and code.</p>
                                            <p>Importantly, they&#8217;re all structured similarly, so if you clearly
                                                understand one, jumping into the next is painless.</p>
                                            <p>We tried to minimize the number of tricks used in each algorithm&#8217;s
                                                implementation, and minimize the differences between otherwise-similar
                                                algorithms. To give some examples of removed tricks: we omit <a
                                                    class="reference external"
                                                    href="https://github.com/haarnoja/sac/blob/108a4229be6f040360fcca983113df9c4ac23a6a/sac/distributions/normal.py#L69">regularization</a>
                                                terms present in the original Soft-Actor Critic code, as well as <a
                                                    class="reference external"
                                                    href="https://github.com/openai/baselines/blob/28aca637d0f13f4415cc5ebb778144154cff3110/baselines/run.py#L131">observation
                                                    normalization</a> from all algorithms. For an example of where
                                                we&#8217;ve removed differences between algorithms: our implementations
                                                of DDPG, TD3, and SAC all follow a convention of running gradient
                                                descent updates after fixed intervals of environment interaction. (By
                                                contrast, other public implementations of these algorithms usually take
                                                slightly different approaches from each other, making them a little bit
                                                harder to compare.)</p>
                                            <p>All algorithms are &#8220;reasonably good&#8221; in the sense that they
                                                achieve roughly the intended performance, but don&#8217;t necessarily
                                                match the best reported results in the literature on every task.
                                                Consequently, be careful if using any of these implementations for
                                                scientific benchmarking comparisons. Details on each
                                                implementation&#8217;s specific performance level can be found on our <a
                                                    class="reference external"
                                                    href="../spinningup/bench.html">benchmarks</a> page.</p>
                                        </div>
                                        <div class="section" id="long-term-support-and-support-history">
                                            <h3><a class="toc-backref" href="#id7">Long-Term Support and Support
                                                    History</a><a class="headerlink"
                                                    href="#long-term-support-and-support-history"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>Spinning Up is currently in maintenance mode. If there are any breaking
                                                bugs, we&#8217;ll repair them to ensure that Spinning Up can continue to
                                                help people study deep RL.</p>
                                            <p>Support history so far:</p>
                                            <ul>
                                                <li>
                                                    <p class="first"><strong>Nov 8, 2018:</strong> Initial release!</p>
                                                </li>
                                                <li>
                                                    <p class="first"><strong>Nov, 2018:</strong> Release was followed by
                                                        a three-week period of high-bandwidth support.</p>
                                                </li>
                                                <li>
                                                    <p class="first"><strong>April, 2019:</strong> Approximately six
                                                        months after release, we conducted an internal review of
                                                        Spinning Up based on feedback from the community. The review
                                                        surfaced interest in a few key features:</p>
                                                    <blockquote>
                                                        <div>
                                                            <ul class="simple">
                                                                <li><strong>Implementations in Other Neural Network
                                                                        Libraries.</strong> Several people expressed
                                                                    interest in seeing Spinning Up use alternatives to
                                                                    Tensorflow v1 for the RL implementations. A few
                                                                    members of the community even developed their own
                                                                    PyTorch versions of Spinning Up algorithms, such as
                                                                    Kashif Rasul&#8217;s <a class="reference external"
                                                                        href="https://github.com/kashif/firedup">Fired
                                                                        Up</a>, Kai Arulkumaran&#8217;s <a
                                                                        class="reference external"
                                                                        href="https://github.com/Kaixhin/spinning-up-basic">Spinning
                                                                        Up Basic</a>, and Misha Laskin&#8217;s <a
                                                                        class="reference external"
                                                                        href="https://github.com/MishaLaskin/torchingup">Torching
                                                                        Up</a>. As a result, making this kind of
                                                                    &#8220;Rosetta Stone&#8221; for deep RL became a
                                                                    high priority for future work.</li>
                                                                <li><strong>Open Source RL Environments.</strong> Many
                                                                    people expressed an interest in seeing Spinning Up
                                                                    use more open source RL environments (eg <a
                                                                        class="reference external"
                                                                        href="https://pybullet.org/wordpress/">PyBullet</a>)
                                                                    for benchmarks, examples, and exercises.</li>
                                                                <li><strong>More Algorithms.</strong> There was some
                                                                    interest in seeing other algorithms included in
                                                                    Spinning Up, especially Deep Q-Networks.</li>
                                                            </ul>
                                                        </div>
                                                    </blockquote>
                                                </li>
                                                <li>
                                                    <p class="first"><strong>Jan, 2020:</strong> The PyTorch update to
                                                        Spinning Up was released!</p>
                                                </li>
                                                <li>
                                                    <p class="first"><strong>Future:</strong> No major updates are
                                                        currently planned for Spinning Up. In the event it makes sense
                                                        for us to release an additional update, following what we found
                                                        in the 6-month review, the next-highest priority features are to
                                                        focus more on open source RL environments and adding algorithms.
                                                    </p>
                                                </li>
                                            </ul>
                                            <p>Additionally, as discussed in the blog post, Spinning Up has been
                                                integrated into the curriculum for our <a class="reference external"
                                                    href="https://openai.com/blog/openai-scholars-spring-2020/">Scholars</a>
                                                and <a class="reference external"
                                                    href="https://openai.com/blog/openai-fellows-fall-2018/">Fellows</a>
                                                programs.</p>
                                        </div>
                                    </div>
                                    <span id="document-user/installation"></span>
                                    <div class="section" id="installation">
                                        <h2><a class="toc-backref" href="#id3">Installation</a><a class="headerlink"
                                                href="#installation" title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal" href="#installation"
                                                        id="id3">Installation</a>
                                                    <ul>
                                                        <li><a class="reference internal" href="#installing-python"
                                                                id="id4">Installing Python</a></li>
                                                        <li><a class="reference internal" href="#installing-openmpi"
                                                                id="id5">Installing OpenMPI</a>
                                                            <ul>
                                                                <li><a class="reference internal" href="#ubuntu"
                                                                        id="id6">Ubuntu</a></li>
                                                                <li><a class="reference internal" href="#mac-os-x"
                                                                        id="id7">Mac OS X</a></li>
                                                            </ul>
                                                        </li>
                                                        <li><a class="reference internal" href="#installing-spinning-up"
                                                                id="id8">Installing Spinning Up</a></li>
                                                        <li><a class="reference internal" href="#check-your-install"
                                                                id="id9">Check Your Install</a></li>
                                                        <li><a class="reference internal"
                                                                href="#installing-mujoco-optional" id="id10">Installing
                                                                MuJoCo (Optional)</a></li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <p>Spinning Up requires Python3, OpenAI Gym, and OpenMPI.</p>
                                        <p>Spinning Up is currently only supported on Linux and OSX. It may be possible
                                            to install on Windows, though this hasn&#8217;t been extensively tested. <a
                                                class="footnote-reference" href="#id2" id="id1">[1]</a></p>
                                        <div class="admonition-you-should-know admonition">
                                            <p class="first admonition-title">You Should Know</p>
                                            <p>Many examples and benchmarks in Spinning Up refer to RL environments that
                                                use the <a class="reference external"
                                                    href="http://www.mujoco.org/index.html">MuJoCo</a> physics engine.
                                                MuJoCo is a proprietary software that requires a license, which is free
                                                to trial and free for students, but otherwise is not free. As a result,
                                                installing it is optional, but because of its importance to the research
                                                community&#8212;it is the de facto standard for benchmarking deep RL
                                                algorithms in continuous control&#8212;it is preferred.</p>
                                            <p class="last">Don&#8217;t worry if you decide not to install MuJoCo,
                                                though. You can definitely get started in RL by running RL algorithms on
                                                the <a class="reference external"
                                                    href="https://gym.openai.com/envs/#classic_control">Classic
                                                    Control</a> and <a class="reference external"
                                                    href="https://gym.openai.com/envs/#box2d">Box2d</a> environments in
                                                Gym, which are totally free to use.</p>
                                        </div>
                                        <table class="docutils footnote" frame="void" id="id2" rules="none">
                                            <colgroup>
                                                <col class="label" />
                                                <col />
                                            </colgroup>
                                            <tbody valign="top">
                                                <tr>
                                                    <td class="label"><a class="fn-backref" href="#id1">[1]</a></td>
                                                    <td>It looks like at least one person has figured out <a
                                                            class="reference external"
                                                            href="https://github.com/openai/spinningup/issues/23">a
                                                            workaround for running on Windows</a>. If you try another
                                                        way and succeed, please let us know how you did it!</td>
                                                </tr>
                                            </tbody>
                                        </table>
                                        <div class="section" id="installing-python">
                                            <h3><a class="toc-backref" href="#id4">Installing Python</a><a
                                                    class="headerlink" href="#installing-python"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>We recommend installing Python through Anaconda. Anaconda is a library
                                                that includes Python and many useful packages for Python, as well as an
                                                environment manager called conda that makes package management simple.
                                            </p>
                                            <p>Follow <a class="reference external"
                                                    href="https://docs.continuum.io/anaconda/install/">the installation
                                                    instructions</a> for Anaconda here. Download and install Anaconda3
                                                (at time of writing, <a class="reference external"
                                                    href="https://repo.anaconda.com/archive/">Anaconda3-5.3.0</a>). Then
                                                create a conda Python 3.6 env for organizing packages used in Spinning
                                                Up:</p>
                                            <div class="highlight-default">
                                                <div class="highlight">
                                                    <pre><span></span><span class="n">conda</span> <span class="n">create</span> <span class="o">-</span><span class="n">n</span> <span class="n">spinningup</span> <span class="n">python</span><span class="o">=</span><span class="mf">3.6</span>
</pre>
                                                </div>
                                            </div>
                                            <p>To use Python from the environment you just created, activate the
                                                environment with:</p>
                                            <div class="highlight-default">
                                                <div class="highlight">
                                                    <pre><span></span><span class="n">conda</span> <span class="n">activate</span> <span class="n">spinningup</span>
</pre>
                                                </div>
                                            </div>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p>If you&#8217;re new to python environments and package management,
                                                    this stuff can quickly get confusing or overwhelming, and
                                                    you&#8217;ll probably hit some snags along the way. (Especially, you
                                                    should expect problems like, &#8220;I just installed this thing, but
                                                    it says it&#8217;s not found when I try to use it!&#8221;) You may
                                                    want to read through some clean explanations about what package
                                                    management is, why it&#8217;s a good idea, and what commands
                                                    you&#8217;ll typically have to execute to correctly use it.</p>
                                                <p class="last"><a class="reference external"
                                                        href="https://medium.freecodecamp.org/why-you-need-python-environments-and-how-to-manage-them-with-conda-85f155f4353c">FreeCodeCamp</a>
                                                    has a good explanation worth reading. There&#8217;s a shorter
                                                    description on <a class="reference external"
                                                        href="https://towardsdatascience.com/environment-management-with-conda-python-2-3-b9961a8a5097">Towards
                                                        Data Science</a> which is also helpful and informative. Finally,
                                                    if you&#8217;re an extremely patient person, you may want to read
                                                    the (dry, but very informative) <a class="reference external"
                                                        href="https://conda.io/docs/user-guide/tasks/manage-environments.html">documentation
                                                        page from Conda</a>.</p>
                                            </div>
                                        </div>
                                        <div class="section" id="installing-openmpi">
                                            <h3><a class="toc-backref" href="#id5">Installing OpenMPI</a><a
                                                    class="headerlink" href="#installing-openmpi"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <div class="section" id="ubuntu">
                                                <h4><a class="toc-backref" href="#id6">Ubuntu</a><a class="headerlink"
                                                        href="#ubuntu" title="Permalink to this headline">¶</a></h4>
                                                <div class="highlight-default">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">sudo</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">update</span> <span class="o">&amp;&amp;</span> <span class="n">sudo</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="n">libopenmpi</span><span class="o">-</span><span class="n">dev</span>
</pre>
                                                    </div>
                                                </div>
                                            </div>
                                            <div class="section" id="mac-os-x">
                                                <h4><a class="toc-backref" href="#id7">Mac OS X</a><a class="headerlink"
                                                        href="#mac-os-x" title="Permalink to this headline">¶</a></h4>
                                                <p>Installation of system packages on Mac requires <a
                                                        class="reference external" href="https://brew.sh">Homebrew</a>.
                                                    With Homebrew installed, run the follwing:</p>
                                                <div class="highlight-default">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">brew</span> <span class="n">install</span> <span class="n">openmpi</span>
</pre>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                        <div class="section" id="installing-spinning-up">
                                            <h3><a class="toc-backref" href="#id8">Installing Spinning Up</a><a
                                                    class="headerlink" href="#installing-spinning-up"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <div class="highlight-default">
                                                <div class="highlight">
                                                    <pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">openai</span><span class="o">/</span><span class="n">spinningup</span><span class="o">.</span><span class="n">git</span>
<span class="n">cd</span> <span class="n">spinningup</span>
<span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">e</span> <span class="o">.</span>
</pre>
                                                </div>
                                            </div>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p class="last">Spinning Up defaults to installing everything in Gym
                                                    <strong>except</strong> the MuJoCo environments. In case you run
                                                    into any trouble with the Gym installation, check out the <a
                                                        class="reference external"
                                                        href="https://github.com/openai/gym">Gym</a> github page for
                                                    help. If you want the MuJoCo environments, see the optional
                                                    installation section below.</p>
                                            </div>
                                        </div>
                                        <div class="section" id="check-your-install">
                                            <h3><a class="toc-backref" href="#id9">Check Your Install</a><a
                                                    class="headerlink" href="#check-your-install"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>To see if you&#8217;ve successfully installed Spinning Up, try running
                                                PPO in the LunarLander-v2 environment with</p>
                                            <div class="highlight-default">
                                                <div class="highlight">
                                                    <pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spinup</span><span class="o">.</span><span class="n">run</span> <span class="n">ppo</span> <span class="o">--</span><span class="n">hid</span> <span class="s2">&quot;[32,32]&quot;</span> <span class="o">--</span><span class="n">env</span> <span class="n">LunarLander</span><span class="o">-</span><span class="n">v2</span> <span class="o">--</span><span class="n">exp_name</span> <span class="n">installtest</span> <span class="o">--</span><span class="n">gamma</span> <span class="mf">0.999</span>
</pre>
                                                </div>
                                            </div>
                                            <p>This might run for around 10 minutes, and you can leave it going in the
                                                background while you continue reading through documentation. This
                                                won&#8217;t train the agent to completion, but will run it for long
                                                enough that you can see <em>some</em> learning progress when the results
                                                come in.</p>
                                            <p>After it finishes training, watch a video of the trained policy with</p>
                                            <div class="highlight-default">
                                                <div class="highlight">
                                                    <pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spinup</span><span class="o">.</span><span class="n">run</span> <span class="n">test_policy</span> <span class="n">data</span><span class="o">/</span><span class="n">installtest</span><span class="o">/</span><span class="n">installtest_s0</span>
</pre>
                                                </div>
                                            </div>
                                            <p>And plot the results with</p>
                                            <div class="highlight-default">
                                                <div class="highlight">
                                                    <pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spinup</span><span class="o">.</span><span class="n">run</span> <span class="n">plot</span> <span class="n">data</span><span class="o">/</span><span class="n">installtest</span><span class="o">/</span><span class="n">installtest_s0</span>
</pre>
                                                </div>
                                            </div>
                                        </div>
                                        <div class="section" id="installing-mujoco-optional">
                                            <h3><a class="toc-backref" href="#id10">Installing MuJoCo (Optional)</a><a
                                                    class="headerlink" href="#installing-mujoco-optional"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>First, go to the <a class="reference external"
                                                    href="https://github.com/openai/mujoco-py">mujoco-py</a> github
                                                page. Follow the installation instructions in the README, which describe
                                                how to install the MuJoCo physics engine and the mujoco-py package
                                                (which allows the use of MuJoCo from Python).</p>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p class="last">In order to use the MuJoCo simulator, you will need to
                                                    get a <a class="reference external"
                                                        href="https://www.roboti.us/license.html">MuJoCo license</a>.
                                                    Free 30-day licenses are available to anyone, and free 1-year
                                                    licenses are available to full-time students.</p>
                                            </div>
                                            <p>Once you have installed MuJoCo, install the corresponding Gym
                                                environments with</p>
                                            <div class="highlight-default">
                                                <div class="highlight">
                                                    <pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">gym</span><span class="p">[</span><span class="n">mujoco</span><span class="p">,</span><span class="n">robotics</span><span class="p">]</span>
</pre>
                                                </div>
                                            </div>
                                            <p>And then check that things are working by running PPO in the Walker2d-v2
                                                environment with</p>
                                            <div class="highlight-default">
                                                <div class="highlight">
                                                    <pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spinup</span><span class="o">.</span><span class="n">run</span> <span class="n">ppo</span> <span class="o">--</span><span class="n">hid</span> <span class="s2">&quot;[32,32]&quot;</span> <span class="o">--</span><span class="n">env</span> <span class="n">Walker2d</span><span class="o">-</span><span class="n">v2</span> <span class="o">--</span><span class="n">exp_name</span> <span class="n">mujocotest</span>
</pre>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    <span id="document-user/algorithms"></span>
                                    <div class="section" id="algorithms">
                                        <h2><a class="toc-backref" href="#id1">Algorithms</a><a class="headerlink"
                                                href="#algorithms" title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal" href="#algorithms"
                                                        id="id1">Algorithms</a>
                                                    <ul>
                                                        <li><a class="reference internal" href="#what-s-included"
                                                                id="id2">What&#8217;s Included</a></li>
                                                        <li><a class="reference internal" href="#why-these-algorithms"
                                                                id="id3">Why These Algorithms?</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#the-on-policy-algorithms" id="id4">The
                                                                        On-Policy Algorithms</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#the-off-policy-algorithms" id="id5">The
                                                                        Off-Policy Algorithms</a></li>
                                                            </ul>
                                                        </li>
                                                        <li><a class="reference internal" href="#code-format"
                                                                id="id6">Code Format</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#the-algorithm-function-pytorch-version"
                                                                        id="id7">The Algorithm Function: PyTorch
                                                                        Version</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#the-algorithm-function-tensorflow-version"
                                                                        id="id8">The Algorithm Function: Tensorflow
                                                                        Version</a></li>
                                                                <li><a class="reference internal" href="#the-core-file"
                                                                        id="id9">The Core File</a></li>
                                                            </ul>
                                                        </li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <div class="section" id="what-s-included">
                                            <h3><a class="toc-backref" href="#id2">What&#8217;s Included</a><a
                                                    class="headerlink" href="#what-s-included"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>The following algorithms are implemented in the Spinning Up package:</p>
                                            <ul class="simple">
                                                <li><a class="reference external" href="../algorithms/vpg.html">Vanilla
                                                        Policy Gradient</a> (VPG)</li>
                                                <li><a class="reference external" href="../algorithms/trpo.html">Trust
                                                        Region Policy Optimization</a> (TRPO)</li>
                                                <li><a class="reference external" href="../algorithms/ppo.html">Proximal
                                                        Policy Optimization</a> (PPO)</li>
                                                <li><a class="reference external" href="../algorithms/ddpg.html">Deep
                                                        Deterministic Policy Gradient</a> (DDPG)</li>
                                                <li><a class="reference external" href="../algorithms/td3.html">Twin
                                                        Delayed DDPG</a> (TD3)</li>
                                                <li><a class="reference external" href="../algorithms/sac.html">Soft
                                                        Actor-Critic</a> (SAC)</li>
                                            </ul>
                                            <p>They are all implemented with <a class="reference external"
                                                    href="https://en.wikipedia.org/wiki/Multilayer_perceptron">MLP</a>
                                                (non-recurrent) actor-critics, making them suitable for fully-observed,
                                                non-image-based RL environments, e.g. the <a class="reference external"
                                                    href="https://gym.openai.com/envs/#mujoco">Gym Mujoco</a>
                                                environments.</p>
                                            <p>Spinning Up has two implementations for each algorithm (except for TRPO):
                                                one that uses <a class="reference external"
                                                    href="https://pytorch.org/">PyTorch</a> as the neural network
                                                library, and one that uses <a class="reference external"
                                                    href="https://www.tensorflow.org/versions/r1.15/api_docs">Tensorflow
                                                    v1</a> as the neural network library. (TRPO is currently only
                                                available in Tensorflow.)</p>
                                        </div>
                                        <div class="section" id="why-these-algorithms">
                                            <h3><a class="toc-backref" href="#id3">Why These Algorithms?</a><a
                                                    class="headerlink" href="#why-these-algorithms"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>We chose the core deep RL algorithms in this package to reflect useful
                                                progressions of ideas from the recent history of the field, culminating
                                                in two algorithms in particular&#8212;PPO and SAC&#8212;which are close
                                                to state of the art on reliability and sample efficiency among
                                                policy-learning algorithms. They also expose some of the trade-offs that
                                                get made in designing and using algorithms in deep RL.</p>
                                            <div class="section" id="the-on-policy-algorithms">
                                                <h4><a class="toc-backref" href="#id4">The On-Policy Algorithms</a><a
                                                        class="headerlink" href="#the-on-policy-algorithms"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>Vanilla Policy Gradient is the most basic, entry-level algorithm in
                                                    the deep RL space because it completely predates the advent of deep
                                                    RL altogether. The core elements of VPG go all the way back to the
                                                    late 80s / early 90s. It started a trail of research which
                                                    ultimately led to stronger algorithms such as TRPO and then PPO soon
                                                    after.</p>
                                                <p>A key feature of this line of work is that all of these algorithms
                                                    are <em>on-policy</em>: that is, they don&#8217;t use old data,
                                                    which makes them weaker on sample efficiency. But this is for a good
                                                    reason: these algorithms directly optimize the objective you care
                                                    about&#8212;policy performance&#8212;and it works out mathematically
                                                    that you need on-policy data to calculate the updates. So, this
                                                    family of algorithms trades off sample efficiency in favor of
                                                    stability&#8212;but you can see the progression of techniques (from
                                                    VPG to TRPO to PPO) working to make up the deficit on sample
                                                    efficiency.</p>
                                            </div>
                                            <div class="section" id="the-off-policy-algorithms">
                                                <h4><a class="toc-backref" href="#id5">The Off-Policy Algorithms</a><a
                                                        class="headerlink" href="#the-off-policy-algorithms"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>DDPG is a similarly foundational algorithm to VPG, although much
                                                    younger&#8212;the theory of deterministic policy gradients, which
                                                    led to DDPG, wasn&#8217;t published until 2014. DDPG is closely
                                                    connected to Q-learning algorithms, and it concurrently learns a
                                                    Q-function and a policy which are updated to improve each other.</p>
                                                <p>Algorithms like DDPG and Q-Learning are <em>off-policy</em>, so they
                                                    are able to reuse old data very efficiently. They gain this benefit
                                                    by exploiting Bellman&#8217;s equations for optimality, which a
                                                    Q-function can be trained to satisfy using <em>any</em> environment
                                                    interaction data (as long as there&#8217;s enough experience from
                                                    the high-reward areas in the environment).</p>
                                                <p>But problematically, there are no guarantees that doing a good job of
                                                    satisfying Bellman&#8217;s equations leads to having great policy
                                                    performance. <em>Empirically</em> one can get great
                                                    performance&#8212;and when it happens, the sample efficiency is
                                                    wonderful&#8212;but the absence of guarantees makes algorithms in
                                                    this class potentially brittle and unstable. TD3 and SAC are
                                                    descendants of DDPG which make use of a variety of insights to
                                                    mitigate these issues.</p>
                                            </div>
                                        </div>
                                        <div class="section" id="code-format">
                                            <h3><a class="toc-backref" href="#id6">Code Format</a><a class="headerlink"
                                                    href="#code-format" title="Permalink to this headline">¶</a></h3>
                                            <p>All implementations in Spinning Up adhere to a standard template. They
                                                are split into two files: an algorithm file, which contains the core
                                                logic of the algorithm, and a core file, which contains various
                                                utilities needed to run the algorithm.</p>
                                            <p>The algorithm file always starts with a class definition for an
                                                experience buffer object, which is used to store information from
                                                agent-environment interactions. Next, there is a single function which
                                                runs the algorithm. The algorithm function follows a template that is
                                                roughly the same across the PyTorch and Tensorflow versions, but
                                                we&#8217;ll break it down for each separately below. Finally,
                                                there&#8217;s some support in each algorithm file for directly running
                                                the algorithm in Gym environments from the command line (though this is
                                                not the recommended way to run the algorithms&#8212;we&#8217;ll describe
                                                how to do that on the <a class="reference external"
                                                    href="../user/running.html">Running Experiments</a> page).</p>
                                            <div class="section" id="the-algorithm-function-pytorch-version">
                                                <h4><a class="toc-backref" href="#id7">The Algorithm Function: PyTorch
                                                        Version</a><a class="headerlink"
                                                        href="#the-algorithm-function-pytorch-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>The algorithm function for a PyTorch implementation performs the
                                                    following tasks in (roughly) this order:</p>
                                                <blockquote>
                                                    <div>
                                                        <ol class="arabic simple">
                                                            <li>Logger setup</li>
                                                            <li>Random seed setting</li>
                                                            <li>Environment instantiation</li>
                                                            <li>Constructing the actor-critic PyTorch module via the
                                                                <code
                                                                    class="docutils literal"><span class="pre">actor_critic</span></code>
                                                                function passed to the algorithm function as an argument
                                                            </li>
                                                            <li>Instantiating the experience buffer</li>
                                                            <li>Setting up callable loss functions that also provide
                                                                diagnostics specific to the algorithm</li>
                                                            <li>Making PyTorch optimizers</li>
                                                            <li>Setting up model saving through the logger</li>
                                                            <li>Setting up an update function that runs one epoch of
                                                                optimization or one step of descent</li>
                                                            <li>Running the main loop of the algorithm:<ol
                                                                    class="loweralpha">
                                                                    <li>Run the agent in the environment</li>
                                                                    <li>Periodically update the parameters of the agent
                                                                        according to the main equations of the algorithm
                                                                    </li>
                                                                    <li>Log key performance metrics and save agent</li>
                                                                </ol>
                                                            </li>
                                                        </ol>
                                                    </div>
                                                </blockquote>
                                            </div>
                                            <div class="section" id="the-algorithm-function-tensorflow-version">
                                                <h4><a class="toc-backref" href="#id8">The Algorithm Function:
                                                        Tensorflow Version</a><a class="headerlink"
                                                        href="#the-algorithm-function-tensorflow-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>The algorithm function for a Tensorflow implementation performs the
                                                    following tasks in (roughly) this order:</p>
                                                <blockquote>
                                                    <div>
                                                        <ol class="arabic simple">
                                                            <li>Logger setup</li>
                                                            <li>Random seed setting</li>
                                                            <li>Environment instantiation</li>
                                                            <li>Making placeholders for the computation graph</li>
                                                            <li>Building the actor-critic computation graph via the
                                                                <code
                                                                    class="docutils literal"><span class="pre">actor_critic</span></code>
                                                                function passed to the algorithm function as an argument
                                                            </li>
                                                            <li>Instantiating the experience buffer</li>
                                                            <li>Building the computation graph for loss functions and
                                                                diagnostics specific to the algorithm</li>
                                                            <li>Making training ops</li>
                                                            <li>Making the TF Session and initializing parameters</li>
                                                            <li>Setting up model saving through the logger</li>
                                                            <li>Defining functions needed for running the main loop of
                                                                the algorithm (e.g. the core update function, get action
                                                                function, and test agent function, depending on the
                                                                algorithm)</li>
                                                            <li>Running the main loop of the algorithm:<ol
                                                                    class="loweralpha">
                                                                    <li>Run the agent in the environment</li>
                                                                    <li>Periodically update the parameters of the agent
                                                                        according to the main equations of the algorithm
                                                                    </li>
                                                                    <li>Log key performance metrics and save agent</li>
                                                                </ol>
                                                            </li>
                                                        </ol>
                                                    </div>
                                                </blockquote>
                                            </div>
                                            <div class="section" id="the-core-file">
                                                <h4><a class="toc-backref" href="#id9">The Core File</a><a
                                                        class="headerlink" href="#the-core-file"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>The core files don&#8217;t adhere as closely as the algorithms files
                                                    to a template, but do have some approximate structure:</p>
                                                <blockquote>
                                                    <div>
                                                        <ol class="arabic simple">
                                                            <li><strong>Tensorflow only:</strong> Functions related to
                                                                making and managing placeholders</li>
                                                            <li>Functions for building sections of computation graph
                                                                relevant to the <code
                                                                    class="docutils literal"><span class="pre">actor_critic</span></code>
                                                                method for a particular algorithm</li>
                                                            <li>Any other useful functions</li>
                                                            <li>Implementations for an MLP actor-critic compatible with
                                                                the algorithm, where both the policy and the value
                                                                function(s) are represented by simple MLPs</li>
                                                        </ol>
                                                    </div>
                                                </blockquote>
                                            </div>
                                        </div>
                                    </div>
                                    <span id="document-user/running"></span>
                                    <div class="section" id="running-experiments">
                                        <h2><a class="toc-backref" href="#id3">Running Experiments</a><a
                                                class="headerlink" href="#running-experiments"
                                                title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal" href="#running-experiments"
                                                        id="id3">Running Experiments</a>
                                                    <ul>
                                                        <li><a class="reference internal"
                                                                href="#launching-from-the-command-line"
                                                                id="id4">Launching from the Command Line</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#choosing-pytorch-or-tensorflow-from-the-command-line"
                                                                        id="id5">Choosing PyTorch or Tensorflow from the
                                                                        Command Line</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#setting-hyperparameters-from-the-command-line"
                                                                        id="id6">Setting Hyperparameters from the
                                                                        Command Line</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#launching-multiple-experiments-at-once"
                                                                        id="id7">Launching Multiple Experiments at
                                                                        Once</a></li>
                                                                <li><a class="reference internal" href="#special-flags"
                                                                        id="id8">Special Flags</a>
                                                                    <ul>
                                                                        <li><a class="reference internal"
                                                                                href="#environment-flag"
                                                                                id="id9">Environment Flag</a></li>
                                                                        <li><a class="reference internal"
                                                                                href="#shortcut-flags"
                                                                                id="id10">Shortcut Flags</a></li>
                                                                        <li><a class="reference internal"
                                                                                href="#config-flags" id="id11">Config
                                                                                Flags</a></li>
                                                                    </ul>
                                                                </li>
                                                                <li><a class="reference internal"
                                                                        href="#where-results-are-saved" id="id12">Where
                                                                        Results are Saved</a>
                                                                    <ul>
                                                                        <li><a class="reference internal"
                                                                                href="#how-is-suffix-determined"
                                                                                id="id13">How is Suffix Determined?</a>
                                                                        </li>
                                                                    </ul>
                                                                </li>
                                                                <li><a class="reference internal" href="#extra"
                                                                        id="id14">Extra</a></li>
                                                            </ul>
                                                        </li>
                                                        <li><a class="reference internal" href="#launching-from-scripts"
                                                                id="id15">Launching from Scripts</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#using-experimentgrid" id="id16">Using
                                                                        ExperimentGrid</a></li>
                                                            </ul>
                                                        </li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <p>One of the best ways to get a feel for deep RL is to run the algorithms and
                                            see how they perform on different tasks. The Spinning Up code library makes
                                            small-scale (local) experiments easy to do, and in this section, we&#8217;ll
                                            discuss two ways to run them: either from the command line, or through
                                            function calls in scripts.</p>
                                        <div class="section" id="launching-from-the-command-line">
                                            <h3><a class="toc-backref" href="#id4">Launching from the Command Line</a><a
                                                    class="headerlink" href="#launching-from-the-command-line"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>Spinning Up ships with <code
                                                    class="docutils literal"><span class="pre">spinup/run.py</span></code>,
                                                a convenient tool that lets you easily launch any algorithm (with any
                                                choices of hyperparameters) from the command line. It also serves as a
                                                thin wrapper over the utilities for watching trained policies and
                                                plotting, although we will not discuss that functionality on this page
                                                (for those details, see the pages on <a class="reference external"
                                                    href="../user/saving_and_loading.html">experiment outputs</a> and <a
                                                    class="reference external"
                                                    href="../user/plotting.html">plotting</a>).</p>
                                            <p>The standard way to run a Spinning Up algorithm from the command line is
                                            </p>
                                            <div class="highlight-default">
                                                <div class="highlight">
                                                    <pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spinup</span><span class="o">.</span><span class="n">run</span> <span class="p">[</span><span class="n">algo</span> <span class="n">name</span><span class="p">]</span> <span class="p">[</span><span class="n">experiment</span> <span class="n">flags</span><span class="p">]</span>
</pre>
                                                </div>
                                            </div>
                                            <p>eg:</p>
                                            <div class="highlight-default">
                                                <div class="highlight">
                                                    <pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spinup</span><span class="o">.</span><span class="n">run</span> <span class="n">ppo</span> <span class="o">--</span><span class="n">env</span> <span class="n">Walker2d</span><span class="o">-</span><span class="n">v2</span> <span class="o">--</span><span class="n">exp_name</span> <span class="n">walker</span>
</pre>
                                                </div>
                                            </div>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p class="last">If you are using ZShell: ZShell interprets square
                                                    brackets as special characters. Spinning Up uses square brackets in
                                                    a few ways for command line arguments; make sure to escape them, or
                                                    try the solution recommended <a class="reference external"
                                                        href="http://kinopyo.com/en/blog/escape-square-bracket-by-default-in-zsh">here</a>
                                                    if you want to escape them by default.</p>
                                            </div>
                                            <div class="admonition-detailed-quickstart-guide admonition">
                                                <p class="first admonition-title">Detailed Quickstart Guide</p>
                                                <div class="highlight-default">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spinup</span><span class="o">.</span><span class="n">run</span> <span class="n">ppo</span> <span class="o">--</span><span class="n">exp_name</span> <span class="n">ppo_ant</span> <span class="o">--</span><span class="n">env</span> <span class="n">Ant</span><span class="o">-</span><span class="n">v2</span> <span class="o">--</span><span class="n">clip_ratio</span> <span class="mf">0.1</span> <span class="mf">0.2</span>
    <span class="o">--</span><span class="n">hid</span><span class="p">[</span><span class="n">h</span><span class="p">]</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">]</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span><span class="mi">32</span><span class="p">]</span> <span class="o">--</span><span class="n">act</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span> <span class="o">--</span><span class="n">seed</span> <span class="mi">0</span> <span class="mi">10</span> <span class="mi">20</span> <span class="o">--</span><span class="n">dt</span>
    <span class="o">--</span><span class="n">data_dir</span> <span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">data</span>
</pre>
                                                    </div>
                                                </div>
                                                <p>runs PPO in the <code
                                                        class="docutils literal"><span class="pre">Ant-v2</span></code>
                                                    Gym environment, with various settings controlled by the flags.</p>
                                                <p>By default, the PyTorch version will run (except for with TRPO, since
                                                    Spinning Up doesn&#8217;t have a PyTorch TRPO yet). Substitute <code
                                                        class="docutils literal"><span class="pre">ppo</span></code>
                                                    with <code
                                                        class="docutils literal"><span class="pre">ppo_tf1</span></code>
                                                    for the Tensorflow version.</p>
                                                <p><code
                                                        class="docutils literal"><span class="pre">clip_ratio</span></code>,
                                                    <code class="docutils literal"><span class="pre">hid</span></code>,
                                                    and <code
                                                        class="docutils literal"><span class="pre">act</span></code> are
                                                    flags to set some algorithm hyperparameters. You can provide
                                                    multiple values for hyperparameters to run multiple experiments.
                                                    Check the docs to see what hyperparameters you can set (click here
                                                    for the <a class="reference external"
                                                        href="../algorithms/ppo.html#spinup.ppo">PPO documentation</a>).
                                                </p>
                                                <p><code class="docutils literal"><span class="pre">hid</span></code>
                                                    and <code
                                                        class="docutils literal"><span class="pre">act</span></code> are
                                                    <a class="reference external"
                                                        href="../user/running.html#shortcut-flags">special shortcut
                                                        flags</a> for setting the hidden sizes and activation function
                                                    for the neural networks trained by the algorithm.</p>
                                                <p>The <code
                                                        class="docutils literal"><span class="pre">seed</span></code>
                                                    flag sets the seed for the random number generator. RL algorithms
                                                    have high variance, so try multiple seeds to get a feel for how
                                                    performance varies.</p>
                                                <p>The <code class="docutils literal"><span class="pre">dt</span></code>
                                                    flag ensures that the save directory names will have timestamps in
                                                    them (otherwise they don&#8217;t, unless you set <code
                                                        class="docutils literal"><span class="pre">FORCE_DATESTAMP=True</span></code>
                                                    in <code
                                                        class="docutils literal"><span class="pre">spinup/user_config.py</span></code>).
                                                </p>
                                                <p>The <code
                                                        class="docutils literal"><span class="pre">data_dir</span></code>
                                                    flag allows you to set the save folder for results. The default
                                                    value is set by <code
                                                        class="docutils literal"><span class="pre">DEFAULT_DATA_DIR</span></code>
                                                    in <code
                                                        class="docutils literal"><span class="pre">spinup/user_config.py</span></code>,
                                                    which will be a subfolder <code
                                                        class="docutils literal"><span class="pre">data</span></code> in
                                                    the <code
                                                        class="docutils literal"><span class="pre">spinningup</span></code>
                                                    folder (unless you change it).</p>
                                                <p><a class="reference external"
                                                        href="../user/running.html#where-results-are-saved">Save
                                                        directory names</a> are based on <code
                                                        class="docutils literal"><span class="pre">exp_name</span></code>
                                                    and any flags which have multiple values. Instead of the full flag,
                                                    a shorthand will appear in the directory name. Shorthands can be
                                                    provided by the user in square brackets after the flag, like <code
                                                        class="docutils literal"><span class="pre">--hid[h]</span></code>;
                                                    otherwise, shorthands are substrings of the flag (<code
                                                        class="docutils literal"><span class="pre">clip_ratio</span></code>
                                                    becomes <code
                                                        class="docutils literal"><span class="pre">cli</span></code>).
                                                    To illustrate, the save directory for the run with <code
                                                        class="docutils literal"><span class="pre">clip_ratio=0.1</span></code>,
                                                    <code
                                                        class="docutils literal"><span class="pre">hid=[32,32]</span></code>,
                                                    and <code
                                                        class="docutils literal"><span class="pre">seed=10</span></code>
                                                    will be:</p>
                                                <div class="last highlight-default">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">YY</span><span class="o">-</span><span class="n">MM</span><span class="o">-</span><span class="n">DD_ppo_ant_cli0</span><span class="o">-</span><span class="mi">1</span><span class="n">_h32</span><span class="o">-</span><span class="mi">32</span><span class="o">/</span><span class="n">YY</span><span class="o">-</span><span class="n">MM</span><span class="o">-</span><span class="n">DD_HH</span><span class="o">-</span><span class="n">MM</span><span class="o">-</span><span class="n">SS</span><span class="o">-</span><span class="n">ppo_ant_cli0</span><span class="o">-</span><span class="mi">1</span><span class="n">_h32</span><span class="o">-</span><span class="mi">32</span><span class="n">_seed10</span>
</pre>
                                                    </div>
                                                </div>
                                            </div>
                                            <div class="section"
                                                id="choosing-pytorch-or-tensorflow-from-the-command-line">
                                                <h4><a class="toc-backref" href="#id5">Choosing PyTorch or Tensorflow
                                                        from the Command Line</a><a class="headerlink"
                                                        href="#choosing-pytorch-or-tensorflow-from-the-command-line"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>To use a PyTorch version of an algorithm, run with</p>
                                                <div class="highlight-default">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spinup</span><span class="o">.</span><span class="n">run</span> <span class="p">[</span><span class="n">algo</span><span class="p">]</span><span class="n">_pytorch</span>
</pre>
                                                    </div>
                                                </div>
                                                <p>To use a Tensorflow version of an algorithm, run with</p>
                                                <div class="highlight-default">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spinup</span><span class="o">.</span><span class="n">run</span> <span class="p">[</span><span class="n">algo</span><span class="p">]</span><span class="n">_tf1</span>
</pre>
                                                    </div>
                                                </div>
                                                <p>If you run <code
                                                        class="docutils literal"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">spinup.run</span> <span class="pre">[algo]</span></code>
                                                    without <code
                                                        class="docutils literal"><span class="pre">_pytorch</span></code>
                                                    or <code
                                                        class="docutils literal"><span class="pre">_tf1</span></code>,
                                                    the runner will look in <code
                                                        class="docutils literal"><span class="pre">spinup/user_config.py</span></code>
                                                    for which version it should default to for that algorithm.</p>
                                            </div>
                                            <div class="section" id="setting-hyperparameters-from-the-command-line">
                                                <h4><a class="toc-backref" href="#id6">Setting Hyperparameters from the
                                                        Command Line</a><a class="headerlink"
                                                        href="#setting-hyperparameters-from-the-command-line"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>Every hyperparameter in every algorithm can be controlled directly
                                                    from the command line. If <code
                                                        class="docutils literal"><span class="pre">kwarg</span></code>
                                                    is a valid keyword arg for the function call of an algorithm, you
                                                    can set values for it with the flag <code
                                                        class="docutils literal"><span class="pre">--kwarg</span></code>.
                                                    To find out what keyword args are available, see either the docs
                                                    page for an algorithm, or try</p>
                                                <div class="highlight-default">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spinup</span><span class="o">.</span><span class="n">run</span> <span class="p">[</span><span class="n">algo</span> <span class="n">name</span><span class="p">]</span> <span class="o">--</span><span class="n">help</span>
</pre>
                                                    </div>
                                                </div>
                                                <p>to see a readout of the docstring.</p>
                                                <div class="admonition-you-should-know admonition">
                                                    <p class="first admonition-title">You Should Know</p>
                                                    <p>Values pass through <code
                                                            class="docutils literal"><span class="pre">eval()</span></code>
                                                        before being used, so you can describe some functions and
                                                        objects directly from the command line. For example:</p>
                                                    <div class="highlight-default">
                                                        <div class="highlight">
                                                            <pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spinup</span><span class="o">.</span><span class="n">run</span> <span class="n">ppo</span> <span class="o">--</span><span class="n">env</span> <span class="n">Walker2d</span><span class="o">-</span><span class="n">v2</span> <span class="o">--</span><span class="n">exp_name</span> <span class="n">walker</span> <span class="o">--</span><span class="n">act</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ELU</span>
</pre>
                                                        </div>
                                                    </div>
                                                    <p class="last">sets <code
                                                            class="docutils literal"><span class="pre">torch.nn.ELU</span></code>
                                                        as the activation function. (Tensorflow equivalent: run <code
                                                            class="docutils literal"><span class="pre">ppo_tf1</span></code>
                                                        with <code
                                                            class="docutils literal"><span class="pre">--act</span> <span class="pre">tf.nn.elu</span></code>.)
                                                    </p>
                                                </div>
                                                <div class="admonition-you-should-know admonition">
                                                    <p class="first admonition-title">You Should Know</p>
                                                    <p>There&#8217;s some nice handling for kwargs that take dict
                                                        values. Instead of having to provide</p>
                                                    <div class="highlight-default">
                                                        <div class="highlight">
                                                            <pre><span></span><span class="o">--</span><span class="n">key</span> <span class="nb">dict</span><span class="p">(</span><span class="n">v1</span><span class="o">=</span><span class="n">value_1</span><span class="p">,</span> <span class="n">v2</span><span class="o">=</span><span class="n">value_2</span><span class="p">)</span>
</pre>
                                                        </div>
                                                    </div>
                                                    <p>you can give</p>
                                                    <div class="highlight-default">
                                                        <div class="highlight">
                                                            <pre><span></span><span class="o">--</span><span class="n">key</span><span class="p">:</span><span class="n">v1</span> <span class="n">value_1</span> <span class="o">--</span><span class="n">key</span><span class="p">:</span><span class="n">v2</span> <span class="n">value_2</span>
</pre>
                                                        </div>
                                                    </div>
                                                    <p class="last">to get the same result.</p>
                                                </div>
                                            </div>
                                            <div class="section" id="launching-multiple-experiments-at-once">
                                                <h4><a class="toc-backref" href="#id7">Launching Multiple Experiments at
                                                        Once</a><a class="headerlink"
                                                        href="#launching-multiple-experiments-at-once"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>You can launch multiple experiments, to be executed <strong>in
                                                        series</strong>, by simply providing more than one value for a
                                                    given argument. (An experiment for each possible combination of
                                                    values will be launched.)</p>
                                                <p>For example, to launch otherwise-equivalent runs with different
                                                    random seeds (0, 10, and 20), do:</p>
                                                <div class="highlight-default">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spinup</span><span class="o">.</span><span class="n">run</span> <span class="n">ppo</span> <span class="o">--</span><span class="n">env</span> <span class="n">Walker2d</span><span class="o">-</span><span class="n">v2</span> <span class="o">--</span><span class="n">exp_name</span> <span class="n">walker</span> <span class="o">--</span><span class="n">seed</span> <span class="mi">0</span> <span class="mi">10</span> <span class="mi">20</span>
</pre>
                                                    </div>
                                                </div>
                                                <p>Experiments don&#8217;t launch in parallel because they soak up
                                                    enough resources that executing several at the same time
                                                    wouldn&#8217;t get a speedup.</p>
                                            </div>
                                            <div class="section" id="special-flags">
                                                <h4><a class="toc-backref" href="#id8">Special Flags</a><a
                                                        class="headerlink" href="#special-flags"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>A few flags receive special treatment.</p>
                                                <div class="section" id="environment-flag">
                                                    <h5><a class="toc-backref" href="#id9">Environment Flag</a><a
                                                            class="headerlink" href="#environment-flag"
                                                            title="Permalink to this headline">¶</a></h5>
                                                    <dl class="option">
                                                        <dt id="cmdoption-env">
                                                            <code class="descname">--env</code><code
                                                                class="descclassname"></code><code
                                                                class="descclassname">, </code><code
                                                                class="descname">--env_name</code><code
                                                                class="descclassname"></code><a class="headerlink"
                                                                href="#cmdoption-env"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p><em>string</em>. The name of an environment in the OpenAI
                                                                Gym. All Spinning Up algorithms are implemented as
                                                                functions that accept <code
                                                                    class="docutils literal"><span class="pre">env_fn</span></code>
                                                                as an argument, where <code
                                                                    class="docutils literal"><span class="pre">env_fn</span></code>
                                                                must be a callable function that builds a copy of the RL
                                                                environment. Since the most common use case is Gym
                                                                environments, though, all of which are built through
                                                                <code
                                                                    class="docutils literal"><span class="pre">gym.make(env_name)</span></code>,
                                                                we allow you to just specify <code
                                                                    class="docutils literal"><span class="pre">env_name</span></code>
                                                                (or <code
                                                                    class="docutils literal"><span class="pre">env</span></code>
                                                                for short) at the command line, which gets converted to
                                                                a lambda-function that builds the correct gym
                                                                environment.</p>
                                                        </dd>
                                                    </dl>

                                                </div>
                                                <div class="section" id="shortcut-flags">
                                                    <h5><a class="toc-backref" href="#id10">Shortcut Flags</a><a
                                                            class="headerlink" href="#shortcut-flags"
                                                            title="Permalink to this headline">¶</a></h5>
                                                    <p>Some algorithm arguments are relatively long, and we enabled
                                                        shortcuts for them:</p>
                                                    <dl class="option">
                                                        <dt id="cmdoption-hid">
                                                            <code class="descname">--hid</code><code
                                                                class="descclassname"></code><code
                                                                class="descclassname">, </code><code
                                                                class="descname">--ac_kwargs</code><code
                                                                class="descclassname">:hidden_sizes</code><a
                                                                class="headerlink" href="#cmdoption-hid"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p><em>list of ints</em>. Sets the sizes of the hidden
                                                                layers in the neural networks (policies and value
                                                                functions).</p>
                                                        </dd>
                                                    </dl>

                                                    <dl class="option">
                                                        <dt id="cmdoption-act">
                                                            <code class="descname">--act</code><code
                                                                class="descclassname"></code><code
                                                                class="descclassname">, </code><code
                                                                class="descname">--ac_kwargs</code><code
                                                                class="descclassname">:activation</code><a
                                                                class="headerlink" href="#cmdoption-act"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p><em>tf op</em>. The activation function for the neural
                                                                networks in the actor and critic.</p>
                                                        </dd>
                                                    </dl>

                                                    <p>These flags are valid for all current Spinning Up algorithms.</p>
                                                </div>
                                                <div class="section" id="config-flags">
                                                    <h5><a class="toc-backref" href="#id11">Config Flags</a><a
                                                            class="headerlink" href="#config-flags"
                                                            title="Permalink to this headline">¶</a></h5>
                                                    <p>These flags are not hyperparameters of any algorithm, but change
                                                        the experimental configuration in some way.</p>
                                                    <dl class="option">
                                                        <dt id="cmdoption-cpu">
                                                            <code class="descname">--cpu</code><code
                                                                class="descclassname"></code><code
                                                                class="descclassname">, </code><code
                                                                class="descname">--num_cpu</code><code
                                                                class="descclassname"></code><a class="headerlink"
                                                                href="#cmdoption-cpu"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p><em>int</em>. If this flag is set, the experiment is
                                                                launched with this many processes, one per cpu,
                                                                connected by MPI. Some algorithms are amenable to this
                                                                sort of parallelization but not all. An error will be
                                                                raised if you try setting <code
                                                                    class="docutils literal"><span class="pre">num_cpu</span></code>
                                                                &gt; 1 for an incompatible algorithm. You can also set
                                                                <code
                                                                    class="docutils literal"><span class="pre">--num_cpu</span> <span class="pre">auto</span></code>,
                                                                which will automatically use as many CPUs as are
                                                                available on the machine.</p>
                                                        </dd>
                                                    </dl>

                                                    <dl class="option">
                                                        <dt id="cmdoption-exp-name">
                                                            <code class="descname">--exp_name</code><code
                                                                class="descclassname"></code><a class="headerlink"
                                                                href="#cmdoption-exp-name"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p><em>string</em>. The experiment name. This is used in
                                                                naming the save directory for each experiment. The
                                                                default is &#8220;cmd&#8221; + [algo name].</p>
                                                        </dd>
                                                    </dl>

                                                    <dl class="option">
                                                        <dt id="cmdoption-data-dir">
                                                            <code class="descname">--data_dir</code><code
                                                                class="descclassname"></code><a class="headerlink"
                                                                href="#cmdoption-data-dir"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p><em>path</em>. Set the base save directory for this
                                                                experiment or set of experiments. If none is given, the
                                                                <code
                                                                    class="docutils literal"><span class="pre">DEFAULT_DATA_DIR</span></code>
                                                                in <code
                                                                    class="docutils literal"><span class="pre">spinup/user_config.py</span></code>
                                                                will be used.</p>
                                                        </dd>
                                                    </dl>

                                                    <dl class="option">
                                                        <dt id="cmdoption-datestamp">
                                                            <code class="descname">--datestamp</code><code
                                                                class="descclassname"></code><a class="headerlink"
                                                                href="#cmdoption-datestamp"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p><em>bool</em>. Include date and time in the name for the
                                                                save directory of the experiment.</p>
                                                        </dd>
                                                    </dl>

                                                </div>
                                            </div>
                                            <div class="section" id="where-results-are-saved">
                                                <h4><a class="toc-backref" href="#id12">Where Results are Saved</a><a
                                                        class="headerlink" href="#where-results-are-saved"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>Results for a particular experiment (a single run of a configuration
                                                    of hyperparameters) are stored in</p>
                                                <div class="highlight-default">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">data_dir</span><span class="o">/</span><span class="p">[</span><span class="n">outer_prefix</span><span class="p">]</span><span class="n">exp_name</span><span class="p">[</span><span class="n">suffix</span><span class="p">]</span><span class="o">/</span><span class="p">[</span><span class="n">inner_prefix</span><span class="p">]</span><span class="n">exp_name</span><span class="p">[</span><span class="n">suffix</span><span class="p">]</span><span class="n">_s</span><span class="p">[</span><span class="n">seed</span><span class="p">]</span>
</pre>
                                                    </div>
                                                </div>
                                                <p>where</p>
                                                <ul class="simple">
                                                    <li><code
                                                            class="docutils literal"><span class="pre">data_dir</span></code>
                                                        is the value of the <code
                                                            class="docutils literal"><span class="pre">--data_dir</span></code>
                                                        flag (defaults to <code
                                                            class="docutils literal"><span class="pre">DEFAULT_DATA_DIR</span></code>
                                                        from <code
                                                            class="docutils literal"><span class="pre">spinup/user_config.py</span></code>
                                                        if <code
                                                            class="docutils literal"><span class="pre">--data_dir</span></code>
                                                        is not given),</li>
                                                    <li>the <code
                                                            class="docutils literal"><span class="pre">outer_prefix</span></code>
                                                        is a <code
                                                            class="docutils literal"><span class="pre">YY-MM-DD_</span></code>
                                                        timestamp if the <code
                                                            class="docutils literal"><span class="pre">--datestamp</span></code>
                                                        flag is raised, otherwise nothing,</li>
                                                    <li>the <code
                                                            class="docutils literal"><span class="pre">inner_prefix</span></code>
                                                        is a <code
                                                            class="docutils literal"><span class="pre">YY-MM-DD_HH-MM-SS-</span></code>
                                                        timestamp if the <code
                                                            class="docutils literal"><span class="pre">--datestamp</span></code>
                                                        flag is raised, otherwise nothing,</li>
                                                    <li>and <code
                                                            class="docutils literal"><span class="pre">suffix</span></code>
                                                        is a special string based on the experiment hyperparameters.
                                                    </li>
                                                </ul>
                                                <div class="section" id="how-is-suffix-determined">
                                                    <h5><a class="toc-backref" href="#id13">How is Suffix
                                                            Determined?</a><a class="headerlink"
                                                            href="#how-is-suffix-determined"
                                                            title="Permalink to this headline">¶</a></h5>
                                                    <p>Suffixes are only included if you run multiple experiments at
                                                        once, and they only include references to hyperparameters that
                                                        differ across experiments, except for random seed. The goal is
                                                        to make sure that results for similar experiments (ones which
                                                        share all params except seed) are grouped in the same folder.
                                                    </p>
                                                    <p>Suffixes are constructed by combining <em>shorthands</em> for
                                                        hyperparameters with their values, where a shorthand is either
                                                        1) constructed automatically from the hyperparameter name or 2)
                                                        supplied by the user. The user can supply a shorthand by writing
                                                        in square brackets after the kwarg flag.</p>
                                                    <p>For example, consider:</p>
                                                    <div class="highlight-default">
                                                        <div class="highlight">
                                                            <pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spinup</span><span class="o">.</span><span class="n">run</span> <span class="n">ddpg_tf1</span> <span class="o">--</span><span class="n">env</span> <span class="n">Hopper</span><span class="o">-</span><span class="n">v2</span> <span class="o">--</span><span class="n">hid</span><span class="p">[</span><span class="n">h</span><span class="p">]</span> <span class="p">[</span><span class="mi">300</span><span class="p">]</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span><span class="mi">128</span><span class="p">]</span> <span class="o">--</span><span class="n">act</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span>
</pre>
                                                        </div>
                                                    </div>
                                                    <p>Here, the <code
                                                            class="docutils literal"><span class="pre">--hid</span></code>
                                                        flag is given a <strong>user-supplied shorthand</strong>, <code
                                                            class="docutils literal"><span class="pre">h</span></code>.
                                                        The <code
                                                            class="docutils literal"><span class="pre">--act</span></code>
                                                        flag is not given a shorthand by the user, so one will be
                                                        constructed for it automatically.</p>
                                                    <p>The suffixes produced in this case are:</p>
                                                    <div class="highlight-default">
                                                        <div class="highlight">
                                                            <pre><span></span><span class="n">_h128</span><span class="o">-</span><span class="mi">128</span><span class="n">_ac</span><span class="o">-</span><span class="n">actrelu</span>
<span class="n">_h128</span><span class="o">-</span><span class="mi">128</span><span class="n">_ac</span><span class="o">-</span><span class="n">acttanh</span>
<span class="n">_h300_ac</span><span class="o">-</span><span class="n">actrelu</span>
<span class="n">_h300_ac</span><span class="o">-</span><span class="n">acttanh</span>
</pre>
                                                        </div>
                                                    </div>
                                                    <p>Note that the <code
                                                            class="docutils literal"><span class="pre">h</span></code>
                                                        was given by the user. the <code
                                                            class="docutils literal"><span class="pre">ac-act</span></code>
                                                        shorthand was constructed from <code
                                                            class="docutils literal"><span class="pre">ac_kwargs:activation</span></code>
                                                        (the true name for the <code
                                                            class="docutils literal"><span class="pre">act</span></code>
                                                        flag).</p>
                                                </div>
                                            </div>
                                            <div class="section" id="extra">
                                                <h4><a class="toc-backref" href="#id14">Extra</a><a class="headerlink"
                                                        href="#extra" title="Permalink to this headline">¶</a></h4>
                                                <div
                                                    class="admonition-you-don-t-actually-need-to-know-this-one admonition">
                                                    <p class="first admonition-title">You Don&#8217;t Actually Need to
                                                        Know This One</p>
                                                    <p>Each individual algorithm is located in a file <code
                                                            class="docutils literal"><span class="pre">spinup/algos/BACKEND/ALGO_NAME/ALGO_NAME.py</span></code>,
                                                        and these files can be run directly from the command line with a
                                                        limited set of arguments (some of which differ from what&#8217;s
                                                        available to <code
                                                            class="docutils literal"><span class="pre">spinup/run.py</span></code>).
                                                        The command line support in the individual algorithm files is
                                                        essentially vestigial, however, and this is <strong>not</strong>
                                                        a recommended way to perform experiments.</p>
                                                    <p class="last">This documentation page will not describe those
                                                        command line calls, and will <em>only</em> describe calls
                                                        through <code
                                                            class="docutils literal"><span class="pre">spinup/run.py</span></code>.
                                                    </p>
                                                </div>
                                            </div>
                                        </div>
                                        <div class="section" id="launching-from-scripts">
                                            <h3><a class="toc-backref" href="#id15">Launching from Scripts</a><a
                                                    class="headerlink" href="#launching-from-scripts"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>Each algorithm is implemented as a python function, which can be imported
                                                directly from the <code
                                                    class="docutils literal"><span class="pre">spinup</span></code>
                                                package, eg</p>
                                            <div class="highlight-default">
                                                <div class="highlight">
                                                    <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">spinup</span> <span class="k">import</span> <span class="n">ppo_pytorch</span> <span class="k">as</span> <span class="n">ppo</span>
</pre>
                                                </div>
                                            </div>
                                            <p>See the documentation page for each algorithm for a complete account of
                                                possible arguments. These methods can be used to set up specialized
                                                custom experiments, for example:</p>
                                            <div class="highlight-python">
                                                <div class="highlight">
                                                    <pre><span></span><span class="kn">from</span> <span class="nn">spinup</span> <span class="kn">import</span> <span class="n">ppo_tf1</span> <span class="k">as</span> <span class="n">ppo</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">gym</span>

<span class="n">env_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;LunarLander-v2&#39;</span><span class="p">)</span>

<span class="n">ac_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span><span class="mi">64</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>

<span class="n">logger_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">output_dir</span><span class="o">=</span><span class="s1">&#39;path/to/output_dir&#39;</span><span class="p">,</span> <span class="n">exp_name</span><span class="o">=</span><span class="s1">&#39;experiment_name&#39;</span><span class="p">)</span>

<span class="n">ppo</span><span class="p">(</span><span class="n">env_fn</span><span class="o">=</span><span class="n">env_fn</span><span class="p">,</span> <span class="n">ac_kwargs</span><span class="o">=</span><span class="n">ac_kwargs</span><span class="p">,</span> <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> <span class="n">logger_kwargs</span><span class="o">=</span><span class="n">logger_kwargs</span><span class="p">)</span>
</pre>
                                                </div>
                                            </div>
                                            <div class="section" id="using-experimentgrid">
                                                <h4><a class="toc-backref" href="#id16">Using ExperimentGrid</a><a
                                                        class="headerlink" href="#using-experimentgrid"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>It&#8217;s often useful in machine learning research to run the same
                                                    algorithm with many possible hyperparameters. Spinning Up ships with
                                                    a simple tool for facilitating this, called <a
                                                        class="reference external"
                                                        href="../utils/run_utils.html#experimentgrid">ExperimentGrid</a>.
                                                </p>
                                                <p>Consider the example in <code
                                                        class="docutils literal"><span class="pre">spinup/examples/pytorch/bench_ppo_cartpole.py</span></code>:
                                                </p>
                                                <div class="highlight-python">
                                                    <table class="highlighttable">
                                                        <tr>
                                                            <td class="linenos">
                                                                <div class="linenodiv">
                                                                    <pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19</pre>
                                                                </div>
                                                            </td>
                                                            <td class="code">
                                                                <div class="highlight">
                                                                    <pre><span></span> <span class="kn">from</span> <span class="nn">spinup.utils.run_utils</span> <span class="kn">import</span> <span class="n">ExperimentGrid</span>
 <span class="kn">from</span> <span class="nn">spinup</span> <span class="kn">import</span> <span class="n">ppo_pytorch</span>
 <span class="kn">import</span> <span class="nn">torch</span>

 <span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
     <span class="kn">import</span> <span class="nn">argparse</span>
     <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
     <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--cpu&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
     <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_runs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
     <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

     <span class="n">eg</span> <span class="o">=</span> <span class="n">ExperimentGrid</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;ppo-pyt-bench&#39;</span><span class="p">)</span>
     <span class="n">eg</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s1">&#39;env_name&#39;</span><span class="p">,</span> <span class="s1">&#39;CartPole-v0&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
     <span class="n">eg</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s1">&#39;seed&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="o">*</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_runs</span><span class="p">)])</span>
     <span class="n">eg</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s1">&#39;epochs&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
     <span class="n">eg</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s1">&#39;steps_per_epoch&#39;</span><span class="p">,</span> <span class="mi">4000</span><span class="p">)</span>
     <span class="n">eg</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s1">&#39;ac_kwargs:hidden_sizes&#39;</span><span class="p">,</span> <span class="p">[(</span><span class="mi">32</span><span class="p">,),</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="mi">64</span><span class="p">)],</span> <span class="s1">&#39;hid&#39;</span><span class="p">)</span>
     <span class="n">eg</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s1">&#39;ac_kwargs:activation&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">],</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
     <span class="n">eg</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">ppo_pytorch</span><span class="p">,</span> <span class="n">num_cpu</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">cpu</span><span class="p">)</span>
</pre>
                                                                </div>
                                                            </td>
                                                        </tr>
                                                    </table>
                                                </div>
                                                <p>(An equivalent Tensorflow example is available in <code
                                                        class="docutils literal"><span class="pre">spinup/examples/tf1/bench_ppo_cartpole.py</span></code>.)
                                                </p>
                                                <p>After making the ExperimentGrid object, parameters are added to it
                                                    with</p>
                                                <div class="highlight-default">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">eg</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">shorthand</span><span class="p">,</span> <span class="n">in_name</span><span class="p">)</span>
</pre>
                                                    </div>
                                                </div>
                                                <p>where <code
                                                        class="docutils literal"><span class="pre">in_name</span></code>
                                                    forces a parameter to appear in the experiment name, even if it has
                                                    the same value across all experiments.</p>
                                                <p>After all parameters have been added,</p>
                                                <div class="highlight-default">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">eg</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">thunk</span><span class="p">,</span> <span class="o">**</span><span class="n">run_kwargs</span><span class="p">)</span>
</pre>
                                                    </div>
                                                </div>
                                                <p>runs all experiments in the grid (one experiment per valid
                                                    configuration), by providing the configurations as kwargs to the
                                                    function <code
                                                        class="docutils literal"><span class="pre">thunk</span></code>.
                                                    <code
                                                        class="docutils literal"><span class="pre">ExperimentGrid.run</span></code>
                                                    uses a function named <a class="reference external"
                                                        href="../utils/run_utils.html#spinup.utils.run_utils.call_experiment">call_experiment</a>
                                                    to launch <code
                                                        class="docutils literal"><span class="pre">thunk</span></code>,
                                                    and <code
                                                        class="docutils literal"><span class="pre">**run_kwargs</span></code>
                                                    specify behaviors for <code
                                                        class="docutils literal"><span class="pre">call_experiment</span></code>.
                                                    See <a class="reference external"
                                                        href="../utils/run_utils.html#experimentgrid">the documentation
                                                        page</a> for details.</p>
                                                <p>Except for the absence of shortcut kwargs (you can&#8217;t use <code
                                                        class="docutils literal"><span class="pre">hid</span></code> for
                                                    <code
                                                        class="docutils literal"><span class="pre">ac_kwargs:hidden_sizes</span></code>
                                                    in <code
                                                        class="docutils literal"><span class="pre">ExperimentGrid</span></code>),
                                                    the basic behavior of <code
                                                        class="docutils literal"><span class="pre">ExperimentGrid</span></code>
                                                    is the same as running things from the command line. (In fact, <code
                                                        class="docutils literal"><span class="pre">spinup.run</span></code>
                                                    uses an <code
                                                        class="docutils literal"><span class="pre">ExperimentGrid</span></code>
                                                    under the hood.)</p>
                                            </div>
                                        </div>
                                    </div>
                                    <span id="document-user/saving_and_loading"></span>
                                    <div class="section" id="experiment-outputs">
                                        <h2><a class="toc-backref" href="#id1">Experiment Outputs</a><a
                                                class="headerlink" href="#experiment-outputs"
                                                title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal" href="#experiment-outputs"
                                                        id="id1">Experiment Outputs</a>
                                                    <ul>
                                                        <li><a class="reference internal" href="#algorithm-outputs"
                                                                id="id2">Algorithm Outputs</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#pytorch-save-directory-info"
                                                                        id="id3">PyTorch Save Directory Info</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#tensorflow-save-directory-info"
                                                                        id="id4">Tensorflow Save Directory Info</a></li>
                                                            </ul>
                                                        </li>
                                                        <li><a class="reference internal"
                                                                href="#save-directory-location" id="id5">Save Directory
                                                                Location</a></li>
                                                        <li><a class="reference internal"
                                                                href="#loading-and-running-trained-policies"
                                                                id="id6">Loading and Running Trained Policies</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#if-environment-saves-successfully"
                                                                        id="id7">If Environment Saves Successfully</a>
                                                                </li>
                                                                <li><a class="reference internal"
                                                                        href="#environment-not-found-error"
                                                                        id="id8">Environment Not Found Error</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#using-trained-value-functions"
                                                                        id="id9">Using Trained Value Functions</a></li>
                                                            </ul>
                                                        </li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <p>In this section we&#8217;ll cover</p>
                                        <ul class="simple">
                                            <li>what outputs come from Spinning Up algorithm implementations,</li>
                                            <li>what formats they&#8217;re stored in and how they&#8217;re organized,
                                            </li>
                                            <li>where they are stored and how you can change that,</li>
                                            <li>and how to load and run trained policies.</li>
                                        </ul>
                                        <div class="admonition-you-should-know admonition">
                                            <p class="first admonition-title">You Should Know</p>
                                            <p class="last">Spinning Up implementations currently have no way to resume
                                                training for partially-trained agents. If you consider this feature
                                                important, please let us know&#8212;or consider it a hacking project!
                                            </p>
                                        </div>
                                        <div class="section" id="algorithm-outputs">
                                            <h3><a class="toc-backref" href="#id2">Algorithm Outputs</a><a
                                                    class="headerlink" href="#algorithm-outputs"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>Each algorithm is set up to save a training run&#8217;s hyperparameter
                                                configuration, learning progress, trained agent and value functions, and
                                                a copy of the environment if possible (to make it easy to load up the
                                                agent and environment simultaneously). The output directory contains the
                                                following:</p>
                                            <table border="1" class="docutils">
                                                <colgroup>
                                                    <col width="20%" />
                                                    <col width="80%" />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr class="row-odd">
                                                        <td colspan="2"><strong>Output Directory Structure</strong></td>
                                                    </tr>
                                                    <tr class="row-even">
                                                        <td><code
                                                                class="docutils literal"><span class="pre">pyt_save/</span></code>
                                                        </td>
                                                        <td>
                                                            <div class="first last line-block">
                                                                <div class="line"><strong>PyTorch implementations
                                                                        only.</strong> A directory containing</div>
                                                                <div class="line">everything needed to restore the
                                                                    trained agent and value</div>
                                                                <div class="line">functions. (<a
                                                                        class="reference internal"
                                                                        href="#details-for-pytorch-saves-below">Details
                                                                        for PyTorch saves below.</a>)</div>
                                                            </div>
                                                        </td>
                                                    </tr>
                                                    <tr class="row-odd">
                                                        <td><code
                                                                class="docutils literal"><span class="pre">tf1_save/</span></code>
                                                        </td>
                                                        <td>
                                                            <div class="first last line-block">
                                                                <div class="line"><strong>Tensorflow implementations
                                                                        only.</strong> A directory containing</div>
                                                                <div class="line">everything needed to restore the
                                                                    trained agent and value</div>
                                                                <div class="line">functions. (<a
                                                                        class="reference internal"
                                                                        href="#details-for-tensorflow-saves-below">Details
                                                                        for Tensorflow saves below.</a>)</div>
                                                            </div>
                                                        </td>
                                                    </tr>
                                                    <tr class="row-even">
                                                        <td><code
                                                                class="docutils literal"><span class="pre">config.json</span></code>
                                                        </td>
                                                        <td>
                                                            <div class="first last line-block">
                                                                <div class="line">A dict containing an
                                                                    as-complete-as-possible description</div>
                                                                <div class="line">of the args and kwargs you used to
                                                                    launch the training</div>
                                                                <div class="line">function. If you passed in something
                                                                    which can&#8217;t be</div>
                                                                <div class="line">serialized to JSON, it should get
                                                                    handled gracefully by the</div>
                                                                <div class="line">logger, and the config file will
                                                                    represent it with a string.</div>
                                                                <div class="line">Note: this is meant for record-keeping
                                                                    only. Launching an</div>
                                                                <div class="line">experiment from a config file is not
                                                                    currently supported.</div>
                                                            </div>
                                                        </td>
                                                    </tr>
                                                    <tr class="row-odd">
                                                        <td><code
                                                                class="docutils literal"><span class="pre">progress.txt</span></code>
                                                        </td>
                                                        <td>
                                                            <div class="first last line-block">
                                                                <div class="line">A tab-separated value file containing
                                                                    records of the metrics</div>
                                                                <div class="line">recorded by the logger throughout
                                                                    training. eg, <code
                                                                        class="docutils literal"><span class="pre">Epoch</span></code>,
                                                                </div>
                                                                <div class="line"><code
                                                                        class="docutils literal"><span class="pre">AverageEpRet</span></code>,
                                                                    etc.</div>
                                                            </div>
                                                        </td>
                                                    </tr>
                                                    <tr class="row-even">
                                                        <td><code
                                                                class="docutils literal"><span class="pre">vars.pkl</span></code>
                                                        </td>
                                                        <td>
                                                            <div class="first last line-block">
                                                                <div class="line">A pickle file containing anything
                                                                    about the algorithm state</div>
                                                                <div class="line">which should get stored. Currently,
                                                                    all algorithms only use</div>
                                                                <div class="line">this to save a copy of the
                                                                    environment.</div>
                                                            </div>
                                                        </td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p class="last">Sometimes environment-saving fails because the
                                                    environment can&#8217;t be pickled, and <code
                                                        class="docutils literal"><span class="pre">vars.pkl</span></code>
                                                    is empty. This is known to be a problem for Gym Box2D environments
                                                    in older versions of Gym, which can&#8217;t be saved in this manner.
                                                </p>
                                            </div>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p class="last">As of 1/30/20, the save directory structure has changed
                                                    slightly. Previously, Tensorflow graphs were saved in the <code
                                                        class="docutils literal"><span class="pre">simple_save/</span></code>
                                                    folder; this has been replaced with <code
                                                        class="docutils literal"><span class="pre">tf1_save/</span></code>.
                                                </p>
                                            </div>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p class="last">The only file in here that you should ever have to use
                                                    &#8220;by hand&#8221; is the <code
                                                        class="docutils literal"><span class="pre">config.json</span></code>
                                                    file. Our agent testing utility will load things from the <code
                                                        class="docutils literal"><span class="pre">tf1_save/</span></code>
                                                    or <code
                                                        class="docutils literal"><span class="pre">pyt_save/</span></code>
                                                    directory, and our plotter interprets the contents of <code
                                                        class="docutils literal"><span class="pre">progress.txt</span></code>,
                                                    and those are the correct tools for interfacing with these outputs.
                                                    But there is no tooling for <code
                                                        class="docutils literal"><span class="pre">config.json</span></code>&#8212;it&#8217;s
                                                    just there so that if you forget what hyperparameters you ran an
                                                    experiment with, you can double-check.</p>
                                            </div>
                                            <div class="section" id="pytorch-save-directory-info">
                                                <h4><a class="toc-backref" href="#id3">PyTorch Save Directory Info</a><a
                                                        class="headerlink" href="#pytorch-save-directory-info"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p id="details-for-pytorch-saves-below">The <code
                                                        class="docutils literal"><span class="pre">pyt_save</span></code>
                                                    directory contains:</p>
                                                <table border="1" class="docutils">
                                                    <colgroup>
                                                        <col width="22%" />
                                                        <col width="78%" />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr class="row-odd">
                                                            <td colspan="2"><strong>Pyt_Save Directory
                                                                    Structure</strong></td>
                                                        </tr>
                                                        <tr class="row-even">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">model.pt</span></code>
                                                            </td>
                                                            <td>
                                                                <div class="first last line-block">
                                                                    <div class="line">A file created with <code
                                                                            class="docutils literal"><span class="pre">torch.save</span></code>,
                                                                        essentially just a</div>
                                                                    <div class="line">pickled PyTorch <code
                                                                            class="docutils literal"><span class="pre">nn.Module</span></code>.
                                                                        Loading it will restore</div>
                                                                    <div class="line">a trained agent as an ActorCritic
                                                                        object with an <code
                                                                            class="docutils literal"><span class="pre">act</span></code>
                                                                    </div>
                                                                    <div class="line">method.</div>
                                                                </div>
                                                            </td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                            </div>
                                            <div class="section" id="tensorflow-save-directory-info">
                                                <h4><a class="toc-backref" href="#id4">Tensorflow Save Directory
                                                        Info</a><a class="headerlink"
                                                        href="#tensorflow-save-directory-info"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p id="details-for-tensorflow-saves-below">The <code
                                                        class="docutils literal"><span class="pre">tf1_save</span></code>
                                                    directory contains:</p>
                                                <table border="1" class="docutils">
                                                    <colgroup>
                                                        <col width="22%" />
                                                        <col width="78%" />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr class="row-odd">
                                                            <td colspan="2"><strong>TF1_Save Directory
                                                                    Structure</strong></td>
                                                        </tr>
                                                        <tr class="row-even">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">variables/</span></code>
                                                            </td>
                                                            <td>
                                                                <div class="first last line-block">
                                                                    <div class="line">A directory containing outputs
                                                                        from the Tensorflow Saver.</div>
                                                                    <div class="line">See documentation for <a
                                                                            class="reference external"
                                                                            href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md">Tensorflow
                                                                            SavedModel</a>.</div>
                                                                </div>
                                                            </td>
                                                        </tr>
                                                        <tr class="row-odd">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">model_info.pkl</span></code>
                                                            </td>
                                                            <td>
                                                                <div class="first last line-block">
                                                                    <div class="line">A dict containing information (map
                                                                        from key to tensor name)</div>
                                                                    <div class="line">which helps us unpack the saved
                                                                        model after loading.</div>
                                                                </div>
                                                            </td>
                                                        </tr>
                                                        <tr class="row-even">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">saved_model.pb</span></code>
                                                            </td>
                                                            <td>
                                                                <div class="first last line-block">
                                                                    <div class="line">A protocol buffer, needed for a <a
                                                                            class="reference external"
                                                                            href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md">Tensorflow
                                                                            SavedModel</a>.</div>
                                                                </div>
                                                            </td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                            </div>
                                        </div>
                                        <div class="section" id="save-directory-location">
                                            <h3><a class="toc-backref" href="#id5">Save Directory Location</a><a
                                                    class="headerlink" href="#save-directory-location"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>Experiment results will, by default, be saved in the same directory as
                                                the Spinning Up package, in a folder called <code
                                                    class="docutils literal"><span class="pre">data</span></code>:</p>
                                            <pre class="literal-block">
spinningup/
    <strong>data/</strong>
        ...
    docs/
        ...
    spinup/
        ...
    LICENSE
    setup.py
</pre>
                                            <p>You can change the default results directory by modifying <code
                                                    class="docutils literal"><span class="pre">DEFAULT_DATA_DIR</span></code>
                                                in <code
                                                    class="docutils literal"><span class="pre">spinup/user_config.py</span></code>.
                                            </p>
                                        </div>
                                        <div class="section" id="loading-and-running-trained-policies">
                                            <h3><a class="toc-backref" href="#id6">Loading and Running Trained
                                                    Policies</a><a class="headerlink"
                                                    href="#loading-and-running-trained-policies"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <div class="section" id="if-environment-saves-successfully">
                                                <h4><a class="toc-backref" href="#id7">If Environment Saves
                                                        Successfully</a><a class="headerlink"
                                                        href="#if-environment-saves-successfully"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>For cases where the environment is successfully saved alongside the
                                                    agent, it&#8217;s a cinch to watch the trained agent act in the
                                                    environment using:</p>
                                                <div class="highlight-default">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spinup</span><span class="o">.</span><span class="n">run</span> <span class="n">test_policy</span> <span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">output_directory</span>
</pre>
                                                    </div>
                                                </div>
                                                <p>There are a few flags for options:</p>
                                                <dl class="option">
                                                    <dt id="cmdoption-l">
                                                        <code class="descname">-l</code><code
                                                            class="descclassname"> L</code><code
                                                            class="descclassname">, </code><code
                                                            class="descname">--len</code><code
                                                            class="descclassname">=L</code><code
                                                            class="descclassname">, </code><code
                                                            class="descname">default</code><code
                                                            class="descclassname">=0</code><a class="headerlink"
                                                            href="#cmdoption-l"
                                                            title="Permalink to this definition">¶</a>
                                                    </dt>
                                                    <dd>
                                                        <p><em>int</em>. Maximum length of test episode / trajectory /
                                                            rollout. The default of 0 means no maximum episode
                                                            length&#8212;episodes only end when the agent has reached a
                                                            terminal state in the environment. (Note: setting L=0 will
                                                            not prevent Gym envs wrapped by TimeLimit wrappers from
                                                            ending when they reach their pre-set maximum episode
                                                            length.)</p>
                                                    </dd>
                                                </dl>

                                                <dl class="option">
                                                    <dt id="cmdoption-n">
                                                        <code class="descname">-n</code><code
                                                            class="descclassname"> N</code><code
                                                            class="descclassname">, </code><code
                                                            class="descname">--episodes</code><code
                                                            class="descclassname">=N</code><code
                                                            class="descclassname">, </code><code
                                                            class="descname">default</code><code
                                                            class="descclassname">=100</code><a class="headerlink"
                                                            href="#cmdoption-n"
                                                            title="Permalink to this definition">¶</a>
                                                    </dt>
                                                    <dd>
                                                        <p><em>int</em>. Number of test episodes to run the agent for.
                                                        </p>
                                                    </dd>
                                                </dl>

                                                <dl class="option">
                                                    <dt id="cmdoption-nr">
                                                        <code class="descname">-nr</code><code
                                                            class="descclassname"></code><code
                                                            class="descclassname">, </code><code
                                                            class="descname">--norender</code><code
                                                            class="descclassname"></code><a class="headerlink"
                                                            href="#cmdoption-nr"
                                                            title="Permalink to this definition">¶</a>
                                                    </dt>
                                                    <dd>
                                                        <p>Do not render the test episodes to the screen. In this case,
                                                            <code
                                                                class="docutils literal"><span class="pre">test_policy</span></code>
                                                            will only print the episode returns and lengths. (Use case:
                                                            the renderer slows down the testing process, and you just
                                                            want to get a fast sense of how the agent is performing, so
                                                            you don&#8217;t particularly care to watch it.)</p>
                                                    </dd>
                                                </dl>

                                                <dl class="option">
                                                    <dt id="cmdoption-i">
                                                        <code class="descname">-i</code><code
                                                            class="descclassname"> I</code><code
                                                            class="descclassname">, </code><code
                                                            class="descname">--itr</code><code
                                                            class="descclassname">=I</code><code
                                                            class="descclassname">, </code><code
                                                            class="descname">default</code><code
                                                            class="descclassname">=-1</code><a class="headerlink"
                                                            href="#cmdoption-i"
                                                            title="Permalink to this definition">¶</a>
                                                    </dt>
                                                    <dd>
                                                        <p><em>int</em>. This is an option for a special case which is
                                                            not supported by algorithms in this package as-shipped, but
                                                            which they are easily modified to do. Use case: Sometimes
                                                            it&#8217;s nice to watch trained agents from many different
                                                            points in training (eg watch at iteration 50, 100, 150,
                                                            etc.). The logger can do this&#8212;save snapshots of the
                                                            agent from those different points, so they can be run and
                                                            watched later. In this case, you use this flag to specify
                                                            which iteration to run. But again: spinup algorithms by
                                                            default only save snapshots of the most recent agent,
                                                            overwriting the old snapshots.</p>
                                                        <p>The default value of this flag means &#8220;use the latest
                                                            snapshot.&#8221;</p>
                                                        <p>To modify an algo so it does produce multiple snapshots, find
                                                            the following line (which is present in all of the
                                                            algorithms):</p>
                                                        <div class="highlight-python">
                                                            <div class="highlight">
                                                                <pre><span></span><span class="n">logger</span><span class="o">.</span><span class="n">save_state</span><span class="p">({</span><span class="s1">&#39;env&#39;</span><span class="p">:</span> <span class="n">env</span><span class="p">},</span> <span class="bp">None</span><span class="p">)</span>
</pre>
                                                            </div>
                                                        </div>
                                                        <p>and tweak it to</p>
                                                        <div class="highlight-python">
                                                            <div class="highlight">
                                                                <pre><span></span><span class="n">logger</span><span class="o">.</span><span class="n">save_state</span><span class="p">({</span><span class="s1">&#39;env&#39;</span><span class="p">:</span> <span class="n">env</span><span class="p">},</span> <span class="n">epoch</span><span class="p">)</span>
</pre>
                                                            </div>
                                                        </div>
                                                        <p>Make sure to then also set <code
                                                                class="docutils literal"><span class="pre">save_freq</span></code>
                                                            to something reasonable (because if it defaults to 1, for
                                                            instance, you&#8217;ll flood your output directory with one
                                                            <code
                                                                class="docutils literal"><span class="pre">save</span></code>
                                                            folder for each snapshot&#8212;which adds up fast).</p>
                                                    </dd>
                                                </dl>

                                                <dl class="option">
                                                    <dt id="cmdoption-d">
                                                        <code class="descname">-d</code><code
                                                            class="descclassname"></code><code
                                                            class="descclassname">, </code><code
                                                            class="descname">--deterministic</code><code
                                                            class="descclassname"></code><a class="headerlink"
                                                            href="#cmdoption-d"
                                                            title="Permalink to this definition">¶</a>
                                                    </dt>
                                                    <dd>
                                                        <p>Another special case, which is only used for SAC. The
                                                            Spinning Up SAC implementation trains a stochastic policy,
                                                            but is evaluated using the deterministic <em>mean</em> of
                                                            the action distribution. <code
                                                                class="docutils literal"><span class="pre">test_policy</span></code>
                                                            will default to using the stochastic policy trained by SAC,
                                                            but you should set the deterministic flag to watch the
                                                            deterministic mean policy (the correct evaluation policy for
                                                            SAC). This flag is not used for any other algorithms.</p>
                                                    </dd>
                                                </dl>

                                            </div>
                                            <div class="section" id="environment-not-found-error">
                                                <h4><a class="toc-backref" href="#id8">Environment Not Found Error</a><a
                                                        class="headerlink" href="#environment-not-found-error"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>If the environment wasn&#8217;t saved successfully, you can expect
                                                    <code
                                                        class="docutils literal"><span class="pre">test_policy.py</span></code>
                                                    to crash with something that looks like</p>
                                                <pre class="literal-block">
Traceback (most recent call last):
  File &quot;spinup/utils/test_policy.py&quot;, line 153, in &lt;module&gt;
    run_policy(env, get_action, args.len, args.episodes, not(args.norender))
  File &quot;spinup/utils/test_policy.py&quot;, line 114, in run_policy
    &quot;and we can't run the agent in it. :( nn Check out the readthedocs &quot; +
AssertionError: Environment not found!

 It looks like the environment wasn't saved, and we can't run the agent in it. :(

 Check out the readthedocs page on Experiment Outputs for how to handle this situation.
</pre>
                                                <p>In this case, watching your agent perform is slightly more of a pain
                                                    but not impossible, as long as you can recreate your environment
                                                    easily. Try the following in IPython:</p>
                                                <div class="highlight-default">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">spinup.utils.test_policy</span> <span class="k">import</span> <span class="n">load_policy_and_env</span><span class="p">,</span> <span class="n">run_policy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">your_env</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span><span class="p">,</span> <span class="n">get_action</span> <span class="o">=</span> <span class="n">load_policy_and_env</span><span class="p">(</span><span class="s1">&#39;/path/to/output_directory&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">env</span> <span class="o">=</span> <span class="n">your_env</span><span class="o">.</span><span class="n">make</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">run_policy</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">get_action</span><span class="p">)</span>
<span class="go">Logging data to /tmp/experiments/1536150702/progress.txt</span>
<span class="go">Episode 0    EpRet -163.830      EpLen 93</span>
<span class="go">Episode 1    EpRet -346.164      EpLen 99</span>
<span class="gp">...</span>
</pre>
                                                    </div>
                                                </div>
                                            </div>
                                            <div class="section" id="using-trained-value-functions">
                                                <h4><a class="toc-backref" href="#id9">Using Trained Value
                                                        Functions</a><a class="headerlink"
                                                        href="#using-trained-value-functions"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>The <code
                                                        class="docutils literal"><span class="pre">test_policy.py</span></code>
                                                    tool doesn&#8217;t help you look at trained value functions, and if
                                                    you want to use those, you will have to do some digging by hand. For
                                                    the PyTorch case, load the saved model file with <code
                                                        class="docutils literal"><span class="pre">torch.load</span></code>
                                                    and check the documentation for each algorithm to see what modules
                                                    the ActorCritic object has. For the Tensorflow case, load the saved
                                                    computation graph with the <a class="reference external"
                                                        href="../utils/logger.html#spinup.utils.logx.restore_tf_graph">restore_tf_graph</a>
                                                    function, and check the documentation for each algorithm to see what
                                                    functions were saved.</p>
                                            </div>
                                        </div>
                                    </div>
                                    <span id="document-user/plotting"></span>
                                    <div class="section" id="plotting-results">
                                        <h2>Plotting Results<a class="headerlink" href="#plotting-results"
                                                title="Permalink to this headline">¶</a></h2>
                                        <p>Spinning Up ships with a simple plotting utility for interpreting results.
                                            Run it with:</p>
                                        <div class="highlight-default">
                                            <div class="highlight">
                                                <pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spinup</span><span class="o">.</span><span class="n">run</span> <span class="n">plot</span> <span class="p">[</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">output_directory</span> <span class="o">...</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">legend</span> <span class="p">[</span><span class="n">LEGEND</span> <span class="o">...</span><span class="p">]]</span>
    <span class="p">[</span><span class="o">--</span><span class="n">xaxis</span> <span class="n">XAXIS</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">value</span> <span class="p">[</span><span class="n">VALUE</span> <span class="o">...</span><span class="p">]]</span> <span class="p">[</span><span class="o">--</span><span class="n">count</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">smooth</span> <span class="n">S</span><span class="p">]</span>
    <span class="p">[</span><span class="o">--</span><span class="n">select</span> <span class="p">[</span><span class="n">SEL</span> <span class="o">...</span><span class="p">]]</span> <span class="p">[</span><span class="o">--</span><span class="n">exclude</span> <span class="p">[</span><span class="n">EXC</span> <span class="o">...</span><span class="p">]]</span>
</pre>
                                            </div>
                                        </div>
                                        <p><strong>Positional Arguments:</strong></p>
                                        <dl class="option">
                                            <dt id="cmdoption-arg-logdir">
                                                <code class="descname">logdir</code><code
                                                    class="descclassname"></code><a class="headerlink"
                                                    href="#cmdoption-arg-logdir"
                                                    title="Permalink to this definition">¶</a>
                                            </dt>
                                            <dd>
                                                <p><em>strings</em>. As many log directories (or prefixes to log
                                                    directories, which the plotter will autocomplete internally) as
                                                    you&#8217;d like to plot from. Logdirs will be searched recursively
                                                    for experiment outputs.</p>
                                                <div class="admonition-you-should-know admonition">
                                                    <p class="first admonition-title">You Should Know</p>
                                                    <p>The internal autocompleting is really handy! Suppose you have run
                                                        several experiments, with the aim of comparing performance
                                                        between different algorithms, resulting in a log directory
                                                        structure of:</p>
                                                    <div class="highlight-default">
                                                        <div class="highlight">
                                                            <pre><span></span><span class="n">data</span><span class="o">/</span>
    <span class="n">bench_algo1</span><span class="o">/</span>
        <span class="n">bench_algo1</span><span class="o">-</span><span class="n">seed0</span><span class="o">/</span>
        <span class="n">bench_algo1</span><span class="o">-</span><span class="n">seed10</span><span class="o">/</span>
    <span class="n">bench_algo2</span><span class="o">/</span>
        <span class="n">bench_algo2</span><span class="o">-</span><span class="n">seed0</span><span class="o">/</span>
        <span class="n">bench_algo2</span><span class="o">-</span><span class="n">seed10</span><span class="o">/</span>
</pre>
                                                        </div>
                                                    </div>
                                                    <p>You can easily produce a graph comparing algo1 and algo2 with:
                                                    </p>
                                                    <div class="highlight-default">
                                                        <div class="highlight">
                                                            <pre><span></span><span class="n">python</span> <span class="n">spinup</span><span class="o">/</span><span class="n">utils</span><span class="o">/</span><span class="n">plot</span><span class="o">.</span><span class="n">py</span> <span class="n">data</span><span class="o">/</span><span class="n">bench_algo</span>
</pre>
                                                        </div>
                                                    </div>
                                                    <p class="last">relying on the autocomplete to find both <code
                                                            class="docutils literal"><span class="pre">data/bench_algo1</span></code>
                                                        and <code
                                                            class="docutils literal"><span class="pre">data/bench_algo2</span></code>.
                                                    </p>
                                                </div>
                                            </dd>
                                        </dl>

                                        <p><strong>Optional Arguments:</strong></p>
                                        <dl class="option">
                                            <dt id="cmdoption-l">
                                                <code class="descname">-l</code><code class="descclassname"></code><code
                                                    class="descclassname">, </code><code
                                                    class="descname">--legend</code><code
                                                    class="descclassname">=[LEGEND ...]</code><a class="headerlink"
                                                    href="#cmdoption-l" title="Permalink to this definition">¶</a>
                                            </dt>
                                            <dd>
                                                <p><em>strings</em>. Optional way to specify legend for the plot. The
                                                    plotter legend will automatically use the <code
                                                        class="docutils literal"><span class="pre">exp_name</span></code>
                                                    from the <code
                                                        class="docutils literal"><span class="pre">config.json</span></code>
                                                    file, unless you tell it otherwise through this flag. This only
                                                    works if you provide a name for each directory that will get
                                                    plotted. (Note: this may not be the same as the number of logdir
                                                    args you provide! Recall that the plotter looks for autocompletes of
                                                    the logdir args: there may be more than one match for a given logdir
                                                    prefix, and you will need to provide a legend string for each one of
                                                    those matches&#8212;unless you have removed some of them as
                                                    candidates via selection or exclusion rules (below).)</p>
                                            </dd>
                                        </dl>

                                        <dl class="option">
                                            <dt id="cmdoption-x">
                                                <code class="descname">-x</code><code class="descclassname"></code><code
                                                    class="descclassname">, </code><code
                                                    class="descname">--xaxis</code><code
                                                    class="descclassname">=XAXIS</code><code
                                                    class="descclassname">, </code><code
                                                    class="descname">default</code><code
                                                    class="descclassname">='TotalEnvInteracts'</code><a
                                                    class="headerlink" href="#cmdoption-x"
                                                    title="Permalink to this definition">¶</a>
                                            </dt>
                                            <dd>
                                                <p><em>string</em>. Pick what column from data is used for the x-axis.
                                                </p>
                                            </dd>
                                        </dl>

                                        <dl class="option">
                                            <dt id="cmdoption-y">
                                                <code class="descname">-y</code><code class="descclassname"></code><code
                                                    class="descclassname">, </code><code
                                                    class="descname">--value</code><code
                                                    class="descclassname">=[VALUE ...]</code><code
                                                    class="descclassname">, </code><code
                                                    class="descname">default</code><code
                                                    class="descclassname">='Performance'</code><a class="headerlink"
                                                    href="#cmdoption-y" title="Permalink to this definition">¶</a>
                                            </dt>
                                            <dd>
                                                <p><em>strings</em>. Pick what columns from data to graph on the y-axis.
                                                    Submitting multiple values will produce multiple graphs. Defaults to
                                                    <code
                                                        class="docutils literal"><span class="pre">Performance</span></code>,
                                                    which is not an actual output of any algorithm. Instead, <code
                                                        class="docutils literal"><span class="pre">Performance</span></code>
                                                    refers to either <code
                                                        class="docutils literal"><span class="pre">AverageEpRet</span></code>,
                                                    the correct performance measure for the on-policy algorithms, or
                                                    <code
                                                        class="docutils literal"><span class="pre">AverageTestEpRet</span></code>,
                                                    the correct performance measure for the off-policy algorithms. The
                                                    plotter will automatically figure out which of <code
                                                        class="docutils literal"><span class="pre">AverageEpRet</span></code>
                                                    or <code
                                                        class="docutils literal"><span class="pre">AverageTestEpRet</span></code>
                                                    to report for each separate logdir.</p>
                                            </dd>
                                        </dl>

                                        <dl class="option">
                                            <dt id="cmdoption-count">
                                                <code class="descname">--count</code><code
                                                    class="descclassname"></code><a class="headerlink"
                                                    href="#cmdoption-count" title="Permalink to this definition">¶</a>
                                            </dt>
                                            <dd>
                                                <p>Optional flag. By default, the plotter shows y-values which are
                                                    averaged across all results that share an <code
                                                        class="docutils literal"><span class="pre">exp_name</span></code>,
                                                    which is typically a set of identical experiments that only vary in
                                                    random seed. But if you&#8217;d like to see all of those curves
                                                    separately, use the <code
                                                        class="docutils literal"><span class="pre">--count</span></code>
                                                    flag.</p>
                                            </dd>
                                        </dl>

                                        <dl class="option">
                                            <dt id="cmdoption-s">
                                                <code class="descname">-s</code><code class="descclassname"></code><code
                                                    class="descclassname">, </code><code
                                                    class="descname">--smooth</code><code
                                                    class="descclassname">=S</code><code
                                                    class="descclassname">, </code><code
                                                    class="descname">default</code><code
                                                    class="descclassname">=1</code><a class="headerlink"
                                                    href="#cmdoption-s" title="Permalink to this definition">¶</a>
                                            </dt>
                                            <dd>
                                                <p><em>int</em>. Smooth data by averaging it over a fixed window. This
                                                    parameter says how wide the averaging window will be.</p>
                                            </dd>
                                        </dl>

                                        <dl class="option">
                                            <dt id="cmdoption-select">
                                                <code class="descname">--select</code><code
                                                    class="descclassname">=[SEL ...]</code><a class="headerlink"
                                                    href="#cmdoption-select" title="Permalink to this definition">¶</a>
                                            </dt>
                                            <dd>
                                                <p><em>strings</em>. Optional selection rule: the plotter will only show
                                                    curves from logdirs that contain all of these substrings.</p>
                                            </dd>
                                        </dl>

                                        <dl class="option">
                                            <dt id="cmdoption-exclude">
                                                <code class="descname">--exclude</code><code
                                                    class="descclassname">=[EXC ...]</code><a class="headerlink"
                                                    href="#cmdoption-exclude" title="Permalink to this definition">¶</a>
                                            </dt>
                                            <dd>
                                                <p><em>strings</em>. Optional exclusion rule: plotter will only show
                                                    curves from logdirs that do not contain these substrings.</p>
                                            </dd>
                                        </dl>

                                    </div>
                                </div>
                                <div class="toctree-wrapper compound">
                                    <span id="document-spinningup/rl_intro"></span>
                                    <div class="section" id="part-1-key-concepts-in-rl">
                                        <h2><a class="toc-backref" href="#id2">Part 1: Key Concepts in RL</a><a
                                                class="headerlink" href="#part-1-key-concepts-in-rl"
                                                title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal" href="#part-1-key-concepts-in-rl"
                                                        id="id2">Part 1: Key Concepts in RL</a>
                                                    <ul>
                                                        <li><a class="reference internal" href="#what-can-rl-do"
                                                                id="id3">What Can RL Do?</a></li>
                                                        <li><a class="reference internal"
                                                                href="#key-concepts-and-terminology" id="id4">Key
                                                                Concepts and Terminology</a></li>
                                                        <li><a class="reference internal" href="#optional-formalism"
                                                                id="id5">(Optional) Formalism</a></li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <p>Welcome to our introduction to reinforcement learning! Here, we aim to
                                            acquaint you with</p>
                                        <ul class="simple">
                                            <li>the language and notation used to discuss the subject,</li>
                                            <li>a high-level explanation of what RL algorithms do (although we mostly
                                                avoid the question of <em>how</em> they do it),</li>
                                            <li>and a little bit of the core math that underlies the algorithms.</li>
                                        </ul>
                                        <p>In a nutshell, RL is the study of agents and how they learn by trial and
                                            error. It formalizes the idea that rewarding or punishing an agent for its
                                            behavior makes it more likely to repeat or forego that behavior in the
                                            future.</p>
                                        <div class="section" id="what-can-rl-do">
                                            <h3><a class="toc-backref" href="#id3">What Can RL Do?</a><a
                                                    class="headerlink" href="#what-can-rl-do"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>RL methods have recently enjoyed a wide variety of successes. For
                                                example, it&#8217;s been used to teach computers to control robots in
                                                simulation...</p>
                                            <video autoplay=""
                                                src="https://d4mucfpksywv.cloudfront.net/openai-baselines-ppo/knocked-over-stand-up.mp4"
                                                loop="" controls=""
                                                style="display: block; margin-left: auto; margin-right: auto; margin-bottom:1.5em; width: 100%; max-width: 720px; max-height: 80vh;">
                                            </video>
                                            <p>...and in the real world...</p>
                                            <div
                                                style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
                                                <iframe src="https://www.youtube.com/embed/jwSbzNHGflM?ecver=1"
                                                    frameborder="0" allowfullscreen
                                                    style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
                                            </div>
                                            <br />
                                            <p>It&#8217;s also famously been used to create breakthrough AIs for
                                                sophisticated strategy games, most notably <a class="reference external"
                                                    href="https://deepmind.com/research/alphago/">Go</a> and <a
                                                    class="reference external"
                                                    href="https://blog.openai.com/openai-five/">Dota</a>, taught
                                                computers to <a class="reference external"
                                                    href="https://deepmind.com/research/dqn/">play Atari games</a> from
                                                raw pixels, and trained simulated robots <a class="reference external"
                                                    href="https://blog.openai.com/deep-reinforcement-learning-from-human-preferences/">to
                                                    follow human instructions</a>.</p>
                                        </div>
                                        <div class="section" id="key-concepts-and-terminology">
                                            <h3><a class="toc-backref" href="#id4">Key Concepts and Terminology</a><a
                                                    class="headerlink" href="#key-concepts-and-terminology"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <div class="figure align-center" id="id1">
                                                <img alt="_images/rl_diagram_transparent_bg.png"
                                                    src="_images/rl_diagram_transparent_bg.png" />
                                                <p class="caption"><span class="caption-text">Agent-environment
                                                        interaction loop.</span></p>
                                            </div>
                                            <p>The main characters of RL are the <strong>agent</strong> and the
                                                <strong>environment</strong>. The environment is the world that the
                                                agent lives in and interacts with. At every step of interaction, the
                                                agent sees a (possibly partial) observation of the state of the world,
                                                and then decides on an action to take. The environment changes when the
                                                agent acts on it, but may also change on its own.</p>
                                            <p>The agent also perceives a <strong>reward</strong> signal from the
                                                environment, a number that tells it how good or bad the current world
                                                state is. The goal of the agent is to maximize its cumulative reward,
                                                called <strong>return</strong>. Reinforcement learning methods are ways
                                                that the agent can learn behaviors to achieve its goal.</p>
                                            <p>To talk more specifically what RL does, we need to introduce additional
                                                terminology. We need to talk about</p>
                                            <ul class="simple">
                                                <li>states and observations,</li>
                                                <li>action spaces,</li>
                                                <li>policies,</li>
                                                <li>trajectories,</li>
                                                <li>different formulations of return,</li>
                                                <li>the RL optimization problem,</li>
                                                <li>and value functions.</li>
                                            </ul>
                                            <div class="section" id="states-and-observations">
                                                <h4>States and Observations<a class="headerlink"
                                                        href="#states-and-observations"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>A <strong>state</strong> <img class="math"
                                                        src="_images/math/96ac51b9afe79581e48f2f3f0ad3faa0f4402cc7.svg"
                                                        alt="s" /> is a complete description of the state of the world.
                                                    There is no information about the world which is hidden from the
                                                    state. An <strong>observation</strong> <img class="math"
                                                        src="_images/math/ca2d5053d03bd8fd9f399e5afbb834202e2d2f2d.svg"
                                                        alt="o" /> is a partial description of a state, which may omit
                                                    information.</p>
                                                <p>In deep RL, we almost always represent states and observations by a
                                                    <a class="reference external"
                                                        href="https://en.wikipedia.org/wiki/Real_coordinate_space">real-valued
                                                        vector, matrix, or higher-order tensor</a>. For instance, a
                                                    visual observation could be represented by the RGB matrix of its
                                                    pixel values; the state of a robot might be represented by its joint
                                                    angles and velocities.</p>
                                                <p>When the agent is able to observe the complete state of the
                                                    environment, we say that the environment is <strong>fully
                                                        observed</strong>. When the agent can only see a partial
                                                    observation, we say that the environment is <strong>partially
                                                        observed</strong>.</p>
                                                <div class="admonition-you-should-know admonition">
                                                    <p class="first admonition-title">You Should Know</p>
                                                    <p>Reinforcement learning notation sometimes puts the symbol for
                                                        state, <img class="math"
                                                            src="_images/math/96ac51b9afe79581e48f2f3f0ad3faa0f4402cc7.svg"
                                                            alt="s" />, in places where it would be technically more
                                                        appropriate to write the symbol for observation, <img
                                                            class="math"
                                                            src="_images/math/ca2d5053d03bd8fd9f399e5afbb834202e2d2f2d.svg"
                                                            alt="o" />. Specifically, this happens when talking about
                                                        how the agent decides an action: we often signal in notation
                                                        that the action is conditioned on the state, when in practice,
                                                        the action is conditioned on the observation because the agent
                                                        does not have access to the state.</p>
                                                    <p class="last">In our guide, we&#8217;ll follow standard
                                                        conventions for notation, but it should be clear from context
                                                        which is meant. If something is unclear, though, please raise an
                                                        issue! Our goal is to teach, not to confuse.</p>
                                                </div>
                                            </div>
                                            <div class="section" id="action-spaces">
                                                <h4>Action Spaces<a class="headerlink" href="#action-spaces"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>Different environments allow different kinds of actions. The set of
                                                    all valid actions in a given environment is often called the
                                                    <strong>action space</strong>. Some environments, like Atari and Go,
                                                    have <strong>discrete action spaces</strong>, where only a finite
                                                    number of moves are available to the agent. Other environments, like
                                                    where the agent controls a robot in a physical world, have
                                                    <strong>continuous action spaces</strong>. In continuous spaces,
                                                    actions are real-valued vectors.</p>
                                                <p>This distinction has some quite-profound consequences for methods in
                                                    deep RL. Some families of algorithms can only be directly applied in
                                                    one case, and would have to be substantially reworked for the other.
                                                </p>
                                            </div>
                                            <div class="section" id="policies">
                                                <h4>Policies<a class="headerlink" href="#policies"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>A <strong>policy</strong> is a rule used by an agent to decide what
                                                    actions to take. It can be deterministic, in which case it is
                                                    usually denoted by <img class="math"
                                                        src="_images/math/123eb57279cfbea38a65e8e129bda64972fedc3d.svg"
                                                        alt="\mu" />:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/73fcacd255a221d20d5d9300acf86e4d3bf5ea1b.svg"
                                                            alt="a_t = \mu(s_t)," /></p>
                                                </div>
                                                <p>or it may be stochastic, in which case it is usually denoted by <img
                                                        class="math"
                                                        src="_images/math/1ae2bd722da01b3a89ffc139af2437c28364a966.svg"
                                                        alt="\pi" />:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/89757355805c4084ac93610e9581c060f2e61610.svg"
                                                            alt="a_t \sim \pi(\cdot | s_t)." /></p>
                                                </div>
                                                <p>Because the policy is essentially the agent&#8217;s brain, it&#8217;s
                                                    not uncommon to substitute the word &#8220;policy&#8221; for
                                                    &#8220;agent&#8221;, eg saying &#8220;The policy is trying to
                                                    maximize reward.&#8221;</p>
                                                <p>In deep RL, we deal with <strong>parameterized policies</strong>:
                                                    policies whose outputs are computable functions that depend on a set
                                                    of parameters (eg the weights and biases of a neural network) which
                                                    we can adjust to change the behavior via some optimization
                                                    algorithm.</p>
                                                <p>We often denote the parameters of such a policy by <img class="math"
                                                        src="_images/math/ce5edddd490112350f4bd555d9390e0e845f754a.svg"
                                                        alt="\theta" /> or <img class="math"
                                                        src="_images/math/3b22abcadf8773922f8db80011611bad8123a783.svg"
                                                        alt="\phi" />, and then write this as a subscript on the policy
                                                    symbol to highlight the connection:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/831f731859658682b2af7e217a76648697c9de46.svg"
                                                            alt="a_t &amp;= \mu_{\theta}(s_t) \\
a_t &amp;\sim \pi_{\theta}(\cdot | s_t)." /></p>
                                                </div>
                                                <div class="section" id="deterministic-policies">
                                                    <h5>Deterministic Policies<a class="headerlink"
                                                            href="#deterministic-policies"
                                                            title="Permalink to this headline">¶</a></h5>
                                                    <p><strong>Example: Deterministic Policies.</strong> Here is a code
                                                        snippet for building a simple deterministic policy for a
                                                        continuous action space in PyTorch, using the <code
                                                            class="docutils literal"><span class="pre">torch.nn</span></code>
                                                        package:</p>
                                                    <div class="highlight-python">
                                                        <div class="highlight">
                                                            <pre><span></span><span class="n">pi_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
              <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">obs_dim</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
              <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
              <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
              <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
              <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">act_dim</span><span class="p">)</span>
            <span class="p">)</span>
</pre>
                                                        </div>
                                                    </div>
                                                    <p>This builds a multi-layer perceptron (MLP) network with two
                                                        hidden layers of size 64 and <img class="math"
                                                            src="_images/math/c65796f3bb56c457e63ebc770e3d775cace08673.svg"
                                                            alt="\tanh" /> activation functions. If <code
                                                            class="docutils literal"><span class="pre">obs</span></code>
                                                        is a Numpy array containing a batch of observations, <code
                                                            class="docutils literal"><span class="pre">pi_net</span></code>
                                                        can be used to obtain a batch of actions as follows:</p>
                                                    <div class="highlight-python">
                                                        <div class="highlight">
                                                            <pre><span></span><span class="n">obs_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">actions</span> <span class="o">=</span> <span class="n">pi_net</span><span class="p">(</span><span class="n">obs_tensor</span><span class="p">)</span>
</pre>
                                                        </div>
                                                    </div>
                                                    <div class="admonition-you-should-know admonition">
                                                        <p class="first admonition-title">You Should Know</p>
                                                        <p class="last">Don&#8217;t worry about it if this neural
                                                            network stuff is unfamiliar to you&#8212;this tutorial will
                                                            focus on RL, and not on the neural network side of things.
                                                            So you can skip this example and come back to it later. But
                                                            we figured that if you already knew, it could be helpful.
                                                        </p>
                                                    </div>
                                                </div>
                                                <div class="section" id="stochastic-policies">
                                                    <h5>Stochastic Policies<a class="headerlink"
                                                            href="#stochastic-policies"
                                                            title="Permalink to this headline">¶</a></h5>
                                                    <p>The two most common kinds of stochastic policies in deep RL are
                                                        <strong>categorical policies</strong> and <strong>diagonal
                                                            Gaussian policies</strong>.</p>
                                                    <p><a class="reference external"
                                                            href="https://en.wikipedia.org/wiki/Categorical_distribution">Categorical</a>
                                                        policies can be used in discrete action spaces, while diagonal
                                                        <a class="reference external"
                                                            href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">Gaussian</a>
                                                        policies are used in continuous action spaces.</p>
                                                    <p>Two key computations are centrally important for using and
                                                        training stochastic policies:</p>
                                                    <ul class="simple">
                                                        <li>sampling actions from the policy,</li>
                                                        <li>and computing log likelihoods of particular actions, <img
                                                                class="math"
                                                                src="_images/math/cc2095cba170e09137c55cb4f1786955b3174336.svg"
                                                                alt="\log \pi_{\theta}(a|s)" />.</li>
                                                    </ul>
                                                    <p>In what follows, we&#8217;ll describe how to do these for both
                                                        categorical and diagonal Gaussian policies.</p>
                                                    <div class="admonition-categorical-policies admonition">
                                                        <p class="first admonition-title">Categorical Policies</p>
                                                        <p>A categorical policy is like a classifier over discrete
                                                            actions. You build the neural network for a categorical
                                                            policy the same way you would for a classifier: the input is
                                                            the observation, followed by some number of layers (possibly
                                                            convolutional or densely-connected, depending on the kind of
                                                            input), and then you have one final linear layer that gives
                                                            you logits for each action, followed by a <a
                                                                class="reference external"
                                                                href="https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax">softmax</a>
                                                            to convert the logits into probabilities.</p>
                                                        <p><strong>Sampling.</strong> Given the probabilities for each
                                                            action, frameworks like PyTorch and Tensorflow have built-in
                                                            tools for sampling. For example, see the documentation for
                                                            <a class="reference external"
                                                                href="https://pytorch.org/docs/stable/distributions.html#categorical">Categorical
                                                                distributions in PyTorch</a>, <a
                                                                class="reference external"
                                                                href="https://pytorch.org/docs/stable/torch.html#torch.multinomial">torch.multinomial</a>,
                                                            <a class="reference external"
                                                                href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/distributions/Categorical">tf.distributions.Categorical</a>,
                                                            or <a class="reference external"
                                                                href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/random/multinomial">tf.multinomial</a>.
                                                        </p>
                                                        <p><strong>Log-Likelihood.</strong> Denote the last layer of
                                                            probabilities as <img class="math"
                                                                src="_images/math/5364a8661022d60da78f14c9bd33124118719454.svg"
                                                                alt="P_{\theta}(s)" />. It is a vector with however many
                                                            entries as there are actions, so we can treat the actions as
                                                            indices for the vector. The log likelihood for an action
                                                            <img class="math"
                                                                src="_images/math/76a319586cd215c8f2075b938fc6f6e07c81714b.svg"
                                                                alt="a" /> can then be obtained by indexing into the
                                                            vector:</p>
                                                        <div class="last math">
                                                            <p><img src="_images/math/ab8f7f4aaa7f1a3d1039ebdee058f297ed712c5a.svg"
                                                                    alt="\log \pi_{\theta}(a|s) = \log \left[P_{\theta}(s)\right]_a." />
                                                            </p>
                                                        </div>
                                                    </div>
                                                    <div class="admonition-diagonal-gaussian-policies admonition">
                                                        <p class="first admonition-title">Diagonal Gaussian Policies</p>
                                                        <p>A multivariate Gaussian distribution (or multivariate normal
                                                            distribution, if you prefer) is described by a mean vector,
                                                            <img class="math"
                                                                src="_images/math/123eb57279cfbea38a65e8e129bda64972fedc3d.svg"
                                                                alt="\mu" />, and a covariance matrix, <img class="math"
                                                                src="_images/math/f03ec2afde0e994f47df68b273d86e3afbfdce80.svg"
                                                                alt="\Sigma" />. A diagonal Gaussian distribution is a
                                                            special case where the covariance matrix only has entries on
                                                            the diagonal. As a result, we can represent it by a vector.
                                                        </p>
                                                        <p>A diagonal Gaussian policy always has a neural network that
                                                            maps from observations to mean actions, <img class="math"
                                                                src="_images/math/6923cb2043e84ea05d3eddbb7436c60659243cb9.svg"
                                                                alt="\mu_{\theta}(s)" />. There are two different ways
                                                            that the covariance matrix is typically represented.</p>
                                                        <p><strong>The first way:</strong> There is a single vector of
                                                            log standard deviations, <img class="math"
                                                                src="_images/math/3276548e12065a40224719e967e02b1538d3c6b2.svg"
                                                                alt="\log \sigma" />, which is <strong>not</strong> a
                                                            function of state: the <img class="math"
                                                                src="_images/math/3276548e12065a40224719e967e02b1538d3c6b2.svg"
                                                                alt="\log \sigma" /> are standalone parameters. (You
                                                            Should Know: our implementations of VPG, TRPO, and PPO do it
                                                            this way.)</p>
                                                        <p><strong>The second way:</strong> There is a neural network
                                                            that maps from states to log standard deviations, <img
                                                                class="math"
                                                                src="_images/math/4015c2b584427ca2a76f50ed03b2c8d0b5b3b350.svg"
                                                                alt="\log \sigma_{\theta}(s)" />. It may optionally
                                                            share some layers with the mean network.</p>
                                                        <p>Note that in both cases we output log standard deviations
                                                            instead of standard deviations directly. This is because log
                                                            stds are free to take on any values in <img class="math"
                                                                src="_images/math/9954b39a284ca1aa0aed2dc3f769404cc4e9f397.svg"
                                                                alt="(-\infty, \infty)" />, while stds must be
                                                            nonnegative. It&#8217;s easier to train parameters if you
                                                            don&#8217;t have to enforce those kinds of constraints. The
                                                            standard deviations can be obtained immediately from the log
                                                            standard deviations by exponentiating them, so we do not
                                                            lose anything by representing them this way.</p>
                                                        <p><strong>Sampling.</strong> Given the mean action <img
                                                                class="math"
                                                                src="_images/math/6923cb2043e84ea05d3eddbb7436c60659243cb9.svg"
                                                                alt="\mu_{\theta}(s)" /> and standard deviation <img
                                                                class="math"
                                                                src="_images/math/cd6cc1a1e8ed7fc447a2ea0e59ad848707631c94.svg"
                                                                alt="\sigma_{\theta}(s)" />, and a vector <img
                                                                class="math"
                                                                src="_images/math/886f88801abbe687ef8480ddd980f4215d2aaa17.svg"
                                                                alt="z" /> of noise from a spherical Gaussian (<img
                                                                class="math"
                                                                src="_images/math/a5a922f10e8b343418b1600a9a1601183673d126.svg"
                                                                alt="z \sim \mathcal{N}(0, I)" />), an action sample can
                                                            be computed with</p>
                                                        <div class="math">
                                                            <p><img src="_images/math/b18a4163a861b1fc18c6a6824af3f5540d4e2468.svg"
                                                                    alt="a = \mu_{\theta}(s) + \sigma_{\theta}(s) \odot z," />
                                                            </p>
                                                        </div>
                                                        <p>where <img class="math"
                                                                src="_images/math/77c323f00609b53862181c31bf0d045c75b29440.svg"
                                                                alt="\odot" /> denotes the elementwise product of two
                                                            vectors. Standard frameworks have built-in ways to generate
                                                            the noise vectors, such as <a class="reference external"
                                                                href="https://pytorch.org/docs/stable/torch.html#torch.normal">torch.normal</a>
                                                            or <a class="reference external"
                                                                href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/random/normal">tf.random_normal</a>.
                                                            Alternatively, you can build distribution objects, eg
                                                            through <a class="reference external"
                                                                href="https://pytorch.org/docs/stable/distributions.html#normal">torch.distributions.Normal</a>
                                                            or <a class="reference external"
                                                                href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/distributions/Normal">tf.distributions.Normal</a>,
                                                            and use them to generate samples. (The advantage of the
                                                            latter approach is that those objects can also calculate
                                                            log-likelihoods for you.)</p>
                                                        <p><strong>Log-Likelihood.</strong> The log-likelihood of a <img
                                                                class="math"
                                                                src="_images/math/a29aa94bd66ac7a6bb3195233fd9a9df8575af86.svg"
                                                                alt="k" /> -dimensional action <img class="math"
                                                                src="_images/math/76a319586cd215c8f2075b938fc6f6e07c81714b.svg"
                                                                alt="a" />, for a diagonal Gaussian with mean <img
                                                                class="math"
                                                                src="_images/math/0b9672dcfd65483d710b61a359dcabea32dab1f6.svg"
                                                                alt="\mu = \mu_{\theta}(s)" /> and standard deviation
                                                            <img class="math"
                                                                src="_images/math/20acc318d574242ee023ecdb36f3651847016480.svg"
                                                                alt="\sigma = \sigma_{\theta}(s)" />, is given by</p>
                                                        <div class="last math">
                                                            <p><img src="_images/math/26f82323a4055444b30fa791238ec90913a12d7b.svg"
                                                                    alt="\log \pi_{\theta}(a|s) = -\frac{1}{2}\left(\sum_{i=1}^k \left(\frac{(a_i - \mu_i)^2}{\sigma_i^2} + 2 \log \sigma_i \right) + k \log 2\pi \right)." />
                                                            </p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                            <div class="section" id="trajectories">
                                                <h4>Trajectories<a class="headerlink" href="#trajectories"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>A trajectory <img class="math"
                                                        src="_images/math/67a5412645decf6424bdd97aed3e9e7601bd784f.svg"
                                                        alt="\tau" /> is a sequence of states and actions in the world,
                                                </p>
                                                <div class="math">
                                                    <p><img src="_images/math/8337d86159a1cd98dfcd0601993d7b6b2fbb54d9.svg"
                                                            alt="\tau = (s_0, a_0, s_1, a_1, ...)." /></p>
                                                </div>
                                                <p>The very first state of the world, <img class="math"
                                                        src="_images/math/bf047f4b5c542c1bfbaf4bf411919f5e1f7ecba8.svg"
                                                        alt="s_0" />, is randomly sampled from the <strong>start-state
                                                        distribution</strong>, sometimes denoted by <img class="math"
                                                        src="_images/math/2d44ad6f01d4e56266daa8e3b35bd4f298e25788.svg"
                                                        alt="\rho_0" />:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/eef23a6502b9cec4bc399bcbce93547c3723643c.svg"
                                                            alt="s_0 \sim \rho_0(\cdot)." /></p>
                                                </div>
                                                <p>State transitions (what happens to the world between the state at
                                                    time <img class="math"
                                                        src="_images/math/7ed8f1921a380f7a5f45b87825838fdced658554.svg"
                                                        alt="t" />, <img class="math"
                                                        src="_images/math/4fcf0bf03c2a691496ce04ade269159d8b89caa5.svg"
                                                        alt="s_t" />, and the state at <img class="math"
                                                        src="_images/math/55c6e4a64640ac5e7b4da87ff4bcf12da93ef252.svg"
                                                        alt="t+1" />, <img class="math"
                                                        src="_images/math/4b669c18a22476afbab2c49bb68525256b416cff.svg"
                                                        alt="s_{t+1}" />), are governed by the natural laws of the
                                                    environment, and depend on only the most recent action, <img
                                                        class="math"
                                                        src="_images/math/39079fcebc9eb2aba4ab3fe7359b34807ceccc0e.svg"
                                                        alt="a_t" />. They can be either deterministic,</p>
                                                <div class="math">
                                                    <p><img src="_images/math/16da6346104894fb6a673473cbfc9ffeba6471fa.svg"
                                                            alt="s_{t+1} = f(s_t, a_t)" /></p>
                                                </div>
                                                <p>or stochastic,</p>
                                                <div class="math">
                                                    <p><img src="_images/math/872390af4f5b2541d17e7ef2bfaecbe1e9746d94.svg"
                                                            alt="s_{t+1} \sim P(\cdot|s_t, a_t)." /></p>
                                                </div>
                                                <p>Actions come from an agent according to its policy.</p>
                                                <div class="admonition-you-should-know admonition">
                                                    <p class="first admonition-title">You Should Know</p>
                                                    <p class="last">Trajectories are also frequently called
                                                        <strong>episodes</strong> or <strong>rollouts</strong>.</p>
                                                </div>
                                            </div>
                                            <div class="section" id="reward-and-return">
                                                <h4>Reward and Return<a class="headerlink" href="#reward-and-return"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>The reward function <img class="math"
                                                        src="_images/math/1f9d30d011e9fe548e999c9bfcf3fccfa27ec3ff.svg"
                                                        alt="R" /> is critically important in reinforcement learning. It
                                                    depends on the current state of the world, the action just taken,
                                                    and the next state of the world:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/6ed565b0911f12c8ef64d93a617d8bb30380d5d5.svg"
                                                            alt="r_t = R(s_t, a_t, s_{t+1})" /></p>
                                                </div>
                                                <p>although frequently this is simplified to just a dependence on the
                                                    current state, <img class="math"
                                                        src="_images/math/4befde40a79499d3655bebda93423e2661036f0d.svg"
                                                        alt="r_t = R(s_t)" />, or state-action pair <img class="math"
                                                        src="_images/math/3a66e4711a16a69ca64bd10d96985363d6e4bc5c.svg"
                                                        alt="r_t = R(s_t,a_t)" />.</p>
                                                <p>The goal of the agent is to maximize some notion of cumulative reward
                                                    over a trajectory, but this actually can mean a few things.
                                                    We&#8217;ll notate all of these cases with <img class="math"
                                                        src="_images/math/c2d6738c058406ade40dcf870311db157ed80e0f.svg"
                                                        alt="R(\tau)" />, and it will either be clear from context which
                                                    case we mean, or it won&#8217;t matter (because the same equations
                                                    will apply to all cases).</p>
                                                <p>One kind of return is the <strong>finite-horizon undiscounted
                                                        return</strong>, which is just the sum of rewards obtained in a
                                                    fixed window of steps:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/b2466507811fc9b9cbe2a0a51fd36034e16f2780.svg"
                                                            alt="R(\tau) = \sum_{t=0}^T r_t." /></p>
                                                </div>
                                                <p>Another kind of return is the <strong>infinite-horizon discounted
                                                        return</strong>, which is the sum of all rewards <em>ever</em>
                                                    obtained by the agent, but discounted by how far off in the future
                                                    they&#8217;re obtained. This formulation of reward includes a
                                                    discount factor <img class="math"
                                                        src="_images/math/7c0000152970a235979a501b70bd05c781a8b1ec.svg"
                                                        alt="\gamma \in (0,1)" />:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/bf49428c66c91a45d7b66a432450ee49a3622348.svg"
                                                            alt="R(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t." /></p>
                                                </div>
                                                <p>Why would we ever want a discount factor, though? Don&#8217;t we just
                                                    want to get <em>all</em> rewards? We do, but the discount factor is
                                                    both intuitively appealing and mathematically convenient. On an
                                                    intuitive level: cash now is better than cash later. Mathematically:
                                                    an infinite-horizon sum of rewards <a class="reference external"
                                                        href="https://en.wikipedia.org/wiki/Convergent_series">may not
                                                        converge</a> to a finite value, and is hard to deal with in
                                                    equations. But with a discount factor and under reasonable
                                                    conditions, the infinite sum converges.</p>
                                                <div class="admonition-you-should-know admonition">
                                                    <p class="first admonition-title">You Should Know</p>
                                                    <p class="last">While the line between these two formulations of
                                                        return are quite stark in RL formalism, deep RL practice tends
                                                        to blur the line a fair bit&#8212;for instance, we frequently
                                                        set up algorithms to optimize the undiscounted return, but use
                                                        discount factors in estimating <strong>value functions</strong>.
                                                    </p>
                                                </div>
                                            </div>
                                            <div class="section" id="the-rl-problem">
                                                <h4>The RL Problem<a class="headerlink" href="#the-rl-problem"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>Whatever the choice of return measure (whether infinite-horizon
                                                    discounted, or finite-horizon undiscounted), and whatever the choice
                                                    of policy, the goal in RL is to select a policy which maximizes
                                                    <strong>expected return</strong> when the agent acts according to
                                                    it.</p>
                                                <p>To talk about expected return, we first have to talk about
                                                    probability distributions over trajectories.</p>
                                                <p>Let&#8217;s suppose that both the environment transitions and the
                                                    policy are stochastic. In this case, the probability of a <img
                                                        class="math"
                                                        src="_images/math/844fe92e58a680253626f9b0706a06c578a4e040.svg"
                                                        alt="T" /> -step trajectory is:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/69369e7fae3098a2f05a79680fbecbf48a4e7f66.svg"
                                                            alt="P(\tau|\pi) = \rho_0 (s_0) \prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \pi(a_t | s_t)." />
                                                    </p>
                                                </div>
                                                <p>The expected return (for whichever measure), denoted by <img
                                                        class="math"
                                                        src="_images/math/89397c4cc47a40c3466507e1330dc380458f762e.svg"
                                                        alt="J(\pi)" />, is then:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/f0d6e3879540e318df14d2c8b68af828b1b350da.svg"
                                                            alt="J(\pi) = \int_{\tau} P(\tau|\pi) R(\tau) = \underE{\tau\sim \pi}{R(\tau)}." />
                                                    </p>
                                                </div>
                                                <p>The central optimization problem in RL can then be expressed by</p>
                                                <div class="math">
                                                    <p><img src="_images/math/2de61070bf8758d03104b4f15df45c8ff5a86f5a.svg"
                                                            alt="\pi^* = \arg \max_{\pi} J(\pi)," /></p>
                                                </div>
                                                <p>with <img class="math"
                                                        src="_images/math/1fbf259ac070c92161e32b93c0f64705a8f18f0a.svg"
                                                        alt="\pi^*" /> being the <strong>optimal policy</strong>.</p>
                                            </div>
                                            <div class="section" id="value-functions">
                                                <h4>Value Functions<a class="headerlink" href="#value-functions"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>It&#8217;s often useful to know the <strong>value</strong> of a
                                                    state, or state-action pair. By value, we mean the expected return
                                                    if you start in that state or state-action pair, and then act
                                                    according to a particular policy forever after. <strong>Value
                                                        functions</strong> are used, one way or another, in almost every
                                                    RL algorithm.</p>
                                                <p>There are four main functions of note here.</p>
                                                <ol class="arabic">
                                                    <li>
                                                        <p class="first">The <strong>On-Policy Value Function</strong>,
                                                            <img class="math"
                                                                src="_images/math/a81303323c25fc13cd0652ca46d7596276e5cb7e.svg"
                                                                alt="V^{\pi}(s)" />, which gives the expected return if
                                                            you start in state <img class="math"
                                                                src="_images/math/96ac51b9afe79581e48f2f3f0ad3faa0f4402cc7.svg"
                                                                alt="s" /> and always act according to policy <img
                                                                class="math"
                                                                src="_images/math/1ae2bd722da01b3a89ffc139af2437c28364a966.svg"
                                                                alt="\pi" />:</p>
                                                        <blockquote>
                                                            <div>
                                                                <div class="math">
                                                                    <p><img src="_images/math/e043709b46c9aa6811953dabd82461db6308fe19.svg"
                                                                            alt="V^{\pi}(s) = \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s\right.}" />
                                                                    </p>
                                                                </div>
                                                            </div>
                                                        </blockquote>
                                                    </li>
                                                    <li>
                                                        <p class="first">The <strong>On-Policy Action-Value
                                                                Function</strong>, <img class="math"
                                                                src="_images/math/86549c5748a6fdd134970fd88f4842bd862a8b25.svg"
                                                                alt="Q^{\pi}(s,a)" />, which gives the expected return
                                                            if you start in state <img class="math"
                                                                src="_images/math/96ac51b9afe79581e48f2f3f0ad3faa0f4402cc7.svg"
                                                                alt="s" />, take an arbitrary action <img class="math"
                                                                src="_images/math/76a319586cd215c8f2075b938fc6f6e07c81714b.svg"
                                                                alt="a" /> (which may not have come from the policy),
                                                            and then forever after act according to policy <img
                                                                class="math"
                                                                src="_images/math/1ae2bd722da01b3a89ffc139af2437c28364a966.svg"
                                                                alt="\pi" />:</p>
                                                        <blockquote>
                                                            <div>
                                                                <div class="math">
                                                                    <p><img src="_images/math/85d41c8c383a96e1ed34fc66f14abd61b132dd28.svg"
                                                                            alt="Q^{\pi}(s,a) = \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s, a_0 = a\right.}" />
                                                                    </p>
                                                                </div>
                                                            </div>
                                                        </blockquote>
                                                    </li>
                                                    <li>
                                                        <p class="first">The <strong>Optimal Value Function</strong>,
                                                            <img class="math"
                                                                src="_images/math/6159ad57fb5294b812e76c6260a65dc5ffa2a5f7.svg"
                                                                alt="V^*(s)" />, which gives the expected return if you
                                                            start in state <img class="math"
                                                                src="_images/math/96ac51b9afe79581e48f2f3f0ad3faa0f4402cc7.svg"
                                                                alt="s" /> and always act according to the
                                                            <em>optimal</em> policy in the environment:</p>
                                                        <blockquote>
                                                            <div>
                                                                <div class="math">
                                                                    <p><img src="_images/math/01d48ea453ecb7b560ea7d42144ae24422fbd0eb.svg"
                                                                            alt="V^*(s) = \max_{\pi} \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s\right.}" />
                                                                    </p>
                                                                </div>
                                                            </div>
                                                        </blockquote>
                                                    </li>
                                                    <li>
                                                        <p class="first">The <strong>Optimal Action-Value
                                                                Function</strong>, <img class="math"
                                                                src="_images/math/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg"
                                                                alt="Q^*(s,a)" />, which gives the expected return if
                                                            you start in state <img class="math"
                                                                src="_images/math/96ac51b9afe79581e48f2f3f0ad3faa0f4402cc7.svg"
                                                                alt="s" />, take an arbitrary action <img class="math"
                                                                src="_images/math/76a319586cd215c8f2075b938fc6f6e07c81714b.svg"
                                                                alt="a" />, and then forever after act according to the
                                                            <em>optimal</em> policy in the environment:</p>
                                                        <blockquote>
                                                            <div>
                                                                <div class="math">
                                                                    <p><img src="_images/math/bc92e8ce1cf0aaa212e144d5ed74e3b115453cb6.svg"
                                                                            alt="Q^*(s,a) = \max_{\pi} \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s, a_0 = a\right.}" />
                                                                    </p>
                                                                </div>
                                                            </div>
                                                        </blockquote>
                                                    </li>
                                                </ol>
                                                <div class="admonition-you-should-know admonition">
                                                    <p class="first admonition-title">You Should Know</p>
                                                    <p class="last">When we talk about value functions, if we do not
                                                        make reference to time-dependence, we only mean expected
                                                        <strong>infinite-horizon discounted return</strong>. Value
                                                        functions for finite-horizon undiscounted return would need to
                                                        accept time as an argument. Can you think about why? Hint: what
                                                        happens when time&#8217;s up?</p>
                                                </div>
                                                <div class="admonition-you-should-know admonition">
                                                    <p class="first admonition-title">You Should Know</p>
                                                    <p>There are two key connections between the value function and the
                                                        action-value function that come up pretty often:</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/5151391b2cd2bfa909a3b5a057b6c93d4191790b.svg"
                                                                alt="V^{\pi}(s) = \underE{a\sim \pi}{Q^{\pi}(s,a)}," />
                                                        </p>
                                                    </div>
                                                    <p>and</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/4cbd255e1ecc9f7083034be12148e8b98cefc2ee.svg"
                                                                alt="V^*(s) = \max_a Q^* (s,a)." /></p>
                                                    </div>
                                                    <p class="last">These relations follow pretty directly from the
                                                        definitions just given: can you prove them?</p>
                                                </div>
                                            </div>
                                            <div class="section" id="the-optimal-q-function-and-the-optimal-action">
                                                <h4>The Optimal Q-Function and the Optimal Action<a class="headerlink"
                                                        href="#the-optimal-q-function-and-the-optimal-action"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>There is an important connection between the optimal action-value
                                                    function <img class="math"
                                                        src="_images/math/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg"
                                                        alt="Q^*(s,a)" /> and the action selected by the optimal policy.
                                                    By definition, <img class="math"
                                                        src="_images/math/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg"
                                                        alt="Q^*(s,a)" /> gives the expected return for starting in
                                                    state <img class="math"
                                                        src="_images/math/96ac51b9afe79581e48f2f3f0ad3faa0f4402cc7.svg"
                                                        alt="s" />, taking (arbitrary) action <img class="math"
                                                        src="_images/math/76a319586cd215c8f2075b938fc6f6e07c81714b.svg"
                                                        alt="a" />, and then acting according to the optimal policy
                                                    forever after.</p>
                                                <p>The optimal policy in <img class="math"
                                                        src="_images/math/96ac51b9afe79581e48f2f3f0ad3faa0f4402cc7.svg"
                                                        alt="s" /> will select whichever action maximizes the expected
                                                    return from starting in <img class="math"
                                                        src="_images/math/96ac51b9afe79581e48f2f3f0ad3faa0f4402cc7.svg"
                                                        alt="s" />. As a result, if we have <img class="math"
                                                        src="_images/math/c2e969d09ae88d847429eac9a8494cc89cabe4bd.svg"
                                                        alt="Q^*" />, we can directly obtain the optimal action, <img
                                                        class="math"
                                                        src="_images/math/baf715aa6a295b7b7d85e1e1123552c5ae705756.svg"
                                                        alt="a^*(s)" />, via</p>
                                                <div class="math">
                                                    <p><img src="_images/math/42490c4d812c9ca1fc226684577900bc8bdd609b.svg"
                                                            alt="a^*(s) = \arg \max_a Q^* (s,a)." /></p>
                                                </div>
                                                <p>Note: there may be multiple actions which maximize <img class="math"
                                                        src="_images/math/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg"
                                                        alt="Q^*(s,a)" />, in which case, all of them are optimal, and
                                                    the optimal policy may randomly select any of them. But there is
                                                    always an optimal policy which deterministically selects an action.
                                                </p>
                                            </div>
                                            <div class="section" id="bellman-equations">
                                                <h4>Bellman Equations<a class="headerlink" href="#bellman-equations"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>All four of the value functions obey special self-consistency
                                                    equations called <strong>Bellman equations</strong>. The basic idea
                                                    behind the Bellman equations is this:</p>
                                                <blockquote>
                                                    <div>The value of your starting point is the reward you expect to
                                                        get from being there, plus the value of wherever you land next.
                                                    </div>
                                                </blockquote>
                                                <p>The Bellman equations for the on-policy value functions are</p>
                                                <div class="math">
                                                    <p><img src="_images/math/7e4a2964e190104a669406ca5e1e320a5da8bae0.svg"
                                                            alt="\begin{align*}
V^{\pi}(s) &amp;= \underE{a \sim \pi \\ s'\sim P}{r(s,a) + \gamma V^{\pi}(s')}, \\
Q^{\pi}(s,a) &amp;= \underE{s'\sim P}{r(s,a) + \gamma \underE{a'\sim \pi}{Q^{\pi}(s',a')}},
\end{align*}" /></p>
                                                </div>
                                                <p>where <img class="math"
                                                        src="_images/math/411171ab57c4bec0d86c9f4b495106ba5d73decc.svg"
                                                        alt="s' \sim P" /> is shorthand for <img class="math"
                                                        src="_images/math/ed45f9d37dbb092727104773ca3a464d46f892b8.svg"
                                                        alt="s' \sim P(\cdot |s,a)" />, indicating that the next state
                                                    <img class="math"
                                                        src="_images/math/6e85fa05d4954e7c1e8037ee1bd163d15bc2e2d6.svg"
                                                        alt="s'" /> is sampled from the environment&#8217;s transition
                                                    rules; <img class="math"
                                                        src="_images/math/e87025074e03131c69c6c5758e873a6224ea5d3a.svg"
                                                        alt="a \sim \pi" /> is shorthand for <img class="math"
                                                        src="_images/math/35c684d9cc672fd0bbacd896f49abdd986f40b02.svg"
                                                        alt="a \sim \pi(\cdot|s)" />; and <img class="math"
                                                        src="_images/math/b3f46cc6cd6c2fa9068013fafbe1b4b029bb8a58.svg"
                                                        alt="a' \sim \pi" /> is shorthand for <img class="math"
                                                        src="_images/math/6eb25f9175aa0471d7a7728ab237a92fef5009e9.svg"
                                                        alt="a' \sim \pi(\cdot|s')" />.</p>
                                                <p>The Bellman equations for the optimal value functions are</p>
                                                <div class="math">
                                                    <p><img src="_images/math/f8ab9b211bc9bb91cde189360051e3bd1f896afa.svg"
                                                            alt="\begin{align*}
V^*(s) &amp;= \max_a \underE{s'\sim P}{r(s,a) + \gamma V^*(s')}, \\
Q^*(s,a) &amp;= \underE{s'\sim P}{r(s,a) + \gamma \max_{a'} Q^*(s',a')}.
\end{align*}" /></p>
                                                </div>
                                                <p>The crucial difference between the Bellman equations for the
                                                    on-policy value functions and the optimal value functions, is the
                                                    absence or presence of the <img class="math"
                                                        src="_images/math/a52117d0fa54eb8449482447cad9c0ab54c757cc.svg"
                                                        alt="\max" /> over actions. Its inclusion reflects the fact that
                                                    whenever the agent gets to choose its action, in order to act
                                                    optimally, it has to pick whichever action leads to the highest
                                                    value.</p>
                                                <div class="admonition-you-should-know admonition">
                                                    <p class="first admonition-title">You Should Know</p>
                                                    <p class="last">The term &#8220;Bellman backup&#8221; comes up quite
                                                        frequently in the RL literature. The Bellman backup for a state,
                                                        or state-action pair, is the right-hand side of the Bellman
                                                        equation: the reward-plus-next-value.</p>
                                                </div>
                                            </div>
                                            <div class="section" id="advantage-functions">
                                                <h4>Advantage Functions<a class="headerlink" href="#advantage-functions"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>Sometimes in RL, we don&#8217;t need to describe how good an action
                                                    is in an absolute sense, but only how much better it is than others
                                                    on average. That is to say, we want to know the relative
                                                    <strong>advantage</strong> of that action. We make this concept
                                                    precise with the <strong>advantage function.</strong></p>
                                                <p>The advantage function <img class="math"
                                                        src="_images/math/09f82f133e9f89a59ba22266639c4968b5641c28.svg"
                                                        alt="A^{\pi}(s,a)" /> corresponding to a policy <img
                                                        class="math"
                                                        src="_images/math/1ae2bd722da01b3a89ffc139af2437c28364a966.svg"
                                                        alt="\pi" /> describes how much better it is to take a specific
                                                    action <img class="math"
                                                        src="_images/math/76a319586cd215c8f2075b938fc6f6e07c81714b.svg"
                                                        alt="a" /> in state <img class="math"
                                                        src="_images/math/96ac51b9afe79581e48f2f3f0ad3faa0f4402cc7.svg"
                                                        alt="s" />, over randomly selecting an action according to <img
                                                        class="math"
                                                        src="_images/math/8d2c2c23f74e7a0cf98b0ee1de016825eb50e2d4.svg"
                                                        alt="\pi(\cdot|s)" />, assuming you act according to <img
                                                        class="math"
                                                        src="_images/math/1ae2bd722da01b3a89ffc139af2437c28364a966.svg"
                                                        alt="\pi" /> forever after. Mathematically, the advantage
                                                    function is defined by</p>
                                                <div class="math">
                                                    <p><img src="_images/math/3748974cc061fb4065fa46dd6271395d59f22040.svg"
                                                            alt="A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)." /></p>
                                                </div>
                                                <div class="admonition-you-should-know admonition">
                                                    <p class="first admonition-title">You Should Know</p>
                                                    <p class="last">We&#8217;ll discuss this more later, but the
                                                        advantage function is crucially important to policy gradient
                                                        methods.</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div class="section" id="optional-formalism">
                                            <h3><a class="toc-backref" href="#id5">(Optional) Formalism</a><a
                                                    class="headerlink" href="#optional-formalism"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>So far, we&#8217;ve discussed the agent&#8217;s environment in an
                                                informal way, but if you try to go digging through the literature,
                                                you&#8217;re likely to run into the standard mathematical formalism for
                                                this setting: <strong>Markov Decision Processes</strong> (MDPs). An MDP
                                                is a 5-tuple, <img class="math"
                                                    src="_images/math/a7e1a4549f45dc56849b1ff857a19a71f9cc02a6.svg"
                                                    alt="\langle S, A, R, P, \rho_0 \rangle" />, where</p>
                                            <ul class="simple">
                                                <li><img class="math"
                                                        src="_images/math/bbe16bfd192df4894eaef8bfe3133325ba462202.svg"
                                                        alt="S" /> is the set of all valid states,</li>
                                                <li><img class="math"
                                                        src="_images/math/a236fe76423c33d18465350c1c36cef9aa8fdc31.svg"
                                                        alt="A" /> is the set of all valid actions,</li>
                                                <li><img class="math"
                                                        src="_images/math/eac18a6e37a9272c1458d3086adb317ecda571e8.svg"
                                                        alt="R : S \times A \times S \to \mathbb{R}" /> is the reward
                                                    function, with <img class="math"
                                                        src="_images/math/444ffe3079b81e8b1c42e462f0b6d63fbeeec6c6.svg"
                                                        alt="r_t = R(s_t, a_t, s_{t+1})" />,</li>
                                                <li><img class="math"
                                                        src="_images/math/3923c00b0df8f8c1003312d5c125275bd10598ba.svg"
                                                        alt="P : S \times A \to \mathcal{P}(S)" /> is the transition
                                                    probability function, with <img class="math"
                                                        src="_images/math/655bf048edfc8ffd3b4655504e874a622ed888ce.svg"
                                                        alt="P(s'|s,a)" /> being the probability of transitioning into
                                                    state <img class="math"
                                                        src="_images/math/6e85fa05d4954e7c1e8037ee1bd163d15bc2e2d6.svg"
                                                        alt="s'" /> if you start in state <img class="math"
                                                        src="_images/math/96ac51b9afe79581e48f2f3f0ad3faa0f4402cc7.svg"
                                                        alt="s" /> and take action <img class="math"
                                                        src="_images/math/76a319586cd215c8f2075b938fc6f6e07c81714b.svg"
                                                        alt="a" />,</li>
                                                <li>and <img class="math"
                                                        src="_images/math/2d44ad6f01d4e56266daa8e3b35bd4f298e25788.svg"
                                                        alt="\rho_0" /> is the starting state distribution.</li>
                                            </ul>
                                            <p>The name Markov Decision Process refers to the fact that the system obeys
                                                the <a class="reference external"
                                                    href="https://en.wikipedia.org/wiki/Markov_property">Markov
                                                    property</a>: transitions only depend on the most recent state and
                                                action, and no prior history.</p>
                                        </div>
                                    </div>
                                    <span id="document-spinningup/rl_intro2"></span>
                                    <div class="section" id="part-2-kinds-of-rl-algorithms">
                                        <h2><a class="toc-backref" href="#id19">Part 2: Kinds of RL Algorithms</a><a
                                                class="headerlink" href="#part-2-kinds-of-rl-algorithms"
                                                title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal" href="#part-2-kinds-of-rl-algorithms"
                                                        id="id19">Part 2: Kinds of RL Algorithms</a>
                                                    <ul>
                                                        <li><a class="reference internal"
                                                                href="#a-taxonomy-of-rl-algorithms" id="id20">A Taxonomy
                                                                of RL Algorithms</a></li>
                                                        <li><a class="reference internal"
                                                                href="#links-to-algorithms-in-taxonomy" id="id21">Links
                                                                to Algorithms in Taxonomy</a></li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <p>Now that we&#8217;ve gone through the basics of RL terminology and notation,
                                            we can cover a little bit of the richer material: the landscape of
                                            algorithms in modern RL, and a description of the kinds of trade-offs that
                                            go into algorithm design.</p>
                                        <div class="section" id="a-taxonomy-of-rl-algorithms">
                                            <h3><a class="toc-backref" href="#id20">A Taxonomy of RL Algorithms</a><a
                                                    class="headerlink" href="#a-taxonomy-of-rl-algorithms"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <div class="figure align-center" id="id18">
                                                <img alt="_images/rl_algorithms_9_15.svg"
                                                    src="_images/rl_algorithms_9_15.svg" />
                                                <p class="caption"><span class="caption-text">A non-exhaustive, but
                                                        useful taxonomy of algorithms in modern RL. <a
                                                            class="reference internal" href="#citations-below">Citations
                                                            below.</a></span></p>
                                            </div>
                                            <p>We&#8217;ll start this section with a disclaimer: it&#8217;s really quite
                                                hard to draw an accurate, all-encompassing taxonomy of algorithms in the
                                                modern RL space, because the modularity of algorithms is not
                                                well-represented by a tree structure. Also, to make something that fits
                                                on a page and is reasonably digestible in an introduction essay, we have
                                                to omit quite a bit of more advanced material (exploration, transfer
                                                learning, meta learning, etc). That said, our goals here are</p>
                                            <ul class="simple">
                                                <li>to highlight the most foundational design choices in deep RL
                                                    algorithms about what to learn and how to learn it,</li>
                                                <li>to expose the trade-offs in those choices,</li>
                                                <li>and to place a few prominent modern algorithms into context with
                                                    respect to those choices.</li>
                                            </ul>
                                            <div class="section" id="model-free-vs-model-based-rl">
                                                <h4>Model-Free vs Model-Based RL<a class="headerlink"
                                                        href="#model-free-vs-model-based-rl"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>One of the most important branching points in an RL algorithm is the
                                                    question of <strong>whether the agent has access to (or learns) a
                                                        model of the environment</strong>. By a model of the
                                                    environment, we mean a function which predicts state transitions and
                                                    rewards.</p>
                                                <p>The main upside to having a model is that <strong>it allows the agent
                                                        to plan</strong> by thinking ahead, seeing what would happen for
                                                    a range of possible choices, and explicitly deciding between its
                                                    options. Agents can then distill the results from planning ahead
                                                    into a learned policy. A particularly famous example of this
                                                    approach is <a class="reference external"
                                                        href="https://arxiv.org/pdf/1712.01815.pdf">AlphaZero</a>. When
                                                    this works, it can result in a substantial improvement in sample
                                                    efficiency over methods that don&#8217;t have a model.</p>
                                                <p>The main downside is that <strong>a ground-truth model of the
                                                        environment is usually not available to the agent.</strong> If
                                                    an agent wants to use a model in this case, it has to learn the
                                                    model purely from experience, which creates several challenges. The
                                                    biggest challenge is that bias in the model can be exploited by the
                                                    agent, resulting in an agent which performs well with respect to the
                                                    learned model, but behaves sub-optimally (or super terribly) in the
                                                    real environment. Model-learning is fundamentally hard, so even
                                                    intense effort&#8212;being willing to throw lots of time and compute
                                                    at it&#8212;can fail to pay off.</p>
                                                <p>Algorithms which use a model are called <strong>model-based</strong>
                                                    methods, and those that don&#8217;t are called
                                                    <strong>model-free</strong>. While model-free methods forego the
                                                    potential gains in sample efficiency from using a model, they tend
                                                    to be easier to implement and tune. As of the time of writing this
                                                    introduction (September 2018), model-free methods are more popular
                                                    and have been more extensively developed and tested than model-based
                                                    methods.</p>
                                            </div>
                                            <div class="section" id="what-to-learn">
                                                <h4>What to Learn<a class="headerlink" href="#what-to-learn"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>Another critical branching point in an RL algorithm is the question
                                                    of <strong>what to learn.</strong> The list of usual suspects
                                                    includes</p>
                                                <ul class="simple">
                                                    <li>policies, either stochastic or deterministic,</li>
                                                    <li>action-value functions (Q-functions),</li>
                                                    <li>value functions,</li>
                                                    <li>and/or environment models.</li>
                                                </ul>
                                                <div class="section" id="what-to-learn-in-model-free-rl">
                                                    <h5>What to Learn in Model-Free RL<a class="headerlink"
                                                            href="#what-to-learn-in-model-free-rl"
                                                            title="Permalink to this headline">¶</a></h5>
                                                    <p>There are two main approaches to representing and training agents
                                                        with model-free RL:</p>
                                                    <p><strong>Policy Optimization.</strong> Methods in this family
                                                        represent a policy explicitly as <img class="math"
                                                            src="_images/math/400068784a9d13ffe96c61f29b4ab26ad5557376.svg"
                                                            alt="\pi_{\theta}(a|s)" />. They optimize the parameters
                                                        <img class="math"
                                                            src="_images/math/ce5edddd490112350f4bd555d9390e0e845f754a.svg"
                                                            alt="\theta" /> either directly by gradient ascent on the
                                                        performance objective <img class="math"
                                                            src="_images/math/96b876944de9cf0f980fe261562e8e07029245bf.svg"
                                                            alt="J(\pi_{\theta})" />, or indirectly, by maximizing local
                                                        approximations of <img class="math"
                                                            src="_images/math/96b876944de9cf0f980fe261562e8e07029245bf.svg"
                                                            alt="J(\pi_{\theta})" />. This optimization is almost always
                                                        performed <strong>on-policy</strong>, which means that each
                                                        update only uses data collected while acting according to the
                                                        most recent version of the policy. Policy optimization also
                                                        usually involves learning an approximator <img class="math"
                                                            src="_images/math/693bb706835fbd5903ad9758837acecd07ef13b1.svg"
                                                            alt="V_{\phi}(s)" /> for the on-policy value function <img
                                                            class="math"
                                                            src="_images/math/a81303323c25fc13cd0652ca46d7596276e5cb7e.svg"
                                                            alt="V^{\pi}(s)" />, which gets used in figuring out how to
                                                        update the policy.</p>
                                                    <p>A couple of examples of policy optimization methods are:</p>
                                                    <ul class="simple">
                                                        <li><a class="reference external"
                                                                href="https://arxiv.org/pdf/1602.01783.pdf">A2C /
                                                                A3C</a>, which performs gradient ascent to directly
                                                            maximize performance,</li>
                                                        <li>and <a class="reference external"
                                                                href="https://arxiv.org/pdf/1707.06347.pdf">PPO</a>,
                                                            whose updates indirectly maximize performance, by instead
                                                            maximizing a <em>surrogate objective</em> function which
                                                            gives a conservative estimate for how much <img class="math"
                                                                src="_images/math/96b876944de9cf0f980fe261562e8e07029245bf.svg"
                                                                alt="J(\pi_{\theta})" /> will change as a result of the
                                                            update.</li>
                                                    </ul>
                                                    <p><strong>Q-Learning.</strong> Methods in this family learn an
                                                        approximator <img class="math"
                                                            src="_images/math/de947d14fdcfaa155ef3301fc39efcf9e6c9449c.svg"
                                                            alt="Q_{\theta}(s,a)" /> for the optimal action-value
                                                        function, <img class="math"
                                                            src="_images/math/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg"
                                                            alt="Q^*(s,a)" />. Typically they use an objective function
                                                        based on the <a class="reference external"
                                                            href="../spinningup/rl_intro.html#bellman-equations">Bellman
                                                            equation</a>. This optimization is almost always performed
                                                        <strong>off-policy</strong>, which means that each update can
                                                        use data collected at any point during training, regardless of
                                                        how the agent was choosing to explore the environment when the
                                                        data was obtained. The corresponding policy is obtained via the
                                                        connection between <img class="math"
                                                            src="_images/math/c2e969d09ae88d847429eac9a8494cc89cabe4bd.svg"
                                                            alt="Q^*" /> and <img class="math"
                                                            src="_images/math/1fbf259ac070c92161e32b93c0f64705a8f18f0a.svg"
                                                            alt="\pi^*" />: the actions taken by the Q-learning agent
                                                        are given by</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/d353412962e458573b92aac78df3fbe0a10d998d.svg"
                                                                alt="a(s) = \arg \max_a Q_{\theta}(s,a)." /></p>
                                                    </div>
                                                    <p>Examples of Q-learning methods include</p>
                                                    <ul class="simple">
                                                        <li><a class="reference external"
                                                                href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">DQN</a>,
                                                            a classic which substantially launched the field of deep RL,
                                                        </li>
                                                        <li>and <a class="reference external"
                                                                href="https://arxiv.org/pdf/1707.06887.pdf">C51</a>, a
                                                            variant that learns a distribution over return whose
                                                            expectation is <img class="math"
                                                                src="_images/math/c2e969d09ae88d847429eac9a8494cc89cabe4bd.svg"
                                                                alt="Q^*" />.</li>
                                                    </ul>
                                                    <p><strong>Trade-offs Between Policy Optimization and
                                                            Q-Learning.</strong> The primary strength of policy
                                                        optimization methods is that they are principled, in the sense
                                                        that <em>you directly optimize for the thing you want.</em> This
                                                        tends to make them stable and reliable. By contrast, Q-learning
                                                        methods only <em>indirectly</em> optimize for agent performance,
                                                        by training <img class="math"
                                                            src="_images/math/713b5ea31ad66705079ea5786dd84e06944402b7.svg"
                                                            alt="Q_{\theta}" /> to satisfy a self-consistency equation.
                                                        There are many failure modes for this kind of learning, so it
                                                        tends to be less stable. <a class="footnote-reference"
                                                            href="#id2" id="id1">[1]</a> But, Q-learning methods gain
                                                        the advantage of being substantially more sample efficient when
                                                        they do work, because they can reuse data more effectively than
                                                        policy optimization techniques.</p>
                                                    <p><strong>Interpolating Between Policy Optimization and
                                                            Q-Learning.</strong> Serendipitously, policy optimization
                                                        and Q-learning are not incompatible (and under some
                                                        circumstances, it turns out, <a class="reference external"
                                                            href="https://arxiv.org/pdf/1704.06440.pdf">equivalent</a>),
                                                        and there exist a range of algorithms that live in between the
                                                        two extremes. Algorithms that live on this spectrum are able to
                                                        carefully trade-off between the strengths and weaknesses of
                                                        either side. Examples include</p>
                                                    <ul class="simple">
                                                        <li><a class="reference external"
                                                                href="https://arxiv.org/pdf/1509.02971.pdf">DDPG</a>, an
                                                            algorithm which concurrently learns a deterministic policy
                                                            and a Q-function by using each to improve the other,</li>
                                                        <li>and <a class="reference external"
                                                                href="https://arxiv.org/pdf/1801.01290.pdf">SAC</a>, a
                                                            variant which uses stochastic policies, entropy
                                                            regularization, and a few other tricks to stabilize learning
                                                            and score higher than DDPG on standard benchmarks.</li>
                                                    </ul>
                                                    <table class="docutils footnote" frame="void" id="id2" rules="none">
                                                        <colgroup>
                                                            <col class="label" />
                                                            <col />
                                                        </colgroup>
                                                        <tbody valign="top">
                                                            <tr>
                                                                <td class="label"><a class="fn-backref"
                                                                        href="#id1">[1]</a></td>
                                                                <td>For more information about how and why Q-learning
                                                                    methods can fail, see 1) this classic paper by <a
                                                                        class="reference external"
                                                                        href="http://web.mit.edu/jnt/www/Papers/J063-97-bvr-td.pdf">Tsitsiklis
                                                                        and van Roy</a>, 2) the (much more recent) <a
                                                                        class="reference external"
                                                                        href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">review
                                                                        by Szepesvari</a> (in section 4.3.2), and 3)
                                                                    chapter 11 of <a class="reference external"
                                                                        href="https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view">Sutton
                                                                        and Barto</a>, especially section 11.3 (on
                                                                    &#8220;the deadly triad&#8221; of function
                                                                    approximation, bootstrapping, and off-policy data,
                                                                    together causing instability in value-learning
                                                                    algorithms).</td>
                                                            </tr>
                                                        </tbody>
                                                    </table>
                                                </div>
                                            </div>
                                            <div class="section" id="what-to-learn-in-model-based-rl">
                                                <h4>What to Learn in Model-Based RL<a class="headerlink"
                                                        href="#what-to-learn-in-model-based-rl"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>Unlike model-free RL, there aren&#8217;t a small number of
                                                    easy-to-define clusters of methods for model-based RL: there are
                                                    many orthogonal ways of using models. We&#8217;ll give a few
                                                    examples, but the list is far from exhaustive. In each case, the
                                                    model may either be given or learned.</p>
                                                <p><strong>Background: Pure Planning.</strong> The most basic approach
                                                    <em>never</em> explicitly represents the policy, and instead, uses
                                                    pure planning techniques like <a class="reference external"
                                                        href="https://en.wikipedia.org/wiki/Model_predictive_control">model-predictive
                                                        control</a> (MPC) to select actions. In MPC, each time the agent
                                                    observes the environment, it computes a plan which is optimal with
                                                    respect to the model, where the plan describes all actions to take
                                                    over some fixed window of time after the present. (Future rewards
                                                    beyond the horizon may be considered by the planning algorithm
                                                    through the use of a learned value function.) The agent then
                                                    executes the first action of the plan, and immediately discards the
                                                    rest of it. It computes a new plan each time it prepares to interact
                                                    with the environment, to avoid using an action from a plan with a
                                                    shorter-than-desired planning horizon.</p>
                                                <ul class="simple">
                                                    <li>The <a class="reference external"
                                                            href="https://sites.google.com/view/mbmf">MBMF</a> work
                                                        explores MPC with learned environment models on some standard
                                                        benchmark tasks for deep RL.</li>
                                                </ul>
                                                <p><strong>Expert Iteration.</strong> A straightforward follow-on to
                                                    pure planning involves using and learning an explicit representation
                                                    of the policy, <img class="math"
                                                        src="_images/math/400068784a9d13ffe96c61f29b4ab26ad5557376.svg"
                                                        alt="\pi_{\theta}(a|s)" />. The agent uses a planning algorithm
                                                    (like Monte Carlo Tree Search) in the model, generating candidate
                                                    actions for the plan by sampling from its current policy. The
                                                    planning algorithm produces an action which is better than what the
                                                    policy alone would have produced, hence it is an
                                                    &#8220;expert&#8221; relative to the policy. The policy is
                                                    afterwards updated to produce an action more like the planning
                                                    algorithm&#8217;s output.</p>
                                                <ul class="simple">
                                                    <li>The <a class="reference external"
                                                            href="https://arxiv.org/pdf/1705.08439.pdf">ExIt</a>
                                                        algorithm uses this approach to train deep neural networks to
                                                        play Hex.</li>
                                                    <li><a class="reference external"
                                                            href="https://arxiv.org/pdf/1712.01815.pdf">AlphaZero</a> is
                                                        another example of this approach.</li>
                                                </ul>
                                                <p><strong>Data Augmentation for Model-Free Methods.</strong> Use a
                                                    model-free RL algorithm to train a policy or Q-function, but either
                                                    1) augment real experiences with fictitious ones in updating the
                                                    agent, or 2) use <em>only</em> fictitous experience for updating the
                                                    agent.</p>
                                                <ul class="simple">
                                                    <li>See <a class="reference external"
                                                            href="https://arxiv.org/pdf/1803.00101.pdf">MBVE</a> for an
                                                        example of augmenting real experiences with fictitious ones.
                                                    </li>
                                                    <li>See <a class="reference external"
                                                            href="https://worldmodels.github.io/">World Models</a> for
                                                        an example of using purely fictitious experience to train the
                                                        agent, which they call &#8220;training in the dream.&#8221;</li>
                                                </ul>
                                                <p><strong>Embedding Planning Loops into Policies.</strong> Another
                                                    approach embeds the planning procedure directly into a policy as a
                                                    subroutine&#8212;so that complete plans become side information for
                                                    the policy&#8212;while training the output of the policy with any
                                                    standard model-free algorithm. The key concept is that in this
                                                    framework, the policy can learn to choose how and when to use the
                                                    plans. This makes model bias less of a problem, because if the model
                                                    is bad for planning in some states, the policy can simply learn to
                                                    ignore it.</p>
                                                <ul class="simple">
                                                    <li>See <a class="reference external"
                                                            href="https://arxiv.org/pdf/1707.06203.pdf">I2A</a> for an
                                                        example of agents being endowed with this style of imagination.
                                                    </li>
                                                </ul>
                                            </div>
                                        </div>
                                        <div class="section" id="links-to-algorithms-in-taxonomy">
                                            <h3><a class="toc-backref" href="#id21">Links to Algorithms in
                                                    Taxonomy</a><a class="headerlink"
                                                    href="#links-to-algorithms-in-taxonomy"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <span class="target" id="citations-below"></span>
                                            <table class="docutils footnote" frame="void" id="id3" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[2]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1602.01783.pdf">A2C /
                                                                A3C</a> (Asynchronous Advantage Actor-Critic): Mnih et
                                                            al, 2016</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id4" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[3]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1707.06347.pdf">PPO</a>
                                                            (Proximal Policy Optimization): Schulman et al, 2017</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id5" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[4]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1502.05477.pdf">TRPO</a>
                                                            (Trust Region Policy Optimization): Schulman et al, 2015
                                                        </td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id6" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[5]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1509.02971.pdf">DDPG</a>
                                                            (Deep Deterministic Policy Gradient): Lillicrap et al, 2015
                                                        </td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id7" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[6]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1802.09477.pdf">TD3</a>
                                                            (Twin Delayed DDPG): Fujimoto et al, 2018</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id8" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[7]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1801.01290.pdf">SAC</a>
                                                            (Soft Actor-Critic): Haarnoja et al, 2018</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id9" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[8]</td>
                                                        <td><a class="reference external"
                                                                href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">DQN</a>
                                                            (Deep Q-Networks): Mnih et al, 2013</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id10" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[9]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1707.06887.pdf">C51</a>
                                                            (Categorical 51-Atom DQN): Bellemare et al, 2017</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id11" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[10]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1710.10044.pdf">QR-DQN</a>
                                                            (Quantile Regression DQN): Dabney et al, 2017</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id12" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[11]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1707.01495.pdf">HER</a>
                                                            (Hindsight Experience Replay): Andrychowicz et al, 2017</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id13" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[12]</td>
                                                        <td><a class="reference external"
                                                                href="https://worldmodels.github.io/">World Models</a>:
                                                            Ha and Schmidhuber, 2018</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id14" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[13]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1707.06203.pdf">I2A</a>
                                                            (Imagination-Augmented Agents): Weber et al, 2017</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id15" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[14]</td>
                                                        <td><a class="reference external"
                                                                href="https://sites.google.com/view/mbmf">MBMF</a>
                                                            (Model-Based RL with Model-Free Fine-Tuning): Nagabandi et
                                                            al, 2017</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id16" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[15]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1803.00101.pdf">MBVE</a>
                                                            (Model-Based Value Expansion): Feinberg et al, 2018</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id17" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[16]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1712.01815.pdf">AlphaZero</a>:
                                                            Silver et al, 2017</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                        </div>
                                    </div>
                                    <span id="document-spinningup/rl_intro3"></span>
                                    <div class="section" id="part-3-intro-to-policy-optimization">
                                        <h2><a class="toc-backref" href="#id7">Part 3: Intro to Policy
                                                Optimization</a><a class="headerlink"
                                                href="#part-3-intro-to-policy-optimization"
                                                title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal"
                                                        href="#part-3-intro-to-policy-optimization" id="id7">Part 3:
                                                        Intro to Policy Optimization</a>
                                                    <ul>
                                                        <li><a class="reference internal"
                                                                href="#deriving-the-simplest-policy-gradient"
                                                                id="id8">Deriving the Simplest Policy Gradient</a></li>
                                                        <li><a class="reference internal"
                                                                href="#implementing-the-simplest-policy-gradient"
                                                                id="id9">Implementing the Simplest Policy Gradient</a>
                                                        </li>
                                                        <li><a class="reference internal"
                                                                href="#expected-grad-log-prob-lemma" id="id10">Expected
                                                                Grad-Log-Prob Lemma</a></li>
                                                        <li><a class="reference internal"
                                                                href="#don-t-let-the-past-distract-you"
                                                                id="id11">Don&#8217;t Let the Past Distract You</a></li>
                                                        <li><a class="reference internal"
                                                                href="#implementing-reward-to-go-policy-gradient"
                                                                id="id12">Implementing Reward-to-Go Policy Gradient</a>
                                                        </li>
                                                        <li><a class="reference internal"
                                                                href="#baselines-in-policy-gradients"
                                                                id="id13">Baselines in Policy Gradients</a></li>
                                                        <li><a class="reference internal"
                                                                href="#other-forms-of-the-policy-gradient"
                                                                id="id14">Other Forms of the Policy Gradient</a></li>
                                                        <li><a class="reference internal" href="#recap"
                                                                id="id15">Recap</a></li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <p>In this section, we&#8217;ll discuss the mathematical foundations of policy
                                            optimization algorithms, and connect the material to sample code. We will
                                            cover three key results in the theory of <strong>policy gradients</strong>:
                                        </p>
                                        <ul class="simple">
                                            <li><a class="reference external"
                                                    href="../spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient">the
                                                    simplest equation</a> describing the gradient of policy performance
                                                with respect to policy parameters,</li>
                                            <li>a rule which allows us to <a class="reference external"
                                                    href="../spinningup/rl_intro3.html#don-t-let-the-past-distract-you">drop
                                                    useless terms</a> from that expression,</li>
                                            <li>and a rule which allows us to <a class="reference external"
                                                    href="../spinningup/rl_intro3.html#baselines-in-policy-gradients">add
                                                    useful terms</a> to that expression.</li>
                                        </ul>
                                        <p>In the end, we&#8217;ll tie those results together and describe the
                                            advantage-based expression for the policy gradient&#8212;the version we use
                                            in our <a class="reference external" href="../algorithms/vpg.html">Vanilla
                                                Policy Gradient</a> implementation.</p>
                                        <div class="section" id="deriving-the-simplest-policy-gradient">
                                            <h3><a class="toc-backref" href="#id8">Deriving the Simplest Policy
                                                    Gradient</a><a class="headerlink"
                                                    href="#deriving-the-simplest-policy-gradient"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>Here, we consider the case of a stochastic, parameterized policy, <img
                                                    class="math"
                                                    src="_images/math/6a71f04b65d9524fb656715cda85d7540a9ddf9f.svg"
                                                    alt="\pi_{\theta}" />. We aim to maximize the expected return <img
                                                    class="math"
                                                    src="_images/math/42bfa236d63e32e501baf89345748061540e102d.svg"
                                                    alt="J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{R(\tau)}" />.
                                                For the purposes of this derivation, we&#8217;ll take <img class="math"
                                                    src="_images/math/c2d6738c058406ade40dcf870311db157ed80e0f.svg"
                                                    alt="R(\tau)" /> to give the <a class="reference external"
                                                    href="../spinningup/rl_intro.html#reward-and-return">finite-horizon
                                                    undiscounted return</a>, but the derivation for the infinite-horizon
                                                discounted return setting is almost identical.</p>
                                            <p>We would like to optimize the policy by gradient ascent, eg</p>
                                            <div class="math">
                                                <p><img src="_images/math/595de3acc68fed2ec24c11420d9abbee013497ac.svg"
                                                        alt="\theta_{k+1} = \theta_k + \alpha \left. \nabla_{\theta} J(\pi_{\theta}) \right|_{\theta_k}." />
                                                </p>
                                            </div>
                                            <p>The gradient of policy performance, <img class="math"
                                                    src="_images/math/fdc185c68404ece5c4deef076c9713af689421a2.svg"
                                                    alt="\nabla_{\theta} J(\pi_{\theta})" />, is called the
                                                <strong>policy gradient</strong>, and algorithms that optimize the
                                                policy this way are called <strong>policy gradient algorithms.</strong>
                                                (Examples include Vanilla Policy Gradient and TRPO. PPO is often
                                                referred to as a policy gradient algorithm, though this is slightly
                                                inaccurate.)</p>
                                            <p>To actually use this algorithm, we need an expression for the policy
                                                gradient which we can numerically compute. This involves two steps: 1)
                                                deriving the analytical gradient of policy performance, which turns out
                                                to have the form of an expected value, and then 2) forming a sample
                                                estimate of that expected value, which can be computed with data from a
                                                finite number of agent-environment interaction steps.</p>
                                            <p>In this subsection, we&#8217;ll find the simplest form of that
                                                expression. In later subsections, we&#8217;ll show how to improve on the
                                                simplest form to get the version we actually use in standard policy
                                                gradient implementations.</p>
                                            <p>We&#8217;ll begin by laying out a few facts which are useful for deriving
                                                the analytical gradient.</p>
                                            <p><strong>1. Probability of a Trajectory.</strong> The probability of a
                                                trajectory <img class="math"
                                                    src="_images/math/59f955ca56f900cb9be25620d7c974afc0e77f32.svg"
                                                    alt="\tau = (s_0, a_0, ..., s_{T+1})" /> given that actions come
                                                from <img class="math"
                                                    src="_images/math/6a71f04b65d9524fb656715cda85d7540a9ddf9f.svg"
                                                    alt="\pi_{\theta}" /> is</p>
                                            <div class="math">
                                                <p><img src="_images/math/cbc185e90569437c9216ea06df6e91244df1972b.svg"
                                                        alt="P(\tau|\theta) = \rho_0 (s_0) \prod_{t=0}^{T} P(s_{t+1}|s_t, a_t) \pi_{\theta}(a_t |s_t)." />
                                                </p>
                                            </div>
                                            <p><strong>2. The Log-Derivative Trick.</strong> The log-derivative trick is
                                                based on a simple rule from calculus: the derivative of <img
                                                    class="math"
                                                    src="_images/math/fe3fe64e081ed066a8e6a1719b6f3fc8ed9b98dc.svg"
                                                    alt="\log x" /> with respect to <img class="math"
                                                    src="_images/math/ea07a4204f1f53321f76d9c7e348199f0d707db1.svg"
                                                    alt="x" /> is <img class="math"
                                                    src="_images/math/37ea41f23266fbdbb332134f34f65bc547e3601d.svg"
                                                    alt="1/x" />. When rearranged and combined with chain rule, we get:
                                            </p>
                                            <div class="math">
                                                <p><img src="_images/math/8025b1c01b73e7f373fa438c15743c5391e2f28f.svg"
                                                        alt="\nabla_{\theta} P(\tau | \theta) = P(\tau | \theta) \nabla_{\theta} \log P(\tau | \theta)." />
                                                </p>
                                            </div>
                                            <p><strong>3. Log-Probability of a Trajectory.</strong> The log-prob of a
                                                trajectory is just</p>
                                            <div class="math">
                                                <p><img src="_images/math/2c8b420444accb2f54391017f28ab21b97bab0ee.svg"
                                                        alt="\log P(\tau|\theta) = \log \rho_0 (s_0) + \sum_{t=0}^{T} \bigg( \log P(s_{t+1}|s_t, a_t)  + \log \pi_{\theta}(a_t |s_t)\bigg)." />
                                                </p>
                                            </div>
                                            <p><strong>4. Gradients of Environment Functions.</strong> The environment
                                                has no dependence on <img class="math"
                                                    src="_images/math/ce5edddd490112350f4bd555d9390e0e845f754a.svg"
                                                    alt="\theta" />, so gradients of <img class="math"
                                                    src="_images/math/cd870494ffa26602c4ede5257356d572045ebb9e.svg"
                                                    alt="\rho_0(s_0)" />, <img class="math"
                                                    src="_images/math/d5f6af878d6a1071f29a9dda286b52ef53ece0e3.svg"
                                                    alt="P(s_{t+1}|s_t, a_t)" />, and <img class="math"
                                                    src="_images/math/c2d6738c058406ade40dcf870311db157ed80e0f.svg"
                                                    alt="R(\tau)" /> are zero.</p>
                                            <p><strong>5. Grad-Log-Prob of a Trajectory.</strong> The gradient of the
                                                log-prob of a trajectory is thus</p>
                                            <div class="math">
                                                <p><img src="_images/math/3ef66d94ee26cfa69015915dbd112ea78fb5e7ba.svg"
                                                        alt="\nabla_{\theta} \log P(\tau | \theta) &amp;= \cancel{\nabla_{\theta} \log \rho_0 (s_0)} + \sum_{t=0}^{T} \bigg( \cancel{\nabla_{\theta} \log P(s_{t+1}|s_t, a_t)}  + \nabla_{\theta} \log \pi_{\theta}(a_t |s_t)\bigg) \\
&amp;= \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t)." /></p>
                                            </div>
                                            <p>Putting it all together, we derive the following:</p>
                                            <div class="admonition-derivation-for-basic-policy-gradient admonition">
                                                <p class="first admonition-title">Derivation for Basic Policy Gradient
                                                </p>
                                                <div class="last math">
                                                    <p><img src="_images/math/b5e135d2ae389147267372abc1c5b20e644ec881.svg"
                                                            alt="\begin{align*}
\nabla_{\theta} J(\pi_{\theta}) &amp;= \nabla_{\theta} \underE{\tau \sim \pi_{\theta}}{R(\tau)} &amp; \\
&amp;= \nabla_{\theta} \int_{\tau} P(\tau|\theta) R(\tau) &amp; \text{Expand expectation} \\
&amp;= \int_{\tau} \nabla_{\theta} P(\tau|\theta) R(\tau) &amp; \text{Bring gradient under integral} \\
&amp;= \int_{\tau} P(\tau|\theta) \nabla_{\theta} \log P(\tau|\theta) R(\tau) &amp; \text{Log-derivative trick} \\
&amp;= \underE{\tau \sim \pi_{\theta}}{\nabla_{\theta} \log P(\tau|\theta) R(\tau)} &amp; \text{Return to expectation form} \\
\therefore \nabla_{\theta} J(\pi_{\theta}) &amp;= \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) R(\tau)} &amp; \text{Expression for grad-log-prob}
\end{align*}" /></p>
                                                </div>
                                            </div>
                                            <p>This is an expectation, which means that we can estimate it with a sample
                                                mean. If we collect a set of trajectories <img class="math"
                                                    src="_images/math/fcfe0c7093afb2538a298abf5bf8dc0bc435fbb0.svg"
                                                    alt="\mathcal{D} = \{\tau_i\}_{i=1,...,N}" /> where each trajectory
                                                is obtained by letting the agent act in the environment using the policy
                                                <img class="math"
                                                    src="_images/math/6a71f04b65d9524fb656715cda85d7540a9ddf9f.svg"
                                                    alt="\pi_{\theta}" />, the policy gradient can be estimated with</p>
                                            <div class="math">
                                                <p><img src="_images/math/3d29a18c0f98b1cdb656ecdf261ee37ffe8bb74b.svg"
                                                        alt="\hat{g} = \frac{1}{|\mathcal{D}|} \sum_{\tau \in \mathcal{D}} \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) R(\tau)," />
                                                </p>
                                            </div>
                                            <p>where <img class="math"
                                                    src="_images/math/9818d8f4b97ecbea56ebfb3c0a8890faca55c4de.svg"
                                                    alt="|\mathcal{D}|" /> is the number of trajectories in <img
                                                    class="math"
                                                    src="_images/math/8c5790e22c3e4fca88012b71d8b024c66699cba5.svg"
                                                    alt="\mathcal{D}" /> (here, <img class="math"
                                                    src="_images/math/9d2283e49e17a5e18105bf77e0cc9382192d2510.svg"
                                                    alt="N" />).</p>
                                            <p>This last expression is the simplest version of the computable expression
                                                we desired. Assuming that we have represented our policy in a way which
                                                allows us to calculate <img class="math"
                                                    src="_images/math/0a5342e24606940a10098d0cdf85d5e5e346ed72.svg"
                                                    alt="\nabla_{\theta} \log \pi_{\theta}(a|s)" />, and if we are able
                                                to run the policy in the environment to collect the trajectory dataset,
                                                we can compute the policy gradient and take an update step.</p>
                                        </div>
                                        <div class="section" id="implementing-the-simplest-policy-gradient">
                                            <h3><a class="toc-backref" href="#id9">Implementing the Simplest Policy
                                                    Gradient</a><a class="headerlink"
                                                    href="#implementing-the-simplest-policy-gradient"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>We give a short PyTorch implementation of this simple version of the
                                                policy gradient algorithm in <code
                                                    class="docutils literal"><span class="pre">spinup/examples/pytorch/pg_math/1_simple_pg.py</span></code>.
                                                (It can also be viewed <a class="reference external"
                                                    href="https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/1_simple_pg.py">on
                                                    github</a>.) It is only 128 lines long, so we highly recommend
                                                reading through it in depth. While we won&#8217;t go through the
                                                entirety of the code here, we&#8217;ll highlight and explain a few
                                                important pieces.</p>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p class="last">This section was previously written with a Tensorflow
                                                    example. The old Tensorflow section can be found <a
                                                        class="reference external"
                                                        href="../spinningup/extra_tf_pg_implementation.html#implementing-the-simplest-policy-gradient">here</a>.
                                                </p>
                                            </div>
                                            <p><strong>1. Making the Policy Network.</strong></p>
                                            <div class="highlight-python">
                                                <table class="highlighttable">
                                                    <tr>
                                                        <td class="linenos">
                                                            <div class="linenodiv">
                                                                <pre>30
31
32
33
34
35
36
37
38
39
40</pre>
                                                            </div>
                                                        </td>
                                                        <td class="code">
                                                            <div class="highlight">
                                                                <pre><span></span><span class="c1"># make core of policy network</span>
<span class="n">logits_net</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">(</span><span class="n">sizes</span><span class="o">=</span><span class="p">[</span><span class="n">obs_dim</span><span class="p">]</span><span class="o">+</span><span class="n">hidden_sizes</span><span class="o">+</span><span class="p">[</span><span class="n">n_acts</span><span class="p">])</span>

<span class="c1"># make function to compute action distribution</span>
<span class="k">def</span> <span class="nf">get_policy</span><span class="p">(</span><span class="n">obs</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">logits_net</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>

<span class="c1"># make action selection function (outputs int actions, sampled from policy)</span>
<span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">obs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">get_policy</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre>
                                                            </div>
                                                        </td>
                                                    </tr>
                                                </table>
                                            </div>
                                            <p>This block builds modules and functions for using a feedforward neural
                                                network categorical policy. (See the <a class="reference external"
                                                    href="../spinningup/rl_intro.html#stochastic-policies">Stochastic
                                                    Policies</a> section in Part 1 for a refresher.) The output from the
                                                <code
                                                    class="docutils literal"><span class="pre">logits_net</span></code>
                                                module can be used to construct log-probabilities and probabilities for
                                                actions, and the <code
                                                    class="docutils literal"><span class="pre">get_action</span></code>
                                                function samples actions based on probabilities computed from the
                                                logits. (Note: this particular <code
                                                    class="docutils literal"><span class="pre">get_action</span></code>
                                                function assumes that there will only be one <code
                                                    class="docutils literal"><span class="pre">obs</span></code>
                                                provided, and therefore only one integer action output. That&#8217;s why
                                                it uses <code
                                                    class="docutils literal"><span class="pre">.item()</span></code>,
                                                which is used to <a class="reference external"
                                                    href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.item">get
                                                    the contents of a Tensor with only one element</a>.)</p>
                                            <p>A lot of work in this example is getting done by the <code
                                                    class="docutils literal"><span class="pre">Categorical</span></code>
                                                object on L36. This is a PyTorch <code
                                                    class="docutils literal"><span class="pre">Distribution</span></code>
                                                object that wraps up some mathematical functions associated with
                                                probability distributions. In particular, it has a method for sampling
                                                from the distribution (which we use on L40) and a method for computing
                                                log probabilities of given samples (which we use later). Since PyTorch
                                                distributions are really useful for RL, check out <a
                                                    class="reference external"
                                                    href="https://pytorch.org/docs/stable/distributions.html">their
                                                    documentation</a> to get a feel for how they work.</p>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p>Friendly reminder! When we talk about a categorical distribution
                                                    having &#8220;logits,&#8221; what we mean is that the probabilities
                                                    for each outcome are given by the Softmax function of the logits.
                                                    That is, the probability for action <img class="math"
                                                        src="_images/math/b42a5fa0aad66603180aff0fc5e346e98a2364ca.svg"
                                                        alt="j" /> under a categorical distribution with logits <img
                                                        class="math"
                                                        src="_images/math/3292288e05ddeecc31b25cde7cc7aeafff61bf43.svg"
                                                        alt="x_j" /> is:</p>
                                                <div class="last math">
                                                    <p><img src="_images/math/54e96f0cfda9010b97808eae635a2200e1482829.svg"
                                                            alt="p_j = \frac{\exp(x_j)}{\sum_{i} \exp(x_i)}" /></p>
                                                </div>
                                            </div>
                                            <p><strong>2. Making the Loss Function.</strong></p>
                                            <div class="highlight-python">
                                                <table class="highlighttable">
                                                    <tr>
                                                        <td class="linenos">
                                                            <div class="linenodiv">
                                                                <pre>42
43
44
45</pre>
                                                            </div>
                                                        </td>
                                                        <td class="code">
                                                            <div class="highlight">
                                                                <pre><span></span><span class="c1"># make loss function whose gradient, for the right data, is policy gradient</span>
<span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">act</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="n">logp</span> <span class="o">=</span> <span class="n">get_policy</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">logp</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre>
                                                            </div>
                                                        </td>
                                                    </tr>
                                                </table>
                                            </div>
                                            <p>In this block, we build a &#8220;loss&#8221; function for the policy
                                                gradient algorithm. When the right data is plugged in, the gradient of
                                                this loss is equal to the policy gradient. The right data means a set of
                                                (state, action, weight) tuples collected while acting according to the
                                                current policy, where the weight for a state-action pair is the return
                                                from the episode to which it belongs. (Although as we will show in later
                                                subsections, there are other values you can plug in for the weight which
                                                also work correctly.)</p>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p>Even though we describe this as a loss function, it is
                                                    <strong>not</strong> a loss function in the typical sense from
                                                    supervised learning. There are two main differences from standard
                                                    loss functions.</p>
                                                <p><strong>1. The data distribution depends on the parameters.</strong>
                                                    A loss function is usually defined on a fixed data distribution
                                                    which is independent of the parameters we aim to optimize. Not so
                                                    here, where the data must be sampled on the most recent policy.</p>
                                                <p><strong>2. It doesn&#8217;t measure performance.</strong> A loss
                                                    function usually evaluates the performance metric that we care
                                                    about. Here, we care about expected return, <img class="math"
                                                        src="_images/math/96b876944de9cf0f980fe261562e8e07029245bf.svg"
                                                        alt="J(\pi_{\theta})" />, but our &#8220;loss&#8221; function
                                                    does not approximate this at all, even in expectation. This
                                                    &#8220;loss&#8221; function is only useful to us because, when
                                                    evaluated at the current parameters, with data generated by the
                                                    current parameters, it has the negative gradient of performance.</p>
                                                <p>But after that first step of gradient descent, there is no more
                                                    connection to performance. This means that minimizing this
                                                    &#8220;loss&#8221; function, for a given batch of data, has
                                                    <em>no</em> guarantee whatsoever of improving expected return. You
                                                    can send this loss to <img class="math"
                                                        src="_images/math/5d7bd7abcf6c1c07becaa8b5fe1a2a000e559a50.svg"
                                                        alt="-\infty" /> and policy performance could crater; in fact,
                                                    it usually will. Sometimes a deep RL researcher might describe this
                                                    outcome as the policy &#8220;overfitting&#8221; to a batch of data.
                                                    This is descriptive, but should not be taken literally because it
                                                    does not refer to generalization error.</p>
                                                <p class="last">We raise this point because it is common for ML
                                                    practitioners to interpret a loss function as a useful signal during
                                                    training&#8212;&#8221;if the loss goes down, all is well.&#8221; In
                                                    policy gradients, this intuition is wrong, and you should only care
                                                    about average return. The loss function means nothing.</p>
                                            </div>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p>The approach used here to make the <code
                                                        class="docutils literal"><span class="pre">logp</span></code>
                                                    tensor&#8211;calling the <code
                                                        class="docutils literal"><span class="pre">log_prob</span></code>
                                                    method of a PyTorch <code
                                                        class="docutils literal"><span class="pre">Categorical</span></code>
                                                    object&#8211;may require some modification to work with other kinds
                                                    of distribution objects.</p>
                                                <p>For example, if you are using a <a class="reference external"
                                                        href="https://pytorch.org/docs/stable/distributions.html#normal">Normal
                                                        distribution</a> (for a diagonal Gaussian policy), the output
                                                    from calling <code
                                                        class="docutils literal"><span class="pre">policy.log_prob(act)</span></code>
                                                    will give you a Tensor containing separate log probabilities for
                                                    each component of each vector-valued action. That is to say, you put
                                                    in a Tensor of shape <code
                                                        class="docutils literal"><span class="pre">(batch,</span> <span class="pre">act_dim)</span></code>,
                                                    and get out a Tensor of shape <code
                                                        class="docutils literal"><span class="pre">(batch,</span> <span class="pre">act_dim)</span></code>,
                                                    when what you need for making an RL loss is a Tensor of shape <code
                                                        class="docutils literal"><span class="pre">(batch,)</span></code>.
                                                    In that case, you would sum up the log probabilities of the action
                                                    components to get the log probabilities of the actions. That is, you
                                                    would compute:</p>
                                                <div class="last highlight-python">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">logp</span> <span class="o">=</span> <span class="n">get_policy</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">act</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre>
                                                    </div>
                                                </div>
                                            </div>
                                            <p><strong>3. Running One Epoch of Training.</strong></p>
                                            <div class="highlight-python">
                                                <table class="highlighttable">
                                                    <tr>
                                                        <td class="linenos">
                                                            <div class="linenodiv">
                                                                <pre> 50
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
 64
 65
 66
 67
 68
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112</pre>
                                                            </div>
                                                        </td>
                                                        <td class="code">
                                                            <div class="highlight">
                                                                <pre><span></span><span class="c1"># for training policy</span>
<span class="k">def</span> <span class="nf">train_one_epoch</span><span class="p">():</span>
    <span class="c1"># make some empty lists for logging.</span>
    <span class="n">batch_obs</span> <span class="o">=</span> <span class="p">[]</span>          <span class="c1"># for observations</span>
    <span class="n">batch_acts</span> <span class="o">=</span> <span class="p">[]</span>         <span class="c1"># for actions</span>
    <span class="n">batch_weights</span> <span class="o">=</span> <span class="p">[]</span>      <span class="c1"># for R(tau) weighting in policy gradient</span>
    <span class="n">batch_rets</span> <span class="o">=</span> <span class="p">[]</span>         <span class="c1"># for measuring episode returns</span>
    <span class="n">batch_lens</span> <span class="o">=</span> <span class="p">[]</span>         <span class="c1"># for measuring episode lengths</span>

    <span class="c1"># reset episode-specific variables</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>       <span class="c1"># first obs comes from starting distribution</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>            <span class="c1"># signal from environment that episode is over</span>
    <span class="n">ep_rews</span> <span class="o">=</span> <span class="p">[]</span>            <span class="c1"># list for rewards accrued throughout ep</span>

    <span class="c1"># render first episode of each epoch</span>
    <span class="n">finished_rendering_this_epoch</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="c1"># collect experience by acting in the environment with current policy</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>

        <span class="c1"># rendering</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">finished_rendering_this_epoch</span><span class="p">)</span> <span class="ow">and</span> <span class="n">render</span><span class="p">:</span>
            <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>

        <span class="c1"># save obs</span>
        <span class="n">batch_obs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">obs</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

        <span class="c1"># act in the environment</span>
        <span class="n">act</span> <span class="o">=</span> <span class="n">get_action</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="n">obs</span><span class="p">,</span> <span class="n">rew</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>

        <span class="c1"># save action, reward</span>
        <span class="n">batch_acts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>
        <span class="n">ep_rews</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rew</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="c1"># if episode is over, record info about episode</span>
            <span class="n">ep_ret</span><span class="p">,</span> <span class="n">ep_len</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">ep_rews</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">ep_rews</span><span class="p">)</span>
            <span class="n">batch_rets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ep_ret</span><span class="p">)</span>
            <span class="n">batch_lens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ep_len</span><span class="p">)</span>

            <span class="c1"># the weight for each logprob(a|s) is R(tau)</span>
            <span class="n">batch_weights</span> <span class="o">+=</span> <span class="p">[</span><span class="n">ep_ret</span><span class="p">]</span> <span class="o">*</span> <span class="n">ep_len</span>

            <span class="c1"># reset episode-specific variables</span>
            <span class="n">obs</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">ep_rews</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(),</span> <span class="bp">False</span><span class="p">,</span> <span class="p">[]</span>

            <span class="c1"># won&#39;t render again this epoch</span>
            <span class="n">finished_rendering_this_epoch</span> <span class="o">=</span> <span class="bp">True</span>

            <span class="c1"># end experience loop if we have enough of it</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_obs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">batch_size</span><span class="p">:</span>
                <span class="k">break</span>

    <span class="c1"># take a single policy gradient update step</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">batch_loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">obs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">batch_obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
                              <span class="n">act</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">batch_acts</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
                              <span class="n">weights</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">batch_weights</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
                              <span class="p">)</span>
    <span class="n">batch_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">batch_loss</span><span class="p">,</span> <span class="n">batch_rets</span><span class="p">,</span> <span class="n">batch_lens</span>
</pre>
                                                            </div>
                                                        </td>
                                                    </tr>
                                                </table>
                                            </div>
                                            <p>The <code
                                                    class="docutils literal"><span class="pre">train_one_epoch()</span></code>
                                                function runs one &#8220;epoch&#8221; of policy gradient, which we
                                                define to be</p>
                                            <ol class="arabic simple">
                                                <li>the experience collection step (L67-102), where the agent acts for
                                                    some number of episodes in the environment using the most recent
                                                    policy, followed by</li>
                                                <li>a single policy gradient update step (L104-111).</li>
                                            </ol>
                                            <p>The main loop of the algorithm just repeatedly calls <code
                                                    class="docutils literal"><span class="pre">train_one_epoch()</span></code>.
                                            </p>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p class="last">If you aren&#8217;t already familiar with optimization
                                                    in PyTorch, observe the pattern for taking one gradient descent step
                                                    as shown in lines 104-111. First, clear the gradient buffers. Then,
                                                    compute the loss function. Then, compute a backward pass on the loss
                                                    function; this accumulates fresh gradients into the gradient
                                                    buffers. Finally, take a step with the optimizer.</p>
                                            </div>
                                        </div>
                                        <div class="section" id="expected-grad-log-prob-lemma">
                                            <h3><a class="toc-backref" href="#id10">Expected Grad-Log-Prob Lemma</a><a
                                                    class="headerlink" href="#expected-grad-log-prob-lemma"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>In this subsection, we will derive an intermediate result which is
                                                extensively used throughout the theory of policy gradients. We will call
                                                it the Expected Grad-Log-Prob (EGLP) lemma. <a
                                                    class="footnote-reference" href="#id2" id="id1">[1]</a></p>
                                            <p><strong>EGLP Lemma.</strong> Suppose that <img class="math"
                                                    src="_images/math/9c44674c334586fd6d417281ea223857ea3ee0d6.svg"
                                                    alt="P_{\theta}" /> is a parameterized probability distribution over
                                                a random variable, <img class="math"
                                                    src="_images/math/ea07a4204f1f53321f76d9c7e348199f0d707db1.svg"
                                                    alt="x" />. Then:</p>
                                            <div class="math">
                                                <p><img src="_images/math/f7a02965e7c07537dfb98391da78ab7613e887f7.svg"
                                                        alt="\underE{x \sim P_{\theta}}{\nabla_{\theta} \log P_{\theta}(x)} = 0." />
                                                </p>
                                            </div>
                                            <div class="admonition-proof admonition">
                                                <p class="first admonition-title">Proof</p>
                                                <p>Recall that all probability distributions are <em>normalized</em>:
                                                </p>
                                                <div class="math">
                                                    <p><img src="_images/math/cc70eeb5c704222ca73cf6d2a7b28b8ae2f2e2aa.svg"
                                                            alt="\int_x P_{\theta}(x) = 1." /></p>
                                                </div>
                                                <p>Take the gradient of both sides of the normalization condition:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/acde4d18e221ccd91e93e6ca4e25ae84d05f29d8.svg"
                                                            alt="\nabla_{\theta} \int_x P_{\theta}(x) = \nabla_{\theta} 1 = 0." />
                                                    </p>
                                                </div>
                                                <p>Use the log derivative trick to get:</p>
                                                <div class="last math">
                                                    <p><img src="_images/math/479e34ecc40103079f87e46e47837f3c066d30bd.svg"
                                                            alt="0 &amp;= \nabla_{\theta} \int_x P_{\theta}(x) \\
&amp;= \int_x \nabla_{\theta} P_{\theta}(x) \\
&amp;= \int_x P_{\theta}(x) \nabla_{\theta} \log P_{\theta}(x) \\
\therefore 0 &amp;= \underE{x \sim P_{\theta}}{\nabla_{\theta} \log P_{\theta}(x)}." /></p>
                                                </div>
                                            </div>
                                            <table class="docutils footnote" frame="void" id="id2" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label"><a class="fn-backref" href="#id1">[1]</a></td>
                                                        <td>The author of this article is not aware of this lemma being
                                                            given a standard name anywhere in the literature. But given
                                                            how often it comes up, it seems pretty worthwhile to give it
                                                            some kind of name for ease of reference.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                        </div>
                                        <div class="section" id="don-t-let-the-past-distract-you">
                                            <h3><a class="toc-backref" href="#id11">Don&#8217;t Let the Past Distract
                                                    You</a><a class="headerlink" href="#don-t-let-the-past-distract-you"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>Examine our most recent expression for the policy gradient:</p>
                                            <div class="math">
                                                <p><img src="_images/math/e8b721fa0eb7fa2aa4b088106518b3ee88ff7707.svg"
                                                        alt="\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) R(\tau)}." />
                                                </p>
                                            </div>
                                            <p>Taking a step with this gradient pushes up the log-probabilities of each
                                                action in proportion to <img class="math"
                                                    src="_images/math/c2d6738c058406ade40dcf870311db157ed80e0f.svg"
                                                    alt="R(\tau)" />, the sum of <em>all rewards ever obtained</em>. But
                                                this doesn&#8217;t make much sense.</p>
                                            <p>Agents should really only reinforce actions on the basis of their
                                                <em>consequences</em>. Rewards obtained before taking an action have no
                                                bearing on how good that action was: only rewards that come
                                                <em>after</em>.</p>
                                            <p>It turns out that this intuition shows up in the math, and we can show
                                                that the policy gradient can also be expressed by</p>
                                            <div class="math">
                                                <p><img src="_images/math/62e6b4e06a1c35fac29e94103988cdc6e940660b.svg"
                                                        alt="\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1})}." />
                                                </p>
                                            </div>
                                            <p>In this form, actions are only reinforced based on rewards obtained after
                                                they are taken.</p>
                                            <p>We&#8217;ll call this form the &#8220;reward-to-go policy
                                                gradient,&#8221; because the sum of rewards after a point in a
                                                trajectory,</p>
                                            <div class="math">
                                                <p><img src="_images/math/d299609cb8b73f294e77708f9cdc6ea0024b6c6c.svg"
                                                        alt="\hat{R}_t \doteq \sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1})," />
                                                </p>
                                            </div>
                                            <p>is called the <strong>reward-to-go</strong> from that point, and this
                                                policy gradient expression depends on the reward-to-go from state-action
                                                pairs.</p>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p class="last"><strong>But how is this better?</strong> A key problem
                                                    with policy gradients is how many sample trajectories are needed to
                                                    get a low-variance sample estimate for them. The formula we started
                                                    with included terms for reinforcing actions proportional to past
                                                    rewards, all of which had zero mean, but nonzero variance: as a
                                                    result, they would just add noise to sample estimates of the policy
                                                    gradient. By removing them, we reduce the number of sample
                                                    trajectories needed.</p>
                                            </div>
                                            <p>An (optional) proof of this claim can be found <a href="#id16"><span
                                                        class="problematic" id="id17">`here`_</span></a>, and it
                                                ultimately depends on the EGLP lemma.</p>
                                        </div>
                                        <div class="section" id="implementing-reward-to-go-policy-gradient">
                                            <h3><a class="toc-backref" href="#id12">Implementing Reward-to-Go Policy
                                                    Gradient</a><a class="headerlink"
                                                    href="#implementing-reward-to-go-policy-gradient"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>We give a short PyTorch implementation of the reward-to-go policy
                                                gradient in <code
                                                    class="docutils literal"><span class="pre">spinup/examples/pytorch/pg_math/2_rtg_pg.py</span></code>.
                                                (It can also be viewed <a class="reference external"
                                                    href="https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/2_rtg_pg.py">on
                                                    github</a>.)</p>
                                            <p>The only thing that has changed from <code
                                                    class="docutils literal"><span class="pre">1_simple_pg.py</span></code>
                                                is that we now use different weights in the loss function. The code
                                                modification is very slight: we add a new function, and change two other
                                                lines. The new function is:</p>
                                            <div class="highlight-python">
                                                <table class="highlighttable">
                                                    <tr>
                                                        <td class="linenos">
                                                            <div class="linenodiv">
                                                                <pre>17
18
19
20
21
22</pre>
                                                            </div>
                                                        </td>
                                                        <td class="code">
                                                            <div class="highlight">
                                                                <pre><span></span><span class="k">def</span> <span class="nf">reward_to_go</span><span class="p">(</span><span class="n">rews</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">rews</span><span class="p">)</span>
    <span class="n">rtgs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">rews</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)):</span>
        <span class="n">rtgs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">rews</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">rtgs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rtgs</span>
</pre>
                                                            </div>
                                                        </td>
                                                    </tr>
                                                </table>
                                            </div>
                                            <p>And then we tweak the old L91-92 from:</p>
                                            <div class="highlight-python">
                                                <table class="highlighttable">
                                                    <tr>
                                                        <td class="linenos">
                                                            <div class="linenodiv">
                                                                <pre>91
92</pre>
                                                            </div>
                                                        </td>
                                                        <td class="code">
                                                            <div class="highlight">
                                                                <pre><span></span>                <span class="c1"># the weight for each logprob(a|s) is R(tau)</span>
                <span class="n">batch_weights</span> <span class="o">+=</span> <span class="p">[</span><span class="n">ep_ret</span><span class="p">]</span> <span class="o">*</span> <span class="n">ep_len</span>
</pre>
                                                            </div>
                                                        </td>
                                                    </tr>
                                                </table>
                                            </div>
                                            <p>to:</p>
                                            <div class="highlight-python">
                                                <table class="highlighttable">
                                                    <tr>
                                                        <td class="linenos">
                                                            <div class="linenodiv">
                                                                <pre>98
99</pre>
                                                            </div>
                                                        </td>
                                                        <td class="code">
                                                            <div class="highlight">
                                                                <pre><span></span>                <span class="c1"># the weight for each logprob(a_t|s_t) is reward-to-go from t</span>
                <span class="n">batch_weights</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="n">reward_to_go</span><span class="p">(</span><span class="n">ep_rews</span><span class="p">))</span>
</pre>
                                                            </div>
                                                        </td>
                                                    </tr>
                                                </table>
                                            </div>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p class="last">This section was previously written with a Tensorflow
                                                    example. The old Tensorflow section can be found <a
                                                        class="reference external"
                                                        href="../spinningup/extra_tf_pg_implementation.html#implementing-reward-to-go-policy-gradient">here</a>.
                                                </p>
                                            </div>
                                        </div>
                                        <div class="section" id="baselines-in-policy-gradients">
                                            <h3><a class="toc-backref" href="#id13">Baselines in Policy Gradients</a><a
                                                    class="headerlink" href="#baselines-in-policy-gradients"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>An immediate consequence of the EGLP lemma is that for any function <img
                                                    class="math"
                                                    src="_images/math/99ac829ad51642abad0797da299214e7ce1da722.svg"
                                                    alt="b" /> which only depends on state,</p>
                                            <div class="math">
                                                <p><img src="_images/math/3bedd2ab2262f396f232d49c8c85621ce5397955.svg"
                                                        alt="\underE{a_t \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t|s_t) b(s_t)} = 0." />
                                                </p>
                                            </div>
                                            <p>This allows us to add or subtract any number of terms like this from our
                                                expression for the policy gradient, without changing it in expectation:
                                            </p>
                                            <div class="math">
                                                <p><img src="_images/math/3a111dcb6e04aa632bd69e9a7e769e06e2530a0a.svg"
                                                        alt="\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \left(\sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1}) - b(s_t)\right)}." />
                                                </p>
                                            </div>
                                            <p>Any function <img class="math"
                                                    src="_images/math/99ac829ad51642abad0797da299214e7ce1da722.svg"
                                                    alt="b" /> used in this way is called a <strong>baseline</strong>.
                                            </p>
                                            <p>The most common choice of baseline is the <a class="reference external"
                                                    href="../spinningup/rl_intro.html#value-functions">on-policy value
                                                    function</a> <img class="math"
                                                    src="_images/math/263857fcb5eaaf46a4e9a5e6ce2be414d95748aa.svg"
                                                    alt="V^{\pi}(s_t)" />. Recall that this is the average return an
                                                agent gets if it starts in state <img class="math"
                                                    src="_images/math/4fcf0bf03c2a691496ce04ade269159d8b89caa5.svg"
                                                    alt="s_t" /> and then acts according to policy <img class="math"
                                                    src="_images/math/1ae2bd722da01b3a89ffc139af2437c28364a966.svg"
                                                    alt="\pi" /> for the rest of its life.</p>
                                            <p>Empirically, the choice <img class="math"
                                                    src="_images/math/67567dbe29590f80b62fd2221481df61e502a450.svg"
                                                    alt="b(s_t) = V^{\pi}(s_t)" /> has the desirable effect of reducing
                                                variance in the sample estimate for the policy gradient. This results in
                                                faster and more stable policy learning. It is also appealing from a
                                                conceptual angle: it encodes the intuition that if an agent gets what it
                                                expected, it should &#8220;feel&#8221; neutral about it.</p>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p>In practice, <img class="math"
                                                        src="_images/math/263857fcb5eaaf46a4e9a5e6ce2be414d95748aa.svg"
                                                        alt="V^{\pi}(s_t)" /> cannot be computed exactly, so it has to
                                                    be approximated. This is usually done with a neural network, <img
                                                        class="math"
                                                        src="_images/math/d313ca98e5fb043c581841a09ea19bce2ce53b04.svg"
                                                        alt="V_{\phi}(s_t)" />, which is updated concurrently with the
                                                    policy (so that the value network always approximates the value
                                                    function of the most recent policy).</p>
                                                <p>The simplest method for learning <img class="math"
                                                        src="_images/math/4be08f55c234dcd60641540fb682afc3e988ae17.svg"
                                                        alt="V_{\phi}" />, used in most implementations of policy
                                                    optimization algorithms (including VPG, TRPO, PPO, and A2C), is to
                                                    minimize a mean-squared-error objective:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/a82208dd637243514710948c4ebbc3c59e9a2e57.svg"
                                                            alt="\phi_k = \arg \min_{\phi} \underE{s_t, \hat{R}_t \sim \pi_k}{\left( V_{\phi}(s_t) - \hat{R}_t \right)^2}," />
                                                    </p>
                                                </div>
                                                <div class="line-block">
                                                    <div class="line"><br /></div>
                                                </div>
                                                <p class="last">where <img class="math"
                                                        src="_images/math/35e7c0260233cac2eb0745c92a34cfaa21e558cb.svg"
                                                        alt="\pi_k" /> is the policy at epoch <img class="math"
                                                        src="_images/math/a29aa94bd66ac7a6bb3195233fd9a9df8575af86.svg"
                                                        alt="k" />. This is done with one or more steps of gradient
                                                    descent, starting from the previous value parameters <img
                                                        class="math"
                                                        src="_images/math/ba40df6b8d4ee942c6856a8d919ce01fc92096f6.svg"
                                                        alt="\phi_{k-1}" />.</p>
                                            </div>
                                        </div>
                                        <div class="section" id="other-forms-of-the-policy-gradient">
                                            <h3><a class="toc-backref" href="#id14">Other Forms of the Policy
                                                    Gradient</a><a class="headerlink"
                                                    href="#other-forms-of-the-policy-gradient"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>What we have seen so far is that the policy gradient has the general form
                                            </p>
                                            <div class="math">
                                                <p><img src="_images/math/eb524fc4ce3052c9058d2221471ac8b302c9c023.svg"
                                                        alt="\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \Phi_t}," />
                                                </p>
                                            </div>
                                            <p>where <img class="math"
                                                    src="_images/math/7bec6ad1be0afd7200e874effc2db6d9cafb3ea4.svg"
                                                    alt="\Phi_t" /> could be any of</p>
                                            <div class="math">
                                                <p><img src="_images/math/cecde3d5124076dfc773c8fa658e61f41cb3efc2.svg"
                                                        alt="\Phi_t &amp;= R(\tau)," /></p>
                                            </div>
                                            <p>or</p>
                                            <div class="math">
                                                <p><img src="_images/math/02a2c10508e4a4c018634a2ba03384350faa7cab.svg"
                                                        alt="\Phi_t &amp;= \sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1})," />
                                                </p>
                                            </div>
                                            <p>or</p>
                                            <div class="math">
                                                <p><img src="_images/math/65fd02144cdac143a61396dc8fe585e8db5f7d81.svg"
                                                        alt="\Phi_t &amp;= \sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1}) - b(s_t)." />
                                                </p>
                                            </div>
                                            <p>All of these choices lead to the same expected value for the policy
                                                gradient, despite having different variances. It turns out that there
                                                are two more valid choices of weights <img class="math"
                                                    src="_images/math/7bec6ad1be0afd7200e874effc2db6d9cafb3ea4.svg"
                                                    alt="\Phi_t" /> which are important to know.</p>
                                            <p><strong>1. On-Policy Action-Value Function.</strong> The choice</p>
                                            <div class="math">
                                                <p><img src="_images/math/90381ce64af2629806c052198d4a0851ff6be36b.svg"
                                                        alt="\Phi_t = Q^{\pi_{\theta}}(s_t, a_t)" /></p>
                                            </div>
                                            <p>is also valid. See <a class="reference external"
                                                    href="../spinningup/extra_pg_proof2.html">this page</a> for an
                                                (optional) proof of this claim.</p>
                                            <p><strong>2. The Advantage Function.</strong> Recall that the <a
                                                    class="reference external"
                                                    href="../spinningup/rl_intro.html#advantage-functions">advantage of
                                                    an action</a>, defined by <img class="math"
                                                    src="_images/math/e2d2e497244979ed8fe62fe2e994a6b49039580c.svg"
                                                    alt="A^{\pi}(s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t)" />,
                                                describes how much better or worse it is than other actions on average
                                                (relative to the current policy). This choice,</p>
                                            <div class="math">
                                                <p><img src="_images/math/e67e0f92306202526b49c7d0a7a6239ba68a7abc.svg"
                                                        alt="\Phi_t = A^{\pi_{\theta}}(s_t, a_t)" /></p>
                                            </div>
                                            <p>is also valid. The proof is that it&#8217;s equivalent to using <img
                                                    class="math"
                                                    src="_images/math/71febd8462276063cccd137802934788492dc5f1.svg"
                                                    alt="\Phi_t = Q^{\pi_{\theta}}(s_t, a_t)" /> and then using a value
                                                function baseline, which we are always free to do.</p>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p class="last">The formulation of policy gradients with advantage
                                                    functions is extremely common, and there are many different ways of
                                                    estimating the advantage function used by different algorithms.</p>
                                            </div>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p>For a more detailed treatment of this topic, you should read the
                                                    paper on <a class="reference external"
                                                        href="https://arxiv.org/pdf/1506.02438.pdf">Generalized
                                                        Advantage Estimation</a> (GAE), which goes into depth about
                                                    different choices of <img class="math"
                                                        src="_images/math/7bec6ad1be0afd7200e874effc2db6d9cafb3ea4.svg"
                                                        alt="\Phi_t" /> in the background sections.</p>
                                                <p class="last">That paper then goes on to describe GAE, a method for
                                                    approximating the advantage function in policy optimization
                                                    algorithms which enjoys widespread use. For instance, Spinning
                                                    Up&#8217;s implementations of VPG, TRPO, and PPO make use of it. As
                                                    a result, we strongly advise you to study it.</p>
                                            </div>
                                        </div>
                                        <div class="section" id="recap">
                                            <h3><a class="toc-backref" href="#id15">Recap</a><a class="headerlink"
                                                    href="#recap" title="Permalink to this headline">¶</a></h3>
                                            <p>In this chapter, we described the basic theory of policy gradient methods
                                                and connected some of the early results to code examples. The interested
                                                student should continue from here by studying how the later results
                                                (value function baselines and the advantage formulation of policy
                                                gradients) translate into Spinning Up&#8217;s implementation of <a
                                                    class="reference external" href="../algorithms/vpg.html">Vanilla
                                                    Policy Gradient</a>.</p>
                                        </div>
                                    </div>
                                </div>
                                <div class="toctree-wrapper compound">
                                    <span id="document-spinningup/spinningup"></span>
                                    <div class="section" id="spinning-up-as-a-deep-rl-researcher">
                                        <h2><a class="toc-backref" href="#id49">Spinning Up as a Deep RL
                                                Researcher</a><a class="headerlink"
                                                href="#spinning-up-as-a-deep-rl-researcher"
                                                title="Permalink to this headline">¶</a></h2>
                                        <p>By Joshua Achiam, October 13th, 2018</p>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal"
                                                        href="#spinning-up-as-a-deep-rl-researcher" id="id49">Spinning
                                                        Up as a Deep RL Researcher</a>
                                                    <ul>
                                                        <li><a class="reference internal" href="#the-right-background"
                                                                id="id50">The Right Background</a></li>
                                                        <li><a class="reference internal" href="#learn-by-doing"
                                                                id="id51">Learn by Doing</a></li>
                                                        <li><a class="reference internal"
                                                                href="#developing-a-research-project"
                                                                id="id52">Developing a Research Project</a></li>
                                                        <li><a class="reference internal"
                                                                href="#doing-rigorous-research-in-rl" id="id53">Doing
                                                                Rigorous Research in RL</a></li>
                                                        <li><a class="reference internal" href="#closing-thoughts"
                                                                id="id54">Closing Thoughts</a></li>
                                                        <li><a class="reference internal" href="#ps-other-resources"
                                                                id="id55">PS: Other Resources</a></li>
                                                        <li><a class="reference internal" href="#references"
                                                                id="id56">References</a></li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <p>If you’re an aspiring deep RL researcher, you’ve probably heard all kinds of
                                            things about deep RL by this point. You know that <a
                                                class="reference external"
                                                href="https://www.alexirpan.com/2018/02/14/rl-hard.html">it’s hard and
                                                it doesn’t always work</a>. That even when you’re following a recipe, <a
                                                class="reference external"
                                                href="https://arxiv.org/pdf/1708.04133.pdf">reproducibility</a> <a
                                                class="reference external"
                                                href="https://arxiv.org/pdf/1709.06560.pdf">is a challenge</a>. And that
                                            if you’re starting from scratch, <a class="reference external"
                                                href="http://amid.fish/reproducing-deep-rl">the learning curve is
                                                incredibly steep</a>. It’s also the case that there are a lot of <a
                                                class="reference external"
                                                href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">great</a>
                                            <a class="reference external"
                                                href="http://rll.berkeley.edu/deeprlcourse/">resources</a> <a
                                                class="reference external"
                                                href="https://sites.google.com/view/deep-rl-bootcamp/lectures">out</a>
                                            <a class="reference external"
                                                href="http://joschu.net/docs/nuts-and-bolts.pdf">there</a>, but the
                                            material is new enough that there’s not a clear, well-charted path to
                                            mastery. The goal of this column is to help you get past the initial hurdle,
                                            and give you a clear sense of how to spin up as a deep RL researcher. In
                                            particular, this will outline a useful curriculum for increasing raw
                                            knowledge, while interleaving it with the odds and ends that lead to better
                                            research.</p>
                                        <div class="section" id="the-right-background">
                                            <h3><a class="toc-backref" href="#id50">The Right Background</a><a
                                                    class="headerlink" href="#the-right-background"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p><strong>Build up a solid mathematical background.</strong> From
                                                probability and statistics, feel comfortable with random variables,
                                                Bayes’ theorem, chain rule of probability, expected values, standard
                                                deviations, and importance sampling. From multivariate calculus,
                                                understand gradients and (optionally, but it’ll help) Taylor series
                                                expansions.</p>
                                            <p><strong>Build up a general knowledge of deep learning.</strong> You don’t
                                                need to know every single special trick and architecture, but the basics
                                                help. Know about standard architectures (<a class="reference external"
                                                    href="http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/">MLP</a>,
                                                <a class="reference external"
                                                    href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">vanilla
                                                    RNN</a>, <a class="reference external"
                                                    href="https://arxiv.org/pdf/1503.04069.pdf">LSTM</a> (<a
                                                    class="reference external"
                                                    href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">also
                                                    see this blog</a>), <a class="reference external"
                                                    href="https://arxiv.org/pdf/1412.3555.pdfv1">GRU</a>, <a
                                                    class="reference external"
                                                    href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/">conv</a>
                                                <a class="reference external"
                                                    href="https://cs231n.github.io/convolutional-networks/">layers</a>,
                                                <a class="reference external"
                                                    href="https://arxiv.org/pdf/1512.03385.pdf">resnets</a>, <a
                                                    class="reference external"
                                                    href="https://arxiv.org/pdf/1409.0473.pdf">attention</a> <a
                                                    class="reference external"
                                                    href="https://arxiv.org/pdf/1706.03762.pdf">mechanisms</a>), common
                                                regularizers (<a class="reference external"
                                                    href="https://papers.nips.cc/paper/563-a-simple-weight-decay-can-improve-generalization.pdf">weight
                                                    decay</a>, <a class="reference external"
                                                    href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf">dropout</a>),
                                                normalization (<a class="reference external"
                                                    href="https://arxiv.org/pdf/1502.03167.pdf">batch norm</a>, <a
                                                    class="reference external"
                                                    href="https://arxiv.org/pdf/1607.06450.pdf">layer norm</a>, <a
                                                    class="reference external"
                                                    href="https://arxiv.org/pdf/1602.07868.pdf">weight norm</a>), and
                                                optimizers (<a class="reference external"
                                                    href="http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/">SGD,
                                                    momentum SGD</a>, <a class="reference external"
                                                    href="https://arxiv.org/pdf/1412.6980.pdf">Adam</a>, <a
                                                    class="reference external"
                                                    href="https://arxiv.org/pdf/1609.04747.pdf">others</a>). Know what
                                                the <a class="reference external"
                                                    href="https://arxiv.org/pdf/1312.6114.pdf">reparameterization
                                                    trick</a> is.</p>
                                            <p><strong>Become familiar with at least one deep learning library.</strong>
                                                <a class="reference external"
                                                    href="https://www.tensorflow.org/">Tensorflow</a> or <a
                                                    class="reference external" href="http://pytorch.org/">PyTorch</a>
                                                would be a good place to start. You don’t need to know how to do
                                                everything, but you should feel pretty confident in implementing a
                                                simple program to do supervised learning.</p>
                                            <p><strong>Get comfortable with the main concepts and terminology in
                                                    RL.</strong> Know what states, actions, trajectories, policies,
                                                rewards, value functions, and action-value functions are. If
                                                you&#8217;re unfamiliar, Spinning Up ships with <a
                                                    class="reference external" href="../spinningup/rl_intro.html">an
                                                    introduction</a> to this material; it&#8217;s also worth checking
                                                out the <a class="reference external"
                                                    href="https://github.com/jachiam/rl-intro/blob/master/Presentation/rl_intro.pdf">RL-Intro</a>
                                                from the OpenAI Hackathon, or the exceptional and thorough <a
                                                    class="reference external"
                                                    href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">overview
                                                    by Lilian Weng</a>. Optionally, if you’re the sort of person who
                                                enjoys mathematical theory, study up on the math of <a
                                                    class="reference external"
                                                    href="http://joschu.net/docs/thesis.pdf">monotonic improvement
                                                    theory</a> (which forms the basis for advanced policy gradient
                                                algorithms), or <a class="reference external"
                                                    href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">classical
                                                    RL algorithms</a> (which despite being superseded by deep RL
                                                algorithms, contain valuable insights that sometimes drive new
                                                research).</p>
                                        </div>
                                        <div class="section" id="learn-by-doing">
                                            <h3><a class="toc-backref" href="#id51">Learn by Doing</a><a
                                                    class="headerlink" href="#learn-by-doing"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p><strong>Write your own implementations.</strong> You should implement as
                                                many of the core deep RL algorithms from scratch as you can, with the
                                                aim of writing the shortest correct implementation of each. This is by
                                                far the best way to develop an understanding of how they work, as well
                                                as intuitions for their specific performance characteristics.</p>
                                            <p><strong>Simplicity is critical.</strong> You should organize your efforts
                                                so that you implement the simplest algorithms first, and only gradually
                                                introduce complexity. If you start off trying to build something with
                                                too many moving parts, odds are good that it will break and you&#8217;ll
                                                lose weeks trying to debug it. This is a common failure mode for people
                                                who are new to deep RL, and if you find yourself stuck in it,
                                                don&#8217;t be discouraged&#8212;but do try to change tack and work on a
                                                simpler algorithm instead, before returning to the more complex thing
                                                later.</p>
                                            <p><strong>Which algorithms?</strong> You should probably start with vanilla
                                                policy gradient (also called <a class="reference external"
                                                    href="https://arxiv.org/pdf/1604.06778.pdf">REINFORCE</a>), <a
                                                    class="reference external"
                                                    href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">DQN</a>, <a
                                                    class="reference external"
                                                    href="https://blog.openai.com/baselines-acktr-a2c/">A2C</a> (the
                                                synchronous version of <a class="reference external"
                                                    href="https://arxiv.org/pdf/1602.01783.pdf">A3C</a>), <a
                                                    class="reference external"
                                                    href="https://arxiv.org/pdf/1707.06347.pdf">PPO</a> (the variant
                                                with the clipped objective), and <a class="reference external"
                                                    href="https://arxiv.org/pdf/1509.02971.pdf">DDPG</a>, approximately
                                                in that order. The simplest versions of all of these can be written in
                                                just a few hundred lines of code (ballpark 250-300), and some of them
                                                even less (for example, <a class="reference external"
                                                    href="https://github.com/jachiam/rl-intro/blob/master/pg_cartpole.py">a
                                                    no-frills version of VPG</a> can be written in about 80 lines).
                                                Write single-threaded code before you try writing parallelized versions
                                                of these algorithms. (Do try to parallelize at least one.)</p>
                                            <p><strong>Focus on understanding.</strong> Writing working RL code requires
                                                clear, detail-oriented understanding of the algorithms. This is because
                                                <strong>broken RL code almost always fails silently,</strong> where the
                                                code appears to run fine except that the agent never learns how to solve
                                                the task. Usually the problem is that something is being calculated with
                                                the wrong equation, or on the wrong distribution, or data is being piped
                                                into the wrong place. Sometimes the only way to find these bugs is to
                                                read the code with a critical eye, know exactly what it should be doing,
                                                and find where it deviates from the correct behavior. Developing that
                                                knowledge requires you to engage with both academic literature and other
                                                existing implementations (when possible), so a good amount of your time
                                                should be spent on that reading.</p>
                                            <p><strong>What to look for in papers:</strong> When implementing an
                                                algorithm based on a paper, scour that paper, especially the ablation
                                                analyses and supplementary material (where available). The ablations
                                                will give you an intuition for what parameters or subroutines have the
                                                biggest impact on getting things to work, which will help you diagnose
                                                bugs. Supplementary material will often give information about specific
                                                details like network architecture and optimization hyperparameters, and
                                                you should try to align your implementation to these details to improve
                                                your chances of getting it working.</p>
                                            <p><strong>But don&#8217;t overfit to paper details.</strong> Sometimes, the
                                                paper prescribes the use of more tricks than are strictly necessary, so
                                                be a bit wary of this, and try out simplifications where possible. For
                                                example, the original DDPG paper suggests a complex neural network
                                                architecture and initialization scheme, as well as batch normalization.
                                                These aren&#8217;t strictly necessary, and some of the best-reported
                                                results for DDPG use simpler networks. As another example, the original
                                                A3C paper uses asynchronous updates from the various actor-learners, but
                                                it turns out that synchronous updates work about as well.</p>
                                            <p><strong>Don&#8217;t overfit to existing implementations either.</strong>
                                                Study <a class="reference external"
                                                    href="https://github.com/openai/baselines">existing</a> <a
                                                    class="reference external"
                                                    href="https://github.com/rll/rllab">implementations</a> for
                                                inspiration, but be careful not to overfit to the engineering details of
                                                those implementations. RL libraries frequently make choices for
                                                abstraction that are good for code reuse between algorithms, but which
                                                are unnecessary if you&#8217;re only writing a single algorithm or
                                                supporting a single use case.</p>
                                            <p><strong>Iterate fast in simple environments.</strong> To debug your
                                                implementations, try them with simple environments where learning should
                                                happen quickly, like CartPole-v0, InvertedPendulum-v0, FrozenLake-v0,
                                                and HalfCheetah-v2 (with a short time horizon&#8212;only 100 or 250
                                                steps instead of the full 1000) from the <a class="reference external"
                                                    href="https://gym.openai.com/">OpenAI Gym</a>. Don’t try to run an
                                                algorithm in Atari or a complex Humanoid environment if you haven’t
                                                first verified that it works on the simplest possible toy task. Your
                                                ideal experiment turnaround-time at the debug stage is &lt;5 minutes (on
                                                your local machine) or slightly longer but not much. These small-scale
                                                experiments don&#8217;t require any special hardware, and can be run
                                                without too much trouble on CPUs.</p>
                                            <p><strong>If it doesn&#8217;t work, assume there&#8217;s a bug.</strong>
                                                Spend a lot of effort searching for bugs before you resort to tweaking
                                                hyperparameters: usually it’s a bug. Bad hyperparameters can
                                                significantly degrade RL performance, but if you&#8217;re using
                                                hyperparameters similar to the ones in papers and standard
                                                implementations, those will probably not be the issue. Also worth
                                                keeping in mind: sometimes things will work in one environment even when
                                                you have a breaking bug, so make sure to test in more than one
                                                environment once your results look promising.</p>
                                            <p><strong>Measure everything.</strong> Do a lot of instrumenting to see
                                                what’s going on under-the-hood. The more stats about the learning
                                                process you read out at each iteration, the easier it is to
                                                debug&#8212;after all, you can’t tell it’s broken if you can’t see that
                                                it’s breaking. I personally like to look at the mean/std/min/max for
                                                cumulative rewards, episode lengths, and value function estimates, along
                                                with the losses for the objectives, and the details of any exploration
                                                parameters (like mean entropy for stochastic policy optimization, or
                                                current epsilon for epsilon-greedy as in DQN). Also, watch videos of
                                                your agent’s performance every now and then; this will give you some
                                                insights you wouldn’t get otherwise.</p>
                                            <p><strong>Scale experiments when things work.</strong> After you have an
                                                implementation of an RL algorithm that seems to work correctly in the
                                                simplest environments, test it out on harder environments. Experiments
                                                at this stage will take longer&#8212;on the order of somewhere between a
                                                few hours and a couple of days, depending. Specialized
                                                hardware&#8212;like a beefy GPU or a 32-core machine&#8212;might be
                                                useful at this point, and you should consider looking into cloud
                                                computing resources like AWS or GCE.</p>
                                            <p><strong>Keep these habits!</strong> These habits are worth keeping beyond
                                                the stage where you’re just learning about deep RL&#8212;they will
                                                accelerate your research!</p>
                                        </div>
                                        <div class="section" id="developing-a-research-project">
                                            <h3><a class="toc-backref" href="#id52">Developing a Research Project</a><a
                                                    class="headerlink" href="#developing-a-research-project"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>Once you feel reasonably comfortable with the basics in deep RL, you
                                                should start pushing on the boundaries and doing research. To get there,
                                                you&#8217;ll need an idea for a project.</p>
                                            <p><strong>Start by exploring the literature to become aware of topics in
                                                    the field.</strong> There are a wide range of topics you might find
                                                interesting: sample efficiency, exploration, transfer learning,
                                                hierarchy, memory, model-based RL, meta learning, and multi-agent, to
                                                name a few. If you&#8217;re looking for inspiration, or just want to get
                                                a rough sense of what&#8217;s out there, check out Spinning Up&#8217;s
                                                <a class="reference external" href="../spinningup/keypapers.html">key
                                                    papers</a> list. Find a paper that you enjoy on one of these
                                                subjects&#8212;something that inspires you&#8212;and read it thoroughly.
                                                Use the related work section and citations to find closely-related
                                                papers and do a deep dive in the literature. You’ll start to figure out
                                                where the unsolved problems are and where you can make an impact.</p>
                                            <p><strong>Approaches to idea-generation:</strong> There are a many
                                                different ways to start thinking about ideas for projects, and the frame
                                                you choose influences how the project might evolve and what risks it
                                                will face. Here are a few examples:</p>
                                            <p><strong>Frame 1: Improving on an Existing Approach.</strong> This is the
                                                incrementalist angle, where you try to get performance gains in an
                                                established problem setting by tweaking an existing algorithm.
                                                Reimplementing prior work is super helpful here, because it exposes you
                                                to the ways that existing algorithms are brittle and could be improved.
                                                A novice will find this the most accessible frame, but it can also be
                                                worthwhile for researchers at any level of experience. While some
                                                researchers find incrementalism less exciting, some of the most
                                                impressive achievements in machine learning have come from work of this
                                                nature.</p>
                                            <p>Because projects like these are tied to existing methods, they are by
                                                nature narrowly scoped and can wrap up quickly (a few months), which may
                                                be desirable (especially when starting out as a researcher). But this
                                                also sets up the risks: it&#8217;s possible that the tweaks you have in
                                                mind for an algorithm may fail to improve it, in which case, unless you
                                                come up with more tweaks, the project is just over and you have no clear
                                                signal on what to do next.</p>
                                            <p><strong>Frame 2: Focusing on Unsolved Benchmarks.</strong> Instead of
                                                thinking about how to improve an existing method, you aim to succeed on
                                                a task that no one has solved before. For example: achieving perfect
                                                generalization from training levels to test levels in the <a
                                                    class="reference external"
                                                    href="https://contest.openai.com/2018-1/">Sonic domain</a> or <a
                                                    class="reference external"
                                                    href="https://blog.openai.com/gym-retro/">Gym Retro</a>. When you
                                                hammer away at an unsolved task, you might try a wide variety of
                                                methods, including prior approaches and new ones that you invent for the
                                                project. It is possible for a novice to approch this kind of problem,
                                                but there will be a steeper learning curve.</p>
                                            <p>Projects in this frame have a broad scope and can go on for a while
                                                (several months to a year-plus). The main risk is that the benchmark is
                                                unsolvable without a substantial breakthrough, meaning that it would be
                                                easy to spend a lot of time without making any progress on it. But even
                                                if a project like this fails, it often leads the researcher to many new
                                                insights that become fertile soil for the next project.</p>
                                            <p><strong>Frame 3: Create a New Problem Setting.</strong> Instead of
                                                thinking about existing methods or current grand challenges, think of an
                                                entirely different conceptual problem that hasn&#8217;t been studied
                                                yet. Then, figure out how to make progress on it. For projects along
                                                these lines, a standard benchmark probably doesn&#8217;t exist yet, and
                                                you will have to design one. This can be a huge challenge, but it’s
                                                worth embracing&#8212;great benchmarks move the whole field forward.</p>
                                            <p>Problems in this frame come up when they come up&#8212;it&#8217;s hard to
                                                go looking for them.</p>
                                            <p><strong>Avoid reinventing the wheel.</strong> When you come up with a
                                                good idea that you want to start testing, that’s great! But while you’re
                                                still in the early stages with it, do the most thorough check you can to
                                                make sure it hasn’t already been done. It can be pretty disheartening to
                                                get halfway through a project, and only then discover that there&#8217;s
                                                already a paper about your idea. It&#8217;s especially frustrating when
                                                the work is concurrent, which happens from time to time! But don’t let
                                                that deter you&#8212;and definitely don’t let it motivate you to plant
                                                flags with not-quite-finished research and over-claim the merits of the
                                                partial work. Do good research and finish out your projects with
                                                complete and thorough investigations, because that’s what counts, and by
                                                far what matters most in the long run.</p>
                                        </div>
                                        <div class="section" id="doing-rigorous-research-in-rl">
                                            <h3><a class="toc-backref" href="#id53">Doing Rigorous Research in RL</a><a
                                                    class="headerlink" href="#doing-rigorous-research-in-rl"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>Now you’ve come up with an idea, and you’re fairly certain it hasn’t been
                                                done. You use the skills you’ve developed to implement it and you start
                                                testing it out on standard domains. It looks like it works! But what
                                                does that mean, and how well does it have to work to be important? This
                                                is one of the hardest parts of research in deep RL. In order to validate
                                                that your proposal is a meaningful contribution, you have to rigorously
                                                prove that it actually gets a performance benefit over the strongest
                                                possible baseline algorithm&#8212;whatever currently achieves SOTA
                                                (state of the art) on your test domains. If you’ve invented a new test
                                                domain, so there’s no previous SOTA, you still need to try out whatever
                                                the most reliable algorithm in the literature is that could plausibly do
                                                well in the new test domain, and then you have to beat that.</p>
                                            <p><strong>Set up fair comparisons.</strong> If you implement your baseline
                                                from scratch&#8212;as opposed to comparing against another paper’s
                                                numbers directly&#8212;it’s important to spend as much time tuning your
                                                baseline as you spend tuning your own algorithm. This will make sure
                                                that comparisons are fair. Also, do your best to hold “all else equal”
                                                even if there are substantial differences between your algorithm and the
                                                baseline. For example, if you’re investigating architecture variants,
                                                keep the number of model parameters approximately equal between your
                                                model and the baseline. Under no circumstances handicap the baseline! It
                                                turns out that the baselines in RL are pretty strong, and getting big,
                                                consistent wins over them can be tricky or require some good insight in
                                                algorithm design.</p>
                                            <p><strong>Remove stochasticity as a confounder.</strong> Beware of random
                                                seeds making things look stronger or weaker than they really are, so run
                                                everything for many random seeds (at least 3, but if you want to be
                                                thorough, do 10 or more). This is really important and deserves a lot of
                                                emphasis: deep RL seems fairly brittle with respect to random seed in a
                                                lot of common use cases. There’s potentially enough variance that two
                                                different groups of random seeds can yield learning curves with
                                                differences so significant that they look like they don’t come from the
                                                same distribution at all (see <a class="reference external"
                                                    href="https://arxiv.org/pdf/1708.04133.pdf">figure 10 here</a>).</p>
                                            <p><strong>Run high-integrity experiments.</strong> Don’t just take the
                                                results from the best or most interesting runs to use in your paper.
                                                Instead, launch new, final experiments&#8212;for all of the methods that
                                                you intend to compare (if you are comparing against your own baseline
                                                implementations)&#8212;and precommit to report on whatever comes out of
                                                that. This is to enforce a weak form of <a class="reference external"
                                                    href="https://cos.io/prereg/">preregistration</a>: you use the
                                                tuning stage to come up with your hypotheses, and you use the final runs
                                                to come up with your conclusions.</p>
                                            <p><strong>Check each claim separately.</strong> Another critical aspect of
                                                doing research is to run an ablation analysis. Any method you propose is
                                                likely to have several key design decisions&#8212;like architecture
                                                choices or regularization techniques, for instance&#8212;each of which
                                                could separately impact performance. The claim you&#8217;ll make in your
                                                work is that those design decisions collectively help, but this is
                                                really a bundle of several claims in disguise: one for each such design
                                                element. By systematically evaluating what would happen if you were to
                                                swap them out with alternate design choices, or remove them entirely,
                                                you can figure out how to correctly attribute credit for the benefits
                                                your method confers. This lets you make each separate claim with a
                                                measure of confidence, and increases the overall strength of your work.
                                            </p>
                                        </div>
                                        <div class="section" id="closing-thoughts">
                                            <h3><a class="toc-backref" href="#id54">Closing Thoughts</a><a
                                                    class="headerlink" href="#closing-thoughts"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>Deep RL is an exciting, fast-moving field, and we need as many people as
                                                possible to go through the open problems and make progress on them.
                                                Hopefully, you feel a bit more prepared to be a part of it after reading
                                                this! And whenever you’re ready, <a class="reference external"
                                                    href="https://jobs.lever.co/openai">let us know</a>.</p>
                                        </div>
                                        <div class="section" id="ps-other-resources">
                                            <h3><a class="toc-backref" href="#id55">PS: Other Resources</a><a
                                                    class="headerlink" href="#ps-other-resources"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>Consider reading through these other informative articles about growing
                                                as a researcher or engineer in this field:</p>
                                            <p><a class="reference external"
                                                    href="https://rockt.github.io/2018/08/29/msc-advice">Advice for
                                                    Short-term Machine Learning Research Projects</a>, by Tim
                                                Rocktäschel, Jakob Foerster and Greg Farquhar.</p>
                                            <p><a class="reference external"
                                                    href="https://80000hours.org/articles/ml-engineering-career-transition-guide/">ML
                                                    Engineering for AI Safety &amp; Robustness: a Google Brain
                                                    Engineer’s Guide to Entering the Field</a>, by Catherine Olsson and
                                                80,000 Hours.</p>
                                        </div>
                                        <div class="section" id="references">
                                            <h3><a class="toc-backref" href="#id56">References</a><a class="headerlink"
                                                    href="#references" title="Permalink to this headline">¶</a></h3>
                                            <table class="docutils footnote" frame="void" id="id1" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[1]</td>
                                                        <td><a class="reference external"
                                                                href="https://www.alexirpan.com/2018/02/14/rl-hard.html">Deep
                                                                Reinforcement Learning Doesn&#8217;t Work Yet</a>, Alex
                                                            Irpan, 2018</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id2" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[2]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1708.04133.pdf">Reproducibility
                                                                of Benchmarked Deep Reinforcement Learning Tasks for
                                                                Continuous Control</a>, Islam et al, 2017</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id3" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[3]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1709.06560.pdf">Deep
                                                                Reinforcement Learning that Matters</a>, Henderson et
                                                            al, 2017</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id4" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[4]</td>
                                                        <td><a class="reference external"
                                                                href="http://amid.fish/reproducing-deep-rl">Lessons
                                                                Learned Reproducing a Deep Reinforcement Learning
                                                                Paper</a>, Matthew Rahtz, 2018</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id5" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[5]</td>
                                                        <td><a class="reference external"
                                                                href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">UCL
                                                                Course on RL</a></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id6" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[6]</td>
                                                        <td><a class="reference external"
                                                                href="http://rll.berkeley.edu/deeprlcourse/">Berkeley
                                                                Deep RL Course</a></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id7" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[7]</td>
                                                        <td><a class="reference external"
                                                                href="https://sites.google.com/view/deep-rl-bootcamp/lectures">Deep
                                                                RL Bootcamp</a></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id8" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[8]</td>
                                                        <td><a class="reference external"
                                                                href="http://joschu.net/docs/nuts-and-bolts.pdf">Nuts
                                                                and Bolts of Deep RL</a>, John Schulman</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id9" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[9]</td>
                                                        <td><a class="reference external"
                                                                href="http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/">Stanford
                                                                Deep Learning Tutorial: Multi-Layer Neural Network</a>
                                                        </td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id10" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[10]</td>
                                                        <td><a class="reference external"
                                                                href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The
                                                                Unreasonable Effectiveness of Recurrent Neural
                                                                Networks</a>, Andrej Karpathy, 2015</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id11" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[11]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1503.04069.pdf">LSTM: A
                                                                Search Space Odyssey</a>, Greff et al, 2015</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id12" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[12]</td>
                                                        <td><a class="reference external"
                                                                href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding
                                                                LSTM Networks</a>, Chris Olah, 2015</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id13" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[13]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1412.3555.pdfv1">Empirical
                                                                Evaluation of Gated Recurrent Neural Networks on
                                                                Sequence Modeling</a>, Chung et al, 2014 (GRU paper)
                                                        </td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id14" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[14]</td>
                                                        <td><a class="reference external"
                                                                href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/">Conv
                                                                Nets: A Modular Perspective</a>, Chris Olah, 2014</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id15" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[15]</td>
                                                        <td><a class="reference external"
                                                                href="https://cs231n.github.io/convolutional-networks/">Stanford
                                                                CS231n, Convolutional Neural Networks for Visual
                                                                Recognition</a></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id16" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[16]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1512.03385.pdf">Deep
                                                                Residual Learning for Image Recognition</a>, He et al,
                                                            2015 (ResNets)</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id17" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[17]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1409.0473.pdf">Neural
                                                                Machine Translation by Jointly Learning to Align and
                                                                Translate</a>, Bahdanau et al, 2014 (Attention
                                                            mechanisms)</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id18" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[18]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is
                                                                All You Need</a>, Vaswani et al, 2017</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id19" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[19]</td>
                                                        <td><a class="reference external"
                                                                href="https://papers.nips.cc/paper/563-a-simple-weight-decay-can-improve-generalization.pdf">A
                                                                Simple Weight Decay Can Improve Generalization</a>,
                                                            Krogh and Hertz, 1992</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id20" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[20]</td>
                                                        <td><a class="reference external"
                                                                href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf">Dropout:
                                                                A Simple Way to Prevent Neural Networks from
                                                                Overfitting</a>, Srivastava et al, 2014</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id21" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[21]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1502.03167.pdf">Batch
                                                                Normalization: Accelerating Deep Network Training by
                                                                Reducing Internal Covariate Shift</a>, Ioffe and
                                                            Szegedy, 2015</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id22" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[22]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1607.06450.pdf">Layer
                                                                Normalization</a>, Ba et al, 2016</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id23" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[23]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1602.07868.pdf">Weight
                                                                Normalization: A Simple Reparameterization to Accelerate
                                                                Training of Deep Neural Networks</a>, Salimans and
                                                            Kingma, 2016</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id24" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[24]</td>
                                                        <td><a class="reference external"
                                                                href="http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/">Stanford
                                                                Deep Learning Tutorial: Stochastic Gradient Descent</a>
                                                        </td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id25" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[25]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1412.6980.pdf">Adam: A
                                                                Method for Stochastic Optimization</a>, Kingma and Ba,
                                                            2014</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id26" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[26]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1609.04747.pdf">An overview
                                                                of gradient descent optimization algorithms</a>,
                                                            Sebastian Ruder, 2016</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id27" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[27]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1312.6114.pdf">Auto-Encoding
                                                                Variational Bayes</a>, Kingma and Welling, 2013
                                                            (Reparameterization trick)</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id28" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[28]</td>
                                                        <td><a class="reference external"
                                                                href="https://www.tensorflow.org/">Tensorflow</a></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id29" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[29]</td>
                                                        <td><a class="reference external"
                                                                href="http://pytorch.org/">PyTorch</a></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id30" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[30]</td>
                                                        <td><a class="reference external"
                                                                href="../spinningup/rl_intro.html">Spinning Up in Deep
                                                                RL: Introduction to RL, Part 1</a></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id31" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[31]</td>
                                                        <td><a class="reference external"
                                                                href="https://github.com/jachiam/rl-intro/blob/master/Presentation/rl_intro.pdf">RL-Intro</a>
                                                            Slides from OpenAI Hackathon, Josh Achiam, 2018</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id32" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[32]</td>
                                                        <td><a class="reference external"
                                                                href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">A
                                                                (Long) Peek into Reinforcement Learning</a>, Lilian
                                                            Weng, 2018</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id33" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[33]</td>
                                                        <td><a class="reference external"
                                                                href="http://joschu.net/docs/thesis.pdf">Optimizing
                                                                Expectations</a>, John Schulman, 2016 (Monotonic
                                                            improvement theory)</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id34" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[34]</td>
                                                        <td><a class="reference external"
                                                                href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">Algorithms
                                                                for Reinforcement Learning</a>, Csaba Szepesvari, 2009
                                                            (Classic RL Algorithms)</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id35" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[35]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1604.06778.pdf">Benchmarking
                                                                Deep Reinforcement Learning for Continuous Control</a>,
                                                            Duan et al, 2016</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id36" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[36]</td>
                                                        <td><a class="reference external"
                                                                href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing
                                                                Atari with Deep Reinforcement Learning</a>, Mnih et al,
                                                            2013 (DQN)</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id37" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[37]</td>
                                                        <td><a class="reference external"
                                                                href="https://blog.openai.com/baselines-acktr-a2c/">OpenAI
                                                                Baselines: ACKTR &amp; A2C</a></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id38" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[38]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1602.01783.pdf">Asynchronous
                                                                Methods for Deep Reinforcement Learning</a>, Mnih et al,
                                                            2016 (A3C)</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id39" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[39]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1707.06347.pdf">Proximal
                                                                Policy Optimization Algorithms</a>, Schulman et al, 2017
                                                            (PPO)</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id40" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[40]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1509.02971.pdf">Continuous
                                                                Control with Deep Reinforcement Learning</a>, Lillicrap
                                                            et al, 2015 (DDPG)</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id41" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[41]</td>
                                                        <td><a class="reference external"
                                                                href="https://github.com/jachiam/rl-intro/blob/master/pg_cartpole.py">RL-Intro
                                                                Policy Gradient Sample Code</a>, Josh Achiam, 2018</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id42" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[42]</td>
                                                        <td><a class="reference external"
                                                                href="https://github.com/openai/baselines">OpenAI
                                                                Baselines</a></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id43" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[43]</td>
                                                        <td><a class="reference external"
                                                                href="https://github.com/rll/rllab">rllab</a></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id44" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[44]</td>
                                                        <td><a class="reference external"
                                                                href="https://gym.openai.com/">OpenAI Gym</a></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id46" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[45]</td>
                                                        <td><a class="reference external"
                                                                href="https://contest.openai.com/2018-1/">OpenAI Retro
                                                                Contest</a></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id47" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[46]</td>
                                                        <td><a class="reference external"
                                                                href="https://blog.openai.com/gym-retro/">OpenAI Gym
                                                                Retro</a></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id48" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[47]</td>
                                                        <td><a class="reference external"
                                                                href="https://cos.io/prereg/">Center for Open
                                                                Science</a>, explaining what preregistration means in
                                                            the context of scientific experiments.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                        </div>
                                    </div>
                                    <span id="document-spinningup/keypapers"></span>
                                    <div class="section" id="key-papers-in-deep-rl">
                                        <h2><a class="toc-backref" href="#id106">Key Papers in Deep RL</a><a
                                                class="headerlink" href="#key-papers-in-deep-rl"
                                                title="Permalink to this headline">¶</a></h2>
                                        <p>What follows is a list of papers in deep RL that are worth reading. This is
                                            <em>far</em> from comprehensive, but should provide a useful starting point
                                            for someone looking to do research in the field.</p>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal" href="#key-papers-in-deep-rl"
                                                        id="id106">Key Papers in Deep RL</a>
                                                    <ul>
                                                        <li><a class="reference internal" href="#model-free-rl"
                                                                id="id107">1. Model-Free RL</a></li>
                                                        <li><a class="reference internal" href="#exploration"
                                                                id="id108">2. Exploration</a></li>
                                                        <li><a class="reference internal"
                                                                href="#transfer-and-multitask-rl" id="id109">3. Transfer
                                                                and Multitask RL</a></li>
                                                        <li><a class="reference internal" href="#hierarchy"
                                                                id="id110">4. Hierarchy</a></li>
                                                        <li><a class="reference internal" href="#memory" id="id111">5.
                                                                Memory</a></li>
                                                        <li><a class="reference internal" href="#model-based-rl"
                                                                id="id112">6. Model-Based RL</a></li>
                                                        <li><a class="reference internal" href="#meta-rl" id="id113">7.
                                                                Meta-RL</a></li>
                                                        <li><a class="reference internal" href="#scaling-rl"
                                                                id="id114">8. Scaling RL</a></li>
                                                        <li><a class="reference internal" href="#rl-in-the-real-world"
                                                                id="id115">9. RL in the Real World</a></li>
                                                        <li><a class="reference internal" href="#safety" id="id116">10.
                                                                Safety</a></li>
                                                        <li><a class="reference internal"
                                                                href="#imitation-learning-and-inverse-reinforcement-learning"
                                                                id="id117">11. Imitation Learning and Inverse
                                                                Reinforcement Learning</a></li>
                                                        <li><a class="reference internal"
                                                                href="#reproducibility-analysis-and-critique"
                                                                id="id118">12. Reproducibility, Analysis, and
                                                                Critique</a></li>
                                                        <li><a class="reference internal"
                                                                href="#bonus-classic-papers-in-rl-theory-or-review"
                                                                id="id119">13. Bonus: Classic Papers in RL Theory or
                                                                Review</a></li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <div class="section" id="model-free-rl">
                                            <h3><a class="toc-backref" href="#id107">1. Model-Free RL</a><a
                                                    class="headerlink" href="#model-free-rl"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <div class="section" id="a-deep-q-learning">
                                                <h4>a. Deep Q-Learning<a class="headerlink" href="#a-deep-q-learning"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <table class="docutils footnote" frame="void" id="id1" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[1]</td>
                                                            <td><a class="reference external"
                                                                    href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing
                                                                    Atari with Deep Reinforcement Learning</a>, Mnih et
                                                                al, 2013. <strong>Algorithm: DQN.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id2" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[2]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1507.06527.pdf">Deep
                                                                    Recurrent Q-Learning for Partially Observable
                                                                    MDPs</a>, Hausknecht and Stone, 2015.
                                                                <strong>Algorithm: Deep Recurrent Q-Learning.</strong>
                                                            </td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id3" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[3]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1511.06581.pdf">Dueling
                                                                    Network Architectures for Deep Reinforcement
                                                                    Learning</a>, Wang et al, 2015. <strong>Algorithm:
                                                                    Dueling DQN.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id4" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[4]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1509.06461.pdf">Deep
                                                                    Reinforcement Learning with Double Q-learning</a>,
                                                                Hasselt et al 2015. <strong>Algorithm: Double
                                                                    DQN.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id5" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[5]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1511.05952.pdf">Prioritized
                                                                    Experience Replay</a>, Schaul et al, 2015.
                                                                <strong>Algorithm: Prioritized Experience Replay
                                                                    (PER).</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id6" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[6]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1710.02298.pdf">Rainbow:
                                                                    Combining Improvements in Deep Reinforcement
                                                                    Learning</a>, Hessel et al, 2017. <strong>Algorithm:
                                                                    Rainbow DQN.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                            </div>
                                            <div class="section" id="b-policy-gradients">
                                                <h4>b. Policy Gradients<a class="headerlink" href="#b-policy-gradients"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <table class="docutils footnote" frame="void" id="id7" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[7]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1602.01783.pdf">Asynchronous
                                                                    Methods for Deep Reinforcement Learning</a>, Mnih et
                                                                al, 2016. <strong>Algorithm: A3C.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id8" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[8]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1502.05477.pdf">Trust
                                                                    Region Policy Optimization</a>, Schulman et al,
                                                                2015. <strong>Algorithm: TRPO.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id9" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[9]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1506.02438.pdf">High-Dimensional
                                                                    Continuous Control Using Generalized Advantage
                                                                    Estimation</a>, Schulman et al, 2015.
                                                                <strong>Algorithm: GAE.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id10" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[10]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1707.06347.pdf">Proximal
                                                                    Policy Optimization Algorithms</a>, Schulman et al,
                                                                2017. <strong>Algorithm: PPO-Clip, PPO-Penalty.</strong>
                                                            </td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id11" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[11]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1707.02286.pdf">Emergence
                                                                    of Locomotion Behaviours in Rich Environments</a>,
                                                                Heess et al, 2017. <strong>Algorithm:
                                                                    PPO-Penalty.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id12" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[12]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1708.05144.pdf">Scalable
                                                                    trust-region method for deep reinforcement learning
                                                                    using Kronecker-factored approximation</a>, Wu et
                                                                al, 2017. <strong>Algorithm: ACKTR.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id13" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[13]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1611.01224.pdf">Sample
                                                                    Efficient Actor-Critic with Experience Replay</a>,
                                                                Wang et al, 2016. <strong>Algorithm: ACER.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id14" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[14]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1801.01290.pdf">Soft
                                                                    Actor-Critic: Off-Policy Maximum Entropy Deep
                                                                    Reinforcement Learning with a Stochastic Actor</a>,
                                                                Haarnoja et al, 2018. <strong>Algorithm: SAC.</strong>
                                                            </td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                            </div>
                                            <div class="section" id="c-deterministic-policy-gradients">
                                                <h4>c. Deterministic Policy Gradients<a class="headerlink"
                                                        href="#c-deterministic-policy-gradients"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <table class="docutils footnote" frame="void" id="id15" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[15]</td>
                                                            <td><a class="reference external"
                                                                    href="http://proceedings.mlr.press/v32/silver14.pdf">Deterministic
                                                                    Policy Gradient Algorithms</a>, Silver et al, 2014.
                                                                <strong>Algorithm: DPG.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id16" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[16]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1509.02971.pdf">Continuous
                                                                    Control With Deep Reinforcement Learning</a>,
                                                                Lillicrap et al, 2015. <strong>Algorithm: DDPG.</strong>
                                                            </td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id17" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[17]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1802.09477.pdf">Addressing
                                                                    Function Approximation Error in Actor-Critic
                                                                    Methods</a>, Fujimoto et al, 2018.
                                                                <strong>Algorithm: TD3.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                            </div>
                                            <div class="section" id="d-distributional-rl">
                                                <h4>d. Distributional RL<a class="headerlink"
                                                        href="#d-distributional-rl"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <table class="docutils footnote" frame="void" id="id18" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[18]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1707.06887.pdf">A
                                                                    Distributional Perspective on Reinforcement
                                                                    Learning</a>, Bellemare et al, 2017.
                                                                <strong>Algorithm: C51.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id19" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[19]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1710.10044.pdf">Distributional
                                                                    Reinforcement Learning with Quantile Regression</a>,
                                                                Dabney et al, 2017. <strong>Algorithm: QR-DQN.</strong>
                                                            </td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id20" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[20]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1806.06923.pdf">Implicit
                                                                    Quantile Networks for Distributional Reinforcement
                                                                    Learning</a>, Dabney et al, 2018. <strong>Algorithm:
                                                                    IQN.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id21" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[21]</td>
                                                            <td><a class="reference external"
                                                                    href="https://openreview.net/forum?id=ByG_3s09KX">Dopamine:
                                                                    A Research Framework for Deep Reinforcement
                                                                    Learning</a>, Anonymous, 2018.
                                                                <strong>Contribution:</strong> Introduces Dopamine, a
                                                                code repository containing implementations of DQN, C51,
                                                                IQN, and Rainbow. <a class="reference external"
                                                                    href="https://github.com/google/dopamine">Code
                                                                    link.</a></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                            </div>
                                            <div class="section"
                                                id="e-policy-gradients-with-action-dependent-baselines">
                                                <h4>e. Policy Gradients with Action-Dependent Baselines<a
                                                        class="headerlink"
                                                        href="#e-policy-gradients-with-action-dependent-baselines"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <table class="docutils footnote" frame="void" id="id22" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[22]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1611.02247.pdf">Q-Prop:
                                                                    Sample-Efficient Policy Gradient with An Off-Policy
                                                                    Critic</a>, Gu et al, 2016. <strong>Algorithm:
                                                                    Q-Prop.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id23" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[23]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1710.11198.pdf">Action-depedent
                                                                    Control Variates for Policy Optimization via
                                                                    Stein&#8217;s Identity</a>, Liu et al, 2017.
                                                                <strong>Algorithm: Stein Control Variates.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id24" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[24]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1802.10031.pdf">The
                                                                    Mirage of Action-Dependent Baselines in
                                                                    Reinforcement Learning</a>, Tucker et al, 2018.
                                                                <strong>Contribution:</strong> interestingly, critiques
                                                                and reevaluates claims from earlier papers (including
                                                                Q-Prop and stein control variates) and finds important
                                                                methodological errors in them.</td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                            </div>
                                            <div class="section" id="f-path-consistency-learning">
                                                <h4>f. Path-Consistency Learning<a class="headerlink"
                                                        href="#f-path-consistency-learning"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <table class="docutils footnote" frame="void" id="id25" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[25]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1702.08892.pdf">Bridging
                                                                    the Gap Between Value and Policy Based Reinforcement
                                                                    Learning</a>, Nachum et al, 2017. <strong>Algorithm:
                                                                    PCL.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id26" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[26]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1707.01891.pdf">Trust-PCL:
                                                                    An Off-Policy Trust Region Method for Continuous
                                                                    Control</a>, Nachum et al, 2017. <strong>Algorithm:
                                                                    Trust-PCL.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                            </div>
                                            <div class="section"
                                                id="g-other-directions-for-combining-policy-learning-and-q-learning">
                                                <h4>g. Other Directions for Combining Policy-Learning and Q-Learning<a
                                                        class="headerlink"
                                                        href="#g-other-directions-for-combining-policy-learning-and-q-learning"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <table class="docutils footnote" frame="void" id="id27" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[27]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1611.01626.pdf">Combining
                                                                    Policy Gradient and Q-learning</a>, O&#8217;Donoghue
                                                                et al, 2016. <strong>Algorithm: PGQL.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id28" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[28]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1704.04651.pdf">The
                                                                    Reactor: A Fast and Sample-Efficient Actor-Critic
                                                                    Agent for Reinforcement Learning</a>, Gruslys et al,
                                                                2017. <strong>Algorithm: Reactor.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id29" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[29]</td>
                                                            <td><a class="reference external"
                                                                    href="http://papers.nips.cc/paper/6974-interpolated-policy-gradient-merging-on-policy-and-off-policy-gradient-estimation-for-deep-reinforcement-learning">Interpolated
                                                                    Policy Gradient: Merging On-Policy and Off-Policy
                                                                    Gradient Estimation for Deep Reinforcement
                                                                    Learning</a>, Gu et al, 2017. <strong>Algorithm:
                                                                    IPG.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id30" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[30]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1704.06440.pdf">Equivalence
                                                                    Between Policy Gradients and Soft Q-Learning</a>,
                                                                Schulman et al, 2017. <strong>Contribution:</strong>
                                                                Reveals a theoretical link between these two families of
                                                                RL algorithms.</td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                            </div>
                                            <div class="section" id="h-evolutionary-algorithms">
                                                <h4>h. Evolutionary Algorithms<a class="headerlink"
                                                        href="#h-evolutionary-algorithms"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <table class="docutils footnote" frame="void" id="id31" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[31]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1703.03864.pdf">Evolution
                                                                    Strategies as a Scalable Alternative to
                                                                    Reinforcement Learning</a>, Salimans et al, 2017.
                                                                <strong>Algorithm: ES.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                            </div>
                                        </div>
                                        <div class="section" id="exploration">
                                            <h3><a class="toc-backref" href="#id108">2. Exploration</a><a
                                                    class="headerlink" href="#exploration"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <div class="section" id="a-intrinsic-motivation">
                                                <h4>a. Intrinsic Motivation<a class="headerlink"
                                                        href="#a-intrinsic-motivation"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <table class="docutils footnote" frame="void" id="id32" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[32]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1605.09674.pdf">VIME:
                                                                    Variational Information Maximizing Exploration</a>,
                                                                Houthooft et al, 2016. <strong>Algorithm: VIME.</strong>
                                                            </td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id33" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[33]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1606.01868.pdf">Unifying
                                                                    Count-Based Exploration and Intrinsic
                                                                    Motivation</a>, Bellemare et al, 2016.
                                                                <strong>Algorithm: CTS-based Pseudocounts.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id34" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[34]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1703.01310.pdf">Count-Based
                                                                    Exploration with Neural Density Models</a>,
                                                                Ostrovski et al, 2017. <strong>Algorithm: PixelCNN-based
                                                                    Pseudocounts.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id35" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[35]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1611.04717.pdf">#Exploration:
                                                                    A Study of Count-Based Exploration for Deep
                                                                    Reinforcement Learning</a>, Tang et al, 2016.
                                                                <strong>Algorithm: Hash-based Counts.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id36" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[36]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1703.01260.pdf">EX2:
                                                                    Exploration with Exemplar Models for Deep
                                                                    Reinforcement Learning</a>, Fu et al, 2017.
                                                                <strong>Algorithm: EX2.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id37" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[37]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1705.05363.pdf">Curiosity-driven
                                                                    Exploration by Self-supervised Prediction</a>,
                                                                Pathak et al, 2017. <strong>Algorithm: Intrinsic
                                                                    Curiosity Module (ICM).</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id38" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[38]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1808.04355.pdf">Large-Scale
                                                                    Study of Curiosity-Driven Learning</a>, Burda et al,
                                                                2018. <strong>Contribution:</strong> Systematic analysis
                                                                of how surprisal-based intrinsic motivation performs in
                                                                a wide variety of environments.</td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id39" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[39]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1810.12894.pdf">Exploration
                                                                    by Random Network Distillation</a>, Burda et al,
                                                                2018. <strong>Algorithm: RND.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                            </div>
                                            <div class="section" id="b-unsupervised-rl">
                                                <h4>b. Unsupervised RL<a class="headerlink" href="#b-unsupervised-rl"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <table class="docutils footnote" frame="void" id="id40" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[40]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1611.07507.pdf">Variational
                                                                    Intrinsic Control</a>, Gregor et al, 2016.
                                                                <strong>Algorithm: VIC.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id41" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[41]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1802.06070.pdf">Diversity
                                                                    is All You Need: Learning Skills without a Reward
                                                                    Function</a>, Eysenbach et al, 2018.
                                                                <strong>Algorithm: DIAYN.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id42" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[42]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1807.10299.pdf">Variational
                                                                    Option Discovery Algorithms</a>, Achiam et al, 2018.
                                                                <strong>Algorithm: VALOR.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                            </div>
                                        </div>
                                        <div class="section" id="transfer-and-multitask-rl">
                                            <h3><a class="toc-backref" href="#id109">3. Transfer and Multitask RL</a><a
                                                    class="headerlink" href="#transfer-and-multitask-rl"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <table class="docutils footnote" frame="void" id="id43" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[43]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1606.04671.pdf">Progressive
                                                                Neural Networks</a>, Rusu et al, 2016.
                                                            <strong>Algorithm: Progressive Networks.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id44" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[44]</td>
                                                        <td><a class="reference external"
                                                                href="http://proceedings.mlr.press/v37/schaul15.pdf">Universal
                                                                Value Function Approximators</a>, Schaul et al, 2015.
                                                            <strong>Algorithm: UVFA.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id45" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[45]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1611.05397.pdf">Reinforcement
                                                                Learning with Unsupervised Auxiliary Tasks</a>,
                                                            Jaderberg et al, 2016. <strong>Algorithm: UNREAL.</strong>
                                                        </td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id46" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[46]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1707.03300.pdf">The
                                                                Intentional Unintentional Agent: Learning to Solve Many
                                                                Continuous Control Tasks Simultaneously</a>, Cabi et al,
                                                            2017. <strong>Algorithm: IU Agent.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id47" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[47]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1701.08734.pdf">PathNet:
                                                                Evolution Channels Gradient Descent in Super Neural
                                                                Networks</a>, Fernando et al, 2017. <strong>Algorithm:
                                                                PathNet.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id48" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[48]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1707.07907.pdf">Mutual
                                                                Alignment Transfer Learning</a>, Wulfmeier et al, 2017.
                                                            <strong>Algorithm: MATL.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id49" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[49]</td>
                                                        <td><a class="reference external"
                                                                href="https://openreview.net/forum?id=rk07ZXZRb&amp;noteId=rk07ZXZRb">Learning
                                                                an Embedding Space for Transferable Robot Skills</a>,
                                                            Hausman et al, 2018.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id50" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[50]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1707.01495.pdf">Hindsight
                                                                Experience Replay</a>, Andrychowicz et al, 2017.
                                                            <strong>Algorithm: Hindsight Experience Replay
                                                                (HER).</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                        </div>
                                        <div class="section" id="hierarchy">
                                            <h3><a class="toc-backref" href="#id110">4. Hierarchy</a><a
                                                    class="headerlink" href="#hierarchy"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <table class="docutils footnote" frame="void" id="id51" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[51]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1606.04695.pdf">Strategic
                                                                Attentive Writer for Learning Macro-Actions</a>,
                                                            Vezhnevets et al, 2016. <strong>Algorithm: STRAW.</strong>
                                                        </td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id52" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[52]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1703.01161.pdf">FeUdal
                                                                Networks for Hierarchical Reinforcement Learning</a>,
                                                            Vezhnevets et al, 2017. <strong>Algorithm: Feudal
                                                                Networks</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id53" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[53]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1805.08296.pdf">Data-Efficient
                                                                Hierarchical Reinforcement Learning</a>, Nachum et al,
                                                            2018. <strong>Algorithm: HIRO.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                        </div>
                                        <div class="section" id="memory">
                                            <h3><a class="toc-backref" href="#id111">5. Memory</a><a class="headerlink"
                                                    href="#memory" title="Permalink to this headline">¶</a></h3>
                                            <table class="docutils footnote" frame="void" id="id54" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[54]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1606.04460.pdf">Model-Free
                                                                Episodic Control</a>, Blundell et al, 2016.
                                                            <strong>Algorithm: MFEC.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id55" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[55]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1703.01988.pdf">Neural
                                                                Episodic Control</a>, Pritzel et al, 2017.
                                                            <strong>Algorithm: NEC.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id56" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[56]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1702.08360.pdf">Neural Map:
                                                                Structured Memory for Deep Reinforcement Learning</a>,
                                                            Parisotto and Salakhutdinov, 2017. <strong>Algorithm: Neural
                                                                Map.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id57" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[57]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1803.10760.pdf">Unsupervised
                                                                Predictive Memory in a Goal-Directed Agent</a>, Wayne et
                                                            al, 2018. <strong>Algorithm: MERLIN.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id58" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[58]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1806.01822.pdf">Relational
                                                                Recurrent Neural Networks</a>, Santoro et al, 2018.
                                                            <strong>Algorithm: RMC.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                        </div>
                                        <div class="section" id="model-based-rl">
                                            <h3><a class="toc-backref" href="#id112">6. Model-Based RL</a><a
                                                    class="headerlink" href="#model-based-rl"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <div class="section" id="a-model-is-learned">
                                                <h4>a. Model is Learned<a class="headerlink" href="#a-model-is-learned"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <table class="docutils footnote" frame="void" id="id59" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[59]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1707.06203.pdf">Imagination-Augmented
                                                                    Agents for Deep Reinforcement Learning</a>, Weber et
                                                                al, 2017. <strong>Algorithm: I2A.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id60" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[60]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1708.02596.pdf">Neural
                                                                    Network Dynamics for Model-Based Deep Reinforcement
                                                                    Learning with Model-Free Fine-Tuning</a>, Nagabandi
                                                                et al, 2017. <strong>Algorithm: MBMF.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id61" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[61]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1803.00101.pdf">Model-Based
                                                                    Value Expansion for Efficient Model-Free
                                                                    Reinforcement Learning</a>, Feinberg et al, 2018.
                                                                <strong>Algorithm: MVE.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id62" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[62]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1807.01675.pdf">Sample-Efficient
                                                                    Reinforcement Learning with Stochastic Ensemble
                                                                    Value Expansion</a>, Buckman et al, 2018.
                                                                <strong>Algorithm: STEVE.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id63" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[63]</td>
                                                            <td><a class="reference external"
                                                                    href="https://openreview.net/forum?id=SJJinbWRZ&amp;noteId=SJJinbWRZ">Model-Ensemble
                                                                    Trust-Region Policy Optimization</a>, Kurutach et
                                                                al, 2018. <strong>Algorithm: ME-TRPO.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id64" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[64]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1809.05214.pdf">Model-Based
                                                                    Reinforcement Learning via Meta-Policy
                                                                    Optimization</a>, Clavera et al, 2018.
                                                                <strong>Algorithm: MB-MPO.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id65" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[65]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1809.01999.pdf">Recurrent
                                                                    World Models Facilitate Policy Evolution</a>, Ha and
                                                                Schmidhuber, 2018.</td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                            </div>
                                            <div class="section" id="b-model-is-given">
                                                <h4>b. Model is Given<a class="headerlink" href="#b-model-is-given"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <table class="docutils footnote" frame="void" id="id66" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[66]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1712.01815.pdf">Mastering
                                                                    Chess and Shogi by Self-Play with a General
                                                                    Reinforcement Learning Algorithm</a>, Silver et al,
                                                                2017. <strong>Algorithm: AlphaZero.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <table class="docutils footnote" frame="void" id="id67" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label">[67]</td>
                                                            <td><a class="reference external"
                                                                    href="https://arxiv.org/pdf/1705.08439.pdf">Thinking
                                                                    Fast and Slow with Deep Learning and Tree
                                                                    Search</a>, Anthony et al, 2017. <strong>Algorithm:
                                                                    ExIt.</strong></td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                            </div>
                                        </div>
                                        <div class="section" id="meta-rl">
                                            <h3><a class="toc-backref" href="#id113">7. Meta-RL</a><a class="headerlink"
                                                    href="#meta-rl" title="Permalink to this headline">¶</a></h3>
                                            <table class="docutils footnote" frame="void" id="id68" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[68]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1611.02779.pdf">RL^2: Fast
                                                                Reinforcement Learning via Slow Reinforcement
                                                                Learning</a>, Duan et al, 2016. <strong>Algorithm:
                                                                RL^2.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id69" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[69]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1611.05763.pdf">Learning to
                                                                Reinforcement Learn</a>, Wang et al, 2016.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id70" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[70]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1703.03400.pdf">Model-Agnostic
                                                                Meta-Learning for Fast Adaptation of Deep Networks</a>,
                                                            Finn et al, 2017. <strong>Algorithm: MAML.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id71" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[71]</td>
                                                        <td><a class="reference external"
                                                                href="https://openreview.net/forum?id=B1DmUzWAW&amp;noteId=B1DmUzWAW">A
                                                                Simple Neural Attentive Meta-Learner</a>, Mishra et al,
                                                            2018. <strong>Algorithm: SNAIL.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                        </div>
                                        <div class="section" id="scaling-rl">
                                            <h3><a class="toc-backref" href="#id114">8. Scaling RL</a><a
                                                    class="headerlink" href="#scaling-rl"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <table class="docutils footnote" frame="void" id="id72" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[72]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1803.02811.pdf">Accelerated
                                                                Methods for Deep Reinforcement Learning</a>, Stooke and
                                                            Abbeel, 2018. <strong>Contribution:</strong> Systematic
                                                            analysis of parallelization in deep RL across algorithms.
                                                        </td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id73" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[73]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1802.01561.pdf">IMPALA:
                                                                Scalable Distributed Deep-RL with Importance Weighted
                                                                Actor-Learner Architectures</a>, Espeholt et al, 2018.
                                                            <strong>Algorithm: IMPALA.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id74" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[74]</td>
                                                        <td><a class="reference external"
                                                                href="https://openreview.net/forum?id=H1Dy---0Z">Distributed
                                                                Prioritized Experience Replay</a>, Horgan et al, 2018.
                                                            <strong>Algorithm: Ape-X.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id75" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[75]</td>
                                                        <td><a class="reference external"
                                                                href="https://openreview.net/forum?id=r1lyTjAqYX">Recurrent
                                                                Experience Replay in Distributed Reinforcement
                                                                Learning</a>, Anonymous, 2018. <strong>Algorithm:
                                                                R2D2.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id76" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[76]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1712.09381.pdf">RLlib:
                                                                Abstractions for Distributed Reinforcement Learning</a>,
                                                            Liang et al, 2017. <strong>Contribution:</strong> A scalable
                                                            library of RL algorithm implementations. <a
                                                                class="reference external"
                                                                href="https://ray.readthedocs.io/en/latest/rllib.html">Documentation
                                                                link.</a></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                        </div>
                                        <div class="section" id="rl-in-the-real-world">
                                            <h3><a class="toc-backref" href="#id115">9. RL in the Real World</a><a
                                                    class="headerlink" href="#rl-in-the-real-world"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <table class="docutils footnote" frame="void" id="id77" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[77]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1809.07731.pdf">Benchmarking
                                                                Reinforcement Learning Algorithms on Real-World
                                                                Robots</a>, Mahmood et al, 2018.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id78" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[78]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1808.00177.pdf">Learning
                                                                Dexterous In-Hand Manipulation</a>, OpenAI, 2018.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id79" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[79]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1806.10293.pdf">QT-Opt:
                                                                Scalable Deep Reinforcement Learning for Vision-Based
                                                                Robotic Manipulation</a>, Kalashnikov et al, 2018.
                                                            <strong>Algorithm: QT-Opt.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id80" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[80]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1811.00260.pdf">Horizon:
                                                                Facebook&#8217;s Open Source Applied Reinforcement
                                                                Learning Platform</a>, Gauci et al, 2018.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                        </div>
                                        <div class="section" id="safety">
                                            <h3><a class="toc-backref" href="#id116">10. Safety</a><a class="headerlink"
                                                    href="#safety" title="Permalink to this headline">¶</a></h3>
                                            <table class="docutils footnote" frame="void" id="id81" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[81]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1606.06565.pdf">Concrete
                                                                Problems in AI Safety</a>, Amodei et al, 2016.
                                                            <strong>Contribution:</strong> establishes a taxonomy of
                                                            safety problems, serving as an important jumping-off point
                                                            for future research. We need to solve these!</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id82" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[82]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1706.03741.pdf">Deep
                                                                Reinforcement Learning From Human Preferences</a>,
                                                            Christiano et al, 2017. <strong>Algorithm: LFP.</strong>
                                                        </td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id83" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[83]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1705.10528.pdf">Constrained
                                                                Policy Optimization</a>, Achiam et al, 2017.
                                                            <strong>Algorithm: CPO.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id84" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[84]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1801.08757.pdf">Safe
                                                                Exploration in Continuous Action Spaces</a>, Dalal et
                                                            al, 2018. <strong>Algorithm: DDPG+Safety Layer.</strong>
                                                        </td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id85" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[85]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1707.05173.pdf">Trial
                                                                without Error: Towards Safe Reinforcement Learning via
                                                                Human Intervention</a>, Saunders et al, 2017.
                                                            <strong>Algorithm: HIRL.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id86" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[86]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1711.06782.pdf">Leave No
                                                                Trace: Learning to Reset for Safe and Autonomous
                                                                Reinforcement Learning</a>, Eysenbach et al, 2017.
                                                            <strong>Algorithm: Leave No Trace.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                        </div>
                                        <div class="section" id="imitation-learning-and-inverse-reinforcement-learning">
                                            <h3><a class="toc-backref" href="#id117">11. Imitation Learning and Inverse
                                                    Reinforcement Learning</a><a class="headerlink"
                                                    href="#imitation-learning-and-inverse-reinforcement-learning"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <table class="docutils footnote" frame="void" id="id87" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[87]</td>
                                                        <td><a class="reference external"
                                                                href="http://www.cs.cmu.edu/~bziebart/publications/thesis-bziebart.pdf">Modeling
                                                                Purposeful Adaptive Behavior with the Principle of
                                                                Maximum Causal Entropy</a>, Ziebart 2010.
                                                            <strong>Contributions:</strong> Crisp formulation of maximum
                                                            entropy IRL.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id88" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[88]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1603.00448.pdf">Guided Cost
                                                                Learning: Deep Inverse Optimal Control via Policy
                                                                Optimization</a>, Finn et al, 2016. <strong>Algorithm:
                                                                GCL.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id89" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[89]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1606.03476.pdf">Generative
                                                                Adversarial Imitation Learning</a>, Ho and Ermon, 2016.
                                                            <strong>Algorithm: GAIL.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id90" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[90]</td>
                                                        <td><a class="reference external"
                                                                href="https://xbpeng.github.io/projects/DeepMimic/2018_TOG_DeepMimic.pdf">DeepMimic:
                                                                Example-Guided Deep Reinforcement Learning of
                                                                Physics-Based Character Skills</a>, Peng et al, 2018.
                                                            <strong>Algorithm: DeepMimic.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id91" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[91]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1810.00821.pdf">Variational
                                                                Discriminator Bottleneck: Improving Imitation Learning,
                                                                Inverse RL, and GANs by Constraining Information
                                                                Flow</a>, Peng et al, 2018. <strong>Algorithm:
                                                                VAIL.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id92" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[92]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1810.05017.pdf">One-Shot
                                                                High-Fidelity Imitation: Training Large-Scale Deep Nets
                                                                with RL</a>, Le Paine et al, 2018. <strong>Algorithm:
                                                                MetaMimic.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                        </div>
                                        <div class="section" id="reproducibility-analysis-and-critique">
                                            <h3><a class="toc-backref" href="#id118">12. Reproducibility, Analysis, and
                                                    Critique</a><a class="headerlink"
                                                    href="#reproducibility-analysis-and-critique"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <table class="docutils footnote" frame="void" id="id93" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[93]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1604.06778.pdf">Benchmarking
                                                                Deep Reinforcement Learning for Continuous Control</a>,
                                                            Duan et al, 2016. <strong>Contribution: rllab.</strong></td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id94" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[94]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1708.04133.pdf">Reproducibility
                                                                of Benchmarked Deep Reinforcement Learning Tasks for
                                                                Continuous Control</a>, Islam et al, 2017.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id95" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[95]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1709.06560.pdf">Deep
                                                                Reinforcement Learning that Matters</a>, Henderson et
                                                            al, 2017.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id96" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[96]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1810.02525.pdf">Where Did My
                                                                Optimum Go?: An Empirical Analysis of Gradient Descent
                                                                Optimization in Policy Gradient Methods</a>, Henderson
                                                            et al, 2018.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id97" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[97]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1811.02553.pdf">Are Deep
                                                                Policy Gradient Algorithms Truly Policy Gradient
                                                                Algorithms?</a>, Ilyas et al, 2018.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id98" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[98]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1803.07055.pdf">Simple
                                                                Random Search Provides a Competitive Approach to
                                                                Reinforcement Learning</a>, Mania et al, 2018.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id99" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[99]</td>
                                                        <td><a class="reference external"
                                                                href="https://arxiv.org/pdf/1907.02057.pdf">Benchmarking
                                                                Model-Based Reinforcement Learning</a>, Wang et al,
                                                            2019.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                        </div>
                                        <div class="section" id="bonus-classic-papers-in-rl-theory-or-review">
                                            <h3><a class="toc-backref" href="#id119">13. Bonus: Classic Papers in RL
                                                    Theory or Review</a><a class="headerlink"
                                                    href="#bonus-classic-papers-in-rl-theory-or-review"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <table class="docutils footnote" frame="void" id="id100" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[100]</td>
                                                        <td><a class="reference external"
                                                                href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf">Policy
                                                                Gradient Methods for Reinforcement Learning with
                                                                Function Approximation</a>, Sutton et al, 2000.
                                                            <strong>Contributions:</strong> Established policy gradient
                                                            theorem and showed convergence of policy gradient algorithm
                                                            for arbitrary policy classes.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id101" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[101]</td>
                                                        <td><a class="reference external"
                                                                href="http://web.mit.edu/jnt/www/Papers/J063-97-bvr-td.pdf">An
                                                                Analysis of Temporal-Difference Learning with Function
                                                                Approximation</a>, Tsitsiklis and Van Roy, 1997.
                                                            <strong>Contributions:</strong> Variety of convergence
                                                            results and counter-examples for value-learning methods in
                                                            RL.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id102" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[102]</td>
                                                        <td><a class="reference external"
                                                                href="http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/Neural-Netw-2008-21-682_4867%5b0%5d.pdf">Reinforcement
                                                                Learning of Motor Skills with Policy Gradients</a>,
                                                            Peters and Schaal, 2008. <strong>Contributions:</strong>
                                                            Thorough review of policy gradient methods at the time, many
                                                            of which are still serviceable descriptions of deep RL
                                                            methods.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id103" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[103]</td>
                                                        <td><a class="reference external"
                                                                href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf">Approximately
                                                                Optimal Approximate Reinforcement Learning</a>, Kakade
                                                            and Langford, 2002. <strong>Contributions:</strong> Early
                                                            roots for monotonic improvement theory, later leading to
                                                            theoretical justification for TRPO and other algorithms.
                                                        </td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id104" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[104]</td>
                                                        <td><a class="reference external"
                                                                href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf">A
                                                                Natural Policy Gradient</a>, Kakade, 2002.
                                                            <strong>Contributions:</strong> Brought natural gradients
                                                            into RL, later leading to TRPO, ACKTR, and several other
                                                            methods in deep RL.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <table class="docutils footnote" frame="void" id="id105" rules="none">
                                                <colgroup>
                                                    <col class="label" />
                                                    <col />
                                                </colgroup>
                                                <tbody valign="top">
                                                    <tr>
                                                        <td class="label">[105]</td>
                                                        <td><a class="reference external"
                                                                href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">Algorithms
                                                                for Reinforcement Learning</a>, Szepesvari, 2009.
                                                            <strong>Contributions:</strong> Unbeatable reference on RL
                                                            before deep RL, containing foundations and theoretical
                                                            background.</td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                        </div>
                                    </div>
                                    <span id="document-spinningup/exercises"></span>
                                    <div class="section" id="exercises">
                                        <h2><a class="toc-backref" href="#id2">Exercises</a><a class="headerlink"
                                                href="#exercises" title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal" href="#exercises"
                                                        id="id2">Exercises</a>
                                                    <ul>
                                                        <li><a class="reference internal"
                                                                href="#problem-set-1-basics-of-implementation"
                                                                id="id3">Problem Set 1: Basics of Implementation</a>
                                                        </li>
                                                        <li><a class="reference internal"
                                                                href="#problem-set-2-algorithm-failure-modes"
                                                                id="id4">Problem Set 2: Algorithm Failure Modes</a></li>
                                                        <li><a class="reference internal" href="#challenges"
                                                                id="id5">Challenges</a></li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <div class="section" id="problem-set-1-basics-of-implementation">
                                            <h3><a class="toc-backref" href="#id3">Problem Set 1: Basics of
                                                    Implementation</a><a class="headerlink"
                                                    href="#problem-set-1-basics-of-implementation"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <div class="admonition-exercise-1-1-gaussian-log-likelihood admonition">
                                                <p class="first admonition-title">Exercise 1.1: Gaussian Log-Likelihood
                                                </p>
                                                <p><strong>Path to Exercise:</strong></p>
                                                <ul class="simple">
                                                    <li>PyTorch version: <code
                                                            class="docutils literal"><span class="pre">spinup/exercises/pytorch/problem_set_1/exercise1_1.py</span></code>
                                                    </li>
                                                    <li>Tensorflow version: <code
                                                            class="docutils literal"><span class="pre">spinup/exercises/tf1/problem_set_1/exercise1_1.py</span></code>
                                                    </li>
                                                </ul>
                                                <p><strong>Path to Solution:</strong></p>
                                                <ul class="simple">
                                                    <li>PyTorch version: <code
                                                            class="docutils literal"><span class="pre">spinup/exercises/pytorch/problem_set_1_solutions/exercise1_1_soln.py</span></code>
                                                    </li>
                                                    <li>Tensorflow version: <code
                                                            class="docutils literal"><span class="pre">spinup/exercises/tf1/problem_set_1_solutions/exercise1_1_soln.py</span></code>
                                                    </li>
                                                </ul>
                                                <p><strong>Instructions.</strong> Write a function that takes in the
                                                    means and log stds of a batch of diagonal Gaussian distributions,
                                                    along with (previously-generated) samples from those distributions,
                                                    and returns the log likelihoods of those samples. (In the Tensorflow
                                                    version, you will write a function that creates computation graph
                                                    operations to do this; in the PyTorch version, you will directly
                                                    operate on given Tensors.)</p>
                                                <p>You may find it useful to review the formula given in <a
                                                        class="reference external"
                                                        href="../spinningup/rl_intro.html#stochastic-policies">this
                                                        section of the RL introduction</a>.</p>
                                                <p>Implement your solution in <code
                                                        class="docutils literal"><span class="pre">exercise1_1.py</span></code>,
                                                    and run that file to automatically check your work.</p>
                                                <p class="last"><strong>Evaluation Criteria.</strong> Your solution will
                                                    be checked by comparing outputs against a known-good implementation,
                                                    using a batch of random inputs.</p>
                                            </div>
                                            <div class="admonition-exercise-1-2-policy-for-ppo admonition">
                                                <p class="first admonition-title">Exercise 1.2: Policy for PPO</p>
                                                <p><strong>Path to Exercise:</strong></p>
                                                <ul class="simple">
                                                    <li>PyTorch version: <code
                                                            class="docutils literal"><span class="pre">spinup/exercises/pytorch/problem_set_1/exercise1_2.py</span></code>
                                                    </li>
                                                    <li>Tensorflow version: <code
                                                            class="docutils literal"><span class="pre">spinup/exercises/tf1/problem_set_1/exercise1_2.py</span></code>
                                                    </li>
                                                </ul>
                                                <p><strong>Path to Solution:</strong></p>
                                                <ul class="simple">
                                                    <li>PyTorch version: <code
                                                            class="docutils literal"><span class="pre">spinup/exercises/pytorch/problem_set_1_solutions/exercise1_2_soln.py</span></code>
                                                    </li>
                                                    <li>Tensorflow version: <code
                                                            class="docutils literal"><span class="pre">spinup/exercises/tf1/problem_set_1_solutions/exercise1_2_soln.py</span></code>
                                                    </li>
                                                </ul>
                                                <p><strong>Instructions.</strong> Implement an MLP diagonal Gaussian
                                                    policy for PPO.</p>
                                                <p>Implement your solution in <code
                                                        class="docutils literal"><span class="pre">exercise1_2.py</span></code>,
                                                    and run that file to automatically check your work.</p>
                                                <p class="last"><strong>Evaluation Criteria.</strong> Your solution will
                                                    be evaluated by running for 20 epochs in the InvertedPendulum-v2 Gym
                                                    environment, and this should take in the ballpark of 3-5 minutes
                                                    (depending on your machine, and other processes you are running in
                                                    the background). The bar for success is reaching an average score of
                                                    over 500 in the last 5 epochs, or getting to a score of 1000 (the
                                                    maximum possible score) in the last 5 epochs.</p>
                                            </div>
                                            <div class="admonition-exercise-1-3-computation-graph-for-td3 admonition">
                                                <p class="first admonition-title">Exercise 1.3: Computation Graph for
                                                    TD3</p>
                                                <p><strong>Path to Exercise.</strong></p>
                                                <ul class="simple">
                                                    <li>PyTorch version: <code
                                                            class="docutils literal"><span class="pre">spinup/exercises/pytorch/problem_set_1/exercise1_3.py</span></code>
                                                    </li>
                                                    <li>Tensorflow version: <code
                                                            class="docutils literal"><span class="pre">spinup/exercises/tf1/problem_set_1/exercise1_3.py</span></code>
                                                    </li>
                                                </ul>
                                                <p><strong>Path to Solution.</strong></p>
                                                <ul class="simple">
                                                    <li>PyTorch version: <code
                                                            class="docutils literal"><span class="pre">spinup/algos/pytorch/td3/td3.py</span></code>
                                                    </li>
                                                    <li>Tensorflow version: <code
                                                            class="docutils literal"><span class="pre">spinup/algos/tf1/td3/td3.py</span></code>
                                                    </li>
                                                </ul>
                                                <p><strong>Instructions.</strong> Implement the main mathematical logic
                                                    for the TD3 algorithm.</p>
                                                <p>As starter code, you are given the entirety of the TD3 algorithm
                                                    except for the main mathematical logic (essentially, the loss
                                                    functions and intermediate calculations needed for them). Find
                                                    &#8220;YOUR CODE HERE&#8221; to begin.</p>
                                                <p>You may find it useful to review the pseudocode in our <a
                                                        class="reference external" href="../algorithms/td3.html">page on
                                                        TD3</a>.</p>
                                                <p>Implement your solution in <code
                                                        class="docutils literal"><span class="pre">exercise1_3.py</span></code>,
                                                    and run that file to see the results of your work. There is no
                                                    automatic checking for this exercise.</p>
                                                <p><strong>Evaluation Criteria.</strong> Evaluate your code by running
                                                    <code
                                                        class="docutils literal"><span class="pre">exercise1_3.py</span></code>
                                                    with HalfCheetah-v2, InvertedPendulum-v2, and one other Gym MuJoCo
                                                    environment of your choosing (set via the <code
                                                        class="docutils literal"><span class="pre">--env</span></code>
                                                    flag). It is set up to use smaller neural networks (hidden sizes
                                                    [128,128]) than typical for TD3, with a maximum episode length of
                                                    150, and to run for only 10 epochs. The goal is to see significant
                                                    learning progress relatively quickly (in terms of wall clock time).
                                                    Experiments will likely take on the order of ~10 minutes.</p>
                                                <p class="last">Use the <code
                                                        class="docutils literal"><span class="pre">--use_soln</span></code>
                                                    flag to run Spinning Up&#8217;s TD3 instead of your implementation.
                                                    Anecdotally, within 10 epochs, the score in HalfCheetah should go
                                                    over 300, and the score in InvertedPendulum should max out at 150.
                                                </p>
                                            </div>
                                        </div>
                                        <div class="section" id="problem-set-2-algorithm-failure-modes">
                                            <h3><a class="toc-backref" href="#id4">Problem Set 2: Algorithm Failure
                                                    Modes</a><a class="headerlink"
                                                    href="#problem-set-2-algorithm-failure-modes"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <div
                                                class="admonition-exercise-2-1-value-function-fitting-in-trpo admonition">
                                                <p class="first admonition-title">Exercise 2.1: Value Function Fitting
                                                    in TRPO</p>
                                                <p><strong>Path to Exercise.</strong> (Not applicable, there is no code
                                                    for this one.)</p>
                                                <p><strong>Path to Solution.</strong> <a class="reference external"
                                                        href="../spinningup/exercise2_1_soln.html">Solution available
                                                        here.</a></p>
                                                <p>Many factors can impact the performance of policy gradient
                                                    algorithms, but few more drastically than the quality of the learned
                                                    value function used for advantage estimation.</p>
                                                <p>In this exercise, you will compare results between runs of TRPO where
                                                    you put lots of effort into fitting the value function (<code
                                                        class="docutils literal"><span class="pre">train_v_iters=80</span></code>),
                                                    versus where you put very little effort into fitting the value
                                                    function (<code
                                                        class="docutils literal"><span class="pre">train_v_iters=0</span></code>).
                                                </p>
                                                <p><strong>Instructions.</strong> Run the following command:</p>
                                                <div class="highlight-default">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spinup</span><span class="o">.</span><span class="n">run</span> <span class="n">trpo</span> <span class="o">--</span><span class="n">env</span> <span class="n">Hopper</span><span class="o">-</span><span class="n">v2</span> <span class="o">--</span><span class="n">train_v_iters</span><span class="p">[</span><span class="n">v</span><span class="p">]</span> <span class="mi">0</span> <span class="mi">80</span> <span class="o">--</span><span class="n">exp_name</span> <span class="n">ex2</span><span class="o">-</span><span class="mi">1</span> <span class="o">--</span><span class="n">epochs</span> <span class="mi">250</span> <span class="o">--</span><span class="n">steps_per_epoch</span> <span class="mi">4000</span> <span class="o">--</span><span class="n">seed</span> <span class="mi">0</span> <span class="mi">10</span> <span class="mi">20</span> <span class="o">--</span><span class="n">dt</span>
</pre>
                                                    </div>
                                                </div>
                                                <p class="last">and plot the results. (These experiments might take ~10
                                                    minutes each, and this command runs six of them.) What do you find?
                                                </p>
                                            </div>
                                            <div class="admonition-exercise-2-2-silent-bug-in-ddpg admonition">
                                                <p class="first admonition-title">Exercise 2.2: Silent Bug in DDPG</p>
                                                <p><strong>Path to Exercise.</strong></p>
                                                <ul class="simple">
                                                    <li>PyTorch version: <code
                                                            class="docutils literal"><span class="pre">spinup/exercises/pytorch/problem_set_2/exercise2_2.py</span></code>
                                                    </li>
                                                    <li>Tensorflow version: <code
                                                            class="docutils literal"><span class="pre">spinup/exercises/tf1/problem_set_2/exercise2_2.py</span></code>
                                                    </li>
                                                </ul>
                                                <p><strong>Path to Solution.</strong> <a class="reference external"
                                                        href="../spinningup/exercise2_2_soln.html">Solution available
                                                        here.</a></p>
                                                <p>The hardest part of writing RL code is dealing with bugs, because
                                                    failures are frequently silent. The code will appear to run
                                                    correctly, but the agent&#8217;s performance will degrade relative
                                                    to a bug-free implementation&#8212;sometimes to the extent that it
                                                    never learns anything.</p>
                                                <p>In this exercise, you will observe a bug in vivo and compare results
                                                    against correct code. The bug is the same (conceptually, if not in
                                                    exact implementation) for both the PyTorch and Tensorflow versions
                                                    of this exercise.</p>
                                                <p><strong>Instructions.</strong> Run <code
                                                        class="docutils literal"><span class="pre">exercise2_2.py</span></code>,
                                                    which will launch DDPG experiments with and without a bug. The
                                                    non-bugged version runs the default Spinning Up implementation of
                                                    DDPG, using a default method for creating the actor and critic
                                                    networks. The bugged version runs the same DDPG code, except uses a
                                                    bugged method for creating the networks.</p>
                                                <p>There will be six experiments in all (three random seeds for each
                                                    case), and each should take in the ballpark of 10 minutes. When
                                                    they&#8217;re finished, plot the results. What is the difference in
                                                    performance with and without the bug?</p>
                                                <p>Without referencing the correct actor-critic code (which is to
                                                    say&#8212;don&#8217;t look in DDPG&#8217;s <code
                                                        class="docutils literal"><span class="pre">core.py</span></code>
                                                    file), try to figure out what the bug is and explain how it breaks
                                                    things.</p>
                                                <p><strong>Hint.</strong> To figure out what&#8217;s going wrong, think
                                                    about how the DDPG code implements the DDPG computation graph. For
                                                    the Tensorflow version, look at this excerpt:</p>
                                                <div class="highlight-python">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="c1"># Bellman backup for Q function</span>
<span class="n">backup</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">r_ph</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">d_ph</span><span class="p">)</span><span class="o">*</span><span class="n">q_pi_targ</span><span class="p">)</span>

<span class="c1"># DDPG losses</span>
<span class="n">pi_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">q_pi</span><span class="p">)</span>
<span class="n">q_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">((</span><span class="n">q</span><span class="o">-</span><span class="n">backup</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre>
                                                    </div>
                                                </div>
                                                <p>How could a bug in the actor-critic code have an impact here?</p>
                                                <p class="last"><strong>Bonus.</strong> Are there any choices of
                                                    hyperparameters which would have hidden the effects of the bug?</p>
                                            </div>
                                        </div>
                                        <div class="section" id="challenges">
                                            <h3><a class="toc-backref" href="#id5">Challenges</a><a class="headerlink"
                                                    href="#challenges" title="Permalink to this headline">¶</a></h3>
                                            <div class="admonition-write-code-from-scratch admonition">
                                                <p class="first admonition-title">Write Code from Scratch</p>
                                                <p class="last">As we suggest in <a class="reference external"
                                                        href="../spinningup/spinningup.html#learn-by-doing">the
                                                        essay</a>, try reimplementing various deep RL algorithms from
                                                    scratch.</p>
                                            </div>
                                            <div class="admonition-requests-for-research admonition">
                                                <p class="first admonition-title">Requests for Research</p>
                                                <p>If you feel comfortable with writing deep learning and deep RL code,
                                                    consider trying to make progress on any of OpenAI&#8217;s standing
                                                    requests for research:</p>
                                                <ul class="last simple">
                                                    <li><a class="reference external"
                                                            href="https://openai.com/requests-for-research/">Requests
                                                            for Research 1</a></li>
                                                    <li><a class="reference external"
                                                            href="https://blog.openai.com/requests-for-research-2/">Requests
                                                            for Research 2</a></li>
                                                </ul>
                                            </div>
                                        </div>
                                    </div>
                                    <span id="document-spinningup/bench"></span>
                                    <div class="section" id="benchmarks-for-spinning-up-implementations">
                                        <h2><a class="toc-backref" href="#id11">Benchmarks for Spinning Up
                                                Implementations</a><a class="headerlink"
                                                href="#benchmarks-for-spinning-up-implementations"
                                                title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal"
                                                        href="#benchmarks-for-spinning-up-implementations"
                                                        id="id11">Benchmarks for Spinning Up Implementations</a>
                                                    <ul>
                                                        <li><a class="reference internal"
                                                                href="#performance-in-each-environment"
                                                                id="id12">Performance in Each Environment</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#halfcheetah-pytorch-versions"
                                                                        id="id13">HalfCheetah: PyTorch Versions</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#halfcheetah-tensorflow-versions"
                                                                        id="id14">HalfCheetah: Tensorflow Versions</a>
                                                                </li>
                                                                <li><a class="reference internal"
                                                                        href="#hopper-pytorch-versions"
                                                                        id="id15">Hopper: PyTorch Versions</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#hopper-tensorflow-versions"
                                                                        id="id16">Hopper: Tensorflow Versions</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#walker2d-pytorch-versions"
                                                                        id="id17">Walker2d: PyTorch Versions</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#walker2d-tensorflow-versions"
                                                                        id="id18">Walker2d: Tensorflow Versions</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#swimmer-pytorch-versions"
                                                                        id="id19">Swimmer: PyTorch Versions</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#swimmer-tensorflow-versions"
                                                                        id="id20">Swimmer: Tensorflow Versions</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#ant-pytorch-versions" id="id21">Ant:
                                                                        PyTorch Versions</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#ant-tensorflow-versions" id="id22">Ant:
                                                                        Tensorflow Versions</a></li>
                                                            </ul>
                                                        </li>
                                                        <li><a class="reference internal" href="#experiment-details"
                                                                id="id23">Experiment Details</a></li>
                                                        <li><a class="reference internal" href="#pytorch-vs-tensorflow"
                                                                id="id24">PyTorch vs Tensorflow</a></li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <p>We benchmarked the Spinning Up algorithm implementations in five environments
                                            from the <a class="reference external"
                                                href="https://gym.openai.com/envs/#mujoco">MuJoCo</a> Gym task suite:
                                            HalfCheetah, Hopper, Walker2d, Swimmer, and Ant.</p>
                                        <div class="section" id="performance-in-each-environment">
                                            <h3><a class="toc-backref" href="#id12">Performance in Each
                                                    Environment</a><a class="headerlink"
                                                    href="#performance-in-each-environment"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <div class="section" id="halfcheetah-pytorch-versions">
                                                <h4><a class="toc-backref" href="#id13">HalfCheetah: PyTorch
                                                        Versions</a><a class="headerlink"
                                                        href="#halfcheetah-pytorch-versions"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <div class="figure align-center" id="id1">
                                                    <img alt="_images/pytorch_halfcheetah_performance.svg"
                                                        src="_images/pytorch_halfcheetah_performance.svg" />
                                                    <p class="caption"><span class="caption-text">3M timestep benchmark
                                                            for HalfCheetah-v3 using <strong>PyTorch</strong>
                                                            implementations.</span></p>
                                                </div>
                                            </div>
                                            <div class="section" id="halfcheetah-tensorflow-versions">
                                                <h4><a class="toc-backref" href="#id14">HalfCheetah: Tensorflow
                                                        Versions</a><a class="headerlink"
                                                        href="#halfcheetah-tensorflow-versions"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <div class="figure align-center" id="id2">
                                                    <img alt="_images/tensorflow_halfcheetah_performance.svg"
                                                        src="_images/tensorflow_halfcheetah_performance.svg" />
                                                    <p class="caption"><span class="caption-text">3M timestep benchmark
                                                            for HalfCheetah-v3 using <strong>Tensorflow</strong>
                                                            implementations.</span></p>
                                                </div>
                                            </div>
                                            <div class="section" id="hopper-pytorch-versions">
                                                <h4><a class="toc-backref" href="#id15">Hopper: PyTorch Versions</a><a
                                                        class="headerlink" href="#hopper-pytorch-versions"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <div class="figure align-center" id="id3">
                                                    <img alt="_images/pytorch_hopper_performance.svg"
                                                        src="_images/pytorch_hopper_performance.svg" />
                                                    <p class="caption"><span class="caption-text">3M timestep benchmark
                                                            for Hopper-v3 using <strong>PyTorch</strong>
                                                            implementations.</span></p>
                                                </div>
                                            </div>
                                            <div class="section" id="hopper-tensorflow-versions">
                                                <h4><a class="toc-backref" href="#id16">Hopper: Tensorflow
                                                        Versions</a><a class="headerlink"
                                                        href="#hopper-tensorflow-versions"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <div class="figure align-center" id="id4">
                                                    <img alt="_images/tensorflow_hopper_performance.svg"
                                                        src="_images/tensorflow_hopper_performance.svg" />
                                                    <p class="caption"><span class="caption-text">3M timestep benchmark
                                                            for Hopper-v3 using <strong>Tensorflow</strong>
                                                            implementations.</span></p>
                                                </div>
                                            </div>
                                            <div class="section" id="walker2d-pytorch-versions">
                                                <h4><a class="toc-backref" href="#id17">Walker2d: PyTorch Versions</a><a
                                                        class="headerlink" href="#walker2d-pytorch-versions"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <div class="figure align-center" id="id5">
                                                    <img alt="_images/pytorch_walker2d_performance.svg"
                                                        src="_images/pytorch_walker2d_performance.svg" />
                                                    <p class="caption"><span class="caption-text">3M timestep benchmark
                                                            for Walker2d-v3 using <strong>PyTorch</strong>
                                                            implementations.</span></p>
                                                </div>
                                            </div>
                                            <div class="section" id="walker2d-tensorflow-versions">
                                                <h4><a class="toc-backref" href="#id18">Walker2d: Tensorflow
                                                        Versions</a><a class="headerlink"
                                                        href="#walker2d-tensorflow-versions"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <div class="figure align-center" id="id6">
                                                    <img alt="_images/tensorflow_walker2d_performance.svg"
                                                        src="_images/tensorflow_walker2d_performance.svg" />
                                                    <p class="caption"><span class="caption-text">3M timestep benchmark
                                                            for Walker2d-v3 using <strong>Tensorflow</strong>
                                                            implementations.</span></p>
                                                </div>
                                            </div>
                                            <div class="section" id="swimmer-pytorch-versions">
                                                <h4><a class="toc-backref" href="#id19">Swimmer: PyTorch Versions</a><a
                                                        class="headerlink" href="#swimmer-pytorch-versions"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <div class="figure align-center" id="id7">
                                                    <img alt="_images/pytorch_swimmer_performance.svg"
                                                        src="_images/pytorch_swimmer_performance.svg" />
                                                    <p class="caption"><span class="caption-text">3M timestep benchmark
                                                            for Swimmer-v3 using <strong>PyTorch</strong>
                                                            implementations.</span></p>
                                                </div>
                                            </div>
                                            <div class="section" id="swimmer-tensorflow-versions">
                                                <h4><a class="toc-backref" href="#id20">Swimmer: Tensorflow
                                                        Versions</a><a class="headerlink"
                                                        href="#swimmer-tensorflow-versions"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <div class="figure align-center" id="id8">
                                                    <img alt="_images/tensorflow_swimmer_performance.svg"
                                                        src="_images/tensorflow_swimmer_performance.svg" />
                                                    <p class="caption"><span class="caption-text">3M timestep benchmark
                                                            for Swimmer-v3 using <strong>Tensorflow</strong>
                                                            implementations.</span></p>
                                                </div>
                                            </div>
                                            <div class="section" id="ant-pytorch-versions">
                                                <h4><a class="toc-backref" href="#id21">Ant: PyTorch Versions</a><a
                                                        class="headerlink" href="#ant-pytorch-versions"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <div class="figure align-center" id="id9">
                                                    <img alt="_images/pytorch_ant_performance.svg"
                                                        src="_images/pytorch_ant_performance.svg" />
                                                    <p class="caption"><span class="caption-text">3M timestep benchmark
                                                            for Ant-v3 using <strong>PyTorch</strong>
                                                            implementations.</span></p>
                                                </div>
                                            </div>
                                            <div class="section" id="ant-tensorflow-versions">
                                                <h4><a class="toc-backref" href="#id22">Ant: Tensorflow Versions</a><a
                                                        class="headerlink" href="#ant-tensorflow-versions"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <div class="figure align-center" id="id10">
                                                    <img alt="_images/tensorflow_ant_performance.svg"
                                                        src="_images/tensorflow_ant_performance.svg" />
                                                    <p class="caption"><span class="caption-text">3M timestep benchmark
                                                            for Ant-v3 using <strong>Tensorflow</strong>
                                                            implementations.</span></p>
                                                </div>
                                            </div>
                                        </div>
                                        <div class="section" id="experiment-details">
                                            <h3><a class="toc-backref" href="#id23">Experiment Details</a><a
                                                    class="headerlink" href="#experiment-details"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p><strong>Random seeds.</strong> All experiments were run for 10 random
                                                seeds each. Graphs show the average (solid line) and std dev (shaded) of
                                                performance over random seed over the course of training.</p>
                                            <p><strong>Performance metric.</strong> Performance for the on-policy
                                                algorithms is measured as the average trajectory return across the batch
                                                collected at each epoch. Performance for the off-policy algorithms is
                                                measured once every 10,000 steps by running the deterministic policy
                                                (or, in the case of SAC, the mean policy) without action noise for ten
                                                trajectories, and reporting the average return over those test
                                                trajectories.</p>
                                            <p><strong>Network architectures.</strong> The on-policy algorithms use
                                                networks of size (64, 32) with tanh units for both the policy and the
                                                value function. The off-policy algorithms use networks of size (256,
                                                256) with relu units.</p>
                                            <p><strong>Batch size.</strong> The on-policy algorithms collected 4000
                                                steps of agent-environment interaction per batch update. The off-policy
                                                algorithms used minibatches of size 100 at each gradient descent step.
                                            </p>
                                            <p>All other hyperparameters are left at default settings for the Spinning
                                                Up implementations. See algorithm pages for details.</p>
                                            <p>Learning curves are smoothed by averaging over a window of 11 epochs.</p>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p>By comparison to the literature, the Spinning Up implementations of
                                                    DDPG, TD3, and SAC are roughly at-parity with the best reported
                                                    results for these algorithms. As a result, you can use the Spinning
                                                    Up implementations of these algorithms for research purposes.</p>
                                                <p class="last">The Spinning Up implementations of VPG, TRPO, and PPO
                                                    are overall a bit weaker than the best reported results for these
                                                    algorithms. This is due to the absence of some standard tricks (such
                                                    as observation normalization and normalized value regression
                                                    targets) from our implementations. For research comparisons, you
                                                    should use the implementations of TRPO or PPO from <a
                                                        class="reference external"
                                                        href="https://github.com/openai/baselines">OpenAI Baselines</a>.
                                                </p>
                                            </div>
                                        </div>
                                        <div class="section" id="pytorch-vs-tensorflow">
                                            <h3><a class="toc-backref" href="#id24">PyTorch vs Tensorflow</a><a
                                                    class="headerlink" href="#pytorch-vs-tensorflow"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>We provide graphs for head-to-head comparisons between the PyTorch and
                                                Tensorflow implementations of each algorithm at the following pages:</p>
                                            <ul class="simple">
                                                <li><a class="reference external"
                                                        href="../spinningup/bench_vpg.html">VPG Head-to-Head</a></li>
                                                <li><a class="reference external"
                                                        href="../spinningup/bench_ppo.html">PPO Head-to-Head</a></li>
                                                <li><a class="reference external"
                                                        href="../spinningup/bench_ddpg.html">DDPG Head-to-Head</a></li>
                                                <li><a class="reference external"
                                                        href="../spinningup/bench_td3.html">TD3 Head-to-Head</a></li>
                                                <li><a class="reference external"
                                                        href="../spinningup/bench_sac.html">SAC Head-to-Head</a></li>
                                            </ul>
                                        </div>
                                    </div>
                                </div>
                                <div class="toctree-wrapper compound">
                                    <span id="document-algorithms/vpg"></span>
                                    <div class="section" id="vanilla-policy-gradient">
                                        <h2><a class="toc-backref" href="#id1">Vanilla Policy Gradient</a><a
                                                class="headerlink" href="#vanilla-policy-gradient"
                                                title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal" href="#vanilla-policy-gradient"
                                                        id="id1">Vanilla Policy Gradient</a>
                                                    <ul>
                                                        <li><a class="reference internal" href="#background"
                                                                id="id2">Background</a>
                                                            <ul>
                                                                <li><a class="reference internal" href="#quick-facts"
                                                                        id="id3">Quick Facts</a></li>
                                                                <li><a class="reference internal" href="#key-equations"
                                                                        id="id4">Key Equations</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#exploration-vs-exploitation"
                                                                        id="id5">Exploration vs. Exploitation</a></li>
                                                                <li><a class="reference internal" href="#pseudocode"
                                                                        id="id6">Pseudocode</a></li>
                                                            </ul>
                                                        </li>
                                                        <li><a class="reference internal" href="#documentation"
                                                                id="id7">Documentation</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#documentation-pytorch-version"
                                                                        id="id8">Documentation: PyTorch Version</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#saved-model-contents-pytorch-version"
                                                                        id="id9">Saved Model Contents: PyTorch
                                                                        Version</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#documentation-tensorflow-version"
                                                                        id="id10">Documentation: Tensorflow Version</a>
                                                                </li>
                                                                <li><a class="reference internal"
                                                                        href="#saved-model-contents-tensorflow-version"
                                                                        id="id11">Saved Model Contents: Tensorflow
                                                                        Version</a></li>
                                                            </ul>
                                                        </li>
                                                        <li><a class="reference internal" href="#references"
                                                                id="id12">References</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#relevant-papers" id="id13">Relevant
                                                                        Papers</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#why-these-papers" id="id14">Why These
                                                                        Papers?</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#other-public-implementations"
                                                                        id="id15">Other Public Implementations</a></li>
                                                            </ul>
                                                        </li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <div class="section" id="background">
                                            <h3><a class="toc-backref" href="#id2">Background</a><a class="headerlink"
                                                    href="#background" title="Permalink to this headline">¶</a></h3>
                                            <p>(Previously: <a class="reference external"
                                                    href="../spinningup/rl_intro3.html">Introduction to RL, Part 3</a>)
                                            </p>
                                            <p>The key idea underlying policy gradients is to push up the probabilities
                                                of actions that lead to higher return, and push down the probabilities
                                                of actions that lead to lower return, until you arrive at the optimal
                                                policy.</p>
                                            <div class="section" id="quick-facts">
                                                <h4><a class="toc-backref" href="#id3">Quick Facts</a><a
                                                        class="headerlink" href="#quick-facts"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <ul class="simple">
                                                    <li>VPG is an on-policy algorithm.</li>
                                                    <li>VPG can be used for environments with either discrete or
                                                        continuous action spaces.</li>
                                                    <li>The Spinning Up implementation of VPG supports parallelization
                                                        with MPI.</li>
                                                </ul>
                                            </div>
                                            <div class="section" id="key-equations">
                                                <h4><a class="toc-backref" href="#id4">Key Equations</a><a
                                                        class="headerlink" href="#key-equations"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>Let <img class="math"
                                                        src="_images/math/6a71f04b65d9524fb656715cda85d7540a9ddf9f.svg"
                                                        alt="\pi_{\theta}" /> denote a policy with parameters <img
                                                        class="math"
                                                        src="_images/math/ce5edddd490112350f4bd555d9390e0e845f754a.svg"
                                                        alt="\theta" />, and <img class="math"
                                                        src="_images/math/96b876944de9cf0f980fe261562e8e07029245bf.svg"
                                                        alt="J(\pi_{\theta})" /> denote the expected finite-horizon
                                                    undiscounted return of the policy. The gradient of <img class="math"
                                                        src="_images/math/96b876944de9cf0f980fe261562e8e07029245bf.svg"
                                                        alt="J(\pi_{\theta})" /> is</p>
                                                <div class="math">
                                                    <p><img src="_images/math/ada1266646d71c941e77e3fd41bba9d92d06b7c2.svg"
                                                            alt="\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{
    \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) A^{\pi_{\theta}}(s_t,a_t)
    }," /></p>
                                                </div>
                                                <p>where <img class="math"
                                                        src="_images/math/67a5412645decf6424bdd97aed3e9e7601bd784f.svg"
                                                        alt="\tau" /> is a trajectory and <img class="math"
                                                        src="_images/math/5441ceb0039c72b114bb209edcd3bbbbe486c02c.svg"
                                                        alt="A^{\pi_{\theta}}" /> is the advantage function for the
                                                    current policy.</p>
                                                <p>The policy gradient algorithm works by updating policy parameters via
                                                    stochastic gradient ascent on policy performance:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/f5198e001f2c6053222b709af633865deb249cdf.svg"
                                                            alt="\theta_{k+1} = \theta_k + \alpha \nabla_{\theta} J(\pi_{\theta_k})" />
                                                    </p>
                                                </div>
                                                <p>Policy gradient implementations typically compute advantage function
                                                    estimates based on the infinite-horizon discounted return, despite
                                                    otherwise using the finite-horizon undiscounted policy gradient
                                                    formula.</p>
                                            </div>
                                            <div class="section" id="exploration-vs-exploitation">
                                                <h4><a class="toc-backref" href="#id5">Exploration vs.
                                                        Exploitation</a><a class="headerlink"
                                                        href="#exploration-vs-exploitation"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>VPG trains a stochastic policy in an on-policy way. This means that
                                                    it explores by sampling actions according to the latest version of
                                                    its stochastic policy. The amount of randomness in action selection
                                                    depends on both initial conditions and the training procedure. Over
                                                    the course of training, the policy typically becomes progressively
                                                    less random, as the update rule encourages it to exploit rewards
                                                    that it has already found. This may cause the policy to get trapped
                                                    in local optima.</p>
                                            </div>
                                            <div class="section" id="pseudocode">
                                                <h4><a class="toc-backref" href="#id6">Pseudocode</a><a
                                                        class="headerlink" href="#pseudocode"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <div class="math">
                                                    <p><img src="_images/math/262538f3077a7be8ce89066abbab523575132996.svg"
                                                            alt="\begin{algorithm}[H]
    \caption{Vanilla Policy Gradient Algorithm}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta_0$, initial value function parameters $\phi_0$
    \FOR{$k = 0,1,2,...$}
    \STATE Collect set of trajectories ${\mathcal D}_k = \{\tau_i\}$ by running policy $\pi_k = \pi(\theta_k)$ in the environment.
    \STATE Compute rewards-to-go $\hat{R}_t$.
    \STATE Compute advantage estimates, $\hat{A}_t$ (using any method of advantage estimation) based on the current value function $V_{\phi_k}$.
    \STATE Estimate policy gradient as
        \begin{equation*}
        \hat{g}_k = \frac{1}{|{\mathcal D}_k|} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T \left. \nabla_{\theta} \log\pi_{\theta}(a_t|s_t)\right|_{\theta_k} \hat{A}_t.
        \end{equation*}
    \STATE Compute policy update, either using standard gradient ascent,
        \begin{equation*}
        \theta_{k+1} = \theta_k + \alpha_k \hat{g}_k,
        \end{equation*}
        or via another gradient ascent algorithm like Adam.
    \STATE Fit value function by regression on mean-squared error:
        \begin{equation*}
        \phi_{k+1} = \arg \min_{\phi} \frac{1}{|{\mathcal D}_k| T} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T\left( V_{\phi} (s_t) - \hat{R}_t \right)^2,
        \end{equation*}
        typically via some gradient descent algorithm.
    \ENDFOR
\end{algorithmic}
\end{algorithm}" /></p>
                                                </div>
                                            </div>
                                        </div>
                                        <div class="section" id="documentation">
                                            <h3><a class="toc-backref" href="#id7">Documentation</a><a
                                                    class="headerlink" href="#documentation"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p class="last">In what follows, we give documentation for the PyTorch
                                                    and Tensorflow implementations of VPG in Spinning Up. They have
                                                    nearly identical function calls and docstrings, except for details
                                                    relating to model construction. However, we include both full
                                                    docstrings for completeness.</p>
                                            </div>
                                            <div class="section" id="documentation-pytorch-version">
                                                <h4><a class="toc-backref" href="#id8">Documentation: PyTorch
                                                        Version</a><a class="headerlink"
                                                        href="#documentation-pytorch-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <dl class="function">
                                                    <dt id="spinup.vpg_pytorch">
                                                        <code class="descclassname">spinup.</code><code
                                                            class="descname">vpg_pytorch</code><span
                                                            class="sig-paren">(</span><em>env_fn</em>,
                                                        <em>actor_critic=&lt;MagicMock spec='str'
                                                            id='140191853362816'&gt;</em>, <em>ac_kwargs={}</em>,
                                                        <em>seed=0</em>, <em>steps_per_epoch=4000</em>,
                                                        <em>epochs=50</em>, <em>gamma=0.99</em>, <em>pi_lr=0.0003</em>,
                                                        <em>vf_lr=0.001</em>, <em>train_v_iters=80</em>,
                                                        <em>lam=0.97</em>, <em>max_ep_len=1000</em>,
                                                        <em>logger_kwargs={}</em>, <em>save_freq=10</em><span
                                                            class="sig-paren">)</span><a class="headerlink"
                                                            href="#spinup.vpg_pytorch"
                                                            title="Permalink to this definition">¶</a>
                                                    </dt>
                                                    <dd>
                                                        <p>Vanilla Policy Gradient</p>
                                                        <p>(with GAE-Lambda for advantage estimation)</p>
                                                        <table class="docutils field-list" frame="void" rules="none">
                                                            <col class="field-name" />
                                                            <col class="field-body" />
                                                            <tbody valign="top">
                                                                <tr class="field-odd field">
                                                                    <th class="field-name">Parameters:</th>
                                                                    <td class="field-body">
                                                                        <ul class="first last simple">
                                                                            <li><strong>env_fn</strong> &#8211; A
                                                                                function which creates a copy of the
                                                                                environment.
                                                                                The environment must satisfy the OpenAI
                                                                                Gym API.</li>
                                                                            <li><strong>actor_critic</strong> &#8211;
                                                                                <p>The constructor method for a PyTorch
                                                                                    Module with a
                                                                                    <code
                                                                                        class="docutils literal"><span class="pre">step</span></code>
                                                                                    method, an <code
                                                                                        class="docutils literal"><span class="pre">act</span></code>
                                                                                    method, a <code
                                                                                        class="docutils literal"><span class="pre">pi</span></code>
                                                                                    module, and a <code
                                                                                        class="docutils literal"><span class="pre">v</span></code>
                                                                                    module. The <code
                                                                                        class="docutils literal"><span class="pre">step</span></code>
                                                                                    method should accept a batch of
                                                                                    observations
                                                                                    and return:
                                                                                </p>
                                                                                <table border="1" class="docutils">
                                                                                    <colgroup>
                                                                                        <col width="17%" />
                                                                                        <col width="25%" />
                                                                                        <col width="58%" />
                                                                                    </colgroup>
                                                                                    <thead valign="bottom">
                                                                                        <tr class="row-odd">
                                                                                            <th class="head">Symbol</th>
                                                                                            <th class="head">Shape</th>
                                                                                            <th class="head">Description
                                                                                            </th>
                                                                                        </tr>
                                                                                    </thead>
                                                                                    <tbody valign="top">
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">a</span></code>
                                                                                            </td>
                                                                                            <td>(batch, act_dim)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Numpy array of
                                                                                                        actions for each
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        observation.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-odd">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">v</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Numpy array of
                                                                                                        value estimates
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        for the provided
                                                                                                        observations.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">logp_a</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Numpy array of
                                                                                                        log probs for
                                                                                                        the</div>
                                                                                                    <div class="line">
                                                                                                        actions in <code
                                                                                                            class="docutils literal"><span class="pre">a</span></code>.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                    </tbody>
                                                                                </table>
                                                                                <p>The <code
                                                                                        class="docutils literal"><span class="pre">act</span></code>
                                                                                    method behaves the same as <code
                                                                                        class="docutils literal"><span class="pre">step</span></code>
                                                                                    but only returns <code
                                                                                        class="docutils literal"><span class="pre">a</span></code>.
                                                                                </p>
                                                                                <p>The <code
                                                                                        class="docutils literal"><span class="pre">pi</span></code>
                                                                                    module&#8217;s forward call should
                                                                                    accept a batch of
                                                                                    observations and optionally a batch
                                                                                    of actions, and return:</p>
                                                                                <table border="1" class="docutils">
                                                                                    <colgroup>
                                                                                        <col width="16%" />
                                                                                        <col width="23%" />
                                                                                        <col width="61%" />
                                                                                    </colgroup>
                                                                                    <thead valign="bottom">
                                                                                        <tr class="row-odd">
                                                                                            <th class="head">Symbol</th>
                                                                                            <th class="head">Shape</th>
                                                                                            <th class="head">Description
                                                                                            </th>
                                                                                        </tr>
                                                                                    </thead>
                                                                                    <tbody valign="top">
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">pi</span></code>
                                                                                            </td>
                                                                                            <td>N/A</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Torch
                                                                                                        Distribution
                                                                                                        object,
                                                                                                        containing</div>
                                                                                                    <div class="line">a
                                                                                                        batch of
                                                                                                        distributions
                                                                                                        describing</div>
                                                                                                    <div class="line">
                                                                                                        the policy for
                                                                                                        the provided
                                                                                                        observations.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-odd">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">logp_a</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Optional (only
                                                                                                        returned if
                                                                                                        batch of</div>
                                                                                                    <div class="line">
                                                                                                        actions is
                                                                                                        given). Tensor
                                                                                                        containing</div>
                                                                                                    <div class="line">
                                                                                                        the log
                                                                                                        probability,
                                                                                                        according to
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        the policy, of
                                                                                                        the provided
                                                                                                        actions.</div>
                                                                                                    <div class="line">If
                                                                                                        actions not
                                                                                                        given, will
                                                                                                        contain</div>
                                                                                                    <div class="line">
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">None</span></code>.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                    </tbody>
                                                                                </table>
                                                                                <p>The <code
                                                                                        class="docutils literal"><span class="pre">v</span></code>
                                                                                    module&#8217;s forward call should
                                                                                    accept a batch of observations
                                                                                    and return:</p>
                                                                                <table border="1" class="docutils">
                                                                                    <colgroup>
                                                                                        <col width="16%" />
                                                                                        <col width="23%" />
                                                                                        <col width="61%" />
                                                                                    </colgroup>
                                                                                    <thead valign="bottom">
                                                                                        <tr class="row-odd">
                                                                                            <th class="head">Symbol</th>
                                                                                            <th class="head">Shape</th>
                                                                                            <th class="head">Description
                                                                                            </th>
                                                                                        </tr>
                                                                                    </thead>
                                                                                    <tbody valign="top">
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">v</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Tensor
                                                                                                        containing the
                                                                                                        value estimates
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        for the provided
                                                                                                        observations.
                                                                                                        (Critical:</div>
                                                                                                    <div class="line">
                                                                                                        make sure to
                                                                                                        flatten this!)
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                    </tbody>
                                                                                </table>
                                                                            </li>
                                                                            <li><strong>ac_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Any kwargs
                                                                                appropriate for the ActorCritic object
                                                                                you provided to VPG.</li>
                                                                            <li><strong>seed</strong> (<em>int</em>)
                                                                                &#8211; Seed for random number
                                                                                generators.</li>
                                                                            <li><strong>steps_per_epoch</strong>
                                                                                (<em>int</em>) &#8211; Number of steps
                                                                                of interaction (state-action pairs)
                                                                                for the agent and the environment in
                                                                                each epoch.</li>
                                                                            <li><strong>epochs</strong> (<em>int</em>)
                                                                                &#8211; Number of epochs of interaction
                                                                                (equivalent to
                                                                                number of policy updates) to perform.
                                                                            </li>
                                                                            <li><strong>gamma</strong> (<em>float</em>)
                                                                                &#8211; Discount factor. (Always between
                                                                                0 and 1.)</li>
                                                                            <li><strong>pi_lr</strong> (<em>float</em>)
                                                                                &#8211; Learning rate for policy
                                                                                optimizer.</li>
                                                                            <li><strong>vf_lr</strong> (<em>float</em>)
                                                                                &#8211; Learning rate for value function
                                                                                optimizer.</li>
                                                                            <li><strong>train_v_iters</strong>
                                                                                (<em>int</em>) &#8211; Number of
                                                                                gradient descent steps to take on
                                                                                value function per epoch.</li>
                                                                            <li><strong>lam</strong> (<em>float</em>)
                                                                                &#8211; Lambda for GAE-Lambda. (Always
                                                                                between 0 and 1,
                                                                                close to 1.)</li>
                                                                            <li><strong>max_ep_len</strong>
                                                                                (<em>int</em>) &#8211; Maximum length of
                                                                                trajectory / episode / rollout.</li>
                                                                            <li><strong>logger_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Keyword args for
                                                                                EpochLogger.</li>
                                                                            <li><strong>save_freq</strong>
                                                                                (<em>int</em>) &#8211; How often (in
                                                                                terms of gap between epochs) to save
                                                                                the current policy and value function.
                                                                            </li>
                                                                        </ul>
                                                                    </td>
                                                                </tr>
                                                            </tbody>
                                                        </table>
                                                    </dd>
                                                </dl>

                                            </div>
                                            <div class="section" id="saved-model-contents-pytorch-version">
                                                <h4><a class="toc-backref" href="#id9">Saved Model Contents: PyTorch
                                                        Version</a><a class="headerlink"
                                                        href="#saved-model-contents-pytorch-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>The PyTorch saved model can be loaded with <code
                                                        class="docutils literal"><span class="pre">ac</span> <span class="pre">=</span> <span class="pre">torch.load('path/to/model.pt')</span></code>,
                                                    yielding an actor-critic object (<code
                                                        class="docutils literal"><span class="pre">ac</span></code>)
                                                    that has the properties described in the docstring for <code
                                                        class="docutils literal"><span class="pre">vpg_pytorch</span></code>.
                                                </p>
                                                <p>You can get actions from this model with</p>
                                                <div class="highlight-python">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">actions</span> <span class="o">=</span> <span class="n">ac</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre>
                                                    </div>
                                                </div>
                                            </div>
                                            <div class="section" id="documentation-tensorflow-version">
                                                <h4><a class="toc-backref" href="#id10">Documentation: Tensorflow
                                                        Version</a><a class="headerlink"
                                                        href="#documentation-tensorflow-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <dl class="function">
                                                    <dt id="spinup.vpg_tf1">
                                                        <code class="descclassname">spinup.</code><code
                                                            class="descname">vpg_tf1</code><span
                                                            class="sig-paren">(</span><em>env_fn</em>,
                                                        <em>actor_critic=&lt;function mlp_actor_critic&gt;</em>,
                                                        <em>ac_kwargs={}</em>, <em>seed=0</em>,
                                                        <em>steps_per_epoch=4000</em>, <em>epochs=50</em>,
                                                        <em>gamma=0.99</em>, <em>pi_lr=0.0003</em>,
                                                        <em>vf_lr=0.001</em>, <em>train_v_iters=80</em>,
                                                        <em>lam=0.97</em>, <em>max_ep_len=1000</em>,
                                                        <em>logger_kwargs={}</em>, <em>save_freq=10</em><span
                                                            class="sig-paren">)</span><a class="headerlink"
                                                            href="#spinup.vpg_tf1"
                                                            title="Permalink to this definition">¶</a>
                                                    </dt>
                                                    <dd>
                                                        <p>Vanilla Policy Gradient</p>
                                                        <p>(with GAE-Lambda for advantage estimation)</p>
                                                        <table class="docutils field-list" frame="void" rules="none">
                                                            <col class="field-name" />
                                                            <col class="field-body" />
                                                            <tbody valign="top">
                                                                <tr class="field-odd field">
                                                                    <th class="field-name">Parameters:</th>
                                                                    <td class="field-body">
                                                                        <ul class="first last simple">
                                                                            <li><strong>env_fn</strong> &#8211; A
                                                                                function which creates a copy of the
                                                                                environment.
                                                                                The environment must satisfy the OpenAI
                                                                                Gym API.</li>
                                                                            <li><strong>actor_critic</strong> &#8211;
                                                                                <p>A function which takes in placeholder
                                                                                    symbols
                                                                                    for state, <code
                                                                                        class="docutils literal"><span class="pre">x_ph</span></code>,
                                                                                    and action, <code
                                                                                        class="docutils literal"><span class="pre">a_ph</span></code>,
                                                                                    and returns the main
                                                                                    outputs from the agent&#8217;s
                                                                                    Tensorflow computation graph:</p>
                                                                                <table border="1" class="docutils">
                                                                                    <colgroup>
                                                                                        <col width="16%" />
                                                                                        <col width="24%" />
                                                                                        <col width="60%" />
                                                                                    </colgroup>
                                                                                    <thead valign="bottom">
                                                                                        <tr class="row-odd">
                                                                                            <th class="head">Symbol</th>
                                                                                            <th class="head">Shape</th>
                                                                                            <th class="head">Description
                                                                                            </th>
                                                                                        </tr>
                                                                                    </thead>
                                                                                    <tbody valign="top">
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">pi</span></code>
                                                                                            </td>
                                                                                            <td>(batch, act_dim)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Samples actions
                                                                                                        from policy
                                                                                                        given</div>
                                                                                                    <div class="line">
                                                                                                        states.</div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-odd">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">logp</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Gives log
                                                                                                        probability,
                                                                                                        according to
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        the policy, of
                                                                                                        taking actions
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">a_ph</span></code>
                                                                                                    </div>
                                                                                                    <div class="line">in
                                                                                                        states <code
                                                                                                            class="docutils literal"><span class="pre">x_ph</span></code>.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">logp_pi</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Gives log
                                                                                                        probability,
                                                                                                        according to
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        the policy, of
                                                                                                        the action
                                                                                                        sampled by</div>
                                                                                                    <div class="line">
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">pi</span></code>.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-odd">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">v</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Gives the value
                                                                                                        estimate for
                                                                                                        states</div>
                                                                                                    <div class="line">in
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">x_ph</span></code>.
                                                                                                        (Critical: make
                                                                                                        sure</div>
                                                                                                    <div class="line">to
                                                                                                        flatten this!)
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                    </tbody>
                                                                                </table>
                                                                            </li>
                                                                            <li><strong>ac_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Any kwargs
                                                                                appropriate for the actor_critic
                                                                                function you provided to VPG.</li>
                                                                            <li><strong>seed</strong> (<em>int</em>)
                                                                                &#8211; Seed for random number
                                                                                generators.</li>
                                                                            <li><strong>steps_per_epoch</strong>
                                                                                (<em>int</em>) &#8211; Number of steps
                                                                                of interaction (state-action pairs)
                                                                                for the agent and the environment in
                                                                                each epoch.</li>
                                                                            <li><strong>epochs</strong> (<em>int</em>)
                                                                                &#8211; Number of epochs of interaction
                                                                                (equivalent to
                                                                                number of policy updates) to perform.
                                                                            </li>
                                                                            <li><strong>gamma</strong> (<em>float</em>)
                                                                                &#8211; Discount factor. (Always between
                                                                                0 and 1.)</li>
                                                                            <li><strong>pi_lr</strong> (<em>float</em>)
                                                                                &#8211; Learning rate for policy
                                                                                optimizer.</li>
                                                                            <li><strong>vf_lr</strong> (<em>float</em>)
                                                                                &#8211; Learning rate for value function
                                                                                optimizer.</li>
                                                                            <li><strong>train_v_iters</strong>
                                                                                (<em>int</em>) &#8211; Number of
                                                                                gradient descent steps to take on
                                                                                value function per epoch.</li>
                                                                            <li><strong>lam</strong> (<em>float</em>)
                                                                                &#8211; Lambda for GAE-Lambda. (Always
                                                                                between 0 and 1,
                                                                                close to 1.)</li>
                                                                            <li><strong>max_ep_len</strong>
                                                                                (<em>int</em>) &#8211; Maximum length of
                                                                                trajectory / episode / rollout.</li>
                                                                            <li><strong>logger_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Keyword args for
                                                                                EpochLogger.</li>
                                                                            <li><strong>save_freq</strong>
                                                                                (<em>int</em>) &#8211; How often (in
                                                                                terms of gap between epochs) to save
                                                                                the current policy and value function.
                                                                            </li>
                                                                        </ul>
                                                                    </td>
                                                                </tr>
                                                            </tbody>
                                                        </table>
                                                    </dd>
                                                </dl>

                                            </div>
                                            <div class="section" id="saved-model-contents-tensorflow-version">
                                                <h4><a class="toc-backref" href="#id11">Saved Model Contents: Tensorflow
                                                        Version</a><a class="headerlink"
                                                        href="#saved-model-contents-tensorflow-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>The computation graph saved by the logger includes:</p>
                                                <table border="1" class="docutils">
                                                    <colgroup>
                                                        <col width="11%" />
                                                        <col width="89%" />
                                                    </colgroup>
                                                    <thead valign="bottom">
                                                        <tr class="row-odd">
                                                            <th class="head">Key</th>
                                                            <th class="head">Value</th>
                                                        </tr>
                                                    </thead>
                                                    <tbody valign="top">
                                                        <tr class="row-even">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">x</span></code>
                                                            </td>
                                                            <td>Tensorflow placeholder for state input.</td>
                                                        </tr>
                                                        <tr class="row-odd">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">pi</span></code>
                                                            </td>
                                                            <td>Samples an action from the agent, conditioned on states
                                                                in <code
                                                                    class="docutils literal"><span class="pre">x</span></code>.
                                                            </td>
                                                        </tr>
                                                        <tr class="row-even">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">v</span></code>
                                                            </td>
                                                            <td>Gives value estimate for states in <code
                                                                    class="docutils literal"><span class="pre">x</span></code>.
                                                            </td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <p>This saved model can be accessed either by</p>
                                                <ul class="simple">
                                                    <li>running the trained policy with the <a
                                                            class="reference external"
                                                            href="../user/saving_and_loading.html#loading-and-running-trained-policies">test_policy.py</a>
                                                        tool,</li>
                                                    <li>or loading the whole saved graph into a program with <a
                                                            class="reference external"
                                                            href="../utils/logger.html#spinup.utils.logx.restore_tf_graph">restore_tf_graph</a>.
                                                    </li>
                                                </ul>
                                            </div>
                                        </div>
                                        <div class="section" id="references">
                                            <h3><a class="toc-backref" href="#id12">References</a><a class="headerlink"
                                                    href="#references" title="Permalink to this headline">¶</a></h3>
                                            <div class="section" id="relevant-papers">
                                                <h4><a class="toc-backref" href="#id13">Relevant Papers</a><a
                                                        class="headerlink" href="#relevant-papers"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <ul class="simple">
                                                    <li><a class="reference external"
                                                            href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf">Policy
                                                            Gradient Methods for Reinforcement Learning with Function
                                                            Approximation</a>, Sutton et al. 2000</li>
                                                    <li><a class="reference external"
                                                            href="http://joschu.net/docs/thesis.pdf">Optimizing
                                                            Expectations: From Deep Reinforcement Learning to Stochastic
                                                            Computation Graphs</a>, Schulman 2016(a)</li>
                                                    <li><a class="reference external"
                                                            href="https://arxiv.org/pdf/1604.06778.pdf">Benchmarking
                                                            Deep Reinforcement Learning for Continuous Control</a>, Duan
                                                        et al. 2016</li>
                                                    <li><a class="reference external"
                                                            href="https://arxiv.org/pdf/1506.02438.pdf">High Dimensional
                                                            Continuous Control Using Generalized Advantage
                                                            Estimation</a>, Schulman et al. 2016(b)</li>
                                                </ul>
                                            </div>
                                            <div class="section" id="why-these-papers">
                                                <h4><a class="toc-backref" href="#id14">Why These Papers?</a><a
                                                        class="headerlink" href="#why-these-papers"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>Sutton 2000 is included because it is a timeless classic of
                                                    reinforcement learning theory, and contains references to the
                                                    earlier work which led to modern policy gradients. Schulman 2016(a)
                                                    is included because Chapter 2 contains a lucid introduction to the
                                                    theory of policy gradient algorithms, including pseudocode. Duan
                                                    2016 is a clear, recent benchmark paper that shows how vanilla
                                                    policy gradient in the deep RL setting (eg with neural network
                                                    policies and Adam as the optimizer) compares with other deep RL
                                                    algorithms. Schulman 2016(b) is included because our implementation
                                                    of VPG makes use of Generalized Advantage Estimation for computing
                                                    the policy gradient.</p>
                                            </div>
                                            <div class="section" id="other-public-implementations">
                                                <h4><a class="toc-backref" href="#id15">Other Public
                                                        Implementations</a><a class="headerlink"
                                                        href="#other-public-implementations"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <ul class="simple">
                                                    <li><a class="reference external"
                                                            href="https://github.com/rll/rllab/blob/master/rllab/algos/vpg.py">rllab</a>
                                                    </li>
                                                    <li><a class="reference external"
                                                            href="https://github.com/ray-project/ray/blob/master/python/ray/rllib/agents/pg">rllib
                                                            (Ray)</a></li>
                                                </ul>
                                            </div>
                                        </div>
                                    </div>
                                    <span id="document-algorithms/trpo"></span>
                                    <div class="section" id="trust-region-policy-optimization">
                                        <h2><a class="toc-backref" href="#id4">Trust Region Policy Optimization</a><a
                                                class="headerlink" href="#trust-region-policy-optimization"
                                                title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal"
                                                        href="#trust-region-policy-optimization" id="id4">Trust Region
                                                        Policy Optimization</a>
                                                    <ul>
                                                        <li><a class="reference internal" href="#background"
                                                                id="id5">Background</a>
                                                            <ul>
                                                                <li><a class="reference internal" href="#quick-facts"
                                                                        id="id6">Quick Facts</a></li>
                                                                <li><a class="reference internal" href="#key-equations"
                                                                        id="id7">Key Equations</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#exploration-vs-exploitation"
                                                                        id="id8">Exploration vs. Exploitation</a></li>
                                                                <li><a class="reference internal" href="#pseudocode"
                                                                        id="id9">Pseudocode</a></li>
                                                            </ul>
                                                        </li>
                                                        <li><a class="reference internal" href="#documentation"
                                                                id="id10">Documentation</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#saved-model-contents" id="id11">Saved
                                                                        Model Contents</a></li>
                                                            </ul>
                                                        </li>
                                                        <li><a class="reference internal" href="#references"
                                                                id="id12">References</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#relevant-papers" id="id13">Relevant
                                                                        Papers</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#why-these-papers" id="id14">Why These
                                                                        Papers?</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#other-public-implementations"
                                                                        id="id15">Other Public Implementations</a></li>
                                                            </ul>
                                                        </li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <div class="section" id="background">
                                            <h3><a class="toc-backref" href="#id5">Background</a><a class="headerlink"
                                                    href="#background" title="Permalink to this headline">¶</a></h3>
                                            <p>(Previously: <a class="reference external"
                                                    href="../algorithms/vpg.html#background">Background for VPG</a>)</p>
                                            <p>TRPO updates policies by taking the largest step possible to improve
                                                performance, while satisfying a special constraint on how close the new
                                                and old policies are allowed to be. The constraint is expressed in terms
                                                of <a class="reference external"
                                                    href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL-Divergence</a>,
                                                a measure of (something like, but not exactly) distance between
                                                probability distributions.</p>
                                            <p>This is different from normal policy gradient, which keeps new and old
                                                policies close in parameter space. But even seemingly small differences
                                                in parameter space can have very large differences in
                                                performance&#8212;so a single bad step can collapse the policy
                                                performance. This makes it dangerous to use large step sizes with
                                                vanilla policy gradients, thus hurting its sample efficiency. TRPO
                                                nicely avoids this kind of collapse, and tends to quickly and
                                                monotonically improve performance.</p>
                                            <div class="section" id="quick-facts">
                                                <h4><a class="toc-backref" href="#id6">Quick Facts</a><a
                                                        class="headerlink" href="#quick-facts"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <ul class="simple">
                                                    <li>TRPO is an on-policy algorithm.</li>
                                                    <li>TRPO can be used for environments with either discrete or
                                                        continuous action spaces.</li>
                                                    <li>The Spinning Up implementation of TRPO supports parallelization
                                                        with MPI.</li>
                                                </ul>
                                            </div>
                                            <div class="section" id="key-equations">
                                                <h4><a class="toc-backref" href="#id7">Key Equations</a><a
                                                        class="headerlink" href="#key-equations"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>Let <img class="math"
                                                        src="_images/math/6a71f04b65d9524fb656715cda85d7540a9ddf9f.svg"
                                                        alt="\pi_{\theta}" /> denote a policy with parameters <img
                                                        class="math"
                                                        src="_images/math/ce5edddd490112350f4bd555d9390e0e845f754a.svg"
                                                        alt="\theta" />. The theoretical TRPO update is:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/23edf1f72f63a4729c40371c1481a36549a0b713.svg"
                                                            alt="\theta_{k+1} = \arg \max_{\theta} \; &amp; {\mathcal L}(\theta_k, \theta) \\
\text{s.t.} \; &amp; \bar{D}_{KL}(\theta || \theta_k) \leq \delta" /></p>
                                                </div>
                                                <p>where <img class="math"
                                                        src="_images/math/0837b005b194415b2b922e42be1df8601b552857.svg"
                                                        alt="{\mathcal L}(\theta_k, \theta)" /> is the <em>surrogate
                                                        advantage</em>, a measure of how policy <img class="math"
                                                        src="_images/math/6a71f04b65d9524fb656715cda85d7540a9ddf9f.svg"
                                                        alt="\pi_{\theta}" /> performs relative to the old policy <img
                                                        class="math"
                                                        src="_images/math/d8bb9f337fa712549e0428223df820773aa1169d.svg"
                                                        alt="\pi_{\theta_k}" /> using data from the old policy:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/ae8edab1e9c727bed15e54d4dda492382538b5fe.svg"
                                                            alt="{\mathcal L}(\theta_k, \theta) = \underE{s,a \sim \pi_{\theta_k}}{
    \frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}}(s,a)
    }," /></p>
                                                </div>
                                                <p>and <img class="math"
                                                        src="_images/math/88396050a58384b85dfaa6fce02cf39d98c78c4b.svg"
                                                        alt="\bar{D}_{KL}(\theta || \theta_k)" /> is an average
                                                    KL-divergence between policies across states visited by the old
                                                    policy:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/78a651e0ce4979bd3e17198594ad952ac20b9b45.svg"
                                                            alt="\bar{D}_{KL}(\theta || \theta_k) = \underE{s \sim \pi_{\theta_k}}{
    D_{KL}\left(\pi_{\theta}(\cdot|s) || \pi_{\theta_k} (\cdot|s) \right)
}." /></p>
                                                </div>
                                                <div class="admonition-you-should-know admonition">
                                                    <p class="first admonition-title">You Should Know</p>
                                                    <p class="last">The objective and constraint are both zero when <img
                                                            class="math"
                                                            src="_images/math/2ae54d61543a208d042466ff3554871467c23d30.svg"
                                                            alt="\theta = \theta_k" />. Furthermore, the gradient of the
                                                        constraint with respect to <img class="math"
                                                            src="_images/math/ce5edddd490112350f4bd555d9390e0e845f754a.svg"
                                                            alt="\theta" /> is zero when <img class="math"
                                                            src="_images/math/2ae54d61543a208d042466ff3554871467c23d30.svg"
                                                            alt="\theta = \theta_k" />. Proving these facts requires
                                                        some subtle command of the relevant math&#8212;it&#8217;s an
                                                        exercise worth doing, whenever you feel ready!</p>
                                                </div>
                                                <p>The theoretical TRPO update isn&#8217;t the easiest to work with, so
                                                    TRPO makes some approximations to get an answer quickly. We Taylor
                                                    expand the objective and constraint to leading order around <img
                                                        class="math"
                                                        src="_images/math/a485f77ef16acbb27539cdfe8286cd6029ccfd26.svg"
                                                        alt="\theta_k" />:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/7cdaa039734ec1d09adcc3e4dc351085823085cf.svg"
                                                            alt="{\mathcal L}(\theta_k, \theta) &amp;\approx g^T (\theta - \theta_k) \\
\bar{D}_{KL}(\theta || \theta_k) &amp; \approx \frac{1}{2} (\theta - \theta_k)^T H (\theta - \theta_k)" /></p>
                                                </div>
                                                <p>resulting in an approximate optimization problem,</p>
                                                <div class="math">
                                                    <p><img src="_images/math/69c9dcbe2fe1c669a1b2cb3a312a479cdfcb27a1.svg"
                                                            alt="\theta_{k+1} = \arg \max_{\theta} \; &amp; g^T (\theta - \theta_k) \\
\text{s.t.} \; &amp; \frac{1}{2} (\theta - \theta_k)^T H (\theta - \theta_k) \leq \delta." /></p>
                                                </div>
                                                <div class="admonition-you-should-know admonition">
                                                    <p class="first admonition-title">You Should Know</p>
                                                    <p class="last">By happy coincidence, the gradient <img class="math"
                                                            src="_images/math/7c8bf3a1920993c53ae254d3f08d697f368af350.svg"
                                                            alt="g" /> of the surrogate advantage function with respect
                                                        to <img class="math"
                                                            src="_images/math/ce5edddd490112350f4bd555d9390e0e845f754a.svg"
                                                            alt="\theta" />, evaluated at <img class="math"
                                                            src="_images/math/2ae54d61543a208d042466ff3554871467c23d30.svg"
                                                            alt="\theta = \theta_k" />, is exactly equal to the policy
                                                        gradient, <img class="math"
                                                            src="_images/math/fdc185c68404ece5c4deef076c9713af689421a2.svg"
                                                            alt="\nabla_{\theta} J(\pi_{\theta})" />! Try proving this,
                                                        if you feel comfortable diving into the math.</p>
                                                </div>
                                                <p>This approximate problem can be analytically solved by the methods of
                                                    Lagrangian duality <a class="footnote-reference" href="#id2"
                                                        id="id1">[1]</a>, yielding the solution:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/e990f7ff0230a8fa93cf1242ea0d49fdf63d05d7.svg"
                                                            alt="\theta_{k+1} = \theta_k + \sqrt{\frac{2 \delta}{g^T H^{-1} g}} H^{-1} g." />
                                                    </p>
                                                </div>
                                                <p>If we were to stop here, and just use this final result, the
                                                    algorithm would be exactly calculating the <a
                                                        class="reference external"
                                                        href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf">Natural
                                                        Policy Gradient</a>. A problem is that, due to the approximation
                                                    errors introduced by the Taylor expansion, this may not satisfy the
                                                    KL constraint, or actually improve the surrogate advantage. TRPO
                                                    adds a modification to this update rule: a backtracking line search,
                                                </p>
                                                <div class="math">
                                                    <p><img src="_images/math/03cabd66ab79d8c17e36fc4247bb46fe0c6dcbfc.svg"
                                                            alt="\theta_{k+1} = \theta_k + \alpha^j \sqrt{\frac{2 \delta}{g^T H^{-1} g}} H^{-1} g," />
                                                    </p>
                                                </div>
                                                <p>where <img class="math"
                                                        src="_images/math/85e2502878c575c6e250a9224be42065ac9844d2.svg"
                                                        alt="\alpha \in (0,1)" /> is the backtracking coefficient, and
                                                    <img class="math"
                                                        src="_images/math/b42a5fa0aad66603180aff0fc5e346e98a2364ca.svg"
                                                        alt="j" /> is the smallest nonnegative integer such that <img
                                                        class="math"
                                                        src="_images/math/3944f0149054734c7f8537d8f9316cd77cbbb143.svg"
                                                        alt="\pi_{\theta_{k+1}}" /> satisfies the KL constraint and
                                                    produces a positive surrogate advantage.</p>
                                                <p>Lastly: computing and storing the matrix inverse, <img class="math"
                                                        src="_images/math/c61d52a1bdbbfa95c007324ae431066f95be2296.svg"
                                                        alt="H^{-1}" />, is painfully expensive when dealing with neural
                                                    network policies with thousands or millions of parameters. TRPO
                                                    sidesteps the issue by using the <a class="reference external"
                                                        href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">conjugate
                                                        gradient</a> algorithm to solve <img class="math"
                                                        src="_images/math/1e5b7619f5aff65751670c7d6b3527e5721a9033.svg"
                                                        alt="Hx = g" /> for <img class="math"
                                                        src="_images/math/a0181d85fab06c9716a1bb2561dbf0f8534ef172.svg"
                                                        alt="x = H^{-1} g" />, requiring only a function which can
                                                    compute the matrix-vector product <img class="math"
                                                        src="_images/math/7c097b2fe748e8a45446bdc5d27721c82b75e969.svg"
                                                        alt="Hx" /> instead of computing and storing the whole matrix
                                                    <img class="math"
                                                        src="_images/math/bf6bcb1745aeab36cdc185e9f75bbfd3998352ce.svg"
                                                        alt="H" /> directly. This is not too hard to do: we set up a
                                                    symbolic operation to calculate</p>
                                                <div class="math">
                                                    <p><img src="_images/math/2b50eb41a25af9e480d1c6facfafe1218624fc35.svg"
                                                            alt="Hx = \nabla_{\theta} \left( \left(\nabla_{\theta} \bar{D}_{KL}(\theta || \theta_k)\right)^T x \right)," />
                                                    </p>
                                                </div>
                                                <p>which gives us the correct output without computing the whole matrix.
                                                </p>
                                                <table class="docutils footnote" frame="void" id="id2" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label"><a class="fn-backref" href="#id1">[1]</a>
                                                            </td>
                                                            <td>See <a class="reference external"
                                                                    href="http://stanford.edu/~boyd/cvxbook/">Convex
                                                                    Optimization</a> by Boyd and Vandenberghe,
                                                                especially chapters 2 through 5.</td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                            </div>
                                            <div class="section" id="exploration-vs-exploitation">
                                                <h4><a class="toc-backref" href="#id8">Exploration vs.
                                                        Exploitation</a><a class="headerlink"
                                                        href="#exploration-vs-exploitation"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>TRPO trains a stochastic policy in an on-policy way. This means that
                                                    it explores by sampling actions according to the latest version of
                                                    its stochastic policy. The amount of randomness in action selection
                                                    depends on both initial conditions and the training procedure. Over
                                                    the course of training, the policy typically becomes progressively
                                                    less random, as the update rule encourages it to exploit rewards
                                                    that it has already found. This may cause the policy to get trapped
                                                    in local optima.</p>
                                            </div>
                                            <div class="section" id="pseudocode">
                                                <h4><a class="toc-backref" href="#id9">Pseudocode</a><a
                                                        class="headerlink" href="#pseudocode"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <div class="math">
                                                    <p><img src="_images/math/5808864ea60ebc3702704717d9f4c3773c90540d.svg"
                                                            alt="\begin{algorithm}[H]
    \caption{Trust Region Policy Optimization}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta_0$, initial value function parameters $\phi_0$
    \STATE Hyperparameters: KL-divergence limit $\delta$, backtracking coefficient $\alpha$, maximum number of backtracking steps $K$
    \FOR{$k = 0,1,2,...$}
    \STATE Collect set of trajectories ${\mathcal D}_k = \{\tau_i\}$ by running policy $\pi_k = \pi(\theta_k)$ in the environment.
    \STATE Compute rewards-to-go $\hat{R}_t$.
    \STATE Compute advantage estimates, $\hat{A}_t$ (using any method of advantage estimation) based on the current value function $V_{\phi_k}$.
    \STATE Estimate policy gradient as
        \begin{equation*}
        \hat{g}_k = \frac{1}{|{\mathcal D}_k|} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T \left. \nabla_{\theta} \log\pi_{\theta}(a_t|s_t)\right|_{\theta_k} \hat{A}_t.
        \end{equation*}
    \STATE Use the conjugate gradient algorithm to compute
        \begin{equation*}
        \hat{x}_k \approx \hat{H}_k^{-1} \hat{g}_k,
        \end{equation*}
        where $\hat{H}_k$ is the Hessian of the sample average KL-divergence.
    \STATE Update the policy by backtracking line search with
        \begin{equation*}
        \theta_{k+1} = \theta_k + \alpha^j \sqrt{ \frac{2\delta}{\hat{x}_k^T \hat{H}_k \hat{x}_k}} \hat{x}_k,
        \end{equation*}
        where $j \in \{0, 1, 2, ... K\}$ is the smallest value which improves the sample loss and satisfies the sample KL-divergence constraint.
    \STATE Fit value function by regression on mean-squared error:
        \begin{equation*}
        \phi_{k+1} = \arg \min_{\phi} \frac{1}{|{\mathcal D}_k| T} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T\left( V_{\phi} (s_t) - \hat{R}_t \right)^2,
        \end{equation*}
        typically via some gradient descent algorithm.
    \ENDFOR
\end{algorithmic}
\end{algorithm}" /></p>
                                                </div>
                                            </div>
                                        </div>
                                        <div class="section" id="documentation">
                                            <h3><a class="toc-backref" href="#id10">Documentation</a><a
                                                    class="headerlink" href="#documentation"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p class="last">Spinning Up currently only has a Tensorflow
                                                    implementation of TRPO.</p>
                                            </div>
                                            <dl class="function">
                                                <dt id="spinup.trpo_tf1">
                                                    <code class="descclassname">spinup.</code><code
                                                        class="descname">trpo_tf1</code><span
                                                        class="sig-paren">(</span><em>env_fn</em>,
                                                    <em>actor_critic=&lt;function mlp_actor_critic&gt;</em>,
                                                    <em>ac_kwargs={}</em>, <em>seed=0</em>,
                                                    <em>steps_per_epoch=4000</em>, <em>epochs=50</em>,
                                                    <em>gamma=0.99</em>, <em>delta=0.01</em>, <em>vf_lr=0.001</em>,
                                                    <em>train_v_iters=80</em>, <em>damping_coeff=0.1</em>,
                                                    <em>cg_iters=10</em>, <em>backtrack_iters=10</em>,
                                                    <em>backtrack_coeff=0.8</em>, <em>lam=0.97</em>,
                                                    <em>max_ep_len=1000</em>, <em>logger_kwargs={}</em>,
                                                    <em>save_freq=10</em>, <em>algo='trpo'</em><span
                                                        class="sig-paren">)</span><a class="headerlink"
                                                        href="#spinup.trpo_tf1"
                                                        title="Permalink to this definition">¶</a>
                                                </dt>
                                                <dd>
                                                    <p>Trust Region Policy Optimization</p>
                                                    <p>(with support for Natural Policy Gradient)</p>
                                                    <table class="docutils field-list" frame="void" rules="none">
                                                        <col class="field-name" />
                                                        <col class="field-body" />
                                                        <tbody valign="top">
                                                            <tr class="field-odd field">
                                                                <th class="field-name">Parameters:</th>
                                                                <td class="field-body">
                                                                    <ul class="first last simple">
                                                                        <li><strong>env_fn</strong> &#8211; A function
                                                                            which creates a copy of the environment.
                                                                            The environment must satisfy the OpenAI Gym
                                                                            API.</li>
                                                                        <li><strong>actor_critic</strong> &#8211; <p>A
                                                                                function which takes in placeholder
                                                                                symbols
                                                                                for state, <code
                                                                                    class="docutils literal"><span class="pre">x_ph</span></code>,
                                                                                and action, <code
                                                                                    class="docutils literal"><span class="pre">a_ph</span></code>,
                                                                                and returns the main
                                                                                outputs from the agent&#8217;s
                                                                                Tensorflow computation graph:</p>
                                                                            <table border="1" class="docutils">
                                                                                <colgroup>
                                                                                    <col width="18%" />
                                                                                    <col width="24%" />
                                                                                    <col width="59%" />
                                                                                </colgroup>
                                                                                <thead valign="bottom">
                                                                                    <tr class="row-odd">
                                                                                        <th class="head">Symbol</th>
                                                                                        <th class="head">Shape</th>
                                                                                        <th class="head">Description
                                                                                        </th>
                                                                                    </tr>
                                                                                </thead>
                                                                                <tbody valign="top">
                                                                                    <tr class="row-even">
                                                                                        <td><code
                                                                                                class="docutils literal"><span class="pre">pi</span></code>
                                                                                        </td>
                                                                                        <td>(batch, act_dim)</td>
                                                                                        <td>
                                                                                            <div
                                                                                                class="first last line-block">
                                                                                                <div class="line">
                                                                                                    Samples actions from
                                                                                                    policy given</div>
                                                                                                <div class="line">
                                                                                                    states.</div>
                                                                                            </div>
                                                                                        </td>
                                                                                    </tr>
                                                                                    <tr class="row-odd">
                                                                                        <td><code
                                                                                                class="docutils literal"><span class="pre">logp</span></code>
                                                                                        </td>
                                                                                        <td>(batch,)</td>
                                                                                        <td>
                                                                                            <div
                                                                                                class="first last line-block">
                                                                                                <div class="line">Gives
                                                                                                    log probability,
                                                                                                    according to</div>
                                                                                                <div class="line">the
                                                                                                    policy, of taking
                                                                                                    actions <code
                                                                                                        class="docutils literal"><span class="pre">a_ph</span></code>
                                                                                                </div>
                                                                                                <div class="line">in
                                                                                                    states <code
                                                                                                        class="docutils literal"><span class="pre">x_ph</span></code>.
                                                                                                </div>
                                                                                            </div>
                                                                                        </td>
                                                                                    </tr>
                                                                                    <tr class="row-even">
                                                                                        <td><code
                                                                                                class="docutils literal"><span class="pre">logp_pi</span></code>
                                                                                        </td>
                                                                                        <td>(batch,)</td>
                                                                                        <td>
                                                                                            <div
                                                                                                class="first last line-block">
                                                                                                <div class="line">Gives
                                                                                                    log probability,
                                                                                                    according to</div>
                                                                                                <div class="line">the
                                                                                                    policy, of the
                                                                                                    action sampled by
                                                                                                </div>
                                                                                                <div class="line"><code
                                                                                                        class="docutils literal"><span class="pre">pi</span></code>.
                                                                                                </div>
                                                                                            </div>
                                                                                        </td>
                                                                                    </tr>
                                                                                    <tr class="row-odd">
                                                                                        <td><code
                                                                                                class="docutils literal"><span class="pre">info</span></code>
                                                                                        </td>
                                                                                        <td>N/A</td>
                                                                                        <td>
                                                                                            <div
                                                                                                class="first last line-block">
                                                                                                <div class="line">A dict
                                                                                                    of any intermediate
                                                                                                    quantities</div>
                                                                                                <div class="line">(from
                                                                                                    calculating the
                                                                                                    policy or log</div>
                                                                                                <div class="line">
                                                                                                    probabilities) which
                                                                                                    are needed for</div>
                                                                                                <div class="line">
                                                                                                    analytically
                                                                                                    computing KL
                                                                                                    divergence.</div>
                                                                                                <div class="line">(eg
                                                                                                    sufficient
                                                                                                    statistics of the
                                                                                                </div>
                                                                                                <div class="line">
                                                                                                    distributions)</div>
                                                                                            </div>
                                                                                        </td>
                                                                                    </tr>
                                                                                    <tr class="row-even">
                                                                                        <td><code
                                                                                                class="docutils literal"><span class="pre">info_phs</span></code>
                                                                                        </td>
                                                                                        <td>N/A</td>
                                                                                        <td>
                                                                                            <div
                                                                                                class="first last line-block">
                                                                                                <div class="line">A dict
                                                                                                    of placeholders for
                                                                                                    old values</div>
                                                                                                <div class="line">of the
                                                                                                    entries in <code
                                                                                                        class="docutils literal"><span class="pre">info</span></code>.
                                                                                                </div>
                                                                                            </div>
                                                                                        </td>
                                                                                    </tr>
                                                                                    <tr class="row-odd">
                                                                                        <td><code
                                                                                                class="docutils literal"><span class="pre">d_kl</span></code>
                                                                                        </td>
                                                                                        <td>()</td>
                                                                                        <td>
                                                                                            <div
                                                                                                class="first last line-block">
                                                                                                <div class="line">A
                                                                                                    symbol for computing
                                                                                                    the mean KL</div>
                                                                                                <div class="line">
                                                                                                    divergence between
                                                                                                    the current policy
                                                                                                </div>
                                                                                                <div class="line">(<code
                                                                                                        class="docutils literal"><span class="pre">pi</span></code>)
                                                                                                    and the old policy
                                                                                                    (as</div>
                                                                                                <div class="line">
                                                                                                    specified by the
                                                                                                    inputs to</div>
                                                                                                <div class="line"><code
                                                                                                        class="docutils literal"><span class="pre">info_phs</span></code>)
                                                                                                    over the batch of
                                                                                                </div>
                                                                                                <div class="line">states
                                                                                                    given in <code
                                                                                                        class="docutils literal"><span class="pre">x_ph</span></code>.
                                                                                                </div>
                                                                                            </div>
                                                                                        </td>
                                                                                    </tr>
                                                                                    <tr class="row-even">
                                                                                        <td><code
                                                                                                class="docutils literal"><span class="pre">v</span></code>
                                                                                        </td>
                                                                                        <td>(batch,)</td>
                                                                                        <td>
                                                                                            <div
                                                                                                class="first last line-block">
                                                                                                <div class="line">Gives
                                                                                                    the value estimate
                                                                                                    for states</div>
                                                                                                <div class="line">in
                                                                                                    <code
                                                                                                        class="docutils literal"><span class="pre">x_ph</span></code>.
                                                                                                    (Critical: make sure
                                                                                                </div>
                                                                                                <div class="line">to
                                                                                                    flatten this!)</div>
                                                                                            </div>
                                                                                        </td>
                                                                                    </tr>
                                                                                </tbody>
                                                                            </table>
                                                                        </li>
                                                                        <li><strong>ac_kwargs</strong> (<em>dict</em>)
                                                                            &#8211; Any kwargs appropriate for the
                                                                            actor_critic
                                                                            function you provided to TRPO.</li>
                                                                        <li><strong>seed</strong> (<em>int</em>) &#8211;
                                                                            Seed for random number generators.</li>
                                                                        <li><strong>steps_per_epoch</strong>
                                                                            (<em>int</em>) &#8211; Number of steps of
                                                                            interaction (state-action pairs)
                                                                            for the agent and the environment in each
                                                                            epoch.</li>
                                                                        <li><strong>epochs</strong> (<em>int</em>)
                                                                            &#8211; Number of epochs of interaction
                                                                            (equivalent to
                                                                            number of policy updates) to perform.</li>
                                                                        <li><strong>gamma</strong> (<em>float</em>)
                                                                            &#8211; Discount factor. (Always between 0
                                                                            and 1.)</li>
                                                                        <li><strong>delta</strong> (<em>float</em>)
                                                                            &#8211; KL-divergence limit for TRPO / NPG
                                                                            update.
                                                                            (Should be small for stability. Values like
                                                                            0.01, 0.05.)</li>
                                                                        <li><strong>vf_lr</strong> (<em>float</em>)
                                                                            &#8211; Learning rate for value function
                                                                            optimizer.</li>
                                                                        <li><strong>train_v_iters</strong>
                                                                            (<em>int</em>) &#8211; Number of gradient
                                                                            descent steps to take on
                                                                            value function per epoch.</li>
                                                                        <li><strong>damping_coeff</strong>
                                                                            (<em>float</em>) &#8211; <p>Artifact for
                                                                                numerical stability, should be
                                                                                smallish. Adjusts Hessian-vector product
                                                                                calculation:</p>
                                                                            <div class="math">
                                                                                <p><img src="_images/math/404dea4d6af56dc327dd856bc640e2699e8135d8.svg"
                                                                                        alt="Hv \rightarrow (\alpha I + H)v" />
                                                                                </p>
                                                                            </div>
                                                                            <p>where <img class="math"
                                                                                    src="_images/math/d8316e40b1057b06d31c2cad3a0d4cc9e75fa2c1.svg"
                                                                                    alt="\alpha" /> is the damping
                                                                                coefficient.
                                                                                Probably don&#8217;t play with this
                                                                                hyperparameter.</p>
                                                                        </li>
                                                                        <li><strong>cg_iters</strong> (<em>int</em>)
                                                                            &#8211; <p>Number of iterations of conjugate
                                                                                gradient to perform.
                                                                                Increasing this will lead to a more
                                                                                accurate approximation
                                                                                to <img class="math"
                                                                                    src="_images/math/e8f8599a36583b493f5c5aaf05228d443ddf1f00.svg"
                                                                                    alt="H^{-1} g" />, and possibly
                                                                                slightly-improved performance,
                                                                                but at the cost of slowing things down.
                                                                            </p>
                                                                            <p>Also probably don&#8217;t play with this
                                                                                hyperparameter.</p>
                                                                        </li>
                                                                        <li><strong>backtrack_iters</strong>
                                                                            (<em>int</em>) &#8211; Maximum number of
                                                                            steps allowed in the
                                                                            backtracking line search. Since the line
                                                                            search usually doesn&#8217;t
                                                                            backtrack, and usually only steps back once
                                                                            when it does, this
                                                                            hyperparameter doesn&#8217;t often matter.
                                                                        </li>
                                                                        <li><strong>backtrack_coeff</strong>
                                                                            (<em>float</em>) &#8211; How far back to
                                                                            step during backtracking line
                                                                            search. (Always between 0 and 1, usually
                                                                            above 0.5.)</li>
                                                                        <li><strong>lam</strong> (<em>float</em>)
                                                                            &#8211; Lambda for GAE-Lambda. (Always
                                                                            between 0 and 1,
                                                                            close to 1.)</li>
                                                                        <li><strong>max_ep_len</strong> (<em>int</em>)
                                                                            &#8211; Maximum length of trajectory /
                                                                            episode / rollout.</li>
                                                                        <li><strong>logger_kwargs</strong>
                                                                            (<em>dict</em>) &#8211; Keyword args for
                                                                            EpochLogger.</li>
                                                                        <li><strong>save_freq</strong> (<em>int</em>)
                                                                            &#8211; How often (in terms of gap between
                                                                            epochs) to save
                                                                            the current policy and value function.</li>
                                                                        <li><strong>algo</strong> &#8211; Either
                                                                            &#8216;trpo&#8217; or &#8216;npg&#8217;:
                                                                            this code supports both, since they are
                                                                            almost the same.</li>
                                                                    </ul>
                                                                </td>
                                                            </tr>
                                                        </tbody>
                                                    </table>
                                                </dd>
                                            </dl>

                                            <div class="section" id="saved-model-contents">
                                                <h4><a class="toc-backref" href="#id11">Saved Model Contents</a><a
                                                        class="headerlink" href="#saved-model-contents"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>The computation graph saved by the logger includes:</p>
                                                <table border="1" class="docutils">
                                                    <colgroup>
                                                        <col width="11%" />
                                                        <col width="89%" />
                                                    </colgroup>
                                                    <thead valign="bottom">
                                                        <tr class="row-odd">
                                                            <th class="head">Key</th>
                                                            <th class="head">Value</th>
                                                        </tr>
                                                    </thead>
                                                    <tbody valign="top">
                                                        <tr class="row-even">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">x</span></code>
                                                            </td>
                                                            <td>Tensorflow placeholder for state input.</td>
                                                        </tr>
                                                        <tr class="row-odd">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">pi</span></code>
                                                            </td>
                                                            <td>Samples an action from the agent, conditioned on states
                                                                in <code
                                                                    class="docutils literal"><span class="pre">x</span></code>.
                                                            </td>
                                                        </tr>
                                                        <tr class="row-even">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">v</span></code>
                                                            </td>
                                                            <td>Gives value estimate for states in <code
                                                                    class="docutils literal"><span class="pre">x</span></code>.
                                                            </td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <p>This saved model can be accessed either by</p>
                                                <ul class="simple">
                                                    <li>running the trained policy with the <a
                                                            class="reference external"
                                                            href="../user/saving_and_loading.html#loading-and-running-trained-policies">test_policy.py</a>
                                                        tool,</li>
                                                    <li>or loading the whole saved graph into a program with <a
                                                            class="reference external"
                                                            href="../utils/logger.html#spinup.utils.logx.restore_tf_graph">restore_tf_graph</a>.
                                                    </li>
                                                </ul>
                                            </div>
                                        </div>
                                        <div class="section" id="references">
                                            <h3><a class="toc-backref" href="#id12">References</a><a class="headerlink"
                                                    href="#references" title="Permalink to this headline">¶</a></h3>
                                            <div class="section" id="relevant-papers">
                                                <h4><a class="toc-backref" href="#id13">Relevant Papers</a><a
                                                        class="headerlink" href="#relevant-papers"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <ul class="simple">
                                                    <li><a class="reference external"
                                                            href="https://arxiv.org/pdf/1502.05477.pdf">Trust Region
                                                            Policy Optimization</a>, Schulman et al. 2015</li>
                                                    <li><a class="reference external"
                                                            href="https://arxiv.org/pdf/1506.02438.pdf">High Dimensional
                                                            Continuous Control Using Generalized Advantage
                                                            Estimation</a>, Schulman et al. 2016</li>
                                                    <li><a class="reference external"
                                                            href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf">Approximately
                                                            Optimal Approximate Reinforcement Learning</a>, Kakade and
                                                        Langford 2002</li>
                                                </ul>
                                            </div>
                                            <div class="section" id="why-these-papers">
                                                <h4><a class="toc-backref" href="#id14">Why These Papers?</a><a
                                                        class="headerlink" href="#why-these-papers"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>Schulman 2015 is included because it is the original paper describing
                                                    TRPO. Schulman 2016 is included because our implementation of TRPO
                                                    makes use of Generalized Advantage Estimation for computing the
                                                    policy gradient. Kakade and Langford 2002 is included because it
                                                    contains theoretical results which motivate and deeply connect to
                                                    the theoretical foundations of TRPO.</p>
                                            </div>
                                            <div class="section" id="other-public-implementations">
                                                <h4><a class="toc-backref" href="#id15">Other Public
                                                        Implementations</a><a class="headerlink"
                                                        href="#other-public-implementations"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <ul class="simple">
                                                    <li><a class="reference external"
                                                            href="https://github.com/openai/baselines/tree/master/baselines/trpo_mpi">Baselines</a>
                                                    </li>
                                                    <li><a class="reference external"
                                                            href="https://github.com/joschu/modular_rl/blob/master/modular_rl/trpo.py">ModularRL</a>
                                                    </li>
                                                    <li><a class="reference external"
                                                            href="https://github.com/rll/rllab/blob/master/rllab/algos/trpo.py">rllab</a>
                                                    </li>
                                                </ul>
                                            </div>
                                        </div>
                                    </div>
                                    <span id="document-algorithms/ppo"></span>
                                    <div class="section" id="proximal-policy-optimization">
                                        <h2><a class="toc-backref" href="#id3">Proximal Policy Optimization</a><a
                                                class="headerlink" href="#proximal-policy-optimization"
                                                title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal" href="#proximal-policy-optimization"
                                                        id="id3">Proximal Policy Optimization</a>
                                                    <ul>
                                                        <li><a class="reference internal" href="#background"
                                                                id="id4">Background</a>
                                                            <ul>
                                                                <li><a class="reference internal" href="#quick-facts"
                                                                        id="id5">Quick Facts</a></li>
                                                                <li><a class="reference internal" href="#key-equations"
                                                                        id="id6">Key Equations</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#exploration-vs-exploitation"
                                                                        id="id7">Exploration vs. Exploitation</a></li>
                                                                <li><a class="reference internal" href="#pseudocode"
                                                                        id="id8">Pseudocode</a></li>
                                                            </ul>
                                                        </li>
                                                        <li><a class="reference internal" href="#documentation"
                                                                id="id9">Documentation</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#documentation-pytorch-version"
                                                                        id="id10">Documentation: PyTorch Version</a>
                                                                </li>
                                                                <li><a class="reference internal"
                                                                        href="#saved-model-contents-pytorch-version"
                                                                        id="id11">Saved Model Contents: PyTorch
                                                                        Version</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#documentation-tensorflow-version"
                                                                        id="id12">Documentation: Tensorflow Version</a>
                                                                </li>
                                                                <li><a class="reference internal"
                                                                        href="#saved-model-contents-tensorflow-version"
                                                                        id="id13">Saved Model Contents: Tensorflow
                                                                        Version</a></li>
                                                            </ul>
                                                        </li>
                                                        <li><a class="reference internal" href="#references"
                                                                id="id14">References</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#relevant-papers" id="id15">Relevant
                                                                        Papers</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#why-these-papers" id="id16">Why These
                                                                        Papers?</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#other-public-implementations"
                                                                        id="id17">Other Public Implementations</a></li>
                                                            </ul>
                                                        </li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <div class="section" id="background">
                                            <h3><a class="toc-backref" href="#id4">Background</a><a class="headerlink"
                                                    href="#background" title="Permalink to this headline">¶</a></h3>
                                            <p>(Previously: <a class="reference external"
                                                    href="../algorithms/trpo.html#background">Background for TRPO</a>)
                                            </p>
                                            <p>PPO is motivated by the same question as TRPO: how can we take the
                                                biggest possible improvement step on a policy using the data we
                                                currently have, without stepping so far that we accidentally cause
                                                performance collapse? Where TRPO tries to solve this problem with a
                                                complex second-order method, PPO is a family of first-order methods that
                                                use a few other tricks to keep new policies close to old. PPO methods
                                                are significantly simpler to implement, and empirically seem to perform
                                                at least as well as TRPO.</p>
                                            <p>There are two primary variants of PPO: PPO-Penalty and PPO-Clip.</p>
                                            <p><strong>PPO-Penalty</strong> approximately solves a KL-constrained update
                                                like TRPO, but penalizes the KL-divergence in the objective function
                                                instead of making it a hard constraint, and automatically adjusts the
                                                penalty coefficient over the course of training so that it&#8217;s
                                                scaled appropriately.</p>
                                            <p><strong>PPO-Clip</strong> doesn&#8217;t have a KL-divergence term in the
                                                objective and doesn&#8217;t have a constraint at all. Instead relies on
                                                specialized clipping in the objective function to remove incentives for
                                                the new policy to get far from the old policy.</p>
                                            <p>Here, we&#8217;ll focus only on PPO-Clip (the primary variant used at
                                                OpenAI).</p>
                                            <div class="section" id="quick-facts">
                                                <h4><a class="toc-backref" href="#id5">Quick Facts</a><a
                                                        class="headerlink" href="#quick-facts"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <ul class="simple">
                                                    <li>PPO is an on-policy algorithm.</li>
                                                    <li>PPO can be used for environments with either discrete or
                                                        continuous action spaces.</li>
                                                    <li>The Spinning Up implementation of PPO supports parallelization
                                                        with MPI.</li>
                                                </ul>
                                            </div>
                                            <div class="section" id="key-equations">
                                                <h4><a class="toc-backref" href="#id6">Key Equations</a><a
                                                        class="headerlink" href="#key-equations"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>PPO-clip updates policies via</p>
                                                <div class="math">
                                                    <p><img src="_images/math/96a52e61318720522e040e433c938ee829d54506.svg"
                                                            alt="\theta_{k+1} = \arg \max_{\theta} \underset{s,a \sim \pi_{\theta_k}}{{\mathrm E}}\left[
    L(s,a,\theta_k, \theta)\right]," /></p>
                                                </div>
                                                <p>typically taking multiple steps of (usually minibatch) SGD to
                                                    maximize the objective. Here <img class="math"
                                                        src="_images/math/3ffe1da701d78dd473975ebd2f875807611f7713.svg"
                                                        alt="L" /> is given by</p>
                                                <div class="math">
                                                    <p><img src="_images/math/99621d5bcaccd056d6ca3aeb48a27bf8cc0e640c.svg"
                                                            alt="L(s,a,\theta_k,\theta) = \min\left(
\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}  A^{\pi_{\theta_k}}(s,a), \;\;
\text{clip}\left(\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}, 1 - \epsilon, 1+\epsilon \right) A^{\pi_{\theta_k}}(s,a)
\right)," /></p>
                                                </div>
                                                <p>in which <img class="math"
                                                        src="_images/math/c589a82739d7aa277bcf45e632d930d1c119b7ef.svg"
                                                        alt="\epsilon" /> is a (small) hyperparameter which roughly says
                                                    how far away the new policy is allowed to go from the old.</p>
                                                <p>This is a pretty complex expression, and it&#8217;s hard to tell at
                                                    first glance what it&#8217;s doing, or how it helps keep the new
                                                    policy close to the old policy. As it turns out, there&#8217;s a
                                                    considerably simplified version <a class="footnote-reference"
                                                        href="#id2" id="id1">[1]</a> of this objective which is a bit
                                                    easier to grapple with (and is also the version we implement in our
                                                    code):</p>
                                                <div class="math">
                                                    <p><img src="_images/math/dd41a29292af3bc58c0c76bc7dba82a7355bf929.svg"
                                                            alt="L(s,a,\theta_k,\theta) = \min\left(
\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}  A^{\pi_{\theta_k}}(s,a), \;\;
g(\epsilon, A^{\pi_{\theta_k}}(s,a))
\right)," /></p>
                                                </div>
                                                <p>where</p>
                                                <div class="math">
                                                    <p><img src="_images/math/39f524858866b80e627840ba77a54360e3bac55e.svg"
                                                            alt="g(\epsilon, A) = \left\{
    \begin{array}{ll}
    (1 + \epsilon) A &amp; A \geq 0 \\
    (1 - \epsilon) A &amp; A &lt; 0.
    \end{array}
    \right." /></p>
                                                </div>
                                                <p>To figure out what intuition to take away from this, let&#8217;s look
                                                    at a single state-action pair <img class="math"
                                                        src="_images/math/4a1b4e2fc586f984a8edafbcae068c3f3c992402.svg"
                                                        alt="(s,a)" />, and think of cases.</p>
                                                <p><strong>Advantage is positive</strong>: Suppose the advantage for
                                                    that state-action pair is positive, in which case its contribution
                                                    to the objective reduces to</p>
                                                <div class="math">
                                                    <p><img src="_images/math/b4e46e01172264315e9e5d6c8bd2ced884d6602c.svg"
                                                            alt="L(s,a,\theta_k,\theta) = \min\left(
\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}, (1 + \epsilon)
\right)  A^{\pi_{\theta_k}}(s,a)." /></p>
                                                </div>
                                                <p>Because the advantage is positive, the objective will increase if the
                                                    action becomes more likely&#8212;that is, if <img class="math"
                                                        src="_images/math/400068784a9d13ffe96c61f29b4ab26ad5557376.svg"
                                                        alt="\pi_{\theta}(a|s)" /> increases. But the min in this term
                                                    puts a limit to how <em>much</em> the objective can increase. Once
                                                    <img class="math"
                                                        src="_images/math/cee08da41b29ab9355f2e4dac94de335c6eff03f.svg"
                                                        alt="\pi_{\theta}(a|s) &gt; (1+\epsilon) \pi_{\theta_k}(a|s)" />,
                                                    the min kicks in and this term hits a ceiling of <img class="math"
                                                        src="_images/math/08d4d3bab53ce2aef0a6fd4d8e0e9f5cd0e4f7ca.svg"
                                                        alt="(1+\epsilon) A^{\pi_{\theta_k}}(s,a)" />. Thus: <em>the new
                                                        policy does not benefit by going far away from the old
                                                        policy</em>.</p>
                                                <p><strong>Advantage is negative</strong>: Suppose the advantage for
                                                    that state-action pair is negative, in which case its contribution
                                                    to the objective reduces to</p>
                                                <div class="math">
                                                    <p><img src="_images/math/b8b23f5e4578125c2d8fbfc66442629ff7a85fb5.svg"
                                                            alt="L(s,a,\theta_k,\theta) = \max\left(
\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}, (1 - \epsilon)
\right)  A^{\pi_{\theta_k}}(s,a)." /></p>
                                                </div>
                                                <p>Because the advantage is negative, the objective will increase if the
                                                    action becomes less likely&#8212;that is, if <img class="math"
                                                        src="_images/math/400068784a9d13ffe96c61f29b4ab26ad5557376.svg"
                                                        alt="\pi_{\theta}(a|s)" /> decreases. But the max in this term
                                                    puts a limit to how <em>much</em> the objective can increase. Once
                                                    <img class="math"
                                                        src="_images/math/82d6b288e893443689bf88b41b1f0f532c54f2f3.svg"
                                                        alt="\pi_{\theta}(a|s) &lt; (1-\epsilon) \pi_{\theta_k}(a|s)" />,
                                                    the max kicks in and this term hits a ceiling of <img class="math"
                                                        src="_images/math/0aea7de5d8df7541d515b563b9c7bb0191e28b32.svg"
                                                        alt="(1-\epsilon) A^{\pi_{\theta_k}}(s,a)" />. Thus, again:
                                                    <em>the new policy does not benefit by going far away from the old
                                                        policy</em>.</p>
                                                <p>What we have seen so far is that clipping serves as a regularizer by
                                                    removing incentives for the policy to change dramatically, and the
                                                    hyperparameter <img class="math"
                                                        src="_images/math/c589a82739d7aa277bcf45e632d930d1c119b7ef.svg"
                                                        alt="\epsilon" /> corresponds to how far away the new policy can
                                                    go from the old while still profiting the objective.</p>
                                                <div class="admonition-you-should-know admonition">
                                                    <p class="first admonition-title">You Should Know</p>
                                                    <p>While this kind of clipping goes a long way towards ensuring
                                                        reasonable policy updates, it is still possible to end up with a
                                                        new policy which is too far from the old policy, and there are a
                                                        bunch of tricks used by different PPO implementations to stave
                                                        this off. In our implementation here, we use a particularly
                                                        simple method: early stopping. If the mean KL-divergence of the
                                                        new policy from the old grows beyond a threshold, we stop taking
                                                        gradient steps.</p>
                                                    <p class="last">When you feel comfortable with the basic math and
                                                        implementation details, it&#8217;s worth checking out other
                                                        implementations to see how they handle this issue!</p>
                                                </div>
                                                <table class="docutils footnote" frame="void" id="id2" rules="none">
                                                    <colgroup>
                                                        <col class="label" />
                                                        <col />
                                                    </colgroup>
                                                    <tbody valign="top">
                                                        <tr>
                                                            <td class="label"><a class="fn-backref" href="#id1">[1]</a>
                                                            </td>
                                                            <td>See <a class="reference external"
                                                                    href="https://drive.google.com/file/d/1PDzn9RPvaXjJFZkGeapMHbHGiWWW20Ey/view?usp=sharing">this
                                                                    note</a> for a derivation of the simplified form of
                                                                the PPO-Clip objective.</td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                            </div>
                                            <div class="section" id="exploration-vs-exploitation">
                                                <h4><a class="toc-backref" href="#id7">Exploration vs.
                                                        Exploitation</a><a class="headerlink"
                                                        href="#exploration-vs-exploitation"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>PPO trains a stochastic policy in an on-policy way. This means that
                                                    it explores by sampling actions according to the latest version of
                                                    its stochastic policy. The amount of randomness in action selection
                                                    depends on both initial conditions and the training procedure. Over
                                                    the course of training, the policy typically becomes progressively
                                                    less random, as the update rule encourages it to exploit rewards
                                                    that it has already found. This may cause the policy to get trapped
                                                    in local optima.</p>
                                            </div>
                                            <div class="section" id="pseudocode">
                                                <h4><a class="toc-backref" href="#id8">Pseudocode</a><a
                                                        class="headerlink" href="#pseudocode"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <div class="math">
                                                    <p><img src="_images/math/e62a8971472597f4b014c2da064f636ffe365ba3.svg"
                                                            alt="\begin{algorithm}[H]
    \caption{PPO-Clip}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta_0$, initial value function parameters $\phi_0$
    \FOR{$k = 0,1,2,...$}
    \STATE Collect set of trajectories ${\mathcal D}_k = \{\tau_i\}$ by running policy $\pi_k = \pi(\theta_k)$ in the environment.
    \STATE Compute rewards-to-go $\hat{R}_t$.
    \STATE Compute advantage estimates, $\hat{A}_t$ (using any method of advantage estimation) based on the current value function $V_{\phi_k}$.
    \STATE Update the policy by maximizing the PPO-Clip objective:
        \begin{equation*}
        \theta_{k+1} = \arg \max_{\theta} \frac{1}{|{\mathcal D}_k| T} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T \min\left(
            \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_k}(a_t|s_t)}  A^{\pi_{\theta_k}}(s_t,a_t), \;\;
            g(\epsilon, A^{\pi_{\theta_k}}(s_t,a_t))
        \right),
        \end{equation*}
        typically via stochastic gradient ascent with Adam.
    \STATE Fit value function by regression on mean-squared error:
        \begin{equation*}
        \phi_{k+1} = \arg \min_{\phi} \frac{1}{|{\mathcal D}_k| T} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T\left( V_{\phi} (s_t) - \hat{R}_t \right)^2,
        \end{equation*}
        typically via some gradient descent algorithm.
    \ENDFOR
\end{algorithmic}
\end{algorithm}" /></p>
                                                </div>
                                            </div>
                                        </div>
                                        <div class="section" id="documentation">
                                            <h3><a class="toc-backref" href="#id9">Documentation</a><a
                                                    class="headerlink" href="#documentation"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p class="last">In what follows, we give documentation for the PyTorch
                                                    and Tensorflow implementations of PPO in Spinning Up. They have
                                                    nearly identical function calls and docstrings, except for details
                                                    relating to model construction. However, we include both full
                                                    docstrings for completeness.</p>
                                            </div>
                                            <div class="section" id="documentation-pytorch-version">
                                                <h4><a class="toc-backref" href="#id10">Documentation: PyTorch
                                                        Version</a><a class="headerlink"
                                                        href="#documentation-pytorch-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <dl class="function">
                                                    <dt id="spinup.ppo_pytorch">
                                                        <code class="descclassname">spinup.</code><code
                                                            class="descname">ppo_pytorch</code><span
                                                            class="sig-paren">(</span><em>env_fn</em>,
                                                        <em>actor_critic=&lt;MagicMock spec='str'
                                                            id='140191856630472'&gt;</em>, <em>ac_kwargs={}</em>,
                                                        <em>seed=0</em>, <em>steps_per_epoch=4000</em>,
                                                        <em>epochs=50</em>, <em>gamma=0.99</em>,
                                                        <em>clip_ratio=0.2</em>, <em>pi_lr=0.0003</em>,
                                                        <em>vf_lr=0.001</em>, <em>train_pi_iters=80</em>,
                                                        <em>train_v_iters=80</em>, <em>lam=0.97</em>,
                                                        <em>max_ep_len=1000</em>, <em>target_kl=0.01</em>,
                                                        <em>logger_kwargs={}</em>, <em>save_freq=10</em><span
                                                            class="sig-paren">)</span><a class="headerlink"
                                                            href="#spinup.ppo_pytorch"
                                                            title="Permalink to this definition">¶</a>
                                                    </dt>
                                                    <dd>
                                                        <p>Proximal Policy Optimization (by clipping),</p>
                                                        <p>with early stopping based on approximate KL</p>
                                                        <table class="docutils field-list" frame="void" rules="none">
                                                            <col class="field-name" />
                                                            <col class="field-body" />
                                                            <tbody valign="top">
                                                                <tr class="field-odd field">
                                                                    <th class="field-name">Parameters:</th>
                                                                    <td class="field-body">
                                                                        <ul class="first last simple">
                                                                            <li><strong>env_fn</strong> &#8211; A
                                                                                function which creates a copy of the
                                                                                environment.
                                                                                The environment must satisfy the OpenAI
                                                                                Gym API.</li>
                                                                            <li><strong>actor_critic</strong> &#8211;
                                                                                <p>The constructor method for a PyTorch
                                                                                    Module with a
                                                                                    <code
                                                                                        class="docutils literal"><span class="pre">step</span></code>
                                                                                    method, an <code
                                                                                        class="docutils literal"><span class="pre">act</span></code>
                                                                                    method, a <code
                                                                                        class="docutils literal"><span class="pre">pi</span></code>
                                                                                    module, and a <code
                                                                                        class="docutils literal"><span class="pre">v</span></code>
                                                                                    module. The <code
                                                                                        class="docutils literal"><span class="pre">step</span></code>
                                                                                    method should accept a batch of
                                                                                    observations
                                                                                    and return:
                                                                                </p>
                                                                                <table border="1" class="docutils">
                                                                                    <colgroup>
                                                                                        <col width="17%" />
                                                                                        <col width="25%" />
                                                                                        <col width="58%" />
                                                                                    </colgroup>
                                                                                    <thead valign="bottom">
                                                                                        <tr class="row-odd">
                                                                                            <th class="head">Symbol</th>
                                                                                            <th class="head">Shape</th>
                                                                                            <th class="head">Description
                                                                                            </th>
                                                                                        </tr>
                                                                                    </thead>
                                                                                    <tbody valign="top">
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">a</span></code>
                                                                                            </td>
                                                                                            <td>(batch, act_dim)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Numpy array of
                                                                                                        actions for each
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        observation.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-odd">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">v</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Numpy array of
                                                                                                        value estimates
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        for the provided
                                                                                                        observations.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">logp_a</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Numpy array of
                                                                                                        log probs for
                                                                                                        the</div>
                                                                                                    <div class="line">
                                                                                                        actions in <code
                                                                                                            class="docutils literal"><span class="pre">a</span></code>.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                    </tbody>
                                                                                </table>
                                                                                <p>The <code
                                                                                        class="docutils literal"><span class="pre">act</span></code>
                                                                                    method behaves the same as <code
                                                                                        class="docutils literal"><span class="pre">step</span></code>
                                                                                    but only returns <code
                                                                                        class="docutils literal"><span class="pre">a</span></code>.
                                                                                </p>
                                                                                <p>The <code
                                                                                        class="docutils literal"><span class="pre">pi</span></code>
                                                                                    module&#8217;s forward call should
                                                                                    accept a batch of
                                                                                    observations and optionally a batch
                                                                                    of actions, and return:</p>
                                                                                <table border="1" class="docutils">
                                                                                    <colgroup>
                                                                                        <col width="16%" />
                                                                                        <col width="23%" />
                                                                                        <col width="61%" />
                                                                                    </colgroup>
                                                                                    <thead valign="bottom">
                                                                                        <tr class="row-odd">
                                                                                            <th class="head">Symbol</th>
                                                                                            <th class="head">Shape</th>
                                                                                            <th class="head">Description
                                                                                            </th>
                                                                                        </tr>
                                                                                    </thead>
                                                                                    <tbody valign="top">
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">pi</span></code>
                                                                                            </td>
                                                                                            <td>N/A</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Torch
                                                                                                        Distribution
                                                                                                        object,
                                                                                                        containing</div>
                                                                                                    <div class="line">a
                                                                                                        batch of
                                                                                                        distributions
                                                                                                        describing</div>
                                                                                                    <div class="line">
                                                                                                        the policy for
                                                                                                        the provided
                                                                                                        observations.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-odd">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">logp_a</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Optional (only
                                                                                                        returned if
                                                                                                        batch of</div>
                                                                                                    <div class="line">
                                                                                                        actions is
                                                                                                        given). Tensor
                                                                                                        containing</div>
                                                                                                    <div class="line">
                                                                                                        the log
                                                                                                        probability,
                                                                                                        according to
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        the policy, of
                                                                                                        the provided
                                                                                                        actions.</div>
                                                                                                    <div class="line">If
                                                                                                        actions not
                                                                                                        given, will
                                                                                                        contain</div>
                                                                                                    <div class="line">
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">None</span></code>.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                    </tbody>
                                                                                </table>
                                                                                <p>The <code
                                                                                        class="docutils literal"><span class="pre">v</span></code>
                                                                                    module&#8217;s forward call should
                                                                                    accept a batch of observations
                                                                                    and return:</p>
                                                                                <table border="1" class="docutils">
                                                                                    <colgroup>
                                                                                        <col width="16%" />
                                                                                        <col width="23%" />
                                                                                        <col width="61%" />
                                                                                    </colgroup>
                                                                                    <thead valign="bottom">
                                                                                        <tr class="row-odd">
                                                                                            <th class="head">Symbol</th>
                                                                                            <th class="head">Shape</th>
                                                                                            <th class="head">Description
                                                                                            </th>
                                                                                        </tr>
                                                                                    </thead>
                                                                                    <tbody valign="top">
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">v</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Tensor
                                                                                                        containing the
                                                                                                        value estimates
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        for the provided
                                                                                                        observations.
                                                                                                        (Critical:</div>
                                                                                                    <div class="line">
                                                                                                        make sure to
                                                                                                        flatten this!)
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                    </tbody>
                                                                                </table>
                                                                            </li>
                                                                            <li><strong>ac_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Any kwargs
                                                                                appropriate for the ActorCritic object
                                                                                you provided to PPO.</li>
                                                                            <li><strong>seed</strong> (<em>int</em>)
                                                                                &#8211; Seed for random number
                                                                                generators.</li>
                                                                            <li><strong>steps_per_epoch</strong>
                                                                                (<em>int</em>) &#8211; Number of steps
                                                                                of interaction (state-action pairs)
                                                                                for the agent and the environment in
                                                                                each epoch.</li>
                                                                            <li><strong>epochs</strong> (<em>int</em>)
                                                                                &#8211; Number of epochs of interaction
                                                                                (equivalent to
                                                                                number of policy updates) to perform.
                                                                            </li>
                                                                            <li><strong>gamma</strong> (<em>float</em>)
                                                                                &#8211; Discount factor. (Always between
                                                                                0 and 1.)</li>
                                                                            <li><strong>clip_ratio</strong>
                                                                                (<em>float</em>) &#8211; Hyperparameter
                                                                                for clipping in the policy objective.
                                                                                Roughly: how far can the new policy go
                                                                                from the old policy while
                                                                                still profiting (improving the objective
                                                                                function)? The new policy
                                                                                can still go farther than the clip_ratio
                                                                                says, but it doesn&#8217;t help
                                                                                on the objective anymore. (Usually
                                                                                small, 0.1 to 0.3.) Typically
                                                                                denoted by <img class="math"
                                                                                    src="_images/math/c589a82739d7aa277bcf45e632d930d1c119b7ef.svg"
                                                                                    alt="\epsilon" />.</li>
                                                                            <li><strong>pi_lr</strong> (<em>float</em>)
                                                                                &#8211; Learning rate for policy
                                                                                optimizer.</li>
                                                                            <li><strong>vf_lr</strong> (<em>float</em>)
                                                                                &#8211; Learning rate for value function
                                                                                optimizer.</li>
                                                                            <li><strong>train_pi_iters</strong>
                                                                                (<em>int</em>) &#8211; Maximum number of
                                                                                gradient descent steps to take
                                                                                on policy loss per epoch. (Early
                                                                                stopping may cause optimizer
                                                                                to take fewer than this.)</li>
                                                                            <li><strong>train_v_iters</strong>
                                                                                (<em>int</em>) &#8211; Number of
                                                                                gradient descent steps to take on
                                                                                value function per epoch.</li>
                                                                            <li><strong>lam</strong> (<em>float</em>)
                                                                                &#8211; Lambda for GAE-Lambda. (Always
                                                                                between 0 and 1,
                                                                                close to 1.)</li>
                                                                            <li><strong>max_ep_len</strong>
                                                                                (<em>int</em>) &#8211; Maximum length of
                                                                                trajectory / episode / rollout.</li>
                                                                            <li><strong>target_kl</strong>
                                                                                (<em>float</em>) &#8211; Roughly what KL
                                                                                divergence we think is appropriate
                                                                                between new and old policies after an
                                                                                update. This will get used
                                                                                for early stopping. (Usually small, 0.01
                                                                                or 0.05.)</li>
                                                                            <li><strong>logger_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Keyword args for
                                                                                EpochLogger.</li>
                                                                            <li><strong>save_freq</strong>
                                                                                (<em>int</em>) &#8211; How often (in
                                                                                terms of gap between epochs) to save
                                                                                the current policy and value function.
                                                                            </li>
                                                                        </ul>
                                                                    </td>
                                                                </tr>
                                                            </tbody>
                                                        </table>
                                                    </dd>
                                                </dl>

                                            </div>
                                            <div class="section" id="saved-model-contents-pytorch-version">
                                                <h4><a class="toc-backref" href="#id11">Saved Model Contents: PyTorch
                                                        Version</a><a class="headerlink"
                                                        href="#saved-model-contents-pytorch-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>The PyTorch saved model can be loaded with <code
                                                        class="docutils literal"><span class="pre">ac</span> <span class="pre">=</span> <span class="pre">torch.load('path/to/model.pt')</span></code>,
                                                    yielding an actor-critic object (<code
                                                        class="docutils literal"><span class="pre">ac</span></code>)
                                                    that has the properties described in the docstring for <code
                                                        class="docutils literal"><span class="pre">ppo_pytorch</span></code>.
                                                </p>
                                                <p>You can get actions from this model with</p>
                                                <div class="highlight-python">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">actions</span> <span class="o">=</span> <span class="n">ac</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre>
                                                    </div>
                                                </div>
                                            </div>
                                            <div class="section" id="documentation-tensorflow-version">
                                                <h4><a class="toc-backref" href="#id12">Documentation: Tensorflow
                                                        Version</a><a class="headerlink"
                                                        href="#documentation-tensorflow-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <dl class="function">
                                                    <dt id="spinup.ppo_tf1">
                                                        <code class="descclassname">spinup.</code><code
                                                            class="descname">ppo_tf1</code><span
                                                            class="sig-paren">(</span><em>env_fn</em>,
                                                        <em>actor_critic=&lt;function mlp_actor_critic&gt;</em>,
                                                        <em>ac_kwargs={}</em>, <em>seed=0</em>,
                                                        <em>steps_per_epoch=4000</em>, <em>epochs=50</em>,
                                                        <em>gamma=0.99</em>, <em>clip_ratio=0.2</em>,
                                                        <em>pi_lr=0.0003</em>, <em>vf_lr=0.001</em>,
                                                        <em>train_pi_iters=80</em>, <em>train_v_iters=80</em>,
                                                        <em>lam=0.97</em>, <em>max_ep_len=1000</em>,
                                                        <em>target_kl=0.01</em>, <em>logger_kwargs={}</em>,
                                                        <em>save_freq=10</em><span class="sig-paren">)</span><a
                                                            class="headerlink" href="#spinup.ppo_tf1"
                                                            title="Permalink to this definition">¶</a>
                                                    </dt>
                                                    <dd>
                                                        <p>Proximal Policy Optimization (by clipping),</p>
                                                        <p>with early stopping based on approximate KL</p>
                                                        <table class="docutils field-list" frame="void" rules="none">
                                                            <col class="field-name" />
                                                            <col class="field-body" />
                                                            <tbody valign="top">
                                                                <tr class="field-odd field">
                                                                    <th class="field-name">Parameters:</th>
                                                                    <td class="field-body">
                                                                        <ul class="first last simple">
                                                                            <li><strong>env_fn</strong> &#8211; A
                                                                                function which creates a copy of the
                                                                                environment.
                                                                                The environment must satisfy the OpenAI
                                                                                Gym API.</li>
                                                                            <li><strong>actor_critic</strong> &#8211;
                                                                                <p>A function which takes in placeholder
                                                                                    symbols
                                                                                    for state, <code
                                                                                        class="docutils literal"><span class="pre">x_ph</span></code>,
                                                                                    and action, <code
                                                                                        class="docutils literal"><span class="pre">a_ph</span></code>,
                                                                                    and returns the main
                                                                                    outputs from the agent&#8217;s
                                                                                    Tensorflow computation graph:</p>
                                                                                <table border="1" class="docutils">
                                                                                    <colgroup>
                                                                                        <col width="16%" />
                                                                                        <col width="24%" />
                                                                                        <col width="60%" />
                                                                                    </colgroup>
                                                                                    <thead valign="bottom">
                                                                                        <tr class="row-odd">
                                                                                            <th class="head">Symbol</th>
                                                                                            <th class="head">Shape</th>
                                                                                            <th class="head">Description
                                                                                            </th>
                                                                                        </tr>
                                                                                    </thead>
                                                                                    <tbody valign="top">
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">pi</span></code>
                                                                                            </td>
                                                                                            <td>(batch, act_dim)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Samples actions
                                                                                                        from policy
                                                                                                        given</div>
                                                                                                    <div class="line">
                                                                                                        states.</div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-odd">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">logp</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Gives log
                                                                                                        probability,
                                                                                                        according to
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        the policy, of
                                                                                                        taking actions
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">a_ph</span></code>
                                                                                                    </div>
                                                                                                    <div class="line">in
                                                                                                        states <code
                                                                                                            class="docutils literal"><span class="pre">x_ph</span></code>.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">logp_pi</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Gives log
                                                                                                        probability,
                                                                                                        according to
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        the policy, of
                                                                                                        the action
                                                                                                        sampled by</div>
                                                                                                    <div class="line">
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">pi</span></code>.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-odd">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">v</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Gives the value
                                                                                                        estimate for
                                                                                                        states</div>
                                                                                                    <div class="line">in
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">x_ph</span></code>.
                                                                                                        (Critical: make
                                                                                                        sure</div>
                                                                                                    <div class="line">to
                                                                                                        flatten this!)
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                    </tbody>
                                                                                </table>
                                                                            </li>
                                                                            <li><strong>ac_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Any kwargs
                                                                                appropriate for the actor_critic
                                                                                function you provided to PPO.</li>
                                                                            <li><strong>seed</strong> (<em>int</em>)
                                                                                &#8211; Seed for random number
                                                                                generators.</li>
                                                                            <li><strong>steps_per_epoch</strong>
                                                                                (<em>int</em>) &#8211; Number of steps
                                                                                of interaction (state-action pairs)
                                                                                for the agent and the environment in
                                                                                each epoch.</li>
                                                                            <li><strong>epochs</strong> (<em>int</em>)
                                                                                &#8211; Number of epochs of interaction
                                                                                (equivalent to
                                                                                number of policy updates) to perform.
                                                                            </li>
                                                                            <li><strong>gamma</strong> (<em>float</em>)
                                                                                &#8211; Discount factor. (Always between
                                                                                0 and 1.)</li>
                                                                            <li><strong>clip_ratio</strong>
                                                                                (<em>float</em>) &#8211; Hyperparameter
                                                                                for clipping in the policy objective.
                                                                                Roughly: how far can the new policy go
                                                                                from the old policy while
                                                                                still profiting (improving the objective
                                                                                function)? The new policy
                                                                                can still go farther than the clip_ratio
                                                                                says, but it doesn&#8217;t help
                                                                                on the objective anymore. (Usually
                                                                                small, 0.1 to 0.3.) Typically
                                                                                denoted by <img class="math"
                                                                                    src="_images/math/c589a82739d7aa277bcf45e632d930d1c119b7ef.svg"
                                                                                    alt="\epsilon" />.</li>
                                                                            <li><strong>pi_lr</strong> (<em>float</em>)
                                                                                &#8211; Learning rate for policy
                                                                                optimizer.</li>
                                                                            <li><strong>vf_lr</strong> (<em>float</em>)
                                                                                &#8211; Learning rate for value function
                                                                                optimizer.</li>
                                                                            <li><strong>train_pi_iters</strong>
                                                                                (<em>int</em>) &#8211; Maximum number of
                                                                                gradient descent steps to take
                                                                                on policy loss per epoch. (Early
                                                                                stopping may cause optimizer
                                                                                to take fewer than this.)</li>
                                                                            <li><strong>train_v_iters</strong>
                                                                                (<em>int</em>) &#8211; Number of
                                                                                gradient descent steps to take on
                                                                                value function per epoch.</li>
                                                                            <li><strong>lam</strong> (<em>float</em>)
                                                                                &#8211; Lambda for GAE-Lambda. (Always
                                                                                between 0 and 1,
                                                                                close to 1.)</li>
                                                                            <li><strong>max_ep_len</strong>
                                                                                (<em>int</em>) &#8211; Maximum length of
                                                                                trajectory / episode / rollout.</li>
                                                                            <li><strong>target_kl</strong>
                                                                                (<em>float</em>) &#8211; Roughly what KL
                                                                                divergence we think is appropriate
                                                                                between new and old policies after an
                                                                                update. This will get used
                                                                                for early stopping. (Usually small, 0.01
                                                                                or 0.05.)</li>
                                                                            <li><strong>logger_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Keyword args for
                                                                                EpochLogger.</li>
                                                                            <li><strong>save_freq</strong>
                                                                                (<em>int</em>) &#8211; How often (in
                                                                                terms of gap between epochs) to save
                                                                                the current policy and value function.
                                                                            </li>
                                                                        </ul>
                                                                    </td>
                                                                </tr>
                                                            </tbody>
                                                        </table>
                                                    </dd>
                                                </dl>

                                            </div>
                                            <div class="section" id="saved-model-contents-tensorflow-version">
                                                <h4><a class="toc-backref" href="#id13">Saved Model Contents: Tensorflow
                                                        Version</a><a class="headerlink"
                                                        href="#saved-model-contents-tensorflow-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>The computation graph saved by the logger includes:</p>
                                                <table border="1" class="docutils">
                                                    <colgroup>
                                                        <col width="11%" />
                                                        <col width="89%" />
                                                    </colgroup>
                                                    <thead valign="bottom">
                                                        <tr class="row-odd">
                                                            <th class="head">Key</th>
                                                            <th class="head">Value</th>
                                                        </tr>
                                                    </thead>
                                                    <tbody valign="top">
                                                        <tr class="row-even">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">x</span></code>
                                                            </td>
                                                            <td>Tensorflow placeholder for state input.</td>
                                                        </tr>
                                                        <tr class="row-odd">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">pi</span></code>
                                                            </td>
                                                            <td>Samples an action from the agent, conditioned on states
                                                                in <code
                                                                    class="docutils literal"><span class="pre">x</span></code>.
                                                            </td>
                                                        </tr>
                                                        <tr class="row-even">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">v</span></code>
                                                            </td>
                                                            <td>Gives value estimate for states in <code
                                                                    class="docutils literal"><span class="pre">x</span></code>.
                                                            </td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <p>This saved model can be accessed either by</p>
                                                <ul class="simple">
                                                    <li>running the trained policy with the <a
                                                            class="reference external"
                                                            href="../user/saving_and_loading.html#loading-and-running-trained-policies">test_policy.py</a>
                                                        tool,</li>
                                                    <li>or loading the whole saved graph into a program with <a
                                                            class="reference external"
                                                            href="../utils/logger.html#spinup.utils.logx.restore_tf_graph">restore_tf_graph</a>.
                                                    </li>
                                                </ul>
                                            </div>
                                        </div>
                                        <div class="section" id="references">
                                            <h3><a class="toc-backref" href="#id14">References</a><a class="headerlink"
                                                    href="#references" title="Permalink to this headline">¶</a></h3>
                                            <div class="section" id="relevant-papers">
                                                <h4><a class="toc-backref" href="#id15">Relevant Papers</a><a
                                                        class="headerlink" href="#relevant-papers"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <ul class="simple">
                                                    <li><a class="reference external"
                                                            href="https://arxiv.org/pdf/1707.06347.pdf">Proximal Policy
                                                            Optimization Algorithms</a>, Schulman et al. 2017</li>
                                                    <li><a class="reference external"
                                                            href="https://arxiv.org/pdf/1506.02438.pdf">High Dimensional
                                                            Continuous Control Using Generalized Advantage
                                                            Estimation</a>, Schulman et al. 2016</li>
                                                    <li><a class="reference external"
                                                            href="https://arxiv.org/pdf/1707.02286.pdf">Emergence of
                                                            Locomotion Behaviours in Rich Environments</a>, Heess et al.
                                                        2017</li>
                                                </ul>
                                            </div>
                                            <div class="section" id="why-these-papers">
                                                <h4><a class="toc-backref" href="#id16">Why These Papers?</a><a
                                                        class="headerlink" href="#why-these-papers"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>Schulman 2017 is included because it is the original paper describing
                                                    PPO. Schulman 2016 is included because our implementation of PPO
                                                    makes use of Generalized Advantage Estimation for computing the
                                                    policy gradient. Heess 2017 is included because it presents a
                                                    large-scale empirical analysis of behaviors learned by PPO agents in
                                                    complex environments (although it uses PPO-penalty instead of
                                                    PPO-clip).</p>
                                            </div>
                                            <div class="section" id="other-public-implementations">
                                                <h4><a class="toc-backref" href="#id17">Other Public
                                                        Implementations</a><a class="headerlink"
                                                        href="#other-public-implementations"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <ul class="simple">
                                                    <li><a class="reference external"
                                                            href="https://github.com/openai/baselines/tree/master/baselines/ppo2">Baselines</a>
                                                    </li>
                                                    <li><a class="reference external"
                                                            href="https://github.com/joschu/modular_rl/blob/master/modular_rl/ppo.py">ModularRL</a>
                                                        (Caution: this implements PPO-penalty instead of PPO-clip.)</li>
                                                    <li><a class="reference external"
                                                            href="https://github.com/rll/rllab/blob/master/rllab/algos/ppo.py">rllab</a>
                                                        (Caution: this implements PPO-penalty instead of PPO-clip.)</li>
                                                    <li><a class="reference external"
                                                            href="https://github.com/ray-project/ray/tree/master/python/ray/rllib/agents/ppo">rllib
                                                            (Ray)</a></li>
                                                </ul>
                                            </div>
                                        </div>
                                    </div>
                                    <span id="document-algorithms/ddpg"></span>
                                    <div class="section" id="deep-deterministic-policy-gradient">
                                        <h2><a class="toc-backref" href="#id1">Deep Deterministic Policy Gradient</a><a
                                                class="headerlink" href="#deep-deterministic-policy-gradient"
                                                title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal"
                                                        href="#deep-deterministic-policy-gradient" id="id1">Deep
                                                        Deterministic Policy Gradient</a>
                                                    <ul>
                                                        <li><a class="reference internal" href="#background"
                                                                id="id2">Background</a>
                                                            <ul>
                                                                <li><a class="reference internal" href="#quick-facts"
                                                                        id="id3">Quick Facts</a></li>
                                                                <li><a class="reference internal" href="#key-equations"
                                                                        id="id4">Key Equations</a>
                                                                    <ul>
                                                                        <li><a class="reference internal"
                                                                                href="#the-q-learning-side-of-ddpg"
                                                                                id="id5">The Q-Learning Side of DDPG</a>
                                                                        </li>
                                                                        <li><a class="reference internal"
                                                                                href="#the-policy-learning-side-of-ddpg"
                                                                                id="id6">The Policy Learning Side of
                                                                                DDPG</a></li>
                                                                    </ul>
                                                                </li>
                                                                <li><a class="reference internal"
                                                                        href="#exploration-vs-exploitation"
                                                                        id="id7">Exploration vs. Exploitation</a></li>
                                                                <li><a class="reference internal" href="#pseudocode"
                                                                        id="id8">Pseudocode</a></li>
                                                            </ul>
                                                        </li>
                                                        <li><a class="reference internal" href="#documentation"
                                                                id="id9">Documentation</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#documentation-pytorch-version"
                                                                        id="id10">Documentation: PyTorch Version</a>
                                                                </li>
                                                                <li><a class="reference internal"
                                                                        href="#saved-model-contents-pytorch-version"
                                                                        id="id11">Saved Model Contents: PyTorch
                                                                        Version</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#documentation-tensorflow-version"
                                                                        id="id12">Documentation: Tensorflow Version</a>
                                                                </li>
                                                                <li><a class="reference internal"
                                                                        href="#saved-model-contents-tensorflow-version"
                                                                        id="id13">Saved Model Contents: Tensorflow
                                                                        Version</a></li>
                                                            </ul>
                                                        </li>
                                                        <li><a class="reference internal" href="#references"
                                                                id="id14">References</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#relevant-papers" id="id15">Relevant
                                                                        Papers</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#why-these-papers" id="id16">Why These
                                                                        Papers?</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#other-public-implementations"
                                                                        id="id17">Other Public Implementations</a></li>
                                                            </ul>
                                                        </li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <div class="section" id="background">
                                            <h3><a class="toc-backref" href="#id2">Background</a><a class="headerlink"
                                                    href="#background" title="Permalink to this headline">¶</a></h3>
                                            <p>(Previously: <a class="reference external"
                                                    href="../spinningup/rl_intro.html#the-optimal-q-function-and-the-optimal-action">Introduction
                                                    to RL Part 1: The Optimal Q-Function and the Optimal Action</a>)</p>
                                            <p>Deep Deterministic Policy Gradient (DDPG) is an algorithm which
                                                concurrently learns a Q-function and a policy. It uses off-policy data
                                                and the Bellman equation to learn the Q-function, and uses the
                                                Q-function to learn the policy.</p>
                                            <p>This approach is closely connected to Q-learning, and is motivated the
                                                same way: if you know the optimal action-value function <img
                                                    class="math"
                                                    src="_images/math/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg"
                                                    alt="Q^*(s,a)" />, then in any given state, the optimal action <img
                                                    class="math"
                                                    src="_images/math/baf715aa6a295b7b7d85e1e1123552c5ae705756.svg"
                                                    alt="a^*(s)" /> can be found by solving</p>
                                            <div class="math">
                                                <p><img src="_images/math/82f049ec26e21eb2bfc6af21e3465707814f4838.svg"
                                                        alt="a^*(s) = \arg \max_a Q^*(s,a)." /></p>
                                            </div>
                                            <p>DDPG interleaves learning an approximator to <img class="math"
                                                    src="_images/math/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg"
                                                    alt="Q^*(s,a)" /> with learning an approximator to <img class="math"
                                                    src="_images/math/baf715aa6a295b7b7d85e1e1123552c5ae705756.svg"
                                                    alt="a^*(s)" />, and it does so in a way which is specifically
                                                adapted for environments with continuous action spaces. But what does it
                                                mean that DDPG is adapted <em>specifically</em> for environments with
                                                continuous action spaces? It relates to how we compute the max over
                                                actions in <img class="math"
                                                    src="_images/math/1f3098d0653722949f8ceeefc8b5c951d99c8274.svg"
                                                    alt="\max_a Q^*(s,a)" />.</p>
                                            <p>When there are a finite number of discrete actions, the max poses no
                                                problem, because we can just compute the Q-values for each action
                                                separately and directly compare them. (This also immediately gives us
                                                the action which maximizes the Q-value.) But when the action space is
                                                continuous, we can&#8217;t exhaustively evaluate the space, and solving
                                                the optimization problem is highly non-trivial. Using a normal
                                                optimization algorithm would make calculating <img class="math"
                                                    src="_images/math/1f3098d0653722949f8ceeefc8b5c951d99c8274.svg"
                                                    alt="\max_a Q^*(s,a)" /> a painfully expensive subroutine. And since
                                                it would need to be run every time the agent wants to take an action in
                                                the environment, this is unacceptable.</p>
                                            <p>Because the action space is continuous, the function <img class="math"
                                                    src="_images/math/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg"
                                                    alt="Q^*(s,a)" /> is presumed to be differentiable with respect to
                                                the action argument. This allows us to set up an efficient,
                                                gradient-based learning rule for a policy <img class="math"
                                                    src="_images/math/3c89236fa57c3dbe71f7c249a07267f83d9c638b.svg"
                                                    alt="\mu(s)" /> which exploits that fact. Then, instead of running
                                                an expensive optimization subroutine each time we wish to compute <img
                                                    class="math"
                                                    src="_images/math/03f01f77446d623f1c933e335f9f81c9a3558c4f.svg"
                                                    alt="\max_a Q(s,a)" />, we can approximate it with <img class="math"
                                                    src="_images/math/8070b852fa94029e80d5811417fd76818a31ec4c.svg"
                                                    alt="\max_a Q(s,a) \approx Q(s,\mu(s))" />. See the Key Equations
                                                section details.</p>
                                            <div class="section" id="quick-facts">
                                                <h4><a class="toc-backref" href="#id3">Quick Facts</a><a
                                                        class="headerlink" href="#quick-facts"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <ul class="simple">
                                                    <li>DDPG is an off-policy algorithm.</li>
                                                    <li>DDPG can only be used for environments with continuous action
                                                        spaces.</li>
                                                    <li>DDPG can be thought of as being deep Q-learning for continuous
                                                        action spaces.</li>
                                                    <li>The Spinning Up implementation of DDPG does not support
                                                        parallelization.</li>
                                                </ul>
                                            </div>
                                            <div class="section" id="key-equations">
                                                <h4><a class="toc-backref" href="#id4">Key Equations</a><a
                                                        class="headerlink" href="#key-equations"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>Here, we&#8217;ll explain the math behind the two parts of DDPG:
                                                    learning a Q function, and learning a policy.</p>
                                                <div class="section" id="the-q-learning-side-of-ddpg">
                                                    <h5><a class="toc-backref" href="#id5">The Q-Learning Side of
                                                            DDPG</a><a class="headerlink"
                                                            href="#the-q-learning-side-of-ddpg"
                                                            title="Permalink to this headline">¶</a></h5>
                                                    <p>First, let&#8217;s recap the Bellman equation describing the
                                                        optimal action-value function, <img class="math"
                                                            src="_images/math/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg"
                                                            alt="Q^*(s,a)" />. It&#8217;s given by</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/3a8b6ce0d6c0b68744b5724403f5d70ed5cda5db.svg"
                                                                alt="Q^*(s,a) = \underset{s' \sim P}{{\mathrm E}}\left[r(s,a) + \gamma \max_{a'} Q^*(s', a')\right]" />
                                                        </p>
                                                    </div>
                                                    <p>where <img class="math"
                                                            src="_images/math/411171ab57c4bec0d86c9f4b495106ba5d73decc.svg"
                                                            alt="s' \sim P" /> is shorthand for saying that the next
                                                        state, <img class="math"
                                                            src="_images/math/6e85fa05d4954e7c1e8037ee1bd163d15bc2e2d6.svg"
                                                            alt="s'" />, is sampled by the environment from a
                                                        distribution <img class="math"
                                                            src="_images/math/400976c62fa52ed70c85d7389f039b5e41473654.svg"
                                                            alt="P(\cdot| s,a)" />.</p>
                                                    <p>This Bellman equation is the starting point for learning an
                                                        approximator to <img class="math"
                                                            src="_images/math/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg"
                                                            alt="Q^*(s,a)" />. Suppose the approximator is a neural
                                                        network <img class="math"
                                                            src="_images/math/521198ffdba43bf32186f95801549cd1502b76c7.svg"
                                                            alt="Q_{\phi}(s,a)" />, with parameters <img class="math"
                                                            src="_images/math/3b22abcadf8773922f8db80011611bad8123a783.svg"
                                                            alt="\phi" />, and that we have collected a set <img
                                                            class="math"
                                                            src="_images/math/452456a08130b84d0c030fdc6e9b05973c5bc8b2.svg"
                                                            alt="{\mathcal D}" /> of transitions <img class="math"
                                                            src="_images/math/4d273c4abe9c8d2805d78e826ee4368ed92841d7.svg"
                                                            alt="(s,a,r,s',d)" /> (where <img class="math"
                                                            src="_images/math/9d61e89bfc1aa6993172a3ac47ab5be75f8e9e81.svg"
                                                            alt="d" /> indicates whether state <img class="math"
                                                            src="_images/math/6e85fa05d4954e7c1e8037ee1bd163d15bc2e2d6.svg"
                                                            alt="s'" /> is terminal). We can set up a
                                                        <strong>mean-squared Bellman error (MSBE)</strong> function,
                                                        which tells us roughly how closely <img class="math"
                                                            src="_images/math/c25464faf1bf4928960905461cbbabe1d2441cb2.svg"
                                                            alt="Q_{\phi}" /> comes to satisfying the Bellman equation:
                                                    </p>
                                                    <div class="math">
                                                        <p><img src="_images/math/31dda6ac0678255c4e192dd6fae4f7ed3c7cd91b.svg"
                                                                alt="L(\phi, {\mathcal D}) = \underset{(s,a,r,s',d) \sim {\mathcal D}}{{\mathrm E}}\left[
    \Bigg( Q_{\phi}(s,a) - \left(r + \gamma (1 - d) \max_{a'} Q_{\phi}(s',a') \right) \Bigg)^2
    \right]" /></p>
                                                    </div>
                                                    <p>Here, in evaluating <img class="math"
                                                            src="_images/math/4591928b993b71d80f43193ffbbbef8e9f3aea10.svg"
                                                            alt="(1-d)" />, we&#8217;ve used a Python convention of
                                                        evaluating <code
                                                            class="docutils literal"><span class="pre">True</span></code>
                                                        to 1 and <code
                                                            class="docutils literal"><span class="pre">False</span></code>
                                                        to zero. Thus, when <code
                                                            class="docutils literal"><span class="pre">d==True</span></code>&#8212;which
                                                        is to say, when <img class="math"
                                                            src="_images/math/6e85fa05d4954e7c1e8037ee1bd163d15bc2e2d6.svg"
                                                            alt="s'" /> is a terminal state&#8212;the Q-function should
                                                        show that the agent gets no additional rewards after the current
                                                        state. (This choice of notation corresponds to what we later
                                                        implement in code.)</p>
                                                    <p>Q-learning algorithms for function approximators, such as DQN
                                                        (and all its variants) and DDPG, are largely based on minimizing
                                                        this MSBE loss function. There are two main tricks employed by
                                                        all of them which are worth describing, and then a specific
                                                        detail for DDPG.</p>
                                                    <p><strong>Trick One: Replay Buffers.</strong> All standard
                                                        algorithms for training a deep neural network to approximate
                                                        <img class="math"
                                                            src="_images/math/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg"
                                                            alt="Q^*(s,a)" /> make use of an experience replay buffer.
                                                        This is the set <img class="math"
                                                            src="_images/math/452456a08130b84d0c030fdc6e9b05973c5bc8b2.svg"
                                                            alt="{\mathcal D}" /> of previous experiences. In order for
                                                        the algorithm to have stable behavior, the replay buffer should
                                                        be large enough to contain a wide range of experiences, but it
                                                        may not always be good to keep everything. If you only use the
                                                        very-most recent data, you will overfit to that and things will
                                                        break; if you use too much experience, you may slow down your
                                                        learning. This may take some tuning to get right.</p>
                                                    <div class="admonition-you-should-know admonition">
                                                        <p class="first admonition-title">You Should Know</p>
                                                        <p class="last">We&#8217;ve mentioned that DDPG is an off-policy
                                                            algorithm: this is as good a point as any to highlight why
                                                            and how. Observe that the replay buffer <em>should</em>
                                                            contain old experiences, even though they might have been
                                                            obtained using an outdated policy. Why are we able to use
                                                            these at all? The reason is that the Bellman equation
                                                            <em>doesn&#8217;t care</em> which transition tuples are
                                                            used, or how the actions were selected, or what happens
                                                            after a given transition, because the optimal Q-function
                                                            should satisfy the Bellman equation for <em>all</em>
                                                            possible transitions. So any transitions that we&#8217;ve
                                                            ever experienced are fair game when trying to fit a
                                                            Q-function approximator via MSBE minimization.</p>
                                                    </div>
                                                    <p><strong>Trick Two: Target Networks.</strong> Q-learning
                                                        algorithms make use of <strong>target networks</strong>. The
                                                        term</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/fac308175faa67be9f5b27260abaf0ae6c4a58bb.svg"
                                                                alt="r + \gamma (1 - d) \max_{a'} Q_{\phi}(s',a')" />
                                                        </p>
                                                    </div>
                                                    <p>is called the <strong>target</strong>, because when we minimize
                                                        the MSBE loss, we are trying to make the Q-function be more like
                                                        this target. Problematically, the target depends on the same
                                                        parameters we are trying to train: <img class="math"
                                                            src="_images/math/3b22abcadf8773922f8db80011611bad8123a783.svg"
                                                            alt="\phi" />. This makes MSBE minimization unstable. The
                                                        solution is to use a set of parameters which comes close to <img
                                                            class="math"
                                                            src="_images/math/3b22abcadf8773922f8db80011611bad8123a783.svg"
                                                            alt="\phi" />, but with a time delay&#8212;that is to say, a
                                                        second network, called the target network, which lags the first.
                                                        The parameters of the target network are denoted <img
                                                            class="math"
                                                            src="_images/math/3d9fb7e74f48ade89cbbcc0f3d1f3cb89a824864.svg"
                                                            alt="\phi_{\text{targ}}" />.</p>
                                                    <p>In DQN-based algorithms, the target network is just copied over
                                                        from the main network every some-fixed-number of steps. In
                                                        DDPG-style algorithms, the target network is updated once per
                                                        main network update by polyak averaging:</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/d417987803ca9f61ac60741880a748129bd66dde.svg"
                                                                alt="\phi_{\text{targ}} \leftarrow \rho \phi_{\text{targ}} + (1 - \rho) \phi," />
                                                        </p>
                                                    </div>
                                                    <p>where <img class="math"
                                                            src="_images/math/b41ecbab285e58fd94a9b544487b74b1d992b0dd.svg"
                                                            alt="\rho" /> is a hyperparameter between 0 and 1 (usually
                                                        close to 1). (This hyperparameter is called <code
                                                            class="docutils literal"><span class="pre">polyak</span></code>
                                                        in our code).</p>
                                                    <p><strong>DDPG Detail: Calculating the Max Over Actions in the
                                                            Target.</strong> As mentioned earlier: computing the maximum
                                                        over actions in the target is a challenge in continuous action
                                                        spaces. DDPG deals with this by using a <strong>target policy
                                                            network</strong> to compute an action which approximately
                                                        maximizes <img class="math"
                                                            src="_images/math/a50d5d2b71fa30f115adf18b0bb1354f967b064a.svg"
                                                            alt="Q_{\phi_{\text{targ}}}" />. The target policy network
                                                        is found the same way as the target Q-function: by polyak
                                                        averaging the policy parameters over the course of training.</p>
                                                    <p>Putting it all together, Q-learning in DDPG is performed by
                                                        minimizing the following MSBE loss with stochastic gradient
                                                        descent:</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/4421120861d55302d76c7e2fd7cc5b2da7aea320.svg"
                                                                alt="L(\phi, {\mathcal D}) = \underset{(s,a,r,s',d) \sim {\mathcal D}}{{\mathrm E}}\left[
    \Bigg( Q_{\phi}(s,a) - \left(r + \gamma (1 - d) Q_{\phi_{\text{targ}}}(s', \mu_{\theta_{\text{targ}}}(s')) \right) \Bigg)^2
    \right]," /></p>
                                                    </div>
                                                    <p>where <img class="math"
                                                            src="_images/math/a325c9e05fa2ccce85eb2384ca00b4888d1c7824.svg"
                                                            alt="\mu_{\theta_{\text{targ}}}" /> is the target policy.
                                                    </p>
                                                </div>
                                                <div class="section" id="the-policy-learning-side-of-ddpg">
                                                    <h5><a class="toc-backref" href="#id6">The Policy Learning Side of
                                                            DDPG</a><a class="headerlink"
                                                            href="#the-policy-learning-side-of-ddpg"
                                                            title="Permalink to this headline">¶</a></h5>
                                                    <p>Policy learning in DDPG is fairly simple. We want to learn a
                                                        deterministic policy <img class="math"
                                                            src="_images/math/6923cb2043e84ea05d3eddbb7436c60659243cb9.svg"
                                                            alt="\mu_{\theta}(s)" /> which gives the action that
                                                        maximizes <img class="math"
                                                            src="_images/math/521198ffdba43bf32186f95801549cd1502b76c7.svg"
                                                            alt="Q_{\phi}(s,a)" />. Because the action space is
                                                        continuous, and we assume the Q-function is differentiable with
                                                        respect to action, we can just perform gradient ascent (with
                                                        respect to policy parameters only) to solve</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/cc4e3565d839e63e871a1cf7e3ce5e95bb616b29.svg"
                                                                alt="\max_{\theta} \underset{s \sim {\mathcal D}}{{\mathrm E}}\left[ Q_{\phi}(s, \mu_{\theta}(s)) \right]." />
                                                        </p>
                                                    </div>
                                                    <p>Note that the Q-function parameters are treated as constants
                                                        here.</p>
                                                </div>
                                            </div>
                                            <div class="section" id="exploration-vs-exploitation">
                                                <h4><a class="toc-backref" href="#id7">Exploration vs.
                                                        Exploitation</a><a class="headerlink"
                                                        href="#exploration-vs-exploitation"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>DDPG trains a deterministic policy in an off-policy way. Because the
                                                    policy is deterministic, if the agent were to explore on-policy, in
                                                    the beginning it would probably not try a wide enough variety of
                                                    actions to find useful learning signals. To make DDPG policies
                                                    explore better, we add noise to their actions at training time. The
                                                    authors of the original DDPG paper recommended time-correlated <a
                                                        class="reference external"
                                                        href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process">OU
                                                        noise</a>, but more recent results suggest that uncorrelated,
                                                    mean-zero Gaussian noise works perfectly well. Since the latter is
                                                    simpler, it is preferred. To facilitate getting higher-quality
                                                    training data, you may reduce the scale of the noise over the course
                                                    of training. (We do not do this in our implementation, and keep
                                                    noise scale fixed throughout.)</p>
                                                <p>At test time, to see how well the policy exploits what it has
                                                    learned, we do not add noise to the actions.</p>
                                                <div class="admonition-you-should-know admonition">
                                                    <p class="first admonition-title">You Should Know</p>
                                                    <p class="last">Our DDPG implementation uses a trick to improve
                                                        exploration at the start of training. For a fixed number of
                                                        steps at the beginning (set with the <code
                                                            class="docutils literal"><span class="pre">start_steps</span></code>
                                                        keyword argument), the agent takes actions which are sampled
                                                        from a uniform random distribution over valid actions. After
                                                        that, it returns to normal DDPG exploration.</p>
                                                </div>
                                            </div>
                                            <div class="section" id="pseudocode">
                                                <h4><a class="toc-backref" href="#id8">Pseudocode</a><a
                                                        class="headerlink" href="#pseudocode"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <div class="math">
                                                    <p><img src="_images/math/5811066e89799e65be299ec407846103fcf1f746.svg"
                                                            alt="\begin{algorithm}[H]
    \caption{Deep Deterministic Policy Gradient}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta$, Q-function parameters $\phi$, empty replay buffer $\mathcal{D}$
    \STATE Set target parameters equal to main parameters $\theta_{\text{targ}} \leftarrow \theta$, $\phi_{\text{targ}} \leftarrow \phi$
    \REPEAT
        \STATE Observe state $s$ and select action $a = \text{clip}(\mu_{\theta}(s) + \epsilon, a_{Low}, a_{High})$, where $\epsilon \sim \mathcal{N}$
        \STATE Execute $a$ in the environment
        \STATE Observe next state $s'$, reward $r$, and done signal $d$ to indicate whether $s'$ is terminal
        \STATE Store $(s,a,r,s',d)$ in replay buffer $\mathcal{D}$
        \STATE If $s'$ is terminal, reset environment state.
        \IF{it's time to update}
            \FOR{however many updates}
                \STATE Randomly sample a batch of transitions, $B = \{ (s,a,r,s',d) \}$ from $\mathcal{D}$
                \STATE Compute targets
                \begin{equation*}
                    y(r,s',d) = r + \gamma (1-d) Q_{\phi_{\text{targ}}}(s', \mu_{\theta_{\text{targ}}}(s'))
                \end{equation*}
                \STATE Update Q-function by one step of gradient descent using
                \begin{equation*}
                    \nabla_{\phi} \frac{1}{|B|}\sum_{(s,a,r,s',d) \in B} \left( Q_{\phi}(s,a) - y(r,s',d) \right)^2
                \end{equation*}
                \STATE Update policy by one step of gradient ascent using
                \begin{equation*}
                    \nabla_{\theta} \frac{1}{|B|}\sum_{s \in B}Q_{\phi}(s, \mu_{\theta}(s))
                \end{equation*}
                \STATE Update target networks with
                \begin{align*}
                    \phi_{\text{targ}} &amp;\leftarrow \rho \phi_{\text{targ}} + (1-\rho) \phi \\
                    \theta_{\text{targ}} &amp;\leftarrow \rho \theta_{\text{targ}} + (1-\rho) \theta
                \end{align*}
            \ENDFOR
        \ENDIF
    \UNTIL{convergence}
\end{algorithmic}
\end{algorithm}" /></p>
                                                </div>
                                            </div>
                                        </div>
                                        <div class="section" id="documentation">
                                            <h3><a class="toc-backref" href="#id9">Documentation</a><a
                                                    class="headerlink" href="#documentation"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p class="last">In what follows, we give documentation for the PyTorch
                                                    and Tensorflow implementations of DDPG in Spinning Up. They have
                                                    nearly identical function calls and docstrings, except for details
                                                    relating to model construction. However, we include both full
                                                    docstrings for completeness.</p>
                                            </div>
                                            <div class="section" id="documentation-pytorch-version">
                                                <h4><a class="toc-backref" href="#id10">Documentation: PyTorch
                                                        Version</a><a class="headerlink"
                                                        href="#documentation-pytorch-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <dl class="function">
                                                    <dt id="spinup.ddpg_pytorch">
                                                        <code class="descclassname">spinup.</code><code
                                                            class="descname">ddpg_pytorch</code><span
                                                            class="sig-paren">(</span><em>env_fn</em>,
                                                        <em>actor_critic=&lt;MagicMock spec='str'
                                                            id='140191853999552'&gt;</em>, <em>ac_kwargs={}</em>,
                                                        <em>seed=0</em>, <em>steps_per_epoch=4000</em>,
                                                        <em>epochs=100</em>, <em>replay_size=1000000</em>,
                                                        <em>gamma=0.99</em>, <em>polyak=0.995</em>,
                                                        <em>pi_lr=0.001</em>, <em>q_lr=0.001</em>,
                                                        <em>batch_size=100</em>, <em>start_steps=10000</em>,
                                                        <em>update_after=1000</em>, <em>update_every=50</em>,
                                                        <em>act_noise=0.1</em>, <em>num_test_episodes=10</em>,
                                                        <em>max_ep_len=1000</em>, <em>logger_kwargs={}</em>,
                                                        <em>save_freq=1</em><span class="sig-paren">)</span><a
                                                            class="headerlink" href="#spinup.ddpg_pytorch"
                                                            title="Permalink to this definition">¶</a>
                                                    </dt>
                                                    <dd>
                                                        <p>Deep Deterministic Policy Gradient (DDPG)</p>
                                                        <table class="docutils field-list" frame="void" rules="none">
                                                            <col class="field-name" />
                                                            <col class="field-body" />
                                                            <tbody valign="top">
                                                                <tr class="field-odd field">
                                                                    <th class="field-name">Parameters:</th>
                                                                    <td class="field-body">
                                                                        <ul class="first last simple">
                                                                            <li><strong>env_fn</strong> &#8211; A
                                                                                function which creates a copy of the
                                                                                environment.
                                                                                The environment must satisfy the OpenAI
                                                                                Gym API.</li>
                                                                            <li><strong>actor_critic</strong> &#8211;
                                                                                <p>The constructor method for a PyTorch
                                                                                    Module with an <code
                                                                                        class="docutils literal"><span class="pre">act</span></code>
                                                                                    method, a <code
                                                                                        class="docutils literal"><span class="pre">pi</span></code>
                                                                                    module, and a <code
                                                                                        class="docutils literal"><span class="pre">q</span></code>
                                                                                    module. The <code
                                                                                        class="docutils literal"><span class="pre">act</span></code>
                                                                                    method and
                                                                                    <code
                                                                                        class="docutils literal"><span class="pre">pi</span></code>
                                                                                    module should accept batches of
                                                                                    observations as inputs,
                                                                                    and <code
                                                                                        class="docutils literal"><span class="pre">q</span></code>
                                                                                    should accept a batch of
                                                                                    observations and a batch of
                                                                                    actions as inputs. When called,
                                                                                    these should return:
                                                                                </p>
                                                                                <table border="1" class="docutils">
                                                                                    <colgroup>
                                                                                        <col width="16%" />
                                                                                        <col width="24%" />
                                                                                        <col width="60%" />
                                                                                    </colgroup>
                                                                                    <thead valign="bottom">
                                                                                        <tr class="row-odd">
                                                                                            <th class="head">Call</th>
                                                                                            <th class="head">Output
                                                                                                Shape</th>
                                                                                            <th class="head">Description
                                                                                            </th>
                                                                                        </tr>
                                                                                    </thead>
                                                                                    <tbody valign="top">
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">act</span></code>
                                                                                            </td>
                                                                                            <td>(batch, act_dim)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Numpy array of
                                                                                                        actions for each
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        observation.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-odd">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">pi</span></code>
                                                                                            </td>
                                                                                            <td>(batch, act_dim)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Tensor
                                                                                                        containing
                                                                                                        actions from
                                                                                                        policy</div>
                                                                                                    <div class="line">
                                                                                                        given
                                                                                                        observations.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">q</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Tensor
                                                                                                        containing the
                                                                                                        current estimate
                                                                                                    </div>
                                                                                                    <div class="line">of
                                                                                                        Q* for the
                                                                                                        provided
                                                                                                        observations
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        and actions.
                                                                                                        (Critical: make
                                                                                                        sure to</div>
                                                                                                    <div class="line">
                                                                                                        flatten this!)
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                    </tbody>
                                                                                </table>
                                                                            </li>
                                                                            <li><strong>ac_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Any kwargs
                                                                                appropriate for the ActorCritic object
                                                                                you provided to DDPG.</li>
                                                                            <li><strong>seed</strong> (<em>int</em>)
                                                                                &#8211; Seed for random number
                                                                                generators.</li>
                                                                            <li><strong>steps_per_epoch</strong>
                                                                                (<em>int</em>) &#8211; Number of steps
                                                                                of interaction (state-action pairs)
                                                                                for the agent and the environment in
                                                                                each epoch.</li>
                                                                            <li><strong>epochs</strong> (<em>int</em>)
                                                                                &#8211; Number of epochs to run and
                                                                                train agent.</li>
                                                                            <li><strong>replay_size</strong>
                                                                                (<em>int</em>) &#8211; Maximum length of
                                                                                replay buffer.</li>
                                                                            <li><strong>gamma</strong> (<em>float</em>)
                                                                                &#8211; Discount factor. (Always between
                                                                                0 and 1.)</li>
                                                                            <li><strong>polyak</strong> (<em>float</em>)
                                                                                &#8211; <p>Interpolation factor in
                                                                                    polyak averaging for target
                                                                                    networks. Target networks are
                                                                                    updated towards main networks
                                                                                    according to:</p>
                                                                                <div class="math">
                                                                                    <p><img src="_images/math/ea6c5fdb8bb78fe30797537bbb28553b9a7706ef.svg"
                                                                                            alt="\theta_{\text{targ}} \leftarrow
\rho \theta_{\text{targ}} + (1-\rho) \theta" /></p>
                                                                                </div>
                                                                                <p>where <img class="math"
                                                                                        src="_images/math/b41ecbab285e58fd94a9b544487b74b1d992b0dd.svg"
                                                                                        alt="\rho" /> is polyak. (Always
                                                                                    between 0 and 1, usually
                                                                                    close to 1.)</p>
                                                                            </li>
                                                                            <li><strong>pi_lr</strong> (<em>float</em>)
                                                                                &#8211; Learning rate for policy.</li>
                                                                            <li><strong>q_lr</strong> (<em>float</em>)
                                                                                &#8211; Learning rate for Q-networks.
                                                                            </li>
                                                                            <li><strong>batch_size</strong>
                                                                                (<em>int</em>) &#8211; Minibatch size
                                                                                for SGD.</li>
                                                                            <li><strong>start_steps</strong>
                                                                                (<em>int</em>) &#8211; Number of steps
                                                                                for uniform-random action selection,
                                                                                before running real policy. Helps
                                                                                exploration.</li>
                                                                            <li><strong>update_after</strong>
                                                                                (<em>int</em>) &#8211; Number of env
                                                                                interactions to collect before
                                                                                starting to do gradient descent updates.
                                                                                Ensures replay buffer
                                                                                is full enough for useful updates.</li>
                                                                            <li><strong>update_every</strong>
                                                                                (<em>int</em>) &#8211; Number of env
                                                                                interactions that should elapse
                                                                                between gradient descent updates. Note:
                                                                                Regardless of how long
                                                                                you wait between updates, the ratio of
                                                                                env steps to gradient steps
                                                                                is locked to 1.</li>
                                                                            <li><strong>act_noise</strong>
                                                                                (<em>float</em>) &#8211; Stddev for
                                                                                Gaussian exploration noise added to
                                                                                policy at training time. (At test time,
                                                                                no noise is added.)</li>
                                                                            <li><strong>num_test_episodes</strong>
                                                                                (<em>int</em>) &#8211; Number of
                                                                                episodes to test the deterministic
                                                                                policy at the end of each epoch.</li>
                                                                            <li><strong>max_ep_len</strong>
                                                                                (<em>int</em>) &#8211; Maximum length of
                                                                                trajectory / episode / rollout.</li>
                                                                            <li><strong>logger_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Keyword args for
                                                                                EpochLogger.</li>
                                                                            <li><strong>save_freq</strong>
                                                                                (<em>int</em>) &#8211; How often (in
                                                                                terms of gap between epochs) to save
                                                                                the current policy and value function.
                                                                            </li>
                                                                        </ul>
                                                                    </td>
                                                                </tr>
                                                            </tbody>
                                                        </table>
                                                    </dd>
                                                </dl>

                                            </div>
                                            <div class="section" id="saved-model-contents-pytorch-version">
                                                <h4><a class="toc-backref" href="#id11">Saved Model Contents: PyTorch
                                                        Version</a><a class="headerlink"
                                                        href="#saved-model-contents-pytorch-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>The PyTorch saved model can be loaded with <code
                                                        class="docutils literal"><span class="pre">ac</span> <span class="pre">=</span> <span class="pre">torch.load('path/to/model.pt')</span></code>,
                                                    yielding an actor-critic object (<code
                                                        class="docutils literal"><span class="pre">ac</span></code>)
                                                    that has the properties described in the docstring for <code
                                                        class="docutils literal"><span class="pre">ddpg_pytorch</span></code>.
                                                </p>
                                                <p>You can get actions from this model with</p>
                                                <div class="highlight-python">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">actions</span> <span class="o">=</span> <span class="n">ac</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre>
                                                    </div>
                                                </div>
                                            </div>
                                            <div class="section" id="documentation-tensorflow-version">
                                                <h4><a class="toc-backref" href="#id12">Documentation: Tensorflow
                                                        Version</a><a class="headerlink"
                                                        href="#documentation-tensorflow-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <dl class="function">
                                                    <dt id="spinup.ddpg_tf1">
                                                        <code class="descclassname">spinup.</code><code
                                                            class="descname">ddpg_tf1</code><span
                                                            class="sig-paren">(</span><em>env_fn</em>,
                                                        <em>actor_critic=&lt;function mlp_actor_critic&gt;</em>,
                                                        <em>ac_kwargs={}</em>, <em>seed=0</em>,
                                                        <em>steps_per_epoch=4000</em>, <em>epochs=100</em>,
                                                        <em>replay_size=1000000</em>, <em>gamma=0.99</em>,
                                                        <em>polyak=0.995</em>, <em>pi_lr=0.001</em>,
                                                        <em>q_lr=0.001</em>, <em>batch_size=100</em>,
                                                        <em>start_steps=10000</em>, <em>update_after=1000</em>,
                                                        <em>update_every=50</em>, <em>act_noise=0.1</em>,
                                                        <em>num_test_episodes=10</em>, <em>max_ep_len=1000</em>,
                                                        <em>logger_kwargs={}</em>, <em>save_freq=1</em><span
                                                            class="sig-paren">)</span><a class="headerlink"
                                                            href="#spinup.ddpg_tf1"
                                                            title="Permalink to this definition">¶</a>
                                                    </dt>
                                                    <dd>
                                                        <p>Deep Deterministic Policy Gradient (DDPG)</p>
                                                        <table class="docutils field-list" frame="void" rules="none">
                                                            <col class="field-name" />
                                                            <col class="field-body" />
                                                            <tbody valign="top">
                                                                <tr class="field-odd field">
                                                                    <th class="field-name">Parameters:</th>
                                                                    <td class="field-body">
                                                                        <ul class="first last simple">
                                                                            <li><strong>env_fn</strong> &#8211; A
                                                                                function which creates a copy of the
                                                                                environment.
                                                                                The environment must satisfy the OpenAI
                                                                                Gym API.</li>
                                                                            <li><strong>actor_critic</strong> &#8211;
                                                                                <p>A function which takes in placeholder
                                                                                    symbols
                                                                                    for state, <code
                                                                                        class="docutils literal"><span class="pre">x_ph</span></code>,
                                                                                    and action, <code
                                                                                        class="docutils literal"><span class="pre">a_ph</span></code>,
                                                                                    and returns the main
                                                                                    outputs from the agent&#8217;s
                                                                                    Tensorflow computation graph:</p>
                                                                                <table border="1" class="docutils">
                                                                                    <colgroup>
                                                                                        <col width="17%" />
                                                                                        <col width="25%" />
                                                                                        <col width="58%" />
                                                                                    </colgroup>
                                                                                    <thead valign="bottom">
                                                                                        <tr class="row-odd">
                                                                                            <th class="head">Symbol</th>
                                                                                            <th class="head">Shape</th>
                                                                                            <th class="head">Description
                                                                                            </th>
                                                                                        </tr>
                                                                                    </thead>
                                                                                    <tbody valign="top">
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">pi</span></code>
                                                                                            </td>
                                                                                            <td>(batch, act_dim)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Deterministically
                                                                                                        computes actions
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        from policy
                                                                                                        given states.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-odd">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">q</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Gives the
                                                                                                        current estimate
                                                                                                        of Q* for</div>
                                                                                                    <div class="line">
                                                                                                        states in <code
                                                                                                            class="docutils literal"><span class="pre">x_ph</span></code>
                                                                                                        and actions in
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">a_ph</span></code>.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">q_pi</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Gives the
                                                                                                        composition of
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">q</span></code>
                                                                                                        and</div>
                                                                                                    <div class="line">
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">pi</span></code>
                                                                                                        for states in
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">x_ph</span></code>:
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        q(x, pi(x)).
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                    </tbody>
                                                                                </table>
                                                                            </li>
                                                                            <li><strong>ac_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Any kwargs
                                                                                appropriate for the actor_critic
                                                                                function you provided to DDPG.</li>
                                                                            <li><strong>seed</strong> (<em>int</em>)
                                                                                &#8211; Seed for random number
                                                                                generators.</li>
                                                                            <li><strong>steps_per_epoch</strong>
                                                                                (<em>int</em>) &#8211; Number of steps
                                                                                of interaction (state-action pairs)
                                                                                for the agent and the environment in
                                                                                each epoch.</li>
                                                                            <li><strong>epochs</strong> (<em>int</em>)
                                                                                &#8211; Number of epochs to run and
                                                                                train agent.</li>
                                                                            <li><strong>replay_size</strong>
                                                                                (<em>int</em>) &#8211; Maximum length of
                                                                                replay buffer.</li>
                                                                            <li><strong>gamma</strong> (<em>float</em>)
                                                                                &#8211; Discount factor. (Always between
                                                                                0 and 1.)</li>
                                                                            <li><strong>polyak</strong> (<em>float</em>)
                                                                                &#8211; <p>Interpolation factor in
                                                                                    polyak averaging for target
                                                                                    networks. Target networks are
                                                                                    updated towards main networks
                                                                                    according to:</p>
                                                                                <div class="math">
                                                                                    <p><img src="_images/math/ea6c5fdb8bb78fe30797537bbb28553b9a7706ef.svg"
                                                                                            alt="\theta_{\text{targ}} \leftarrow
\rho \theta_{\text{targ}} + (1-\rho) \theta" /></p>
                                                                                </div>
                                                                                <p>where <img class="math"
                                                                                        src="_images/math/b41ecbab285e58fd94a9b544487b74b1d992b0dd.svg"
                                                                                        alt="\rho" /> is polyak. (Always
                                                                                    between 0 and 1, usually
                                                                                    close to 1.)</p>
                                                                            </li>
                                                                            <li><strong>pi_lr</strong> (<em>float</em>)
                                                                                &#8211; Learning rate for policy.</li>
                                                                            <li><strong>q_lr</strong> (<em>float</em>)
                                                                                &#8211; Learning rate for Q-networks.
                                                                            </li>
                                                                            <li><strong>batch_size</strong>
                                                                                (<em>int</em>) &#8211; Minibatch size
                                                                                for SGD.</li>
                                                                            <li><strong>start_steps</strong>
                                                                                (<em>int</em>) &#8211; Number of steps
                                                                                for uniform-random action selection,
                                                                                before running real policy. Helps
                                                                                exploration.</li>
                                                                            <li><strong>update_after</strong>
                                                                                (<em>int</em>) &#8211; Number of env
                                                                                interactions to collect before
                                                                                starting to do gradient descent updates.
                                                                                Ensures replay buffer
                                                                                is full enough for useful updates.</li>
                                                                            <li><strong>update_every</strong>
                                                                                (<em>int</em>) &#8211; Number of env
                                                                                interactions that should elapse
                                                                                between gradient descent updates. Note:
                                                                                Regardless of how long
                                                                                you wait between updates, the ratio of
                                                                                env steps to gradient steps
                                                                                is locked to 1.</li>
                                                                            <li><strong>act_noise</strong>
                                                                                (<em>float</em>) &#8211; Stddev for
                                                                                Gaussian exploration noise added to
                                                                                policy at training time. (At test time,
                                                                                no noise is added.)</li>
                                                                            <li><strong>num_test_episodes</strong>
                                                                                (<em>int</em>) &#8211; Number of
                                                                                episodes to test the deterministic
                                                                                policy at the end of each epoch.</li>
                                                                            <li><strong>max_ep_len</strong>
                                                                                (<em>int</em>) &#8211; Maximum length of
                                                                                trajectory / episode / rollout.</li>
                                                                            <li><strong>logger_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Keyword args for
                                                                                EpochLogger.</li>
                                                                            <li><strong>save_freq</strong>
                                                                                (<em>int</em>) &#8211; How often (in
                                                                                terms of gap between epochs) to save
                                                                                the current policy and value function.
                                                                            </li>
                                                                        </ul>
                                                                    </td>
                                                                </tr>
                                                            </tbody>
                                                        </table>
                                                    </dd>
                                                </dl>

                                            </div>
                                            <div class="section" id="saved-model-contents-tensorflow-version">
                                                <h4><a class="toc-backref" href="#id13">Saved Model Contents: Tensorflow
                                                        Version</a><a class="headerlink"
                                                        href="#saved-model-contents-tensorflow-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>The computation graph saved by the logger includes:</p>
                                                <table border="1" class="docutils">
                                                    <colgroup>
                                                        <col width="10%" />
                                                        <col width="90%" />
                                                    </colgroup>
                                                    <thead valign="bottom">
                                                        <tr class="row-odd">
                                                            <th class="head">Key</th>
                                                            <th class="head">Value</th>
                                                        </tr>
                                                    </thead>
                                                    <tbody valign="top">
                                                        <tr class="row-even">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">x</span></code>
                                                            </td>
                                                            <td>Tensorflow placeholder for state input.</td>
                                                        </tr>
                                                        <tr class="row-odd">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">a</span></code>
                                                            </td>
                                                            <td>Tensorflow placeholder for action input.</td>
                                                        </tr>
                                                        <tr class="row-even">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">pi</span></code>
                                                            </td>
                                                            <td>
                                                                <div class="first last line-block">
                                                                    <div class="line">Deterministically computes an
                                                                        action from the agent, conditioned</div>
                                                                    <div class="line">on states in <code
                                                                            class="docutils literal"><span class="pre">x</span></code>.
                                                                    </div>
                                                                </div>
                                                            </td>
                                                        </tr>
                                                        <tr class="row-odd">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">q</span></code>
                                                            </td>
                                                            <td>Gives action-value estimate for states in <code
                                                                    class="docutils literal"><span class="pre">x</span></code>
                                                                and actions in <code
                                                                    class="docutils literal"><span class="pre">a</span></code>.
                                                            </td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <p>This saved model can be accessed either by</p>
                                                <ul class="simple">
                                                    <li>running the trained policy with the <a
                                                            class="reference external"
                                                            href="../user/saving_and_loading.html#loading-and-running-trained-policies">test_policy.py</a>
                                                        tool,</li>
                                                    <li>or loading the whole saved graph into a program with <a
                                                            class="reference external"
                                                            href="../utils/logger.html#spinup.utils.logx.restore_tf_graph">restore_tf_graph</a>.
                                                    </li>
                                                </ul>
                                            </div>
                                        </div>
                                        <div class="section" id="references">
                                            <h3><a class="toc-backref" href="#id14">References</a><a class="headerlink"
                                                    href="#references" title="Permalink to this headline">¶</a></h3>
                                            <div class="section" id="relevant-papers">
                                                <h4><a class="toc-backref" href="#id15">Relevant Papers</a><a
                                                        class="headerlink" href="#relevant-papers"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <ul class="simple">
                                                    <li><a class="reference external"
                                                            href="http://proceedings.mlr.press/v32/silver14.pdf">Deterministic
                                                            Policy Gradient Algorithms</a>, Silver et al. 2014</li>
                                                    <li><a class="reference external"
                                                            href="https://arxiv.org/pdf/1509.02971.pdf">Continuous
                                                            Control With Deep Reinforcement Learning</a>, Lillicrap et
                                                        al. 2016</li>
                                                </ul>
                                            </div>
                                            <div class="section" id="why-these-papers">
                                                <h4><a class="toc-backref" href="#id16">Why These Papers?</a><a
                                                        class="headerlink" href="#why-these-papers"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>Silver 2014 is included because it establishes the theory underlying
                                                    deterministic policy gradients (DPG). Lillicrap 2016 is included
                                                    because it adapts the theoretically-grounded DPG algorithm to the
                                                    deep RL setting, giving DDPG.</p>
                                            </div>
                                            <div class="section" id="other-public-implementations">
                                                <h4><a class="toc-backref" href="#id17">Other Public
                                                        Implementations</a><a class="headerlink"
                                                        href="#other-public-implementations"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <ul class="simple">
                                                    <li><a class="reference external"
                                                            href="https://github.com/openai/baselines/tree/master/baselines/ddpg">Baselines</a>
                                                    </li>
                                                    <li><a class="reference external"
                                                            href="https://github.com/rll/rllab/blob/master/rllab/algos/ddpg.py">rllab</a>
                                                    </li>
                                                    <li><a class="reference external"
                                                            href="https://github.com/ray-project/ray/tree/master/python/ray/rllib/agents/ddpg">rllib
                                                            (Ray)</a></li>
                                                    <li><a class="reference external"
                                                            href="https://github.com/sfujim/TD3">TD3 release repo</a>
                                                    </li>
                                                </ul>
                                            </div>
                                        </div>
                                    </div>
                                    <span id="document-algorithms/td3"></span>
                                    <div class="section" id="twin-delayed-ddpg">
                                        <h2><a class="toc-backref" href="#id1">Twin Delayed DDPG</a><a
                                                class="headerlink" href="#twin-delayed-ddpg"
                                                title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal" href="#twin-delayed-ddpg"
                                                        id="id1">Twin Delayed DDPG</a>
                                                    <ul>
                                                        <li><a class="reference internal" href="#background"
                                                                id="id2">Background</a>
                                                            <ul>
                                                                <li><a class="reference internal" href="#quick-facts"
                                                                        id="id3">Quick Facts</a></li>
                                                                <li><a class="reference internal" href="#key-equations"
                                                                        id="id4">Key Equations</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#exploration-vs-exploitation"
                                                                        id="id5">Exploration vs. Exploitation</a></li>
                                                                <li><a class="reference internal" href="#pseudocode"
                                                                        id="id6">Pseudocode</a></li>
                                                            </ul>
                                                        </li>
                                                        <li><a class="reference internal" href="#documentation"
                                                                id="id7">Documentation</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#documentation-pytorch-version"
                                                                        id="id8">Documentation: PyTorch Version</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#saved-model-contents-pytorch-version"
                                                                        id="id9">Saved Model Contents: PyTorch
                                                                        Version</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#documentation-tensorflow-version"
                                                                        id="id10">Documentation: Tensorflow Version</a>
                                                                </li>
                                                                <li><a class="reference internal"
                                                                        href="#saved-model-contents-tensorflow-version"
                                                                        id="id11">Saved Model Contents: Tensorflow
                                                                        Version</a></li>
                                                            </ul>
                                                        </li>
                                                        <li><a class="reference internal" href="#references"
                                                                id="id12">References</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#relevant-papers" id="id13">Relevant
                                                                        Papers</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#other-public-implementations"
                                                                        id="id14">Other Public Implementations</a></li>
                                                            </ul>
                                                        </li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <div class="section" id="background">
                                            <h3><a class="toc-backref" href="#id2">Background</a><a class="headerlink"
                                                    href="#background" title="Permalink to this headline">¶</a></h3>
                                            <p>(Previously: <a class="reference external"
                                                    href="../algorithms/ddpg.html#background">Background for DDPG</a>)
                                            </p>
                                            <p>While DDPG can achieve great performance sometimes, it is frequently
                                                brittle with respect to hyperparameters and other kinds of tuning. A
                                                common failure mode for DDPG is that the learned Q-function begins to
                                                dramatically overestimate Q-values, which then leads to the policy
                                                breaking, because it exploits the errors in the Q-function. Twin Delayed
                                                DDPG (TD3) is an algorithm that addresses this issue by introducing
                                                three critical tricks:</p>
                                            <p><strong>Trick One: Clipped Double-Q Learning.</strong> TD3 learns
                                                <em>two</em> Q-functions instead of one (hence &#8220;twin&#8221;), and
                                                uses the smaller of the two Q-values to form the targets in the Bellman
                                                error loss functions.</p>
                                            <p><strong>Trick Two: &#8220;Delayed&#8221; Policy Updates.</strong> TD3
                                                updates the policy (and target networks) less frequently than the
                                                Q-function. The paper recommends one policy update for every two
                                                Q-function updates.</p>
                                            <p><strong>Trick Three: Target Policy Smoothing.</strong> TD3 adds noise to
                                                the target action, to make it harder for the policy to exploit
                                                Q-function errors by smoothing out Q along changes in action.</p>
                                            <p>Together, these three tricks result in substantially improved performance
                                                over baseline DDPG.</p>
                                            <div class="section" id="quick-facts">
                                                <h4><a class="toc-backref" href="#id3">Quick Facts</a><a
                                                        class="headerlink" href="#quick-facts"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <ul class="simple">
                                                    <li>TD3 is an off-policy algorithm.</li>
                                                    <li>TD3 can only be used for environments with continuous action
                                                        spaces.</li>
                                                    <li>The Spinning Up implementation of TD3 does not support
                                                        parallelization.</li>
                                                </ul>
                                            </div>
                                            <div class="section" id="key-equations">
                                                <h4><a class="toc-backref" href="#id4">Key Equations</a><a
                                                        class="headerlink" href="#key-equations"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>TD3 concurrently learns two Q-functions, <img class="math"
                                                        src="_images/math/8795d42bd263dcbe55d123e7466b2dd5091490a7.svg"
                                                        alt="Q_{\phi_1}" /> and <img class="math"
                                                        src="_images/math/ac4e235414bfd47d449e3440ba29ae8470d3952e.svg"
                                                        alt="Q_{\phi_2}" />, by mean square Bellman error minimization,
                                                    in almost the same way that DDPG learns its single Q-function. To
                                                    show exactly how TD3 does this and how it differs from normal DDPG,
                                                    we&#8217;ll work from the innermost part of the loss function
                                                    outwards.</p>
                                                <p>First: <strong>target policy smoothing</strong>. Actions used to form
                                                    the Q-learning target are based on the target policy, <img
                                                        class="math"
                                                        src="_images/math/a325c9e05fa2ccce85eb2384ca00b4888d1c7824.svg"
                                                        alt="\mu_{\theta_{\text{targ}}}" />, but with clipped noise
                                                    added on each dimension of the action. After adding the clipped
                                                    noise, the target action is then clipped to lie in the valid action
                                                    range (all valid actions, <img class="math"
                                                        src="_images/math/76a319586cd215c8f2075b938fc6f6e07c81714b.svg"
                                                        alt="a" />, satisfy <img class="math"
                                                        src="_images/math/a5132668c0af8733656505c5fb6c1dff4a7907a1.svg"
                                                        alt="a_{Low} \leq a \leq a_{High}" />). The target actions are
                                                    thus:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/8efd61c40551db4eddb3f780d2804cac34c8ae52.svg"
                                                            alt="a'(s') = \text{clip}\left(\mu_{\theta_{\text{targ}}}(s') + \text{clip}(\epsilon,-c,c), a_{Low}, a_{High}\right), \;\;\;\;\; \epsilon \sim \mathcal{N}(0, \sigma)" />
                                                    </p>
                                                </div>
                                                <p>Target policy smoothing essentially serves as a regularizer for the
                                                    algorithm. It addresses a particular failure mode that can happen in
                                                    DDPG: if the Q-function approximator develops an incorrect sharp
                                                    peak for some actions, the policy will quickly exploit that peak and
                                                    then have brittle or incorrect behavior. This can be averted by
                                                    smoothing out the Q-function over similar actions, which target
                                                    policy smoothing is designed to do.</p>
                                                <p>Next: <strong>clipped double-Q learning</strong>. Both Q-functions
                                                    use a single target, calculated using whichever of the two
                                                    Q-functions gives a smaller target value:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/70901eaea34c31e03bb878d7a710a33cb75d1143.svg"
                                                            alt="y(r,s',d) = r + \gamma (1 - d) \min_{i=1,2} Q_{\phi_{i, \text{targ}}}(s', a'(s'))," />
                                                    </p>
                                                </div>
                                                <p>and then both are learned by regressing to this target:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/7d5c18f49a242cc3eec554f717fe4f3bfc119bab.svg"
                                                            alt="L(\phi_1, {\mathcal D}) = \underE{(s,a,r,s',d) \sim {\mathcal D}}{
    \Bigg( Q_{\phi_1}(s,a) - y(r,s',d) \Bigg)^2
    }," /></p>
                                                </div>
                                                <div class="math">
                                                    <p><img src="_images/math/cd73726a8a3845ade467aed57714912f868f6b36.svg"
                                                            alt="L(\phi_2, {\mathcal D}) = \underE{(s,a,r,s',d) \sim {\mathcal D}}{
    \Bigg( Q_{\phi_2}(s,a) - y(r,s',d) \Bigg)^2
    }." /></p>
                                                </div>
                                                <p>Using the smaller Q-value for the target, and regressing towards
                                                    that, helps fend off overestimation in the Q-function.</p>
                                                <p>Lastly: the policy is learned just by maximizing <img class="math"
                                                        src="_images/math/8795d42bd263dcbe55d123e7466b2dd5091490a7.svg"
                                                        alt="Q_{\phi_1}" />:</p>
                                                <div class="math">
                                                    <p><img src="_images/math/9ed1a541005a48d51b624c3b329897064ec2c065.svg"
                                                            alt="\max_{\theta} \underset{s \sim {\mathcal D}}{{\mathrm E}}\left[ Q_{\phi_1}(s, \mu_{\theta}(s)) \right]," />
                                                    </p>
                                                </div>
                                                <p>which is pretty much unchanged from DDPG. However, in TD3, the policy
                                                    is updated less frequently than the Q-functions are. This helps damp
                                                    the volatility that normally arises in DDPG because of how a policy
                                                    update changes the target.</p>
                                            </div>
                                            <div class="section" id="exploration-vs-exploitation">
                                                <h4><a class="toc-backref" href="#id5">Exploration vs.
                                                        Exploitation</a><a class="headerlink"
                                                        href="#exploration-vs-exploitation"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>TD3 trains a deterministic policy in an off-policy way. Because the
                                                    policy is deterministic, if the agent were to explore on-policy, in
                                                    the beginning it would probably not try a wide enough variety of
                                                    actions to find useful learning signals. To make TD3 policies
                                                    explore better, we add noise to their actions at training time,
                                                    typically uncorrelated mean-zero Gaussian noise. To facilitate
                                                    getting higher-quality training data, you may reduce the scale of
                                                    the noise over the course of training. (We do not do this in our
                                                    implementation, and keep noise scale fixed throughout.)</p>
                                                <p>At test time, to see how well the policy exploits what it has
                                                    learned, we do not add noise to the actions.</p>
                                                <div class="admonition-you-should-know admonition">
                                                    <p class="first admonition-title">You Should Know</p>
                                                    <p class="last">Our TD3 implementation uses a trick to improve
                                                        exploration at the start of training. For a fixed number of
                                                        steps at the beginning (set with the <code
                                                            class="docutils literal"><span class="pre">start_steps</span></code>
                                                        keyword argument), the agent takes actions which are sampled
                                                        from a uniform random distribution over valid actions. After
                                                        that, it returns to normal TD3 exploration.</p>
                                                </div>
                                            </div>
                                            <div class="section" id="pseudocode">
                                                <h4><a class="toc-backref" href="#id6">Pseudocode</a><a
                                                        class="headerlink" href="#pseudocode"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <div class="math">
                                                    <p><img src="_images/math/b7dfe8fa3a703b9657dcecb624c4457926e0ce8a.svg"
                                                            alt="\begin{algorithm}[H]
    \caption{Twin Delayed DDPG}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta$, Q-function parameters $\phi_1$, $\phi_2$, empty replay buffer $\mathcal{D}$
    \STATE Set target parameters equal to main parameters $\theta_{\text{targ}} \leftarrow \theta$, $\phi_{\text{targ},1} \leftarrow \phi_1$, $\phi_{\text{targ},2} \leftarrow \phi_2$
    \REPEAT
        \STATE Observe state $s$ and select action $a = \text{clip}(\mu_{\theta}(s) + \epsilon, a_{Low}, a_{High})$, where $\epsilon \sim \mathcal{N}$
        \STATE Execute $a$ in the environment
        \STATE Observe next state $s'$, reward $r$, and done signal $d$ to indicate whether $s'$ is terminal
        \STATE Store $(s,a,r,s',d)$ in replay buffer $\mathcal{D}$
        \STATE If $s'$ is terminal, reset environment state.
        \IF{it's time to update}
            \FOR{$j$ in range(however many updates)}
                \STATE Randomly sample a batch of transitions, $B = \{ (s,a,r,s',d) \}$ from $\mathcal{D}$
                \STATE Compute target actions
                \begin{equation*}
                    a'(s') = \text{clip}\left(\mu_{\theta_{\text{targ}}}(s') + \text{clip}(\epsilon,-c,c), a_{Low}, a_{High}\right), \;\;\;\;\; \epsilon \sim \mathcal{N}(0, \sigma)
                \end{equation*}
                \STATE Compute targets
                \begin{equation*}
                    y(r,s',d) = r + \gamma (1-d) \min_{i=1,2} Q_{\phi_{\text{targ},i}}(s', a'(s'))
                \end{equation*}
                \STATE Update Q-functions by one step of gradient descent using
                \begin{align*}
                    &amp; \nabla_{\phi_i} \frac{1}{|B|}\sum_{(s,a,r,s',d) \in B} \left( Q_{\phi_i}(s,a) - y(r,s',d) \right)^2 &amp;&amp; \text{for } i=1,2
                \end{align*}
                \IF{ $j \mod$ \texttt{policy\_delay} $ = 0$}
                    \STATE Update policy by one step of gradient ascent using
                    \begin{equation*}
                        \nabla_{\theta} \frac{1}{|B|}\sum_{s \in B}Q_{\phi_1}(s, \mu_{\theta}(s))
                    \end{equation*}
                    \STATE Update target networks with
                    \begin{align*}
                        \phi_{\text{targ},i} &amp;\leftarrow \rho \phi_{\text{targ}, i} + (1-\rho) \phi_i &amp;&amp; \text{for } i=1,2\\
                        \theta_{\text{targ}} &amp;\leftarrow \rho \theta_{\text{targ}} + (1-\rho) \theta
                    \end{align*}
                \ENDIF
            \ENDFOR
        \ENDIF
    \UNTIL{convergence}
\end{algorithmic}
\end{algorithm}" /></p>
                                                </div>
                                            </div>
                                        </div>
                                        <div class="section" id="documentation">
                                            <h3><a class="toc-backref" href="#id7">Documentation</a><a
                                                    class="headerlink" href="#documentation"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p class="last">In what follows, we give documentation for the PyTorch
                                                    and Tensorflow implementations of TD3 in Spinning Up. They have
                                                    nearly identical function calls and docstrings, except for details
                                                    relating to model construction. However, we include both full
                                                    docstrings for completeness.</p>
                                            </div>
                                            <div class="section" id="documentation-pytorch-version">
                                                <h4><a class="toc-backref" href="#id8">Documentation: PyTorch
                                                        Version</a><a class="headerlink"
                                                        href="#documentation-pytorch-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <dl class="function">
                                                    <dt id="spinup.td3_pytorch">
                                                        <code class="descclassname">spinup.</code><code
                                                            class="descname">td3_pytorch</code><span
                                                            class="sig-paren">(</span><em>env_fn</em>,
                                                        <em>actor_critic=&lt;MagicMock spec='str'
                                                            id='140191853475208'&gt;</em>, <em>ac_kwargs={}</em>,
                                                        <em>seed=0</em>, <em>steps_per_epoch=4000</em>,
                                                        <em>epochs=100</em>, <em>replay_size=1000000</em>,
                                                        <em>gamma=0.99</em>, <em>polyak=0.995</em>,
                                                        <em>pi_lr=0.001</em>, <em>q_lr=0.001</em>,
                                                        <em>batch_size=100</em>, <em>start_steps=10000</em>,
                                                        <em>update_after=1000</em>, <em>update_every=50</em>,
                                                        <em>act_noise=0.1</em>, <em>target_noise=0.2</em>,
                                                        <em>noise_clip=0.5</em>, <em>policy_delay=2</em>,
                                                        <em>num_test_episodes=10</em>, <em>max_ep_len=1000</em>,
                                                        <em>logger_kwargs={}</em>, <em>save_freq=1</em><span
                                                            class="sig-paren">)</span><a class="headerlink"
                                                            href="#spinup.td3_pytorch"
                                                            title="Permalink to this definition">¶</a>
                                                    </dt>
                                                    <dd>
                                                        <p>Twin Delayed Deep Deterministic Policy Gradient (TD3)</p>
                                                        <table class="docutils field-list" frame="void" rules="none">
                                                            <col class="field-name" />
                                                            <col class="field-body" />
                                                            <tbody valign="top">
                                                                <tr class="field-odd field">
                                                                    <th class="field-name">Parameters:</th>
                                                                    <td class="field-body">
                                                                        <ul class="first last simple">
                                                                            <li><strong>env_fn</strong> &#8211; A
                                                                                function which creates a copy of the
                                                                                environment.
                                                                                The environment must satisfy the OpenAI
                                                                                Gym API.</li>
                                                                            <li><strong>actor_critic</strong> &#8211;
                                                                                <p>The constructor method for a PyTorch
                                                                                    Module with an <code
                                                                                        class="docutils literal"><span class="pre">act</span></code>
                                                                                    method, a <code
                                                                                        class="docutils literal"><span class="pre">pi</span></code>
                                                                                    module, a <code
                                                                                        class="docutils literal"><span class="pre">q1</span></code>
                                                                                    module, and a <code
                                                                                        class="docutils literal"><span class="pre">q2</span></code>
                                                                                    module.
                                                                                    The <code
                                                                                        class="docutils literal"><span class="pre">act</span></code>
                                                                                    method and <code
                                                                                        class="docutils literal"><span class="pre">pi</span></code>
                                                                                    module should accept batches of
                                                                                    observations as inputs, and <code
                                                                                        class="docutils literal"><span class="pre">q1</span></code>
                                                                                    and <code
                                                                                        class="docutils literal"><span class="pre">q2</span></code>
                                                                                    should accept a batch
                                                                                    of observations and a batch of
                                                                                    actions as inputs. When called,
                                                                                    these should return:</p>
                                                                                <table border="1" class="docutils">
                                                                                    <colgroup>
                                                                                        <col width="15%" />
                                                                                        <col width="22%" />
                                                                                        <col width="63%" />
                                                                                    </colgroup>
                                                                                    <thead valign="bottom">
                                                                                        <tr class="row-odd">
                                                                                            <th class="head">Call</th>
                                                                                            <th class="head">Output
                                                                                                Shape</th>
                                                                                            <th class="head">Description
                                                                                            </th>
                                                                                        </tr>
                                                                                    </thead>
                                                                                    <tbody valign="top">
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">act</span></code>
                                                                                            </td>
                                                                                            <td>(batch, act_dim)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Numpy array of
                                                                                                        actions for each
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        observation.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-odd">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">pi</span></code>
                                                                                            </td>
                                                                                            <td>(batch, act_dim)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Tensor
                                                                                                        containing
                                                                                                        actions from
                                                                                                        policy</div>
                                                                                                    <div class="line">
                                                                                                        given
                                                                                                        observations.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">q1</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Tensor
                                                                                                        containing one
                                                                                                        current estimate
                                                                                                    </div>
                                                                                                    <div class="line">of
                                                                                                        Q* for the
                                                                                                        provided
                                                                                                        observations
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        and actions.
                                                                                                        (Critical: make
                                                                                                        sure to</div>
                                                                                                    <div class="line">
                                                                                                        flatten this!)
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-odd">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">q2</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Tensor
                                                                                                        containing the
                                                                                                        other current
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        estimate of Q*
                                                                                                        for the provided
                                                                                                        observations
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        and actions.
                                                                                                        (Critical: make
                                                                                                        sure to</div>
                                                                                                    <div class="line">
                                                                                                        flatten this!)
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                    </tbody>
                                                                                </table>
                                                                            </li>
                                                                            <li><strong>ac_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Any kwargs
                                                                                appropriate for the ActorCritic object
                                                                                you provided to TD3.</li>
                                                                            <li><strong>seed</strong> (<em>int</em>)
                                                                                &#8211; Seed for random number
                                                                                generators.</li>
                                                                            <li><strong>steps_per_epoch</strong>
                                                                                (<em>int</em>) &#8211; Number of steps
                                                                                of interaction (state-action pairs)
                                                                                for the agent and the environment in
                                                                                each epoch.</li>
                                                                            <li><strong>epochs</strong> (<em>int</em>)
                                                                                &#8211; Number of epochs to run and
                                                                                train agent.</li>
                                                                            <li><strong>replay_size</strong>
                                                                                (<em>int</em>) &#8211; Maximum length of
                                                                                replay buffer.</li>
                                                                            <li><strong>gamma</strong> (<em>float</em>)
                                                                                &#8211; Discount factor. (Always between
                                                                                0 and 1.)</li>
                                                                            <li><strong>polyak</strong> (<em>float</em>)
                                                                                &#8211; <p>Interpolation factor in
                                                                                    polyak averaging for target
                                                                                    networks. Target networks are
                                                                                    updated towards main networks
                                                                                    according to:</p>
                                                                                <div class="math">
                                                                                    <p><img src="_images/math/ea6c5fdb8bb78fe30797537bbb28553b9a7706ef.svg"
                                                                                            alt="\theta_{\text{targ}} \leftarrow
\rho \theta_{\text{targ}} + (1-\rho) \theta" /></p>
                                                                                </div>
                                                                                <p>where <img class="math"
                                                                                        src="_images/math/b41ecbab285e58fd94a9b544487b74b1d992b0dd.svg"
                                                                                        alt="\rho" /> is polyak. (Always
                                                                                    between 0 and 1, usually
                                                                                    close to 1.)</p>
                                                                            </li>
                                                                            <li><strong>pi_lr</strong> (<em>float</em>)
                                                                                &#8211; Learning rate for policy.</li>
                                                                            <li><strong>q_lr</strong> (<em>float</em>)
                                                                                &#8211; Learning rate for Q-networks.
                                                                            </li>
                                                                            <li><strong>batch_size</strong>
                                                                                (<em>int</em>) &#8211; Minibatch size
                                                                                for SGD.</li>
                                                                            <li><strong>start_steps</strong>
                                                                                (<em>int</em>) &#8211; Number of steps
                                                                                for uniform-random action selection,
                                                                                before running real policy. Helps
                                                                                exploration.</li>
                                                                            <li><strong>update_after</strong>
                                                                                (<em>int</em>) &#8211; Number of env
                                                                                interactions to collect before
                                                                                starting to do gradient descent updates.
                                                                                Ensures replay buffer
                                                                                is full enough for useful updates.</li>
                                                                            <li><strong>update_every</strong>
                                                                                (<em>int</em>) &#8211; Number of env
                                                                                interactions that should elapse
                                                                                between gradient descent updates. Note:
                                                                                Regardless of how long
                                                                                you wait between updates, the ratio of
                                                                                env steps to gradient steps
                                                                                is locked to 1.</li>
                                                                            <li><strong>act_noise</strong>
                                                                                (<em>float</em>) &#8211; Stddev for
                                                                                Gaussian exploration noise added to
                                                                                policy at training time. (At test time,
                                                                                no noise is added.)</li>
                                                                            <li><strong>target_noise</strong>
                                                                                (<em>float</em>) &#8211; Stddev for
                                                                                smoothing noise added to target
                                                                                policy.</li>
                                                                            <li><strong>noise_clip</strong>
                                                                                (<em>float</em>) &#8211; Limit for
                                                                                absolute value of target policy
                                                                                smoothing noise.</li>
                                                                            <li><strong>policy_delay</strong>
                                                                                (<em>int</em>) &#8211; Policy will only
                                                                                be updated once every
                                                                                policy_delay times for each update of
                                                                                the Q-networks.</li>
                                                                            <li><strong>num_test_episodes</strong>
                                                                                (<em>int</em>) &#8211; Number of
                                                                                episodes to test the deterministic
                                                                                policy at the end of each epoch.</li>
                                                                            <li><strong>max_ep_len</strong>
                                                                                (<em>int</em>) &#8211; Maximum length of
                                                                                trajectory / episode / rollout.</li>
                                                                            <li><strong>logger_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Keyword args for
                                                                                EpochLogger.</li>
                                                                            <li><strong>save_freq</strong>
                                                                                (<em>int</em>) &#8211; How often (in
                                                                                terms of gap between epochs) to save
                                                                                the current policy and value function.
                                                                            </li>
                                                                        </ul>
                                                                    </td>
                                                                </tr>
                                                            </tbody>
                                                        </table>
                                                    </dd>
                                                </dl>

                                            </div>
                                            <div class="section" id="saved-model-contents-pytorch-version">
                                                <h4><a class="toc-backref" href="#id9">Saved Model Contents: PyTorch
                                                        Version</a><a class="headerlink"
                                                        href="#saved-model-contents-pytorch-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>The PyTorch saved model can be loaded with <code
                                                        class="docutils literal"><span class="pre">ac</span> <span class="pre">=</span> <span class="pre">torch.load('path/to/model.pt')</span></code>,
                                                    yielding an actor-critic object (<code
                                                        class="docutils literal"><span class="pre">ac</span></code>)
                                                    that has the properties described in the docstring for <code
                                                        class="docutils literal"><span class="pre">td3_pytorch</span></code>.
                                                </p>
                                                <p>You can get actions from this model with</p>
                                                <div class="highlight-python">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">actions</span> <span class="o">=</span> <span class="n">ac</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre>
                                                    </div>
                                                </div>
                                            </div>
                                            <div class="section" id="documentation-tensorflow-version">
                                                <h4><a class="toc-backref" href="#id10">Documentation: Tensorflow
                                                        Version</a><a class="headerlink"
                                                        href="#documentation-tensorflow-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <dl class="function">
                                                    <dt id="spinup.td3_tf1">
                                                        <code class="descclassname">spinup.</code><code
                                                            class="descname">td3_tf1</code><span
                                                            class="sig-paren">(</span><em>env_fn</em>,
                                                        <em>actor_critic=&lt;function mlp_actor_critic&gt;</em>,
                                                        <em>ac_kwargs={}</em>, <em>seed=0</em>,
                                                        <em>steps_per_epoch=4000</em>, <em>epochs=100</em>,
                                                        <em>replay_size=1000000</em>, <em>gamma=0.99</em>,
                                                        <em>polyak=0.995</em>, <em>pi_lr=0.001</em>,
                                                        <em>q_lr=0.001</em>, <em>batch_size=100</em>,
                                                        <em>start_steps=10000</em>, <em>update_after=1000</em>,
                                                        <em>update_every=50</em>, <em>act_noise=0.1</em>,
                                                        <em>target_noise=0.2</em>, <em>noise_clip=0.5</em>,
                                                        <em>policy_delay=2</em>, <em>num_test_episodes=10</em>,
                                                        <em>max_ep_len=1000</em>, <em>logger_kwargs={}</em>,
                                                        <em>save_freq=1</em><span class="sig-paren">)</span><a
                                                            class="headerlink" href="#spinup.td3_tf1"
                                                            title="Permalink to this definition">¶</a>
                                                    </dt>
                                                    <dd>
                                                        <p>Twin Delayed Deep Deterministic Policy Gradient (TD3)</p>
                                                        <table class="docutils field-list" frame="void" rules="none">
                                                            <col class="field-name" />
                                                            <col class="field-body" />
                                                            <tbody valign="top">
                                                                <tr class="field-odd field">
                                                                    <th class="field-name">Parameters:</th>
                                                                    <td class="field-body">
                                                                        <ul class="first last simple">
                                                                            <li><strong>env_fn</strong> &#8211; A
                                                                                function which creates a copy of the
                                                                                environment.
                                                                                The environment must satisfy the OpenAI
                                                                                Gym API.</li>
                                                                            <li><strong>actor_critic</strong> &#8211;
                                                                                <p>A function which takes in placeholder
                                                                                    symbols
                                                                                    for state, <code
                                                                                        class="docutils literal"><span class="pre">x_ph</span></code>,
                                                                                    and action, <code
                                                                                        class="docutils literal"><span class="pre">a_ph</span></code>,
                                                                                    and returns the main
                                                                                    outputs from the agent&#8217;s
                                                                                    Tensorflow computation graph:</p>
                                                                                <table border="1" class="docutils">
                                                                                    <colgroup>
                                                                                        <col width="17%" />
                                                                                        <col width="25%" />
                                                                                        <col width="58%" />
                                                                                    </colgroup>
                                                                                    <thead valign="bottom">
                                                                                        <tr class="row-odd">
                                                                                            <th class="head">Symbol</th>
                                                                                            <th class="head">Shape</th>
                                                                                            <th class="head">Description
                                                                                            </th>
                                                                                        </tr>
                                                                                    </thead>
                                                                                    <tbody valign="top">
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">pi</span></code>
                                                                                            </td>
                                                                                            <td>(batch, act_dim)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Deterministically
                                                                                                        computes actions
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        from policy
                                                                                                        given states.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-odd">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">q1</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Gives one
                                                                                                        estimate of Q*
                                                                                                        for</div>
                                                                                                    <div class="line">
                                                                                                        states in <code
                                                                                                            class="docutils literal"><span class="pre">x_ph</span></code>
                                                                                                        and actions in
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">a_ph</span></code>.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">q2</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Gives another
                                                                                                        estimate of Q*
                                                                                                        for</div>
                                                                                                    <div class="line">
                                                                                                        states in <code
                                                                                                            class="docutils literal"><span class="pre">x_ph</span></code>
                                                                                                        and actions in
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">a_ph</span></code>.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-odd">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">q1_pi</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Gives the
                                                                                                        composition of
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">q1</span></code>
                                                                                                        and</div>
                                                                                                    <div class="line">
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">pi</span></code>
                                                                                                        for states in
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">x_ph</span></code>:
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        q1(x, pi(x)).
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                    </tbody>
                                                                                </table>
                                                                            </li>
                                                                            <li><strong>ac_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Any kwargs
                                                                                appropriate for the actor_critic
                                                                                function you provided to TD3.</li>
                                                                            <li><strong>seed</strong> (<em>int</em>)
                                                                                &#8211; Seed for random number
                                                                                generators.</li>
                                                                            <li><strong>steps_per_epoch</strong>
                                                                                (<em>int</em>) &#8211; Number of steps
                                                                                of interaction (state-action pairs)
                                                                                for the agent and the environment in
                                                                                each epoch.</li>
                                                                            <li><strong>epochs</strong> (<em>int</em>)
                                                                                &#8211; Number of epochs to run and
                                                                                train agent.</li>
                                                                            <li><strong>replay_size</strong>
                                                                                (<em>int</em>) &#8211; Maximum length of
                                                                                replay buffer.</li>
                                                                            <li><strong>gamma</strong> (<em>float</em>)
                                                                                &#8211; Discount factor. (Always between
                                                                                0 and 1.)</li>
                                                                            <li><strong>polyak</strong> (<em>float</em>)
                                                                                &#8211; <p>Interpolation factor in
                                                                                    polyak averaging for target
                                                                                    networks. Target networks are
                                                                                    updated towards main networks
                                                                                    according to:</p>
                                                                                <div class="math">
                                                                                    <p><img src="_images/math/ea6c5fdb8bb78fe30797537bbb28553b9a7706ef.svg"
                                                                                            alt="\theta_{\text{targ}} \leftarrow
\rho \theta_{\text{targ}} + (1-\rho) \theta" /></p>
                                                                                </div>
                                                                                <p>where <img class="math"
                                                                                        src="_images/math/b41ecbab285e58fd94a9b544487b74b1d992b0dd.svg"
                                                                                        alt="\rho" /> is polyak. (Always
                                                                                    between 0 and 1, usually
                                                                                    close to 1.)</p>
                                                                            </li>
                                                                            <li><strong>pi_lr</strong> (<em>float</em>)
                                                                                &#8211; Learning rate for policy.</li>
                                                                            <li><strong>q_lr</strong> (<em>float</em>)
                                                                                &#8211; Learning rate for Q-networks.
                                                                            </li>
                                                                            <li><strong>batch_size</strong>
                                                                                (<em>int</em>) &#8211; Minibatch size
                                                                                for SGD.</li>
                                                                            <li><strong>start_steps</strong>
                                                                                (<em>int</em>) &#8211; Number of steps
                                                                                for uniform-random action selection,
                                                                                before running real policy. Helps
                                                                                exploration.</li>
                                                                            <li><strong>update_after</strong>
                                                                                (<em>int</em>) &#8211; Number of env
                                                                                interactions to collect before
                                                                                starting to do gradient descent updates.
                                                                                Ensures replay buffer
                                                                                is full enough for useful updates.</li>
                                                                            <li><strong>update_every</strong>
                                                                                (<em>int</em>) &#8211; Number of env
                                                                                interactions that should elapse
                                                                                between gradient descent updates. Note:
                                                                                Regardless of how long
                                                                                you wait between updates, the ratio of
                                                                                env steps to gradient steps
                                                                                is locked to 1.</li>
                                                                            <li><strong>act_noise</strong>
                                                                                (<em>float</em>) &#8211; Stddev for
                                                                                Gaussian exploration noise added to
                                                                                policy at training time. (At test time,
                                                                                no noise is added.)</li>
                                                                            <li><strong>target_noise</strong>
                                                                                (<em>float</em>) &#8211; Stddev for
                                                                                smoothing noise added to target
                                                                                policy.</li>
                                                                            <li><strong>noise_clip</strong>
                                                                                (<em>float</em>) &#8211; Limit for
                                                                                absolute value of target policy
                                                                                smoothing noise.</li>
                                                                            <li><strong>policy_delay</strong>
                                                                                (<em>int</em>) &#8211; Policy will only
                                                                                be updated once every
                                                                                policy_delay times for each update of
                                                                                the Q-networks.</li>
                                                                            <li><strong>num_test_episodes</strong>
                                                                                (<em>int</em>) &#8211; Number of
                                                                                episodes to test the deterministic
                                                                                policy at the end of each epoch.</li>
                                                                            <li><strong>max_ep_len</strong>
                                                                                (<em>int</em>) &#8211; Maximum length of
                                                                                trajectory / episode / rollout.</li>
                                                                            <li><strong>logger_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Keyword args for
                                                                                EpochLogger.</li>
                                                                            <li><strong>save_freq</strong>
                                                                                (<em>int</em>) &#8211; How often (in
                                                                                terms of gap between epochs) to save
                                                                                the current policy and value function.
                                                                            </li>
                                                                        </ul>
                                                                    </td>
                                                                </tr>
                                                            </tbody>
                                                        </table>
                                                    </dd>
                                                </dl>

                                            </div>
                                            <div class="section" id="saved-model-contents-tensorflow-version">
                                                <h4><a class="toc-backref" href="#id11">Saved Model Contents: Tensorflow
                                                        Version</a><a class="headerlink"
                                                        href="#saved-model-contents-tensorflow-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>The computation graph saved by the logger includes:</p>
                                                <table border="1" class="docutils">
                                                    <colgroup>
                                                        <col width="9%" />
                                                        <col width="91%" />
                                                    </colgroup>
                                                    <thead valign="bottom">
                                                        <tr class="row-odd">
                                                            <th class="head">Key</th>
                                                            <th class="head">Value</th>
                                                        </tr>
                                                    </thead>
                                                    <tbody valign="top">
                                                        <tr class="row-even">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">x</span></code>
                                                            </td>
                                                            <td>Tensorflow placeholder for state input.</td>
                                                        </tr>
                                                        <tr class="row-odd">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">a</span></code>
                                                            </td>
                                                            <td>Tensorflow placeholder for action input.</td>
                                                        </tr>
                                                        <tr class="row-even">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">pi</span></code>
                                                            </td>
                                                            <td>
                                                                <div class="first last line-block">
                                                                    <div class="line">Deterministically computes an
                                                                        action from the agent, conditioned</div>
                                                                    <div class="line">on states in <code
                                                                            class="docutils literal"><span class="pre">x</span></code>.
                                                                    </div>
                                                                </div>
                                                            </td>
                                                        </tr>
                                                        <tr class="row-odd">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">q1</span></code>
                                                            </td>
                                                            <td>Gives one action-value estimate for states in <code
                                                                    class="docutils literal"><span class="pre">x</span></code>
                                                                and actions in <code
                                                                    class="docutils literal"><span class="pre">a</span></code>.
                                                            </td>
                                                        </tr>
                                                        <tr class="row-even">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">q2</span></code>
                                                            </td>
                                                            <td>Gives the other action-value estimate for states in
                                                                <code
                                                                    class="docutils literal"><span class="pre">x</span></code>
                                                                and actions in <code
                                                                    class="docutils literal"><span class="pre">a</span></code>.
                                                            </td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <p>This saved model can be accessed either by</p>
                                                <ul class="simple">
                                                    <li>running the trained policy with the <a
                                                            class="reference external"
                                                            href="../user/saving_and_loading.html#loading-and-running-trained-policies">test_policy.py</a>
                                                        tool,</li>
                                                    <li>or loading the whole saved graph into a program with <a
                                                            class="reference external"
                                                            href="../utils/logger.html#spinup.utils.logx.restore_tf_graph">restore_tf_graph</a>.
                                                    </li>
                                                </ul>
                                            </div>
                                        </div>
                                        <div class="section" id="references">
                                            <h3><a class="toc-backref" href="#id12">References</a><a class="headerlink"
                                                    href="#references" title="Permalink to this headline">¶</a></h3>
                                            <div class="section" id="relevant-papers">
                                                <h4><a class="toc-backref" href="#id13">Relevant Papers</a><a
                                                        class="headerlink" href="#relevant-papers"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <ul class="simple">
                                                    <li><a class="reference external"
                                                            href="https://arxiv.org/pdf/1802.09477.pdf">Addressing
                                                            Function Approximation Error in Actor-Critic Methods</a>,
                                                        Fujimoto et al, 2018</li>
                                                </ul>
                                            </div>
                                            <div class="section" id="other-public-implementations">
                                                <h4><a class="toc-backref" href="#id14">Other Public
                                                        Implementations</a><a class="headerlink"
                                                        href="#other-public-implementations"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <ul class="simple">
                                                    <li><a class="reference external"
                                                            href="https://github.com/sfujim/TD3">TD3 release repo</a>
                                                    </li>
                                                </ul>
                                            </div>
                                        </div>
                                    </div>
                                    <span id="document-algorithms/sac"></span>
                                    <div class="section" id="soft-actor-critic">
                                        <h2><a class="toc-backref" href="#id2">Soft Actor-Critic</a><a
                                                class="headerlink" href="#soft-actor-critic"
                                                title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal" href="#soft-actor-critic"
                                                        id="id2">Soft Actor-Critic</a>
                                                    <ul>
                                                        <li><a class="reference internal" href="#background"
                                                                id="id3">Background</a>
                                                            <ul>
                                                                <li><a class="reference internal" href="#quick-facts"
                                                                        id="id4">Quick Facts</a></li>
                                                                <li><a class="reference internal" href="#key-equations"
                                                                        id="id5">Key Equations</a>
                                                                    <ul>
                                                                        <li><a class="reference internal"
                                                                                href="#entropy-regularized-reinforcement-learning"
                                                                                id="id6">Entropy-Regularized
                                                                                Reinforcement Learning</a></li>
                                                                        <li><a class="reference internal" href="#id1"
                                                                                id="id7">Soft Actor-Critic</a></li>
                                                                    </ul>
                                                                </li>
                                                                <li><a class="reference internal"
                                                                        href="#exploration-vs-exploitation"
                                                                        id="id8">Exploration vs. Exploitation</a></li>
                                                                <li><a class="reference internal" href="#pseudocode"
                                                                        id="id9">Pseudocode</a></li>
                                                            </ul>
                                                        </li>
                                                        <li><a class="reference internal" href="#documentation"
                                                                id="id10">Documentation</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#documentation-pytorch-version"
                                                                        id="id11">Documentation: PyTorch Version</a>
                                                                </li>
                                                                <li><a class="reference internal"
                                                                        href="#saved-model-contents-pytorch-version"
                                                                        id="id12">Saved Model Contents: PyTorch
                                                                        Version</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#documentation-tensorflow-version"
                                                                        id="id13">Documentation: Tensorflow Version</a>
                                                                </li>
                                                                <li><a class="reference internal"
                                                                        href="#saved-model-contents-tensorflow-version"
                                                                        id="id14">Saved Model Contents: Tensorflow
                                                                        Version</a></li>
                                                            </ul>
                                                        </li>
                                                        <li><a class="reference internal" href="#references"
                                                                id="id15">References</a>
                                                            <ul>
                                                                <li><a class="reference internal"
                                                                        href="#relevant-papers" id="id16">Relevant
                                                                        Papers</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#other-public-implementations"
                                                                        id="id17">Other Public Implementations</a></li>
                                                            </ul>
                                                        </li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <div class="section" id="background">
                                            <h3><a class="toc-backref" href="#id3">Background</a><a class="headerlink"
                                                    href="#background" title="Permalink to this headline">¶</a></h3>
                                            <p>(Previously: <a class="reference external"
                                                    href="../algorithms/td3.html#background">Background for TD3</a>)</p>
                                            <p>Soft Actor Critic (SAC) is an algorithm that optimizes a stochastic
                                                policy in an off-policy way, forming a bridge between stochastic policy
                                                optimization and DDPG-style approaches. It isn&#8217;t a direct
                                                successor to TD3 (having been published roughly concurrently), but it
                                                incorporates the clipped double-Q trick, and due to the inherent
                                                stochasticity of the policy in SAC, it also winds up benefiting from
                                                something like target policy smoothing.</p>
                                            <p>A central feature of SAC is <strong>entropy regularization.</strong> The
                                                policy is trained to maximize a trade-off between expected return and <a
                                                    class="reference external"
                                                    href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">entropy</a>,
                                                a measure of randomness in the policy. This has a close connection to
                                                the exploration-exploitation trade-off: increasing entropy results in
                                                more exploration, which can accelerate learning later on. It can also
                                                prevent the policy from prematurely converging to a bad local optimum.
                                            </p>
                                            <div class="section" id="quick-facts">
                                                <h4><a class="toc-backref" href="#id4">Quick Facts</a><a
                                                        class="headerlink" href="#quick-facts"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <ul class="simple">
                                                    <li>SAC is an off-policy algorithm.</li>
                                                    <li>The version of SAC implemented here can only be used for
                                                        environments with continuous action spaces.</li>
                                                    <li>An alternate version of SAC, which slightly changes the policy
                                                        update rule, can be implemented to handle discrete action
                                                        spaces.</li>
                                                    <li>The Spinning Up implementation of SAC does not support
                                                        parallelization.</li>
                                                </ul>
                                            </div>
                                            <div class="section" id="key-equations">
                                                <h4><a class="toc-backref" href="#id5">Key Equations</a><a
                                                        class="headerlink" href="#key-equations"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>To explain Soft Actor Critic, we first have to introduce the
                                                    entropy-regularized reinforcement learning setting. In
                                                    entropy-regularized RL, there are slightly-different equations for
                                                    value functions.</p>
                                                <div class="section" id="entropy-regularized-reinforcement-learning">
                                                    <h5><a class="toc-backref" href="#id6">Entropy-Regularized
                                                            Reinforcement Learning</a><a class="headerlink"
                                                            href="#entropy-regularized-reinforcement-learning"
                                                            title="Permalink to this headline">¶</a></h5>
                                                    <p>Entropy is a quantity which, roughly speaking, says how random a
                                                        random variable is. If a coin is weighted so that it almost
                                                        always comes up heads, it has low entropy; if it&#8217;s evenly
                                                        weighted and has a half chance of either outcome, it has high
                                                        entropy.</p>
                                                    <p>Let <img class="math"
                                                            src="_images/math/ea07a4204f1f53321f76d9c7e348199f0d707db1.svg"
                                                            alt="x" /> be a random variable with probability mass or
                                                        density function <img class="math"
                                                            src="_images/math/4204ba416334e663d7bd7c6457d737ba3cbbfe46.svg"
                                                            alt="P" />. The entropy <img class="math"
                                                            src="_images/math/bf6bcb1745aeab36cdc185e9f75bbfd3998352ce.svg"
                                                            alt="H" /> of <img class="math"
                                                            src="_images/math/ea07a4204f1f53321f76d9c7e348199f0d707db1.svg"
                                                            alt="x" /> is computed from its distribution <img
                                                            class="math"
                                                            src="_images/math/4204ba416334e663d7bd7c6457d737ba3cbbfe46.svg"
                                                            alt="P" /> according to</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/1bf89d5228652e14d82657fe9f1499b136f54094.svg"
                                                                alt="H(P) = \underE{x \sim P}{-\log P(x)}." /></p>
                                                    </div>
                                                    <p>In entropy-regularized reinforcement learning, the agent gets a
                                                        bonus reward at each time step proportional to the entropy of
                                                        the policy at that timestep. This changes <a
                                                            class="reference external"
                                                            href="../spinningup/rl_intro.html#the-rl-problem">the RL
                                                            problem</a> to:</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/b86bf499707114c8789946df649871c5b9185b9d.svg"
                                                                alt="\pi^* = \arg \max_{\pi} \underE{\tau \sim \pi}{ \sum_{t=0}^{\infty} \gamma^t \bigg( R(s_t, a_t, s_{t+1}) + \alpha H\left(\pi(\cdot|s_t)\right) \bigg)}," />
                                                        </p>
                                                    </div>
                                                    <p>where <img class="math"
                                                            src="_images/math/900375490edee0019a5c54a311bf91de801a1642.svg"
                                                            alt="\alpha &gt; 0" /> is the trade-off coefficient. (Note:
                                                        we&#8217;re assuming an infinite-horizon discounted setting
                                                        here, and we&#8217;ll do the same for the rest of this page.) We
                                                        can now define the slightly-different value functions in this
                                                        setting. <img class="math"
                                                            src="_images/math/fbed8ae629f7512710c5352ca50e8f629d7f34e4.svg"
                                                            alt="V^{\pi}" /> is changed to include the entropy bonuses
                                                        from every timestep:</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/dda9cebd308c7fe5313f6bf4cbce8d15af046279.svg"
                                                                alt="V^{\pi}(s) = \underE{\tau \sim \pi}{ \left. \sum_{t=0}^{\infty} \gamma^t \bigg( R(s_t, a_t, s_{t+1}) + \alpha H\left(\pi(\cdot|s_t)\right) \bigg) \right| s_0 = s}" />
                                                        </p>
                                                    </div>
                                                    <p><img class="math"
                                                            src="_images/math/2bbd8ab5668fe92f59056f58c9f75a01c929e37d.svg"
                                                            alt="Q^{\pi}" /> is changed to include the entropy bonuses
                                                        from every timestep <em>except the first</em>:</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/3c1b1d100a914b01d2f537fd11bdd1159921cad2.svg"
                                                                alt="Q^{\pi}(s,a) = \underE{\tau \sim \pi}{ \left. \sum_{t=0}^{\infty} \gamma^t  R(s_t, a_t, s_{t+1}) + \alpha \sum_{t=1}^{\infty} \gamma^t H\left(\pi(\cdot|s_t)\right)\right| s_0 = s, a_0 = a}" />
                                                        </p>
                                                    </div>
                                                    <p>With these definitions, <img class="math"
                                                            src="_images/math/fbed8ae629f7512710c5352ca50e8f629d7f34e4.svg"
                                                            alt="V^{\pi}" /> and <img class="math"
                                                            src="_images/math/2bbd8ab5668fe92f59056f58c9f75a01c929e37d.svg"
                                                            alt="Q^{\pi}" /> are connected by:</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/46d0852616c131f3d5aa2d1798328141904a764d.svg"
                                                                alt="V^{\pi}(s) = \underE{a \sim \pi}{Q^{\pi}(s,a)} + \alpha H\left(\pi(\cdot|s)\right)" />
                                                        </p>
                                                    </div>
                                                    <p>and the Bellman equation for <img class="math"
                                                            src="_images/math/2bbd8ab5668fe92f59056f58c9f75a01c929e37d.svg"
                                                            alt="Q^{\pi}" /> is</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/8010672f1e8269ce985f901728e7224faa07731e.svg"
                                                                alt="Q^{\pi}(s,a) &amp;= \underE{s' \sim P \\ a' \sim \pi}{R(s,a,s') + \gamma\left(Q^{\pi}(s',a') + \alpha H\left(\pi(\cdot|s')\right) \right)} \\
&amp;= \underE{s' \sim P}{R(s,a,s') + \gamma V^{\pi}(s')}." /></p>
                                                    </div>
                                                    <div class="admonition-you-should-know admonition">
                                                        <p class="first admonition-title">You Should Know</p>
                                                        <p class="last">The way we&#8217;ve set up the value functions
                                                            in the entropy-regularized setting is a little bit
                                                            arbitrary, and actually we could have done it differently
                                                            (eg make <img class="math"
                                                                src="_images/math/2bbd8ab5668fe92f59056f58c9f75a01c929e37d.svg"
                                                                alt="Q^{\pi}" /> include the entropy bonus at the first
                                                            timestep). The choice of definition may vary slightly across
                                                            papers on the subject.</p>
                                                    </div>
                                                </div>
                                                <div class="section" id="id1">
                                                    <h5><a class="toc-backref" href="#id7">Soft Actor-Critic</a><a
                                                            class="headerlink" href="#id1"
                                                            title="Permalink to this headline">¶</a></h5>
                                                    <p>SAC concurrently learns a policy <img class="math"
                                                            src="_images/math/6a71f04b65d9524fb656715cda85d7540a9ddf9f.svg"
                                                            alt="\pi_{\theta}" /> and two Q-functions <img class="math"
                                                            src="_images/math/a4f90f64839041d3c84ac2dde832e76f9d6db7b6.svg"
                                                            alt="Q_{\phi_1}, Q_{\phi_2}" />. There are two variants of
                                                        SAC that are currently standard: one that uses a fixed entropy
                                                        regularization coefficient <img class="math"
                                                            src="_images/math/d8316e40b1057b06d31c2cad3a0d4cc9e75fa2c1.svg"
                                                            alt="\alpha" />, and another that enforces an entropy
                                                        constraint by varying <img class="math"
                                                            src="_images/math/d8316e40b1057b06d31c2cad3a0d4cc9e75fa2c1.svg"
                                                            alt="\alpha" /> over the course of training. For simplicity,
                                                        Spinning Up makes use of the version with a fixed entropy
                                                        regularization coefficient, but the entropy-constrained variant
                                                        is generally preferred by practitioners.</p>
                                                    <div class="admonition-you-should-know admonition">
                                                        <p class="first admonition-title">You Should Know</p>
                                                        <p class="last">The SAC algorithm has changed a little bit over
                                                            time. An older version of SAC also learns a value function
                                                            <img class="math"
                                                                src="_images/math/f8b8aa6de09a776f6aa37138d773730ba9e623c7.svg"
                                                                alt="V_{\psi}" /> in addition to the Q-functions; this
                                                            page will focus on the modern version that omits the extra
                                                            value function.</p>
                                                    </div>
                                                    <p><strong>Learning Q.</strong> The Q-functions are learned in a
                                                        similar way to TD3, but with a few key differences.</p>
                                                    <p>First, what&#8217;s similar?</p>
                                                    <ol class="arabic simple">
                                                        <li>Like in TD3, both Q-functions are learned with MSBE
                                                            minimization, by regressing to a single shared target.</li>
                                                        <li>Like in TD3, the shared target is computed using target
                                                            Q-networks, and the target Q-networks are obtained by polyak
                                                            averaging the Q-network parameters over the course of
                                                            training.</li>
                                                        <li>Like in TD3, the shared target makes use of the
                                                            <strong>clipped double-Q</strong> trick.</li>
                                                    </ol>
                                                    <p>What&#8217;s different?</p>
                                                    <ol class="arabic simple">
                                                        <li>Unlike in TD3, the target also includes a term that comes
                                                            from SAC&#8217;s use of entropy regularization.</li>
                                                        <li>Unlike in TD3, the next-state actions used in the target
                                                            come from the <strong>current policy</strong> instead of a
                                                            target policy.</li>
                                                        <li>Unlike in TD3, there is no explicit target policy smoothing.
                                                            TD3 trains a deterministic policy, and so it accomplishes
                                                            smoothing by adding random noise to the next-state actions.
                                                            SAC trains a stochastic policy, and so the noise from that
                                                            stochasticity is sufficient to get a similar effect.</li>
                                                    </ol>
                                                    <p>Before we give the final form of the Q-loss, let’s take a moment
                                                        to discuss how the contribution from entropy regularization
                                                        comes in. We&#8217;ll start by taking our recursive Bellman
                                                        equation for the entropy-regularized <img class="math"
                                                            src="_images/math/2bbd8ab5668fe92f59056f58c9f75a01c929e37d.svg"
                                                            alt="Q^{\pi}" /> from earlier, and rewriting it a little bit
                                                        by using the definition of entropy:</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/1557c0c7205cbb2928eb3305b2df207e79bc70fe.svg"
                                                                alt="Q^{\pi}(s,a) &amp;= \underE{s' \sim P \\ a' \sim \pi}{R(s,a,s') + \gamma\left(Q^{\pi}(s',a') + \alpha H\left(\pi(\cdot|s')\right) \right)} \\
&amp;= \underE{s' \sim P \\ a' \sim \pi}{R(s,a,s') + \gamma\left(Q^{\pi}(s',a') - \alpha \log \pi(a'|s') \right)}" />
                                                        </p>
                                                    </div>
                                                    <p>The RHS is an expectation over next states (which come from the
                                                        replay buffer) and next actions (which come from the current
                                                        policy, and <strong>not</strong> the replay buffer). Since
                                                        it&#8217;s an expectation, we can approximate it with samples:
                                                    </p>
                                                    <div class="math">
                                                        <p><img src="_images/math/aa74b233b0820048f096edb81f0b3321730d71a8.svg"
                                                                alt="Q^{\pi}(s,a) &amp;\approx r + \gamma\left(Q^{\pi}(s',\tilde{a}') - \alpha \log \pi(\tilde{a}'|s') \right), \;\;\;\;\;  \tilde{a}' \sim \pi(\cdot|s')." />
                                                        </p>
                                                    </div>
                                                    <div class="admonition-you-should-know admonition">
                                                        <p class="first admonition-title">You Should Know</p>
                                                        <p class="last">We switch next action notation to <img
                                                                class="math"
                                                                src="_images/math/f1523bc2b6ea2ca935e184990079e62313c3321f.svg"
                                                                alt="\tilde{a}'" />, instead of <img class="math"
                                                                src="_images/math/3200e4a6949b896a76b0e83a40edb16602433fd0.svg"
                                                                alt="a'" />, to highlight that the next actions have to
                                                            be sampled fresh from the policy (whereas by contrast, <img
                                                                class="math"
                                                                src="_images/math/5a3ac7a81362ac174d142bab198b4bd5a9e2dcee.svg"
                                                                alt="r" /> and <img class="math"
                                                                src="_images/math/6e85fa05d4954e7c1e8037ee1bd163d15bc2e2d6.svg"
                                                                alt="s'" /> should come from the replay buffer).</p>
                                                    </div>
                                                    <p>SAC sets up the MSBE loss for each Q-function using this kind of
                                                        sample approximation for the target. The only thing still
                                                        undetermined here is which Q-function gets used to compute the
                                                        sample backup: like TD3, SAC uses the clipped double-Q trick,
                                                        and takes the minimum Q-value between the two Q approximators.
                                                    </p>
                                                    <p>Putting it all together, the loss functions for the Q-networks in
                                                        SAC are:</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/0bd81fc5d1cb03a33d6477f5ff10ed879ea393ec.svg"
                                                                alt="L(\phi_i, {\mathcal D}) = \underset{(s,a,r,s',d) \sim {\mathcal D}}{{\mathrm E}}\left[
    \Bigg( Q_{\phi_i}(s,a) - y(r,s',d) \Bigg)^2
    \right]," /></p>
                                                    </div>
                                                    <p>where the target is given by</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/fc03ff9e9f818fb31b7724907e2b43d5101d2ab8.svg"
                                                                alt="y(r, s', d) = r + \gamma (1 - d) \left( \min_{j=1,2} Q_{\phi_{\text{targ},j}}(s', \tilde{a}') - \alpha \log \pi_{\theta}(\tilde{a}'|s') \right), \;\;\;\;\; \tilde{a}' \sim \pi_{\theta}(\cdot|s')." />
                                                        </p>
                                                    </div>
                                                    <p><strong>Learning the Policy.</strong> The policy should, in each
                                                        state, act to maximize the expected future return plus expected
                                                        future entropy. That is, it should maximize <img class="math"
                                                            src="_images/math/a81303323c25fc13cd0652ca46d7596276e5cb7e.svg"
                                                            alt="V^{\pi}(s)" />, which we expand out into</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/5ff58df73caef07f6309a1460fe57b1c34e3b374.svg"
                                                                alt="V^{\pi}(s) &amp;= \underE{a \sim \pi}{Q^{\pi}(s,a)} + \alpha H\left(\pi(\cdot|s)\right) \\
&amp;= \underE{a \sim \pi}{Q^{\pi}(s,a) - \alpha \log \pi(a|s)}." /></p>
                                                    </div>
                                                    <p>The way we optimize the policy makes use of the
                                                        <strong>reparameterization trick</strong>, in which a sample
                                                        from <img class="math"
                                                            src="_images/math/e57f13375048b8f7343f9066b6553bc282afa326.svg"
                                                            alt="\pi_{\theta}(\cdot|s)" /> is drawn by computing a
                                                        deterministic function of state, policy parameters, and
                                                        independent noise. To illustrate: following the authors of the
                                                        SAC paper, we use a squashed Gaussian policy, which means that
                                                        samples are obtained according to</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/dac3ddc2ea35e8233b8bc0a905273712793ab1cb.svg"
                                                                alt="\tilde{a}_{\theta}(s, \xi) = \tanh\left( \mu_{\theta}(s) + \sigma_{\theta}(s) \odot \xi \right), \;\;\;\;\; \xi \sim \mathcal{N}(0, I)." />
                                                        </p>
                                                    </div>
                                                    <div class="admonition-you-should-know admonition">
                                                        <p class="first admonition-title">You Should Know</p>
                                                        <p>This policy has two key differences from the policies we use
                                                            in the other policy optimization algorithms:</p>
                                                        <p><strong>1. The squashing function.</strong> The <img
                                                                class="math"
                                                                src="_images/math/c65796f3bb56c457e63ebc770e3d775cace08673.svg"
                                                                alt="\tanh" /> in the SAC policy ensures that actions
                                                            are bounded to a finite range. This is absent in the VPG,
                                                            TRPO, and PPO policies. It also changes the distribution:
                                                            before the <img class="math"
                                                                src="_images/math/c65796f3bb56c457e63ebc770e3d775cace08673.svg"
                                                                alt="\tanh" /> the SAC policy is a factored Gaussian
                                                            like the other algorithms&#8217; policies, but after the
                                                            <img class="math"
                                                                src="_images/math/c65796f3bb56c457e63ebc770e3d775cace08673.svg"
                                                                alt="\tanh" /> it is not. (You can still compute the
                                                            log-probabilities of actions in closed form, though: see the
                                                            paper appendix for details.)</p>
                                                        <p class="last"><strong>2. The way standard deviations are
                                                                parameterized.</strong> In VPG, TRPO, and PPO, we
                                                            represent the log std devs with state-independent parameter
                                                            vectors. In SAC, we represent the log std devs as outputs
                                                            from the neural network, meaning that they depend on state
                                                            in a complex way. SAC with state-independent log std devs,
                                                            in our experience, did not work. (Can you think of why? Or
                                                            better yet: run an experiment to verify?)</p>
                                                    </div>
                                                    <p>The reparameterization trick allows us to rewrite the expectation
                                                        over actions (which contains a pain point: the distribution
                                                        depends on the policy parameters) into an expectation over noise
                                                        (which removes the pain point: the distribution now has no
                                                        dependence on parameters):</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/5713f9f99ea3532e3cbde89eac91328eb8549409.svg"
                                                                alt="\underE{a \sim \pi_{\theta}}{Q^{\pi_{\theta}}(s,a) - \alpha \log \pi_{\theta}(a|s)} = \underE{\xi \sim \mathcal{N}}{Q^{\pi_{\theta}}(s,\tilde{a}_{\theta}(s,\xi)) - \alpha \log \pi_{\theta}(\tilde{a}_{\theta}(s,\xi)|s)}" />
                                                        </p>
                                                    </div>
                                                    <p>To get the policy loss, the final step is that we need to
                                                        substitute <img class="math"
                                                            src="_images/math/9c39112fd52e66e3062f93c502ade0eb9381d957.svg"
                                                            alt="Q^{\pi_{\theta}}" /> with one of our function
                                                        approximators. Unlike in TD3, which uses <img class="math"
                                                            src="_images/math/8795d42bd263dcbe55d123e7466b2dd5091490a7.svg"
                                                            alt="Q_{\phi_1}" /> (just the first Q approximator), SAC
                                                        uses <img class="math"
                                                            src="_images/math/e5d14ed1b7128d64d43af73b7d0b189c6afda8ec.svg"
                                                            alt="\min_{j=1,2} Q_{\phi_j}" /> (the minimum of the two Q
                                                        approximators). The policy is thus optimized according to</p>
                                                    <div class="math">
                                                        <p><img src="_images/math/bdbe4cabbba4687b310d99e8fa67ed314339bd31.svg"
                                                                alt="\max_{\theta} \underE{s \sim \mathcal{D} \\ \xi \sim \mathcal{N}}{\min_{j=1,2} Q_{\phi_j}(s,\tilde{a}_{\theta}(s,\xi)) - \alpha \log \pi_{\theta}(\tilde{a}_{\theta}(s,\xi)|s)}," />
                                                        </p>
                                                    </div>
                                                    <p>which is almost the same as the DDPG and TD3 policy optimization,
                                                        except for the min-double-Q trick, the stochasticity, and the
                                                        entropy term.</p>
                                                </div>
                                            </div>
                                            <div class="section" id="exploration-vs-exploitation">
                                                <h4><a class="toc-backref" href="#id8">Exploration vs.
                                                        Exploitation</a><a class="headerlink"
                                                        href="#exploration-vs-exploitation"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>SAC trains a stochastic policy with entropy regularization, and
                                                    explores in an on-policy way. The entropy regularization coefficient
                                                    <img class="math"
                                                        src="_images/math/d8316e40b1057b06d31c2cad3a0d4cc9e75fa2c1.svg"
                                                        alt="\alpha" /> explicitly controls the explore-exploit
                                                    tradeoff, with higher <img class="math"
                                                        src="_images/math/d8316e40b1057b06d31c2cad3a0d4cc9e75fa2c1.svg"
                                                        alt="\alpha" /> corresponding to more exploration, and lower
                                                    <img class="math"
                                                        src="_images/math/d8316e40b1057b06d31c2cad3a0d4cc9e75fa2c1.svg"
                                                        alt="\alpha" /> corresponding to more exploitation. The right
                                                    coefficient (the one which leads to the stablest / highest-reward
                                                    learning) may vary from environment to environment, and could
                                                    require careful tuning.</p>
                                                <p>At test time, to see how well the policy exploits what it has
                                                    learned, we remove stochasticity and use the mean action instead of
                                                    a sample from the distribution. This tends to improve performance
                                                    over the original stochastic policy.</p>
                                                <div class="admonition-you-should-know admonition">
                                                    <p class="first admonition-title">You Should Know</p>
                                                    <p class="last">Our SAC implementation uses a trick to improve
                                                        exploration at the start of training. For a fixed number of
                                                        steps at the beginning (set with the <code
                                                            class="docutils literal"><span class="pre">start_steps</span></code>
                                                        keyword argument), the agent takes actions which are sampled
                                                        from a uniform random distribution over valid actions. After
                                                        that, it returns to normal SAC exploration.</p>
                                                </div>
                                            </div>
                                            <div class="section" id="pseudocode">
                                                <h4><a class="toc-backref" href="#id9">Pseudocode</a><a
                                                        class="headerlink" href="#pseudocode"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <div class="math">
                                                    <p><img src="_images/math/c01f4994ae4aacf299a6b3ceceedfe0a14d4b874.svg"
                                                            alt="\begin{algorithm}[H]
    \caption{Soft Actor-Critic}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta$, Q-function parameters $\phi_1$, $\phi_2$, empty replay buffer $\mathcal{D}$
    \STATE Set target parameters equal to main parameters $\phi_{\text{targ},1} \leftarrow \phi_1$, $\phi_{\text{targ},2} \leftarrow \phi_2$
    \REPEAT
        \STATE Observe state $s$ and select action $a \sim \pi_{\theta}(\cdot|s)$
        \STATE Execute $a$ in the environment
        \STATE Observe next state $s'$, reward $r$, and done signal $d$ to indicate whether $s'$ is terminal
        \STATE Store $(s,a,r,s',d)$ in replay buffer $\mathcal{D}$
        \STATE If $s'$ is terminal, reset environment state.
        \IF{it's time to update}
            \FOR{$j$ in range(however many updates)}
                \STATE Randomly sample a batch of transitions, $B = \{ (s,a,r,s',d) \}$ from $\mathcal{D}$
                \STATE Compute targets for the Q functions:
                \begin{align*}
                    y (r,s',d) &amp;= r + \gamma (1-d) \left(\min_{i=1,2} Q_{\phi_{\text{targ}, i}} (s', \tilde{a}') - \alpha \log \pi_{\theta}(\tilde{a}'|s')\right), &amp;&amp; \tilde{a}' \sim \pi_{\theta}(\cdot|s')
                \end{align*}
                \STATE Update Q-functions by one step of gradient descent using
                \begin{align*}
                    &amp; \nabla_{\phi_i} \frac{1}{|B|}\sum_{(s,a,r,s',d) \in B} \left( Q_{\phi_i}(s,a) - y(r,s',d) \right)^2 &amp;&amp; \text{for } i=1,2
                \end{align*}
                \STATE Update policy by one step of gradient ascent using
                \begin{equation*}
                    \nabla_{\theta} \frac{1}{|B|}\sum_{s \in B} \Big(\min_{i=1,2} Q_{\phi_i}(s, \tilde{a}_{\theta}(s)) - \alpha \log \pi_{\theta} \left(\left. \tilde{a}_{\theta}(s) \right| s\right) \Big),
                \end{equation*}
                where $\tilde{a}_{\theta}(s)$ is a sample from $\pi_{\theta}(\cdot|s)$ which is differentiable wrt $\theta$ via the reparametrization trick.
                \STATE Update target networks with
                \begin{align*}
                    \phi_{\text{targ},i} &amp;\leftarrow \rho \phi_{\text{targ}, i} + (1-\rho) \phi_i &amp;&amp; \text{for } i=1,2
                \end{align*}
            \ENDFOR
        \ENDIF
    \UNTIL{convergence}
\end{algorithmic}
\end{algorithm}" /></p>
                                                </div>
                                            </div>
                                        </div>
                                        <div class="section" id="documentation">
                                            <h3><a class="toc-backref" href="#id10">Documentation</a><a
                                                    class="headerlink" href="#documentation"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p class="last">In what follows, we give documentation for the PyTorch
                                                    and Tensorflow implementations of SAC in Spinning Up. They have
                                                    nearly identical function calls and docstrings, except for details
                                                    relating to model construction. However, we include both full
                                                    docstrings for completeness.</p>
                                            </div>
                                            <div class="section" id="documentation-pytorch-version">
                                                <h4><a class="toc-backref" href="#id11">Documentation: PyTorch
                                                        Version</a><a class="headerlink"
                                                        href="#documentation-pytorch-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <dl class="function">
                                                    <dt id="spinup.sac_pytorch">
                                                        <code class="descclassname">spinup.</code><code
                                                            class="descname">sac_pytorch</code><span
                                                            class="sig-paren">(</span><em>env_fn</em>,
                                                        <em>actor_critic=&lt;MagicMock spec='str'
                                                            id='140191856645456'&gt;</em>, <em>ac_kwargs={}</em>,
                                                        <em>seed=0</em>, <em>steps_per_epoch=4000</em>,
                                                        <em>epochs=100</em>, <em>replay_size=1000000</em>,
                                                        <em>gamma=0.99</em>, <em>polyak=0.995</em>, <em>lr=0.001</em>,
                                                        <em>alpha=0.2</em>, <em>batch_size=100</em>,
                                                        <em>start_steps=10000</em>, <em>update_after=1000</em>,
                                                        <em>update_every=50</em>, <em>num_test_episodes=10</em>,
                                                        <em>max_ep_len=1000</em>, <em>logger_kwargs={}</em>,
                                                        <em>save_freq=1</em><span class="sig-paren">)</span><a
                                                            class="headerlink" href="#spinup.sac_pytorch"
                                                            title="Permalink to this definition">¶</a>
                                                    </dt>
                                                    <dd>
                                                        <p>Soft Actor-Critic (SAC)</p>
                                                        <table class="docutils field-list" frame="void" rules="none">
                                                            <col class="field-name" />
                                                            <col class="field-body" />
                                                            <tbody valign="top">
                                                                <tr class="field-odd field">
                                                                    <th class="field-name">Parameters:</th>
                                                                    <td class="field-body">
                                                                        <ul class="first last simple">
                                                                            <li><strong>env_fn</strong> &#8211; A
                                                                                function which creates a copy of the
                                                                                environment.
                                                                                The environment must satisfy the OpenAI
                                                                                Gym API.</li>
                                                                            <li><strong>actor_critic</strong> &#8211;
                                                                                <p>The constructor method for a PyTorch
                                                                                    Module with an <code
                                                                                        class="docutils literal"><span class="pre">act</span></code>
                                                                                    method, a <code
                                                                                        class="docutils literal"><span class="pre">pi</span></code>
                                                                                    module, a <code
                                                                                        class="docutils literal"><span class="pre">q1</span></code>
                                                                                    module, and a <code
                                                                                        class="docutils literal"><span class="pre">q2</span></code>
                                                                                    module.
                                                                                    The <code
                                                                                        class="docutils literal"><span class="pre">act</span></code>
                                                                                    method and <code
                                                                                        class="docutils literal"><span class="pre">pi</span></code>
                                                                                    module should accept batches of
                                                                                    observations as inputs, and <code
                                                                                        class="docutils literal"><span class="pre">q1</span></code>
                                                                                    and <code
                                                                                        class="docutils literal"><span class="pre">q2</span></code>
                                                                                    should accept a batch
                                                                                    of observations and a batch of
                                                                                    actions as inputs. When called,
                                                                                    <code
                                                                                        class="docutils literal"><span class="pre">act</span></code>,
                                                                                    <code
                                                                                        class="docutils literal"><span class="pre">q1</span></code>,
                                                                                    and <code
                                                                                        class="docutils literal"><span class="pre">q2</span></code>
                                                                                    should return:
                                                                                </p>
                                                                                <table border="1" class="docutils">
                                                                                    <colgroup>
                                                                                        <col width="15%" />
                                                                                        <col width="22%" />
                                                                                        <col width="63%" />
                                                                                    </colgroup>
                                                                                    <thead valign="bottom">
                                                                                        <tr class="row-odd">
                                                                                            <th class="head">Call</th>
                                                                                            <th class="head">Output
                                                                                                Shape</th>
                                                                                            <th class="head">Description
                                                                                            </th>
                                                                                        </tr>
                                                                                    </thead>
                                                                                    <tbody valign="top">
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">act</span></code>
                                                                                            </td>
                                                                                            <td>(batch, act_dim)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Numpy array of
                                                                                                        actions for each
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        observation.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-odd">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">q1</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Tensor
                                                                                                        containing one
                                                                                                        current estimate
                                                                                                    </div>
                                                                                                    <div class="line">of
                                                                                                        Q* for the
                                                                                                        provided
                                                                                                        observations
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        and actions.
                                                                                                        (Critical: make
                                                                                                        sure to</div>
                                                                                                    <div class="line">
                                                                                                        flatten this!)
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">q2</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Tensor
                                                                                                        containing the
                                                                                                        other current
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        estimate of Q*
                                                                                                        for the provided
                                                                                                        observations
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        and actions.
                                                                                                        (Critical: make
                                                                                                        sure to</div>
                                                                                                    <div class="line">
                                                                                                        flatten this!)
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                    </tbody>
                                                                                </table>
                                                                                <p>Calling <code
                                                                                        class="docutils literal"><span class="pre">pi</span></code>
                                                                                    should return:</p>
                                                                                <table border="1" class="docutils">
                                                                                    <colgroup>
                                                                                        <col width="16%" />
                                                                                        <col width="23%" />
                                                                                        <col width="61%" />
                                                                                    </colgroup>
                                                                                    <thead valign="bottom">
                                                                                        <tr class="row-odd">
                                                                                            <th class="head">Symbol</th>
                                                                                            <th class="head">Shape</th>
                                                                                            <th class="head">Description
                                                                                            </th>
                                                                                        </tr>
                                                                                    </thead>
                                                                                    <tbody valign="top">
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">a</span></code>
                                                                                            </td>
                                                                                            <td>(batch, act_dim)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Tensor
                                                                                                        containing
                                                                                                        actions from
                                                                                                        policy</div>
                                                                                                    <div class="line">
                                                                                                        given
                                                                                                        observations.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-odd">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">logp_pi</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Tensor
                                                                                                        containing log
                                                                                                        probabilities of
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        actions in <code
                                                                                                            class="docutils literal"><span class="pre">a</span></code>.
                                                                                                        Importantly:
                                                                                                        gradients</div>
                                                                                                    <div class="line">
                                                                                                        should be able
                                                                                                        to flow back
                                                                                                        into <code
                                                                                                            class="docutils literal"><span class="pre">a</span></code>.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                    </tbody>
                                                                                </table>
                                                                            </li>
                                                                            <li><strong>ac_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Any kwargs
                                                                                appropriate for the ActorCritic object
                                                                                you provided to SAC.</li>
                                                                            <li><strong>seed</strong> (<em>int</em>)
                                                                                &#8211; Seed for random number
                                                                                generators.</li>
                                                                            <li><strong>steps_per_epoch</strong>
                                                                                (<em>int</em>) &#8211; Number of steps
                                                                                of interaction (state-action pairs)
                                                                                for the agent and the environment in
                                                                                each epoch.</li>
                                                                            <li><strong>epochs</strong> (<em>int</em>)
                                                                                &#8211; Number of epochs to run and
                                                                                train agent.</li>
                                                                            <li><strong>replay_size</strong>
                                                                                (<em>int</em>) &#8211; Maximum length of
                                                                                replay buffer.</li>
                                                                            <li><strong>gamma</strong> (<em>float</em>)
                                                                                &#8211; Discount factor. (Always between
                                                                                0 and 1.)</li>
                                                                            <li><strong>polyak</strong> (<em>float</em>)
                                                                                &#8211; <p>Interpolation factor in
                                                                                    polyak averaging for target
                                                                                    networks. Target networks are
                                                                                    updated towards main networks
                                                                                    according to:</p>
                                                                                <div class="math">
                                                                                    <p><img src="_images/math/ea6c5fdb8bb78fe30797537bbb28553b9a7706ef.svg"
                                                                                            alt="\theta_{\text{targ}} \leftarrow
\rho \theta_{\text{targ}} + (1-\rho) \theta" /></p>
                                                                                </div>
                                                                                <p>where <img class="math"
                                                                                        src="_images/math/b41ecbab285e58fd94a9b544487b74b1d992b0dd.svg"
                                                                                        alt="\rho" /> is polyak. (Always
                                                                                    between 0 and 1, usually
                                                                                    close to 1.)</p>
                                                                            </li>
                                                                            <li><strong>lr</strong> (<em>float</em>)
                                                                                &#8211; Learning rate (used for both
                                                                                policy and value learning).</li>
                                                                            <li><strong>alpha</strong> (<em>float</em>)
                                                                                &#8211; Entropy regularization
                                                                                coefficient. (Equivalent to
                                                                                inverse of reward scale in the original
                                                                                SAC paper.)</li>
                                                                            <li><strong>batch_size</strong>
                                                                                (<em>int</em>) &#8211; Minibatch size
                                                                                for SGD.</li>
                                                                            <li><strong>start_steps</strong>
                                                                                (<em>int</em>) &#8211; Number of steps
                                                                                for uniform-random action selection,
                                                                                before running real policy. Helps
                                                                                exploration.</li>
                                                                            <li><strong>update_after</strong>
                                                                                (<em>int</em>) &#8211; Number of env
                                                                                interactions to collect before
                                                                                starting to do gradient descent updates.
                                                                                Ensures replay buffer
                                                                                is full enough for useful updates.</li>
                                                                            <li><strong>update_every</strong>
                                                                                (<em>int</em>) &#8211; Number of env
                                                                                interactions that should elapse
                                                                                between gradient descent updates. Note:
                                                                                Regardless of how long
                                                                                you wait between updates, the ratio of
                                                                                env steps to gradient steps
                                                                                is locked to 1.</li>
                                                                            <li><strong>num_test_episodes</strong>
                                                                                (<em>int</em>) &#8211; Number of
                                                                                episodes to test the deterministic
                                                                                policy at the end of each epoch.</li>
                                                                            <li><strong>max_ep_len</strong>
                                                                                (<em>int</em>) &#8211; Maximum length of
                                                                                trajectory / episode / rollout.</li>
                                                                            <li><strong>logger_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Keyword args for
                                                                                EpochLogger.</li>
                                                                            <li><strong>save_freq</strong>
                                                                                (<em>int</em>) &#8211; How often (in
                                                                                terms of gap between epochs) to save
                                                                                the current policy and value function.
                                                                            </li>
                                                                        </ul>
                                                                    </td>
                                                                </tr>
                                                            </tbody>
                                                        </table>
                                                    </dd>
                                                </dl>

                                            </div>
                                            <div class="section" id="saved-model-contents-pytorch-version">
                                                <h4><a class="toc-backref" href="#id12">Saved Model Contents: PyTorch
                                                        Version</a><a class="headerlink"
                                                        href="#saved-model-contents-pytorch-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>The PyTorch saved model can be loaded with <code
                                                        class="docutils literal"><span class="pre">ac</span> <span class="pre">=</span> <span class="pre">torch.load('path/to/model.pt')</span></code>,
                                                    yielding an actor-critic object (<code
                                                        class="docutils literal"><span class="pre">ac</span></code>)
                                                    that has the properties described in the docstring for <code
                                                        class="docutils literal"><span class="pre">sac_pytorch</span></code>.
                                                </p>
                                                <p>You can get actions from this model with</p>
                                                <div class="highlight-python">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="n">actions</span> <span class="o">=</span> <span class="n">ac</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre>
                                                    </div>
                                                </div>
                                            </div>
                                            <div class="section" id="documentation-tensorflow-version">
                                                <h4><a class="toc-backref" href="#id13">Documentation: Tensorflow
                                                        Version</a><a class="headerlink"
                                                        href="#documentation-tensorflow-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <dl class="function">
                                                    <dt id="spinup.sac_tf1">
                                                        <code class="descclassname">spinup.</code><code
                                                            class="descname">sac_tf1</code><span
                                                            class="sig-paren">(</span><em>env_fn</em>,
                                                        <em>actor_critic=&lt;function mlp_actor_critic&gt;</em>,
                                                        <em>ac_kwargs={}</em>, <em>seed=0</em>,
                                                        <em>steps_per_epoch=4000</em>, <em>epochs=100</em>,
                                                        <em>replay_size=1000000</em>, <em>gamma=0.99</em>,
                                                        <em>polyak=0.995</em>, <em>lr=0.001</em>, <em>alpha=0.2</em>,
                                                        <em>batch_size=100</em>, <em>start_steps=10000</em>,
                                                        <em>update_after=1000</em>, <em>update_every=50</em>,
                                                        <em>num_test_episodes=10</em>, <em>max_ep_len=1000</em>,
                                                        <em>logger_kwargs={}</em>, <em>save_freq=1</em><span
                                                            class="sig-paren">)</span><a class="headerlink"
                                                            href="#spinup.sac_tf1"
                                                            title="Permalink to this definition">¶</a>
                                                    </dt>
                                                    <dd>
                                                        <p>Soft Actor-Critic (SAC)</p>
                                                        <table class="docutils field-list" frame="void" rules="none">
                                                            <col class="field-name" />
                                                            <col class="field-body" />
                                                            <tbody valign="top">
                                                                <tr class="field-odd field">
                                                                    <th class="field-name">Parameters:</th>
                                                                    <td class="field-body">
                                                                        <ul class="first last simple">
                                                                            <li><strong>env_fn</strong> &#8211; A
                                                                                function which creates a copy of the
                                                                                environment.
                                                                                The environment must satisfy the OpenAI
                                                                                Gym API.</li>
                                                                            <li><strong>actor_critic</strong> &#8211;
                                                                                <p>A function which takes in placeholder
                                                                                    symbols
                                                                                    for state, <code
                                                                                        class="docutils literal"><span class="pre">x_ph</span></code>,
                                                                                    and action, <code
                                                                                        class="docutils literal"><span class="pre">a_ph</span></code>,
                                                                                    and returns the main
                                                                                    outputs from the agent&#8217;s
                                                                                    Tensorflow computation graph:</p>
                                                                                <table border="1" class="docutils">
                                                                                    <colgroup>
                                                                                        <col width="16%" />
                                                                                        <col width="23%" />
                                                                                        <col width="61%" />
                                                                                    </colgroup>
                                                                                    <thead valign="bottom">
                                                                                        <tr class="row-odd">
                                                                                            <th class="head">Symbol</th>
                                                                                            <th class="head">Shape</th>
                                                                                            <th class="head">Description
                                                                                            </th>
                                                                                        </tr>
                                                                                    </thead>
                                                                                    <tbody valign="top">
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">mu</span></code>
                                                                                            </td>
                                                                                            <td>(batch, act_dim)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Computes mean
                                                                                                        actions from
                                                                                                        policy</div>
                                                                                                    <div class="line">
                                                                                                        given states.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-odd">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">pi</span></code>
                                                                                            </td>
                                                                                            <td>(batch, act_dim)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Samples actions
                                                                                                        from policy
                                                                                                        given</div>
                                                                                                    <div class="line">
                                                                                                        states.</div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">logp_pi</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Gives log
                                                                                                        probability,
                                                                                                        according to
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        the policy, of
                                                                                                        the action
                                                                                                        sampled by</div>
                                                                                                    <div class="line">
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">pi</span></code>.
                                                                                                        Critical: must
                                                                                                        be
                                                                                                        differentiable
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        with respect to
                                                                                                        policy
                                                                                                        parameters all
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        the way through
                                                                                                        action sampling.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-odd">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">q1</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Gives one
                                                                                                        estimate of Q*
                                                                                                        for</div>
                                                                                                    <div class="line">
                                                                                                        states in <code
                                                                                                            class="docutils literal"><span class="pre">x_ph</span></code>
                                                                                                        and actions in
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">a_ph</span></code>.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                        <tr class="row-even">
                                                                                            <td><code
                                                                                                    class="docutils literal"><span class="pre">q2</span></code>
                                                                                            </td>
                                                                                            <td>(batch,)</td>
                                                                                            <td>
                                                                                                <div
                                                                                                    class="first last line-block">
                                                                                                    <div class="line">
                                                                                                        Gives another
                                                                                                        estimate of Q*
                                                                                                        for</div>
                                                                                                    <div class="line">
                                                                                                        states in <code
                                                                                                            class="docutils literal"><span class="pre">x_ph</span></code>
                                                                                                        and actions in
                                                                                                    </div>
                                                                                                    <div class="line">
                                                                                                        <code
                                                                                                            class="docutils literal"><span class="pre">a_ph</span></code>.
                                                                                                    </div>
                                                                                                </div>
                                                                                            </td>
                                                                                        </tr>
                                                                                    </tbody>
                                                                                </table>
                                                                            </li>
                                                                            <li><strong>ac_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Any kwargs
                                                                                appropriate for the actor_critic
                                                                                function you provided to SAC.</li>
                                                                            <li><strong>seed</strong> (<em>int</em>)
                                                                                &#8211; Seed for random number
                                                                                generators.</li>
                                                                            <li><strong>steps_per_epoch</strong>
                                                                                (<em>int</em>) &#8211; Number of steps
                                                                                of interaction (state-action pairs)
                                                                                for the agent and the environment in
                                                                                each epoch.</li>
                                                                            <li><strong>epochs</strong> (<em>int</em>)
                                                                                &#8211; Number of epochs to run and
                                                                                train agent.</li>
                                                                            <li><strong>replay_size</strong>
                                                                                (<em>int</em>) &#8211; Maximum length of
                                                                                replay buffer.</li>
                                                                            <li><strong>gamma</strong> (<em>float</em>)
                                                                                &#8211; Discount factor. (Always between
                                                                                0 and 1.)</li>
                                                                            <li><strong>polyak</strong> (<em>float</em>)
                                                                                &#8211; <p>Interpolation factor in
                                                                                    polyak averaging for target
                                                                                    networks. Target networks are
                                                                                    updated towards main networks
                                                                                    according to:</p>
                                                                                <div class="math">
                                                                                    <p><img src="_images/math/ea6c5fdb8bb78fe30797537bbb28553b9a7706ef.svg"
                                                                                            alt="\theta_{\text{targ}} \leftarrow
\rho \theta_{\text{targ}} + (1-\rho) \theta" /></p>
                                                                                </div>
                                                                                <p>where <img class="math"
                                                                                        src="_images/math/b41ecbab285e58fd94a9b544487b74b1d992b0dd.svg"
                                                                                        alt="\rho" /> is polyak. (Always
                                                                                    between 0 and 1, usually
                                                                                    close to 1.)</p>
                                                                            </li>
                                                                            <li><strong>lr</strong> (<em>float</em>)
                                                                                &#8211; Learning rate (used for both
                                                                                policy and value learning).</li>
                                                                            <li><strong>alpha</strong> (<em>float</em>)
                                                                                &#8211; Entropy regularization
                                                                                coefficient. (Equivalent to
                                                                                inverse of reward scale in the original
                                                                                SAC paper.)</li>
                                                                            <li><strong>batch_size</strong>
                                                                                (<em>int</em>) &#8211; Minibatch size
                                                                                for SGD.</li>
                                                                            <li><strong>start_steps</strong>
                                                                                (<em>int</em>) &#8211; Number of steps
                                                                                for uniform-random action selection,
                                                                                before running real policy. Helps
                                                                                exploration.</li>
                                                                            <li><strong>update_after</strong>
                                                                                (<em>int</em>) &#8211; Number of env
                                                                                interactions to collect before
                                                                                starting to do gradient descent updates.
                                                                                Ensures replay buffer
                                                                                is full enough for useful updates.</li>
                                                                            <li><strong>update_every</strong>
                                                                                (<em>int</em>) &#8211; Number of env
                                                                                interactions that should elapse
                                                                                between gradient descent updates. Note:
                                                                                Regardless of how long
                                                                                you wait between updates, the ratio of
                                                                                env steps to gradient steps
                                                                                is locked to 1.</li>
                                                                            <li><strong>num_test_episodes</strong>
                                                                                (<em>int</em>) &#8211; Number of
                                                                                episodes to test the deterministic
                                                                                policy at the end of each epoch.</li>
                                                                            <li><strong>max_ep_len</strong>
                                                                                (<em>int</em>) &#8211; Maximum length of
                                                                                trajectory / episode / rollout.</li>
                                                                            <li><strong>logger_kwargs</strong>
                                                                                (<em>dict</em>) &#8211; Keyword args for
                                                                                EpochLogger.</li>
                                                                            <li><strong>save_freq</strong>
                                                                                (<em>int</em>) &#8211; How often (in
                                                                                terms of gap between epochs) to save
                                                                                the current policy and value function.
                                                                            </li>
                                                                        </ul>
                                                                    </td>
                                                                </tr>
                                                            </tbody>
                                                        </table>
                                                    </dd>
                                                </dl>

                                            </div>
                                            <div class="section" id="saved-model-contents-tensorflow-version">
                                                <h4><a class="toc-backref" href="#id14">Saved Model Contents: Tensorflow
                                                        Version</a><a class="headerlink"
                                                        href="#saved-model-contents-tensorflow-version"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>The computation graph saved by the logger includes:</p>
                                                <table border="1" class="docutils">
                                                    <colgroup>
                                                        <col width="9%" />
                                                        <col width="91%" />
                                                    </colgroup>
                                                    <thead valign="bottom">
                                                        <tr class="row-odd">
                                                            <th class="head">Key</th>
                                                            <th class="head">Value</th>
                                                        </tr>
                                                    </thead>
                                                    <tbody valign="top">
                                                        <tr class="row-even">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">x</span></code>
                                                            </td>
                                                            <td>Tensorflow placeholder for state input.</td>
                                                        </tr>
                                                        <tr class="row-odd">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">a</span></code>
                                                            </td>
                                                            <td>Tensorflow placeholder for action input.</td>
                                                        </tr>
                                                        <tr class="row-even">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">mu</span></code>
                                                            </td>
                                                            <td>Deterministically computes mean action from the agent,
                                                                given states in <code
                                                                    class="docutils literal"><span class="pre">x</span></code>.
                                                            </td>
                                                        </tr>
                                                        <tr class="row-odd">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">pi</span></code>
                                                            </td>
                                                            <td>Samples an action from the agent, conditioned on states
                                                                in <code
                                                                    class="docutils literal"><span class="pre">x</span></code>.
                                                            </td>
                                                        </tr>
                                                        <tr class="row-even">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">q1</span></code>
                                                            </td>
                                                            <td>Gives one action-value estimate for states in <code
                                                                    class="docutils literal"><span class="pre">x</span></code>
                                                                and actions in <code
                                                                    class="docutils literal"><span class="pre">a</span></code>.
                                                            </td>
                                                        </tr>
                                                        <tr class="row-odd">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">q2</span></code>
                                                            </td>
                                                            <td>Gives the other action-value estimate for states in
                                                                <code
                                                                    class="docutils literal"><span class="pre">x</span></code>
                                                                and actions in <code
                                                                    class="docutils literal"><span class="pre">a</span></code>.
                                                            </td>
                                                        </tr>
                                                        <tr class="row-even">
                                                            <td><code
                                                                    class="docutils literal"><span class="pre">v</span></code>
                                                            </td>
                                                            <td>Gives the value estimate for states in <code
                                                                    class="docutils literal"><span class="pre">x</span></code>.
                                                            </td>
                                                        </tr>
                                                    </tbody>
                                                </table>
                                                <p>This saved model can be accessed either by</p>
                                                <ul class="simple">
                                                    <li>running the trained policy with the <a
                                                            class="reference external"
                                                            href="../user/saving_and_loading.html#loading-and-running-trained-policies">test_policy.py</a>
                                                        tool,</li>
                                                    <li>or loading the whole saved graph into a program with <a
                                                            class="reference external"
                                                            href="../utils/logger.html#spinup.utils.logx.restore_tf_graph">restore_tf_graph</a>.
                                                    </li>
                                                </ul>
                                                <p>Note: for SAC, the correct evaluation policy is given by <code
                                                        class="docutils literal"><span class="pre">mu</span></code> and
                                                    not by <code
                                                        class="docutils literal"><span class="pre">pi</span></code>. The
                                                    policy <code
                                                        class="docutils literal"><span class="pre">pi</span></code> may
                                                    be thought of as the exploration policy, while <code
                                                        class="docutils literal"><span class="pre">mu</span></code> is
                                                    the exploitation policy.</p>
                                            </div>
                                        </div>
                                        <div class="section" id="references">
                                            <h3><a class="toc-backref" href="#id15">References</a><a class="headerlink"
                                                    href="#references" title="Permalink to this headline">¶</a></h3>
                                            <div class="section" id="relevant-papers">
                                                <h4><a class="toc-backref" href="#id16">Relevant Papers</a><a
                                                        class="headerlink" href="#relevant-papers"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <ul class="simple">
                                                    <li><a class="reference external"
                                                            href="https://arxiv.org/pdf/1801.01290.pdf">Soft
                                                            Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement
                                                            Learning with a Stochastic Actor</a>, Haarnoja et al, 2018
                                                    </li>
                                                    <li><a class="reference external"
                                                            href="https://arxiv.org/pdf/1812.05905.pdf">Soft
                                                            Actor-Critic Algorithms and Applications</a>, Haarnoja et
                                                        al, 2018</li>
                                                    <li><a class="reference external"
                                                            href="https://arxiv.org/pdf/1812.11103.pdf">Learning to Walk
                                                            via Deep Reinforcement Learning</a>, Haarnoja et al, 2018
                                                    </li>
                                                </ul>
                                            </div>
                                            <div class="section" id="other-public-implementations">
                                                <h4><a class="toc-backref" href="#id17">Other Public
                                                        Implementations</a><a class="headerlink"
                                                        href="#other-public-implementations"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <ul class="simple">
                                                    <li><a class="reference external"
                                                            href="https://github.com/haarnoja/sac">SAC release repo</a>
                                                        (original &#8220;official&#8221; codebase)</li>
                                                    <li><a class="reference external"
                                                            href="https://github.com/rail-berkeley/softlearning">Softlearning
                                                            repo</a> (current &#8220;official&#8221; codebase)</li>
                                                    <li><a class="reference external"
                                                            href="https://github.com/denisyarats/pytorch_sac">Yarats and
                                                            Kostrikov repo</a></li>
                                                </ul>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="toctree-wrapper compound">
                                    <span id="document-utils/logger"></span>
                                    <div class="section" id="logger">
                                        <h2><a class="toc-backref" href="#id2">Logger</a><a class="headerlink"
                                                href="#logger" title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal" href="#logger" id="id2">Logger</a>
                                                    <ul>
                                                        <li><a class="reference internal" href="#using-a-logger"
                                                                id="id3">Using a Logger</a>
                                                            <ul>
                                                                <li><a class="reference internal" href="#examples"
                                                                        id="id4">Examples</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#logging-and-pytorch" id="id5">Logging and
                                                                        PyTorch</a></li>
                                                                <li><a class="reference internal"
                                                                        href="#logging-and-mpi" id="id6">Logging and
                                                                        MPI</a></li>
                                                            </ul>
                                                        </li>
                                                        <li><a class="reference internal" href="#logger-classes"
                                                                id="id7">Logger Classes</a></li>
                                                        <li><a class="reference internal"
                                                                href="#loading-saved-models-pytorch-only"
                                                                id="id8">Loading Saved Models (PyTorch Only)</a></li>
                                                        <li><a class="reference internal"
                                                                href="#loading-saved-graphs-tensorflow-only"
                                                                id="id9">Loading Saved Graphs (Tensorflow Only)</a></li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <div class="section" id="using-a-logger">
                                            <h3><a class="toc-backref" href="#id3">Using a Logger</a><a
                                                    class="headerlink" href="#using-a-logger"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>Spinning Up ships with basic logging tools, implemented in the classes <a
                                                    class="reference external"
                                                    href="../utils/logger.html#spinup.utils.logx.Logger">Logger</a> and
                                                <a class="reference external"
                                                    href="../utils/logger.html#spinup.utils.logx.EpochLogger">EpochLogger</a>.
                                                The Logger class contains most of the basic functionality for saving
                                                diagnostics, hyperparameter configurations, the state of a training run,
                                                and the trained model. The EpochLogger class adds a thin layer on top of
                                                that to make it easy to track the average, standard deviation, min, and
                                                max value of a diagnostic over each epoch and across MPI workers.</p>
                                            <div class="admonition-you-should-know admonition">
                                                <p class="first admonition-title">You Should Know</p>
                                                <p class="last">All Spinning Up algorithm implementations use an
                                                    EpochLogger.</p>
                                            </div>
                                            <div class="section" id="examples">
                                                <h4><a class="toc-backref" href="#id4">Examples</a><a class="headerlink"
                                                        href="#examples" title="Permalink to this headline">¶</a></h4>
                                                <p>First, let&#8217;s look at a simple example of how an EpochLogger
                                                    keeps track of a diagnostic value:</p>
                                                <div class="highlight-default">
                                                    <div class="highlight">
                                                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">spinup.utils.logx</span> <span class="k">import</span> <span class="n">EpochLogger</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">epoch_logger</span> <span class="o">=</span> <span class="n">EpochLogger</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
<span class="go">        epoch_logger.store(Test=i)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">epoch_logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span><span class="s1">&#39;Test&#39;</span><span class="p">,</span> <span class="n">with_min_and_max</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">epoch_logger</span><span class="o">.</span><span class="n">dump_tabular</span><span class="p">()</span>
<span class="go">-------------------------------------</span>
<span class="go">|     AverageTest |             4.5 |</span>
<span class="go">|         StdTest |            2.87 |</span>
<span class="go">|         MaxTest |               9 |</span>
<span class="go">|         MinTest |               0 |</span>
<span class="go">-------------------------------------</span>
</pre>
                                                    </div>
                                                </div>
                                                <p>The <code
                                                        class="docutils literal"><span class="pre">store</span></code>
                                                    method is used to save all values of <code
                                                        class="docutils literal"><span class="pre">Test</span></code> to
                                                    the <code
                                                        class="docutils literal"><span class="pre">epoch_logger</span></code>&#8216;s
                                                    internal state. Then, when <code
                                                        class="docutils literal"><span class="pre">log_tabular</span></code>
                                                    is called, it computes the average, standard deviation, min, and max
                                                    of <code
                                                        class="docutils literal"><span class="pre">Test</span></code>
                                                    over all of the values in the internal state. The internal state is
                                                    wiped clean after the call to <code
                                                        class="docutils literal"><span class="pre">log_tabular</span></code>
                                                    (to prevent leakage into the statistics at the next epoch). Finally,
                                                    <code
                                                        class="docutils literal"><span class="pre">dump_tabular</span></code>
                                                    is called to write the diagnostics to file and to stdout.</p>
                                                <p>Next, let&#8217;s look at a full training procedure with the logger
                                                    embedded, to highlight configuration and model saving as well as
                                                    diagnostic logging:</p>
                                                <div class="highlight-python">
                                                    <table class="highlighttable">
                                                        <tr>
                                                            <td class="linenos">
                                                                <div class="linenodiv">
                                                                    <pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69</pre>
                                                                </div>
                                                            </td>
                                                            <td class="code">
                                                                <div class="highlight">
                                                                    <pre><span></span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
 <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
 <span class="kn">import</span> <span class="nn">time</span>
 <span class="kn">from</span> <span class="nn">spinup.utils.logx</span> <span class="kn">import</span> <span class="n">EpochLogger</span>


 <span class="k">def</span> <span class="nf">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">hidden_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,),</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span> <span class="n">output_activation</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
     <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">hidden_sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
         <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>
     <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="n">hidden_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="n">output_activation</span><span class="p">)</span>


 <span class="c1"># Simple script for training an MLP on MNIST.</span>
 <span class="k">def</span> <span class="nf">train_mnist</span><span class="p">(</span><span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                 <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                 <span class="n">logger_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(),</span> <span class="n">save_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>

<span class="hll">     <span class="n">logger</span> <span class="o">=</span> <span class="n">EpochLogger</span><span class="p">(</span><span class="o">**</span><span class="n">logger_kwargs</span><span class="p">)</span>
</span><span class="hll">     <span class="n">logger</span><span class="o">.</span><span class="n">save_config</span><span class="p">(</span><span class="nb">locals</span><span class="p">())</span>
</span>
     <span class="c1"># Load and preprocess MNIST data</span>
     <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
     <span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>

     <span class="c1"># Define inputs &amp; main outputs from computation graph</span>
     <span class="n">x_ph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">))</span>
     <span class="n">y_ph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,))</span>
     <span class="n">logits</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">(</span><span class="n">x_ph</span><span class="p">,</span> <span class="n">hidden_sizes</span><span class="o">=</span><span class="p">[</span><span class="n">hidden_size</span><span class="p">]</span><span class="o">*</span><span class="n">layers</span> <span class="o">+</span> <span class="p">[</span><span class="mi">10</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
     <span class="n">predict</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

     <span class="c1"># Define loss function, accuracy, and training op</span>
     <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">y_ph</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
     <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">softmax_cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
     <span class="n">acc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">y_ph</span><span class="p">,</span> <span class="n">predict</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
     <span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">()</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

     <span class="c1"># Prepare session</span>
     <span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
     <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

     <span class="c1"># Setup model saving</span>
<span class="hll">     <span class="n">logger</span><span class="o">.</span><span class="n">setup_tf_saver</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_ph</span><span class="p">},</span>
</span><span class="hll">                                 <span class="n">outputs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;logits&#39;</span><span class="p">:</span> <span class="n">logits</span><span class="p">,</span> <span class="s1">&#39;predict&#39;</span><span class="p">:</span> <span class="n">predict</span><span class="p">})</span>
</span>
     <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

     <span class="c1"># Run main training loop</span>
     <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
         <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps_per_epoch</span><span class="p">):</span>
             <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">),</span> <span class="mi">32</span><span class="p">)</span>
             <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">x_ph</span><span class="p">:</span> <span class="n">x_train</span><span class="p">[</span><span class="n">idxs</span><span class="p">],</span>
                          <span class="n">y_ph</span><span class="p">:</span> <span class="n">y_train</span><span class="p">[</span><span class="n">idxs</span><span class="p">]}</span>
             <span class="n">outs</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="n">train_op</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>
<span class="hll">             <span class="n">logger</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">Loss</span><span class="o">=</span><span class="n">outs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Acc</span><span class="o">=</span><span class="n">outs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span>
         <span class="c1"># Save model</span>
         <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">%</span> <span class="n">save_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">==</span> <span class="n">epochs</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
<span class="hll">             <span class="n">logger</span><span class="o">.</span><span class="n">save_state</span><span class="p">(</span><span class="n">state_dict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(),</span> <span class="n">itr</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</span>
         <span class="c1"># Log info about epoch</span>
<span class="hll">         <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
</span><span class="hll">         <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span><span class="s1">&#39;Acc&#39;</span><span class="p">,</span> <span class="n">with_min_and_max</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class="hll">         <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">,</span> <span class="n">average_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class="hll">         <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span><span class="s1">&#39;TotalGradientSteps&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">steps_per_epoch</span><span class="p">)</span>
</span><span class="hll">         <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span><span class="s1">&#39;Time&#39;</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start_time</span><span class="p">)</span>
</span><span class="hll">         <span class="n">logger</span><span class="o">.</span><span class="n">dump_tabular</span><span class="p">()</span>
</span>
 <span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
     <span class="n">train_mnist</span><span class="p">()</span>
</pre>
                                                                </div>
                                                            </td>
                                                        </tr>
                                                    </table>
                                                </div>
                                                <p>In this example, observe that</p>
                                                <ul class="simple">
                                                    <li>On line 19, <a class="reference external"
                                                            href="../utils/logger.html#spinup.utils.logx.Logger.save_config">logger.save_config</a>
                                                        is used to save the hyperparameter configuration to a JSON file.
                                                    </li>
                                                    <li>On lines 42 and 43, <a class="reference external"
                                                            href="../utils/logger.html#spinup.utils.logx.Logger.setup_tf_saver">logger.setup_tf_saver</a>
                                                        is used to prepare the logger to save the key elements of the
                                                        computation graph.</li>
                                                    <li>On line 54, diagnostics are saved to the logger&#8217;s internal
                                                        state via <a class="reference external"
                                                            href="../utils/logger.html#spinup.utils.logx.EpochLogger.store">logger.store</a>.
                                                    </li>
                                                    <li>On line 58, the computation graph is saved once per epoch via <a
                                                            class="reference external"
                                                            href="../utils/logger.html#spinup.utils.logx.Logger.save_state">logger.save_state</a>.
                                                    </li>
                                                    <li>On lines 61-66, <a class="reference external"
                                                            href="../utils/logger.html#spinup.utils.logx.EpochLogger.log_tabular">logger.log_tabular</a>
                                                        and <a class="reference external"
                                                            href="../utils/logger.html#spinup.utils.logx.Logger.dump_tabular">logger.dump_tabular</a>
                                                        are used to write the epoch diagnostics to file. Note that the
                                                        keys passed into <a class="reference external"
                                                            href="../utils/logger.html#spinup.utils.logx.EpochLogger.log_tabular">logger.log_tabular</a>
                                                        are the same as the keys passed into <a
                                                            class="reference external"
                                                            href="../utils/logger.html#spinup.utils.logx.EpochLogger.store">logger.store</a>.
                                                    </li>
                                                </ul>
                                            </div>
                                            <div class="section" id="logging-and-pytorch">
                                                <h4><a class="toc-backref" href="#id5">Logging and PyTorch</a><a
                                                        class="headerlink" href="#logging-and-pytorch"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <p>The preceding example was given in Tensorflow. For PyTorch,
                                                    everything is the same except for L42-43: instead of <code
                                                        class="docutils literal"><span class="pre">logger.setup_tf_saver</span></code>,
                                                    you would use <code
                                                        class="docutils literal"><span class="pre">logger.setup_pytorch_saver</span></code>,
                                                    and you would pass it <a class="reference external"
                                                        href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">a
                                                        PyTorch module</a> (the network you are training) as an
                                                    argument.</p>
                                                <p>The behavior of <code
                                                        class="docutils literal"><span class="pre">logger.save_state</span></code>
                                                    is the same as in the Tensorflow case: each time it is called,
                                                    it&#8217;ll save the latest version of the PyTorch module.</p>
                                            </div>
                                            <div class="section" id="logging-and-mpi">
                                                <h4><a class="toc-backref" href="#id6">Logging and MPI</a><a
                                                        class="headerlink" href="#logging-and-mpi"
                                                        title="Permalink to this headline">¶</a></h4>
                                                <div class="admonition-you-should-know admonition">
                                                    <p class="first admonition-title">You Should Know</p>
                                                    <p class="last">Several algorithms in RL are easily parallelized by
                                                        using MPI to average gradients and/or other key quantities. The
                                                        Spinning Up loggers are designed to be well-behaved when using
                                                        MPI: things will only get written to stdout and to file from the
                                                        process with rank 0. But information from other processes
                                                        isn&#8217;t lost if you use the EpochLogger: everything which is
                                                        passed into EpochLogger via <code
                                                            class="docutils literal"><span class="pre">store</span></code>,
                                                        regardless of which process it&#8217;s stored in, gets used to
                                                        compute average/std/min/max values for a diagnostic.</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div class="section" id="logger-classes">
                                            <h3><a class="toc-backref" href="#id7">Logger Classes</a><a
                                                    class="headerlink" href="#logger-classes"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <dl class="class">
                                                <dt id="spinup.utils.logx.Logger">
                                                    <em class="property">class </em><code
                                                        class="descclassname">spinup.utils.logx.</code><code
                                                        class="descname">Logger</code><span
                                                        class="sig-paren">(</span><em>output_dir=None</em>,
                                                    <em>output_fname='progress.txt'</em>, <em>exp_name=None</em><span
                                                        class="sig-paren">)</span><a class="reference internal"
                                                        href="_modules/spinup/utils/logx.html#Logger"><span
                                                            class="viewcode-link">[source]</span></a><a
                                                        class="headerlink" href="#spinup.utils.logx.Logger"
                                                        title="Permalink to this definition">¶</a>
                                                </dt>
                                                <dd>
                                                    <p>A general-purpose logger.</p>
                                                    <p>Makes it easy to save diagnostics, hyperparameter configurations,
                                                        the
                                                        state of a training run, and the trained model.</p>
                                                    <dl class="method">
                                                        <dt id="spinup.utils.logx.Logger.__init__">
                                                            <code class="descname">__init__</code><span
                                                                class="sig-paren">(</span><em>output_dir=None</em>,
                                                            <em>output_fname='progress.txt'</em>,
                                                            <em>exp_name=None</em><span class="sig-paren">)</span><a
                                                                class="reference internal"
                                                                href="_modules/spinup/utils/logx.html#Logger.__init__"><span
                                                                    class="viewcode-link">[source]</span></a><a
                                                                class="headerlink"
                                                                href="#spinup.utils.logx.Logger.__init__"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p>Initialize a Logger.</p>
                                                            <table class="docutils field-list" frame="void"
                                                                rules="none">
                                                                <col class="field-name" />
                                                                <col class="field-body" />
                                                                <tbody valign="top">
                                                                    <tr class="field-odd field">
                                                                        <th class="field-name">Parameters:</th>
                                                                        <td class="field-body">
                                                                            <ul class="first last simple">
                                                                                <li><strong>output_dir</strong>
                                                                                    (<em>string</em>) &#8211; A
                                                                                    directory for saving results to. If
                                                                                    <code
                                                                                        class="docutils literal"><span class="pre">None</span></code>,
                                                                                    defaults to a temp directory of the
                                                                                    form
                                                                                    <code
                                                                                        class="docutils literal"><span class="pre">/tmp/experiments/somerandomnumber</span></code>.
                                                                                </li>
                                                                                <li><strong>output_fname</strong>
                                                                                    (<em>string</em>) &#8211; Name for
                                                                                    the tab-separated-value file
                                                                                    containing metrics logged throughout
                                                                                    a training run.
                                                                                    Defaults to <code
                                                                                        class="docutils literal"><span class="pre">progress.txt</span></code>.
                                                                                </li>
                                                                                <li><strong>exp_name</strong>
                                                                                    (<em>string</em>) &#8211; Experiment
                                                                                    name. If you run multiple training
                                                                                    runs and give them all the same
                                                                                    <code
                                                                                        class="docutils literal"><span class="pre">exp_name</span></code>,
                                                                                    the plotter
                                                                                    will know to group them. (Use case:
                                                                                    if you run the same
                                                                                    hyperparameter configuration with
                                                                                    multiple random seeds, you
                                                                                    should give them all the same <code
                                                                                        class="docutils literal"><span class="pre">exp_name</span></code>.)
                                                                                </li>
                                                                            </ul>
                                                                        </td>
                                                                    </tr>
                                                                </tbody>
                                                            </table>
                                                        </dd>
                                                    </dl>

                                                    <dl class="method">
                                                        <dt id="spinup.utils.logx.Logger.dump_tabular">
                                                            <code class="descname">dump_tabular</code><span
                                                                class="sig-paren">(</span><span
                                                                class="sig-paren">)</span><a class="reference internal"
                                                                href="_modules/spinup/utils/logx.html#Logger.dump_tabular"><span
                                                                    class="viewcode-link">[source]</span></a><a
                                                                class="headerlink"
                                                                href="#spinup.utils.logx.Logger.dump_tabular"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p>Write all of the diagnostics from the current iteration.
                                                            </p>
                                                            <p>Writes both to stdout, and to the output file.</p>
                                                        </dd>
                                                    </dl>

                                                    <dl class="method">
                                                        <dt id="spinup.utils.logx.Logger.log">
                                                            <code class="descname">log</code><span
                                                                class="sig-paren">(</span><em>msg</em>,
                                                            <em>color='green'</em><span class="sig-paren">)</span><a
                                                                class="reference internal"
                                                                href="_modules/spinup/utils/logx.html#Logger.log"><span
                                                                    class="viewcode-link">[source]</span></a><a
                                                                class="headerlink" href="#spinup.utils.logx.Logger.log"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p>Print a colorized message to stdout.</p>
                                                        </dd>
                                                    </dl>

                                                    <dl class="method">
                                                        <dt id="spinup.utils.logx.Logger.log_tabular">
                                                            <code class="descname">log_tabular</code><span
                                                                class="sig-paren">(</span><em>key</em>,
                                                            <em>val</em><span class="sig-paren">)</span><a
                                                                class="reference internal"
                                                                href="_modules/spinup/utils/logx.html#Logger.log_tabular"><span
                                                                    class="viewcode-link">[source]</span></a><a
                                                                class="headerlink"
                                                                href="#spinup.utils.logx.Logger.log_tabular"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p>Log a value of some diagnostic.</p>
                                                            <p>Call this only once for each diagnostic quantity, each
                                                                iteration.
                                                                After using <code
                                                                    class="docutils literal"><span class="pre">log_tabular</span></code>
                                                                to store values for each diagnostic,
                                                                make sure to call <code
                                                                    class="docutils literal"><span class="pre">dump_tabular</span></code>
                                                                to write them out to file and
                                                                stdout (otherwise they will not get saved anywhere).</p>
                                                        </dd>
                                                    </dl>

                                                    <dl class="method">
                                                        <dt id="spinup.utils.logx.Logger.save_config">
                                                            <code class="descname">save_config</code><span
                                                                class="sig-paren">(</span><em>config</em><span
                                                                class="sig-paren">)</span><a class="reference internal"
                                                                href="_modules/spinup/utils/logx.html#Logger.save_config"><span
                                                                    class="viewcode-link">[source]</span></a><a
                                                                class="headerlink"
                                                                href="#spinup.utils.logx.Logger.save_config"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p>Log an experiment configuration.</p>
                                                            <p>Call this once at the top of your experiment, passing in
                                                                all important
                                                                config vars as a dict. This will serialize the config to
                                                                JSON, while
                                                                handling anything which can&#8217;t be serialized in a
                                                                graceful way (writing
                                                                as informative a string as possible).</p>
                                                            <p>Example use:</p>
                                                            <div class="highlight-python">
                                                                <div class="highlight">
                                                                    <pre><span></span><span class="n">logger</span> <span class="o">=</span> <span class="n">EpochLogger</span><span class="p">(</span><span class="o">**</span><span class="n">logger_kwargs</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">save_config</span><span class="p">(</span><span class="nb">locals</span><span class="p">())</span>
</pre>
                                                                </div>
                                                            </div>
                                                        </dd>
                                                    </dl>

                                                    <dl class="method">
                                                        <dt id="spinup.utils.logx.Logger.save_state">
                                                            <code class="descname">save_state</code><span
                                                                class="sig-paren">(</span><em>state_dict</em>,
                                                            <em>itr=None</em><span class="sig-paren">)</span><a
                                                                class="reference internal"
                                                                href="_modules/spinup/utils/logx.html#Logger.save_state"><span
                                                                    class="viewcode-link">[source]</span></a><a
                                                                class="headerlink"
                                                                href="#spinup.utils.logx.Logger.save_state"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p>Saves the state of an experiment.</p>
                                                            <p>To be clear: this is about saving <em>state</em>, not
                                                                logging diagnostics.
                                                                All diagnostic logging is separate from this function.
                                                                This function
                                                                will save whatever is in <code
                                                                    class="docutils literal"><span class="pre">state_dict</span></code>&#8212;usually
                                                                just a copy of the
                                                                environment&#8212;and the most recent parameters for the
                                                                model you
                                                                previously set up saving for with <code
                                                                    class="docutils literal"><span class="pre">setup_tf_saver</span></code>.
                                                            </p>
                                                            <p>Call with any frequency you prefer. If you only want to
                                                                maintain a
                                                                single state and overwrite it at each call with the most
                                                                recent
                                                                version, leave <code
                                                                    class="docutils literal"><span class="pre">itr=None</span></code>.
                                                                If you want to keep all of the states you
                                                                save, provide unique (increasing) values for
                                                                &#8216;itr&#8217;.</p>
                                                            <table class="docutils field-list" frame="void"
                                                                rules="none">
                                                                <col class="field-name" />
                                                                <col class="field-body" />
                                                                <tbody valign="top">
                                                                    <tr class="field-odd field">
                                                                        <th class="field-name">Parameters:</th>
                                                                        <td class="field-body">
                                                                            <ul class="first last simple">
                                                                                <li><strong>state_dict</strong>
                                                                                    (<em>dict</em>) &#8211; Dictionary
                                                                                    containing essential elements to
                                                                                    describe the current state of
                                                                                    training.</li>
                                                                                <li><strong>itr</strong> &#8211; An int,
                                                                                    or None. Current iteration of
                                                                                    training.</li>
                                                                            </ul>
                                                                        </td>
                                                                    </tr>
                                                                </tbody>
                                                            </table>
                                                        </dd>
                                                    </dl>

                                                    <dl class="method">
                                                        <dt id="spinup.utils.logx.Logger.setup_pytorch_saver">
                                                            <code class="descname">setup_pytorch_saver</code><span
                                                                class="sig-paren">(</span><em>what_to_save</em><span
                                                                class="sig-paren">)</span><a class="reference internal"
                                                                href="_modules/spinup/utils/logx.html#Logger.setup_pytorch_saver"><span
                                                                    class="viewcode-link">[source]</span></a><a
                                                                class="headerlink"
                                                                href="#spinup.utils.logx.Logger.setup_pytorch_saver"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p>Set up easy model saving for a single PyTorch model.</p>
                                                            <p>Because PyTorch saving and loading is especially
                                                                painless, this is
                                                                very minimal; we just need references to whatever we
                                                                would like to
                                                                pickle. This is integrated into the logger because the
                                                                logger
                                                                knows where the user would like to save information
                                                                about this
                                                                training run.</p>
                                                            <table class="docutils field-list" frame="void"
                                                                rules="none">
                                                                <col class="field-name" />
                                                                <col class="field-body" />
                                                                <tbody valign="top">
                                                                    <tr class="field-odd field">
                                                                        <th class="field-name">Parameters:</th>
                                                                        <td class="field-body">
                                                                            <strong>what_to_save</strong> &#8211; Any
                                                                            PyTorch model or serializable object
                                                                            containing
                                                                            PyTorch models.</td>
                                                                    </tr>
                                                                </tbody>
                                                            </table>
                                                        </dd>
                                                    </dl>

                                                    <dl class="method">
                                                        <dt id="spinup.utils.logx.Logger.setup_tf_saver">
                                                            <code class="descname">setup_tf_saver</code><span
                                                                class="sig-paren">(</span><em>sess</em>,
                                                            <em>inputs</em>, <em>outputs</em><span
                                                                class="sig-paren">)</span><a class="reference internal"
                                                                href="_modules/spinup/utils/logx.html#Logger.setup_tf_saver"><span
                                                                    class="viewcode-link">[source]</span></a><a
                                                                class="headerlink"
                                                                href="#spinup.utils.logx.Logger.setup_tf_saver"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p>Set up easy model saving for tensorflow.</p>
                                                            <p>Call once, after defining your computation graph but
                                                                before training.</p>
                                                            <table class="docutils field-list" frame="void"
                                                                rules="none">
                                                                <col class="field-name" />
                                                                <col class="field-body" />
                                                                <tbody valign="top">
                                                                    <tr class="field-odd field">
                                                                        <th class="field-name">Parameters:</th>
                                                                        <td class="field-body">
                                                                            <ul class="first last simple">
                                                                                <li><strong>sess</strong> &#8211; The
                                                                                    Tensorflow session in which you
                                                                                    train your computation
                                                                                    graph.</li>
                                                                                <li><strong>inputs</strong>
                                                                                    (<em>dict</em>) &#8211; A dictionary
                                                                                    that maps from keys of your choice
                                                                                    to the tensorflow placeholders that
                                                                                    serve as inputs to the
                                                                                    computation graph. Make sure that
                                                                                    <em>all</em> of the placeholders
                                                                                    needed for your outputs are
                                                                                    included!</li>
                                                                                <li><strong>outputs</strong>
                                                                                    (<em>dict</em>) &#8211; A dictionary
                                                                                    that maps from keys of your choice
                                                                                    to the outputs from your computation
                                                                                    graph.</li>
                                                                            </ul>
                                                                        </td>
                                                                    </tr>
                                                                </tbody>
                                                            </table>
                                                        </dd>
                                                    </dl>

                                                </dd>
                                            </dl>

                                            <dl class="class">
                                                <dt id="spinup.utils.logx.EpochLogger">
                                                    <em class="property">class </em><code
                                                        class="descclassname">spinup.utils.logx.</code><code
                                                        class="descname">EpochLogger</code><span
                                                        class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span
                                                        class="sig-paren">)</span><a class="reference internal"
                                                        href="_modules/spinup/utils/logx.html#EpochLogger"><span
                                                            class="viewcode-link">[source]</span></a><a
                                                        class="headerlink" href="#spinup.utils.logx.EpochLogger"
                                                        title="Permalink to this definition">¶</a>
                                                </dt>
                                                <dd>
                                                    <p>Bases: <a class="reference internal"
                                                            href="#spinup.utils.logx.Logger"
                                                            title="spinup.utils.logx.Logger"><code
                                                                class="xref py py-class docutils literal"><span class="pre">spinup.utils.logx.Logger</span></code></a>
                                                    </p>
                                                    <p>A variant of Logger tailored for tracking average values over
                                                        epochs.</p>
                                                    <p>Typical use case: there is some quantity which is calculated many
                                                        times
                                                        throughout an epoch, and at the end of the epoch, you would like
                                                        to
                                                        report the average / std / min / max value of that quantity.</p>
                                                    <p>With an EpochLogger, each time the quantity is calculated, you
                                                        would
                                                        use</p>
                                                    <div class="highlight-python">
                                                        <div class="highlight">
                                                            <pre><span></span><span class="n">epoch_logger</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">NameOfQuantity</span><span class="o">=</span><span class="n">quantity_value</span><span class="p">)</span>
</pre>
                                                        </div>
                                                    </div>
                                                    <p>to load it into the EpochLogger&#8217;s state. Then at the end of
                                                        the epoch, you
                                                        would use</p>
                                                    <div class="highlight-python">
                                                        <div class="highlight">
                                                            <pre><span></span><span class="n">epoch_logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span><span class="n">NameOfQuantity</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">)</span>
</pre>
                                                        </div>
                                                    </div>
                                                    <p>to record the desired values.</p>
                                                    <dl class="method">
                                                        <dt id="spinup.utils.logx.EpochLogger.get_stats">
                                                            <code class="descname">get_stats</code><span
                                                                class="sig-paren">(</span><em>key</em><span
                                                                class="sig-paren">)</span><a class="reference internal"
                                                                href="_modules/spinup/utils/logx.html#EpochLogger.get_stats"><span
                                                                    class="viewcode-link">[source]</span></a><a
                                                                class="headerlink"
                                                                href="#spinup.utils.logx.EpochLogger.get_stats"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p>Lets an algorithm ask the logger for mean/std/min/max of
                                                                a diagnostic.</p>
                                                        </dd>
                                                    </dl>

                                                    <dl class="method">
                                                        <dt id="spinup.utils.logx.EpochLogger.log_tabular">
                                                            <code class="descname">log_tabular</code><span
                                                                class="sig-paren">(</span><em>key</em>,
                                                            <em>val=None</em>, <em>with_min_and_max=False</em>,
                                                            <em>average_only=False</em><span
                                                                class="sig-paren">)</span><a class="reference internal"
                                                                href="_modules/spinup/utils/logx.html#EpochLogger.log_tabular"><span
                                                                    class="viewcode-link">[source]</span></a><a
                                                                class="headerlink"
                                                                href="#spinup.utils.logx.EpochLogger.log_tabular"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p>Log a value or possibly the mean/std/min/max values of a
                                                                diagnostic.</p>
                                                            <table class="docutils field-list" frame="void"
                                                                rules="none">
                                                                <col class="field-name" />
                                                                <col class="field-body" />
                                                                <tbody valign="top">
                                                                    <tr class="field-odd field">
                                                                        <th class="field-name">Parameters:</th>
                                                                        <td class="field-body">
                                                                            <ul class="first last simple">
                                                                                <li><strong>key</strong>
                                                                                    (<em>string</em>) &#8211; The name
                                                                                    of the diagnostic. If you are
                                                                                    logging a
                                                                                    diagnostic whose state has
                                                                                    previously been saved with
                                                                                    <code
                                                                                        class="docutils literal"><span class="pre">store</span></code>,
                                                                                    the key here has to match the key
                                                                                    you used there.
                                                                                </li>
                                                                                <li><strong>val</strong> &#8211; A value
                                                                                    for the diagnostic. If you have
                                                                                    previously saved
                                                                                    values for this key via <code
                                                                                        class="docutils literal"><span class="pre">store</span></code>,
                                                                                    do <em>not</em> provide a <code
                                                                                        class="docutils literal"><span class="pre">val</span></code>
                                                                                    here.</li>
                                                                                <li><strong>with_min_and_max</strong>
                                                                                    (<em>bool</em>) &#8211; If true, log
                                                                                    min and max values of the
                                                                                    diagnostic over the epoch.</li>
                                                                                <li><strong>average_only</strong>
                                                                                    (<em>bool</em>) &#8211; If true, do
                                                                                    not log the standard deviation
                                                                                    of the diagnostic over the epoch.
                                                                                </li>
                                                                            </ul>
                                                                        </td>
                                                                    </tr>
                                                                </tbody>
                                                            </table>
                                                        </dd>
                                                    </dl>

                                                    <dl class="method">
                                                        <dt id="spinup.utils.logx.EpochLogger.store">
                                                            <code class="descname">store</code><span
                                                                class="sig-paren">(</span><em>**kwargs</em><span
                                                                class="sig-paren">)</span><a class="reference internal"
                                                                href="_modules/spinup/utils/logx.html#EpochLogger.store"><span
                                                                    class="viewcode-link">[source]</span></a><a
                                                                class="headerlink"
                                                                href="#spinup.utils.logx.EpochLogger.store"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p>Save something into the epoch_logger&#8217;s current
                                                                state.</p>
                                                            <p>Provide an arbitrary number of keyword arguments with
                                                                numerical
                                                                values.</p>
                                                        </dd>
                                                    </dl>

                                                </dd>
                                            </dl>

                                        </div>
                                        <div class="section" id="loading-saved-models-pytorch-only">
                                            <h3><a class="toc-backref" href="#id8">Loading Saved Models (PyTorch
                                                    Only)</a><a class="headerlink"
                                                    href="#loading-saved-models-pytorch-only"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>To load an actor-critic model saved by a PyTorch Spinning Up
                                                implementation, run:</p>
                                            <div class="highlight-python">
                                                <div class="highlight">
                                                    <pre><span></span><span class="n">ac</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;path/to/model.pt&#39;</span><span class="p">)</span>
</pre>
                                                </div>
                                            </div>
                                            <p>When you use this method to load an actor-critic model, you can minimally
                                                expect it to have an <code
                                                    class="docutils literal"><span class="pre">act</span></code> method
                                                that allows you to sample actions from the policy, given observations:
                                            </p>
                                            <div class="highlight-python">
                                                <div class="highlight">
                                                    <pre><span></span><span class="n">actions</span> <span class="o">=</span> <span class="n">ac</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre>
                                                </div>
                                            </div>
                                        </div>
                                        <div class="section" id="loading-saved-graphs-tensorflow-only">
                                            <h3><a class="toc-backref" href="#id9">Loading Saved Graphs (Tensorflow
                                                    Only)</a><a class="headerlink"
                                                    href="#loading-saved-graphs-tensorflow-only"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <dl class="function">
                                                <dt id="spinup.utils.logx.restore_tf_graph">
                                                    <code class="descclassname">spinup.utils.logx.</code><code
                                                        class="descname">restore_tf_graph</code><span
                                                        class="sig-paren">(</span><em>sess</em>, <em>fpath</em><span
                                                        class="sig-paren">)</span><a class="reference internal"
                                                        href="_modules/spinup/utils/logx.html#restore_tf_graph"><span
                                                            class="viewcode-link">[source]</span></a><a
                                                        class="headerlink" href="#spinup.utils.logx.restore_tf_graph"
                                                        title="Permalink to this definition">¶</a>
                                                </dt>
                                                <dd>
                                                    <p>Loads graphs saved by Logger.</p>
                                                    <p>Will output a dictionary whose keys and values are from the
                                                        &#8216;inputs&#8217;
                                                        and &#8216;outputs&#8217; dict you specified with
                                                        logger.setup_tf_saver().</p>
                                                    <table class="docutils field-list" frame="void" rules="none">
                                                        <col class="field-name" />
                                                        <col class="field-body" />
                                                        <tbody valign="top">
                                                            <tr class="field-odd field">
                                                                <th class="field-name">Parameters:</th>
                                                                <td class="field-body">
                                                                    <ul class="first simple">
                                                                        <li><strong>sess</strong> &#8211; A Tensorflow
                                                                            session.</li>
                                                                        <li><strong>fpath</strong> &#8211; Filepath to
                                                                            save directory.</li>
                                                                    </ul>
                                                                </td>
                                                            </tr>
                                                            <tr class="field-even field">
                                                                <th class="field-name">Returns:</th>
                                                                <td class="field-body">
                                                                    <p class="first last">A dictionary mapping from keys
                                                                        to tensors in the computation graph
                                                                        loaded from <code
                                                                            class="docutils literal"><span class="pre">fpath</span></code>.
                                                                    </p>
                                                                </td>
                                                            </tr>
                                                        </tbody>
                                                    </table>
                                                </dd>
                                            </dl>

                                            <p>When you use this method to restore a graph saved by a Tensorflow
                                                Spinning Up implementation, you can minimally expect it to include the
                                                following:</p>
                                            <table border="1" class="docutils">
                                                <colgroup>
                                                    <col width="11%" />
                                                    <col width="89%" />
                                                </colgroup>
                                                <thead valign="bottom">
                                                    <tr class="row-odd">
                                                        <th class="head">Key</th>
                                                        <th class="head">Value</th>
                                                    </tr>
                                                </thead>
                                                <tbody valign="top">
                                                    <tr class="row-even">
                                                        <td><code
                                                                class="docutils literal"><span class="pre">x</span></code>
                                                        </td>
                                                        <td>Tensorflow placeholder for state input.</td>
                                                    </tr>
                                                    <tr class="row-odd">
                                                        <td><code
                                                                class="docutils literal"><span class="pre">pi</span></code>
                                                        </td>
                                                        <td>
                                                            <div class="first last line-block">
                                                                <div class="line">Samples an action from the agent,
                                                                    conditioned</div>
                                                                <div class="line">on states in <code
                                                                        class="docutils literal"><span class="pre">x</span></code>.
                                                                </div>
                                                            </div>
                                                        </td>
                                                    </tr>
                                                </tbody>
                                            </table>
                                            <p>The relevant value functions for an algorithm are also typically stored.
                                                For details of what else gets saved by a given algorithm, see its
                                                documentation page.</p>
                                        </div>
                                    </div>
                                    <span id="document-utils/plotter"></span>
                                    <div class="section" id="plotter">
                                        <h2>Plotter<a class="headerlink" href="#plotter"
                                                title="Permalink to this headline">¶</a></h2>
                                        <p>See the page on <a class="reference external"
                                                href="../user/plotting.html">plotting results</a> for documentation of
                                            the plotter.</p>
                                    </div>
                                    <span id="document-utils/mpi"></span>
                                    <div class="section" id="mpi-tools">
                                        <h2><a class="toc-backref" href="#id1">MPI Tools</a><a class="headerlink"
                                                href="#mpi-tools" title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal" href="#mpi-tools" id="id1">MPI
                                                        Tools</a>
                                                    <ul>
                                                        <li><a class="reference internal"
                                                                href="#module-spinup.utils.mpi_tools" id="id2">Core MPI
                                                                Utilities</a></li>
                                                        <li><a class="reference internal" href="#mpi-pytorch-utilities"
                                                                id="id3">MPI + PyTorch Utilities</a></li>
                                                        <li><a class="reference internal"
                                                                href="#mpi-tensorflow-utilities" id="id4">MPI +
                                                                Tensorflow Utilities</a></li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <div class="section" id="module-spinup.utils.mpi_tools">
                                            <span id="core-mpi-utilities"></span>
                                            <h3><a class="toc-backref" href="#id2">Core MPI Utilities</a><a
                                                    class="headerlink" href="#module-spinup.utils.mpi_tools"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <dl class="function">
                                                <dt id="spinup.utils.mpi_tools.mpi_avg">
                                                    <code class="descclassname">spinup.utils.mpi_tools.</code><code
                                                        class="descname">mpi_avg</code><span
                                                        class="sig-paren">(</span><em>x</em><span
                                                        class="sig-paren">)</span><a class="reference internal"
                                                        href="_modules/spinup/utils/mpi_tools.html#mpi_avg"><span
                                                            class="viewcode-link">[source]</span></a><a
                                                        class="headerlink" href="#spinup.utils.mpi_tools.mpi_avg"
                                                        title="Permalink to this definition">¶</a>
                                                </dt>
                                                <dd>
                                                    <p>Average a scalar or vector over MPI processes.</p>
                                                </dd>
                                            </dl>

                                            <dl class="function">
                                                <dt id="spinup.utils.mpi_tools.mpi_fork">
                                                    <code class="descclassname">spinup.utils.mpi_tools.</code><code
                                                        class="descname">mpi_fork</code><span
                                                        class="sig-paren">(</span><em>n</em>,
                                                    <em>bind_to_core=False</em><span class="sig-paren">)</span><a
                                                        class="reference internal"
                                                        href="_modules/spinup/utils/mpi_tools.html#mpi_fork"><span
                                                            class="viewcode-link">[source]</span></a><a
                                                        class="headerlink" href="#spinup.utils.mpi_tools.mpi_fork"
                                                        title="Permalink to this definition">¶</a>
                                                </dt>
                                                <dd>
                                                    <p>Re-launches the current script with workers linked by MPI.</p>
                                                    <p>Also, terminates the original process that launched it.</p>
                                                    <p>Taken almost without modification from the Baselines function of
                                                        the
                                                        <a class="reference external"
                                                            href="https://github.com/openai/baselines/blob/master/baselines/common/mpi_fork.py">same
                                                            name</a>.
                                                    </p>
                                                    <table class="docutils field-list" frame="void" rules="none">
                                                        <col class="field-name" />
                                                        <col class="field-body" />
                                                        <tbody valign="top">
                                                            <tr class="field-odd field">
                                                                <th class="field-name">Parameters:</th>
                                                                <td class="field-body">
                                                                    <ul class="first last simple">
                                                                        <li><strong>n</strong> (<em>int</em>) &#8211;
                                                                            Number of process to split into.</li>
                                                                        <li><strong>bind_to_core</strong>
                                                                            (<em>bool</em>) &#8211; Bind each MPI
                                                                            process to a core.</li>
                                                                    </ul>
                                                                </td>
                                                            </tr>
                                                        </tbody>
                                                    </table>
                                                </dd>
                                            </dl>

                                            <dl class="function">
                                                <dt id="spinup.utils.mpi_tools.mpi_statistics_scalar">
                                                    <code class="descclassname">spinup.utils.mpi_tools.</code><code
                                                        class="descname">mpi_statistics_scalar</code><span
                                                        class="sig-paren">(</span><em>x</em>,
                                                    <em>with_min_and_max=False</em><span class="sig-paren">)</span><a
                                                        class="reference internal"
                                                        href="_modules/spinup/utils/mpi_tools.html#mpi_statistics_scalar"><span
                                                            class="viewcode-link">[source]</span></a><a
                                                        class="headerlink"
                                                        href="#spinup.utils.mpi_tools.mpi_statistics_scalar"
                                                        title="Permalink to this definition">¶</a>
                                                </dt>
                                                <dd>
                                                    <p>Get mean/std and optional min/max of scalar x across MPI
                                                        processes.</p>
                                                    <table class="docutils field-list" frame="void" rules="none">
                                                        <col class="field-name" />
                                                        <col class="field-body" />
                                                        <tbody valign="top">
                                                            <tr class="field-odd field">
                                                                <th class="field-name">Parameters:</th>
                                                                <td class="field-body">
                                                                    <ul class="first last simple">
                                                                        <li><strong>x</strong> &#8211; An array
                                                                            containing samples of the scalar to produce
                                                                            statistics
                                                                            for.</li>
                                                                        <li><strong>with_min_and_max</strong>
                                                                            (<em>bool</em>) &#8211; If true, return min
                                                                            and max of x in
                                                                            addition to mean and std.</li>
                                                                    </ul>
                                                                </td>
                                                            </tr>
                                                        </tbody>
                                                    </table>
                                                </dd>
                                            </dl>

                                            <dl class="function">
                                                <dt id="spinup.utils.mpi_tools.num_procs">
                                                    <code class="descclassname">spinup.utils.mpi_tools.</code><code
                                                        class="descname">num_procs</code><span
                                                        class="sig-paren">(</span><span class="sig-paren">)</span><a
                                                        class="reference internal"
                                                        href="_modules/spinup/utils/mpi_tools.html#num_procs"><span
                                                            class="viewcode-link">[source]</span></a><a
                                                        class="headerlink" href="#spinup.utils.mpi_tools.num_procs"
                                                        title="Permalink to this definition">¶</a>
                                                </dt>
                                                <dd>
                                                    <p>Count active MPI processes.</p>
                                                </dd>
                                            </dl>

                                            <dl class="function">
                                                <dt id="spinup.utils.mpi_tools.proc_id">
                                                    <code class="descclassname">spinup.utils.mpi_tools.</code><code
                                                        class="descname">proc_id</code><span
                                                        class="sig-paren">(</span><span class="sig-paren">)</span><a
                                                        class="reference internal"
                                                        href="_modules/spinup/utils/mpi_tools.html#proc_id"><span
                                                            class="viewcode-link">[source]</span></a><a
                                                        class="headerlink" href="#spinup.utils.mpi_tools.proc_id"
                                                        title="Permalink to this definition">¶</a>
                                                </dt>
                                                <dd>
                                                    <p>Get rank of calling process.</p>
                                                </dd>
                                            </dl>

                                        </div>
                                        <div class="section" id="mpi-pytorch-utilities">
                                            <h3><a class="toc-backref" href="#id3">MPI + PyTorch Utilities</a><a
                                                    class="headerlink" href="#mpi-pytorch-utilities"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p><code
                                                    class="docutils literal"><span class="pre">spinup.utils.mpi_pytorch</span></code>
                                                contains a few tools to make it easy to do data-parallel PyTorch
                                                optimization across MPI processes. The two main ingredients are syncing
                                                parameters and averaging gradients before they are used by the adaptive
                                                optimizer. Also there&#8217;s a hacky fix for a problem where the
                                                PyTorch instance in each separate process tries to get too many threads,
                                                and they start to clobber each other.</p>
                                            <p>The pattern for using these tools looks something like this:</p>
                                            <ol class="arabic simple">
                                                <li>At the beginning of the training script, call <code
                                                        class="docutils literal"><span class="pre">setup_pytorch_for_mpi()</span></code>.
                                                    (Avoids clobbering problem.)</li>
                                                <li>After you&#8217;ve constructed a PyTorch module, call <code
                                                        class="docutils literal"><span class="pre">sync_params(module)</span></code>.
                                                </li>
                                                <li>Then, during gradient descent, call <code
                                                        class="docutils literal"><span class="pre">mpi_avg_grads</span></code>
                                                    after the backward pass, like so:</li>
                                            </ol>
                                            <div class="highlight-python">
                                                <div class="highlight">
                                                    <pre><span></span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">mpi_avg_grads</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>   <span class="c1"># averages gradient buffers across MPI processes!</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre>
                                                </div>
                                            </div>
                                            <span class="target" id="module-spinup.utils.mpi_pytorch"></span>
                                            <dl class="function">
                                                <dt id="spinup.utils.mpi_pytorch.mpi_avg_grads">
                                                    <code class="descclassname">spinup.utils.mpi_pytorch.</code><code
                                                        class="descname">mpi_avg_grads</code><span
                                                        class="sig-paren">(</span><em>module</em><span
                                                        class="sig-paren">)</span><a class="reference internal"
                                                        href="_modules/spinup/utils/mpi_pytorch.html#mpi_avg_grads"><span
                                                            class="viewcode-link">[source]</span></a><a
                                                        class="headerlink"
                                                        href="#spinup.utils.mpi_pytorch.mpi_avg_grads"
                                                        title="Permalink to this definition">¶</a>
                                                </dt>
                                                <dd>
                                                    <p>Average contents of gradient buffers across MPI processes.</p>
                                                </dd>
                                            </dl>

                                            <dl class="function">
                                                <dt id="spinup.utils.mpi_pytorch.setup_pytorch_for_mpi">
                                                    <code class="descclassname">spinup.utils.mpi_pytorch.</code><code
                                                        class="descname">setup_pytorch_for_mpi</code><span
                                                        class="sig-paren">(</span><span class="sig-paren">)</span><a
                                                        class="reference internal"
                                                        href="_modules/spinup/utils/mpi_pytorch.html#setup_pytorch_for_mpi"><span
                                                            class="viewcode-link">[source]</span></a><a
                                                        class="headerlink"
                                                        href="#spinup.utils.mpi_pytorch.setup_pytorch_for_mpi"
                                                        title="Permalink to this definition">¶</a>
                                                </dt>
                                                <dd>
                                                    <p>Avoid slowdowns caused by each separate process&#8217;s PyTorch
                                                        using
                                                        more than its fair share of CPU resources.</p>
                                                </dd>
                                            </dl>

                                            <dl class="function">
                                                <dt id="spinup.utils.mpi_pytorch.sync_params">
                                                    <code class="descclassname">spinup.utils.mpi_pytorch.</code><code
                                                        class="descname">sync_params</code><span
                                                        class="sig-paren">(</span><em>module</em><span
                                                        class="sig-paren">)</span><a class="reference internal"
                                                        href="_modules/spinup/utils/mpi_pytorch.html#sync_params"><span
                                                            class="viewcode-link">[source]</span></a><a
                                                        class="headerlink" href="#spinup.utils.mpi_pytorch.sync_params"
                                                        title="Permalink to this definition">¶</a>
                                                </dt>
                                                <dd>
                                                    <p>Sync all parameters of module across all MPI processes.</p>
                                                </dd>
                                            </dl>

                                        </div>
                                        <div class="section" id="mpi-tensorflow-utilities">
                                            <h3><a class="toc-backref" href="#id4">MPI + Tensorflow Utilities</a><a
                                                    class="headerlink" href="#mpi-tensorflow-utilities"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>The <code
                                                    class="docutils literal"><span class="pre">spinup.utils.mpi_tf</span></code>
                                                contains a a few tools to make it easy to use the AdamOptimizer across
                                                many MPI processes. This is a bit hacky&#8212;if you&#8217;re looking
                                                for something more sophisticated and general-purpose, consider <a
                                                    class="reference external"
                                                    href="https://github.com/uber/horovod">horovod</a>.</p>
                                            <span class="target" id="module-spinup.utils.mpi_tf"></span>
                                            <dl class="class">
                                                <dt id="spinup.utils.mpi_tf.MpiAdamOptimizer">
                                                    <em class="property">class </em><code
                                                        class="descclassname">spinup.utils.mpi_tf.</code><code
                                                        class="descname">MpiAdamOptimizer</code><span
                                                        class="sig-paren">(</span><em>**kwargs</em><span
                                                        class="sig-paren">)</span><a class="reference internal"
                                                        href="_modules/spinup/utils/mpi_tf.html#MpiAdamOptimizer"><span
                                                            class="viewcode-link">[source]</span></a><a
                                                        class="headerlink" href="#spinup.utils.mpi_tf.MpiAdamOptimizer"
                                                        title="Permalink to this definition">¶</a>
                                                </dt>
                                                <dd>
                                                    <p>Adam optimizer that averages gradients across MPI processes.</p>
                                                    <p>The compute_gradients method is taken from Baselines <a
                                                            class="reference external"
                                                            href="https://github.com/openai/baselines/blob/master/baselines/common/mpi_adam_optimizer.py">MpiAdamOptimizer</a>.
                                                        For documentation on method arguments, see the Tensorflow docs
                                                        page for
                                                        the base <a class="reference external"
                                                            href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer">AdamOptimizer</a>.
                                                    </p>
                                                    <dl class="method">
                                                        <dt id="spinup.utils.mpi_tf.MpiAdamOptimizer.apply_gradients">
                                                            <code class="descname">apply_gradients</code><span
                                                                class="sig-paren">(</span><em>grads_and_vars</em>,
                                                            <em>global_step=None</em>, <em>name=None</em><span
                                                                class="sig-paren">)</span><a class="reference internal"
                                                                href="_modules/spinup/utils/mpi_tf.html#MpiAdamOptimizer.apply_gradients"><span
                                                                    class="viewcode-link">[source]</span></a><a
                                                                class="headerlink"
                                                                href="#spinup.utils.mpi_tf.MpiAdamOptimizer.apply_gradients"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p>Same as normal apply_gradients, except sync params after
                                                                update.</p>
                                                        </dd>
                                                    </dl>

                                                    <dl class="method">
                                                        <dt id="spinup.utils.mpi_tf.MpiAdamOptimizer.compute_gradients">
                                                            <code class="descname">compute_gradients</code><span
                                                                class="sig-paren">(</span><em>loss</em>,
                                                            <em>var_list</em>, <em>**kwargs</em><span
                                                                class="sig-paren">)</span><a class="reference internal"
                                                                href="_modules/spinup/utils/mpi_tf.html#MpiAdamOptimizer.compute_gradients"><span
                                                                    class="viewcode-link">[source]</span></a><a
                                                                class="headerlink"
                                                                href="#spinup.utils.mpi_tf.MpiAdamOptimizer.compute_gradients"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p>Same as normal compute_gradients, except average grads
                                                                over processes.</p>
                                                        </dd>
                                                    </dl>

                                                </dd>
                                            </dl>

                                            <dl class="function">
                                                <dt id="spinup.utils.mpi_tf.sync_all_params">
                                                    <code class="descclassname">spinup.utils.mpi_tf.</code><code
                                                        class="descname">sync_all_params</code><span
                                                        class="sig-paren">(</span><span class="sig-paren">)</span><a
                                                        class="reference internal"
                                                        href="_modules/spinup/utils/mpi_tf.html#sync_all_params"><span
                                                            class="viewcode-link">[source]</span></a><a
                                                        class="headerlink" href="#spinup.utils.mpi_tf.sync_all_params"
                                                        title="Permalink to this definition">¶</a>
                                                </dt>
                                                <dd>
                                                    <p>Sync all tf variables across MPI processes.</p>
                                                </dd>
                                            </dl>

                                        </div>
                                    </div>
                                    <span id="document-utils/run_utils"></span>
                                    <div class="section" id="run-utils">
                                        <h2><a class="toc-backref" href="#id1">Run Utils</a><a class="headerlink"
                                                href="#run-utils" title="Permalink to this headline">¶</a></h2>
                                        <div class="contents topic" id="table-of-contents">
                                            <p class="topic-title first">Table of Contents</p>
                                            <ul class="simple">
                                                <li><a class="reference internal" href="#run-utils" id="id1">Run
                                                        Utils</a>
                                                    <ul>
                                                        <li><a class="reference internal" href="#experimentgrid"
                                                                id="id2">ExperimentGrid</a></li>
                                                        <li><a class="reference internal" href="#calling-experiments"
                                                                id="id3">Calling Experiments</a></li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </div>
                                        <div class="section" id="experimentgrid">
                                            <h3><a class="toc-backref" href="#id2">ExperimentGrid</a><a
                                                    class="headerlink" href="#experimentgrid"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <p>Spinning Up ships with a tool called ExperimentGrid for making
                                                hyperparameter ablations easier. This is based on (but simpler than) <a
                                                    class="reference external"
                                                    href="https://github.com/rll/rllab/blob/master/rllab/misc/instrument.py#L173">the
                                                    rllab tool</a> called VariantGenerator.</p>
                                            <dl class="class">
                                                <dt id="spinup.utils.run_utils.ExperimentGrid">
                                                    <em class="property">class </em><code
                                                        class="descclassname">spinup.utils.run_utils.</code><code
                                                        class="descname">ExperimentGrid</code><span
                                                        class="sig-paren">(</span><em>name=''</em><span
                                                        class="sig-paren">)</span><a class="reference internal"
                                                        href="_modules/spinup/utils/run_utils.html#ExperimentGrid"><span
                                                            class="viewcode-link">[source]</span></a><a
                                                        class="headerlink" href="#spinup.utils.run_utils.ExperimentGrid"
                                                        title="Permalink to this definition">¶</a>
                                                </dt>
                                                <dd>
                                                    <p>Tool for running many experiments given hyperparameter ranges.
                                                    </p>
                                                    <dl class="method">
                                                        <dt id="spinup.utils.run_utils.ExperimentGrid.add">
                                                            <code class="descname">add</code><span
                                                                class="sig-paren">(</span><em>key</em>, <em>vals</em>,
                                                            <em>shorthand=None</em>, <em>in_name=False</em><span
                                                                class="sig-paren">)</span><a class="reference internal"
                                                                href="_modules/spinup/utils/run_utils.html#ExperimentGrid.add"><span
                                                                    class="viewcode-link">[source]</span></a><a
                                                                class="headerlink"
                                                                href="#spinup.utils.run_utils.ExperimentGrid.add"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p>Add a parameter (key) to the grid config, with potential
                                                                values (vals).</p>
                                                            <p>By default, if a shorthand isn&#8217;t given, one is
                                                                automatically generated
                                                                from the key using the first three letters of each
                                                                colon-separated
                                                                term. To disable this behavior, change <code
                                                                    class="docutils literal"><span class="pre">DEFAULT_SHORTHAND</span></code>
                                                                in the
                                                                <code
                                                                    class="docutils literal"><span class="pre">spinup/user_config.py</span></code>
                                                                file to <code
                                                                    class="docutils literal"><span class="pre">False</span></code>.
                                                            </p>
                                                            <table class="docutils field-list" frame="void"
                                                                rules="none">
                                                                <col class="field-name" />
                                                                <col class="field-body" />
                                                                <tbody valign="top">
                                                                    <tr class="field-odd field">
                                                                        <th class="field-name">Parameters:</th>
                                                                        <td class="field-body">
                                                                            <ul class="first last simple">
                                                                                <li><strong>key</strong>
                                                                                    (<em>string</em>) &#8211; Name of
                                                                                    parameter.</li>
                                                                                <li><strong>vals</strong>
                                                                                    (<em>value</em><em> or </em><em>list
                                                                                        of values</em>) &#8211; Allowed
                                                                                    values of parameter.</li>
                                                                                <li><strong>shorthand</strong>
                                                                                    (<em>string</em>) &#8211; Optional,
                                                                                    shortened name of parameter. For
                                                                                    example, maybe the parameter <code
                                                                                        class="docutils literal"><span class="pre">steps_per_epoch</span></code>
                                                                                    is shortened
                                                                                    to <code
                                                                                        class="docutils literal"><span class="pre">steps</span></code>.
                                                                                </li>
                                                                                <li><strong>in_name</strong>
                                                                                    (<em>bool</em>) &#8211; When
                                                                                    constructing variant names, force
                                                                                    the
                                                                                    inclusion of this parameter into the
                                                                                    name.</li>
                                                                            </ul>
                                                                        </td>
                                                                    </tr>
                                                                </tbody>
                                                            </table>
                                                        </dd>
                                                    </dl>

                                                    <dl class="method">
                                                        <dt id="spinup.utils.run_utils.ExperimentGrid.print">
                                                            <code class="descname">print</code><span
                                                                class="sig-paren">(</span><span
                                                                class="sig-paren">)</span><a class="reference internal"
                                                                href="_modules/spinup/utils/run_utils.html#ExperimentGrid.print"><span
                                                                    class="viewcode-link">[source]</span></a><a
                                                                class="headerlink"
                                                                href="#spinup.utils.run_utils.ExperimentGrid.print"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p>Print a helpful report about the experiment grid.</p>
                                                        </dd>
                                                    </dl>

                                                    <dl class="method">
                                                        <dt id="spinup.utils.run_utils.ExperimentGrid.run">
                                                            <code class="descname">run</code><span
                                                                class="sig-paren">(</span><em>thunk</em>,
                                                            <em>num_cpu=1</em>, <em>data_dir=None</em>,
                                                            <em>datestamp=False</em><span class="sig-paren">)</span><a
                                                                class="reference internal"
                                                                href="_modules/spinup/utils/run_utils.html#ExperimentGrid.run"><span
                                                                    class="viewcode-link">[source]</span></a><a
                                                                class="headerlink"
                                                                href="#spinup.utils.run_utils.ExperimentGrid.run"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p>Run each variant in the grid with function
                                                                &#8216;thunk&#8217;.</p>
                                                            <p>Note: &#8216;thunk&#8217; must be either a callable
                                                                function, or a string. If it is
                                                                a string, it must be the name of a parameter whose
                                                                values are all
                                                                callable functions.</p>
                                                            <p>Uses <code
                                                                    class="docutils literal"><span class="pre">call_experiment</span></code>
                                                                to actually launch each experiment, and gives
                                                                each variant a name using <code
                                                                    class="docutils literal"><span class="pre">self.variant_name()</span></code>.
                                                            </p>
                                                            <p>Maintenance note: the args for ExperimentGrid.run should
                                                                track closely
                                                                to the args for call_experiment. However, <code
                                                                    class="docutils literal"><span class="pre">seed</span></code>
                                                                is omitted because
                                                                we presume the user may add it as a parameter in the
                                                                grid.</p>
                                                        </dd>
                                                    </dl>

                                                    <dl class="method">
                                                        <dt id="spinup.utils.run_utils.ExperimentGrid.variant_name">
                                                            <code class="descname">variant_name</code><span
                                                                class="sig-paren">(</span><em>variant</em><span
                                                                class="sig-paren">)</span><a class="reference internal"
                                                                href="_modules/spinup/utils/run_utils.html#ExperimentGrid.variant_name"><span
                                                                    class="viewcode-link">[source]</span></a><a
                                                                class="headerlink"
                                                                href="#spinup.utils.run_utils.ExperimentGrid.variant_name"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p>Given a variant (dict of valid param/value pairs), make
                                                                an exp_name.</p>
                                                            <p>A variant&#8217;s name is constructed as the grid name
                                                                (if you&#8217;ve given it
                                                                one), plus param names (or shorthands if available) and
                                                                values
                                                                separated by underscores.</p>
                                                            <p>Note: if <code
                                                                    class="docutils literal"><span class="pre">seed</span></code>
                                                                is a parameter, it is not included in the name.</p>
                                                        </dd>
                                                    </dl>

                                                    <dl class="method">
                                                        <dt id="spinup.utils.run_utils.ExperimentGrid.variants">
                                                            <code class="descname">variants</code><span
                                                                class="sig-paren">(</span><span
                                                                class="sig-paren">)</span><a class="reference internal"
                                                                href="_modules/spinup/utils/run_utils.html#ExperimentGrid.variants"><span
                                                                    class="viewcode-link">[source]</span></a><a
                                                                class="headerlink"
                                                                href="#spinup.utils.run_utils.ExperimentGrid.variants"
                                                                title="Permalink to this definition">¶</a>
                                                        </dt>
                                                        <dd>
                                                            <p>Makes a list of dicts, where each dict is a valid config
                                                                in the grid.</p>
                                                            <p>There is special handling for variant parameters whose
                                                                names take
                                                                the form</p>
                                                            <blockquote>
                                                                <div><code
                                                                        class="docutils literal"><span class="pre">'full:param:name'</span></code>.
                                                                </div>
                                                            </blockquote>
                                                            <p>The colons are taken to indicate that these parameters
                                                                should
                                                                have a nested dict structure. eg, if there are two
                                                                params,</p>
                                                            <blockquote>
                                                                <div>
                                                                    <table border="1" class="docutils">
                                                                        <colgroup>
                                                                            <col width="87%" />
                                                                            <col width="13%" />
                                                                        </colgroup>
                                                                        <thead valign="bottom">
                                                                            <tr class="row-odd">
                                                                                <th class="head">Key</th>
                                                                                <th class="head">Val</th>
                                                                            </tr>
                                                                        </thead>
                                                                        <tbody valign="top">
                                                                            <tr class="row-even">
                                                                                <td><code
                                                                                        class="docutils literal"><span class="pre">'base:param:a'</span></code>
                                                                                </td>
                                                                                <td>1</td>
                                                                            </tr>
                                                                            <tr class="row-odd">
                                                                                <td><code
                                                                                        class="docutils literal"><span class="pre">'base:param:b'</span></code>
                                                                                </td>
                                                                                <td>2</td>
                                                                            </tr>
                                                                        </tbody>
                                                                    </table>
                                                                </div>
                                                            </blockquote>
                                                            <p>the variant dict will have the structure</p>
                                                            <div class="highlight-default">
                                                                <div class="highlight">
                                                                    <pre><span></span><span class="n">variant</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">base</span><span class="p">:</span> <span class="p">{</span>
        <span class="n">param</span> <span class="p">:</span> <span class="p">{</span>
            <span class="n">a</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">b</span> <span class="p">:</span> <span class="mi">2</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
</pre>
                                                                </div>
                                                            </div>
                                                        </dd>
                                                    </dl>

                                                </dd>
                                            </dl>

                                        </div>
                                        <div class="section" id="calling-experiments">
                                            <h3><a class="toc-backref" href="#id3">Calling Experiments</a><a
                                                    class="headerlink" href="#calling-experiments"
                                                    title="Permalink to this headline">¶</a></h3>
                                            <dl class="function">
                                                <dt id="spinup.utils.run_utils.call_experiment">
                                                    <code class="descclassname">spinup.utils.run_utils.</code><code
                                                        class="descname">call_experiment</code><span
                                                        class="sig-paren">(</span><em>exp_name</em>, <em>thunk</em>,
                                                    <em>seed=0</em>, <em>num_cpu=1</em>, <em>data_dir=None</em>,
                                                    <em>datestamp=False</em>, <em>**kwargs</em><span
                                                        class="sig-paren">)</span><a class="reference internal"
                                                        href="_modules/spinup/utils/run_utils.html#call_experiment"><span
                                                            class="viewcode-link">[source]</span></a><a
                                                        class="headerlink"
                                                        href="#spinup.utils.run_utils.call_experiment"
                                                        title="Permalink to this definition">¶</a>
                                                </dt>
                                                <dd>
                                                    <p>Run a function (thunk) with hyperparameters (kwargs), plus
                                                        configuration.</p>
                                                    <p>This wraps a few pieces of functionality which are useful when
                                                        you want
                                                        to run many experiments in sequence, including logger
                                                        configuration and
                                                        splitting into multiple processes for MPI.</p>
                                                    <p>There&#8217;s also a SpinningUp-specific convenience added into
                                                        executing the
                                                        thunk: if <code
                                                            class="docutils literal"><span class="pre">env_name</span></code>
                                                        is one of the kwargs passed to call_experiment, it&#8217;s
                                                        assumed that the thunk accepts an argument called <code
                                                            class="docutils literal"><span class="pre">env_fn</span></code>,
                                                        and that
                                                        the <code
                                                            class="docutils literal"><span class="pre">env_fn</span></code>
                                                        should make a gym environment with the given <code
                                                            class="docutils literal"><span class="pre">env_name</span></code>.
                                                    </p>
                                                    <p>The way the experiment is actually executed is slightly
                                                        complicated: the
                                                        function is serialized to a string, and then <code
                                                            class="docutils literal"><span class="pre">run_entrypoint.py</span></code>
                                                        is
                                                        executed in a subprocess call with the serialized string as an
                                                        argument.
                                                        <code
                                                            class="docutils literal"><span class="pre">run_entrypoint.py</span></code>
                                                        unserializes the function call and executes it.
                                                        We choose to do it this way&#8212;instead of just calling the
                                                        function
                                                        directly here&#8212;to avoid leaking state between successive
                                                        experiments.
                                                    </p>
                                                    <table class="docutils field-list" frame="void" rules="none">
                                                        <col class="field-name" />
                                                        <col class="field-body" />
                                                        <tbody valign="top">
                                                            <tr class="field-odd field">
                                                                <th class="field-name">Parameters:</th>
                                                                <td class="field-body">
                                                                    <ul class="first last simple">
                                                                        <li><strong>exp_name</strong> (<em>string</em>)
                                                                            &#8211; Name for experiment.</li>
                                                                        <li><strong>thunk</strong> (<em>callable</em>)
                                                                            &#8211; A python function.</li>
                                                                        <li><strong>seed</strong> (<em>int</em>) &#8211;
                                                                            Seed for random number generators.</li>
                                                                        <li><strong>num_cpu</strong> (<em>int</em>)
                                                                            &#8211; Number of MPI processes to split
                                                                            into. Also accepts
                                                                            &#8216;auto&#8217;, which will set up as
                                                                            many procs as there are cpus on
                                                                            the machine.</li>
                                                                        <li><strong>data_dir</strong> (<em>string</em>)
                                                                            &#8211; Used in configuring the logger, to
                                                                            decide where
                                                                            to store experiment results. Note: if left
                                                                            as None, data_dir will
                                                                            default to <code
                                                                                class="docutils literal"><span class="pre">DEFAULT_DATA_DIR</span></code>
                                                                            from <code
                                                                                class="docutils literal"><span class="pre">spinup/user_config.py</span></code>.
                                                                        </li>
                                                                        <li><strong>**kwargs</strong> &#8211; All kwargs
                                                                            to pass to thunk.</li>
                                                                    </ul>
                                                                </td>
                                                            </tr>
                                                        </tbody>
                                                    </table>
                                                </dd>
                                            </dl>

                                            <dl class="function">
                                                <dt id="spinup.utils.run_utils.setup_logger_kwargs">
                                                    <code class="descclassname">spinup.utils.run_utils.</code><code
                                                        class="descname">setup_logger_kwargs</code><span
                                                        class="sig-paren">(</span><em>exp_name</em>, <em>seed=None</em>,
                                                    <em>data_dir=None</em>, <em>datestamp=False</em><span
                                                        class="sig-paren">)</span><a class="reference internal"
                                                        href="_modules/spinup/utils/run_utils.html#setup_logger_kwargs"><span
                                                            class="viewcode-link">[source]</span></a><a
                                                        class="headerlink"
                                                        href="#spinup.utils.run_utils.setup_logger_kwargs"
                                                        title="Permalink to this definition">¶</a>
                                                </dt>
                                                <dd>
                                                    <p>Sets up the output_dir for a logger and returns a dict for logger
                                                        kwargs.</p>
                                                    <p>If no seed is given and datestamp is false,</p>
                                                    <div class="highlight-default">
                                                        <div class="highlight">
                                                            <pre><span></span><span class="n">output_dir</span> <span class="o">=</span> <span class="n">data_dir</span><span class="o">/</span><span class="n">exp_name</span>
</pre>
                                                        </div>
                                                    </div>
                                                    <p>If a seed is given and datestamp is false,</p>
                                                    <div class="highlight-default">
                                                        <div class="highlight">
                                                            <pre><span></span><span class="n">output_dir</span> <span class="o">=</span> <span class="n">data_dir</span><span class="o">/</span><span class="n">exp_name</span><span class="o">/</span><span class="n">exp_name_s</span><span class="p">[</span><span class="n">seed</span><span class="p">]</span>
</pre>
                                                        </div>
                                                    </div>
                                                    <p>If datestamp is true, amend to</p>
                                                    <div class="highlight-default">
                                                        <div class="highlight">
                                                            <pre><span></span><span class="n">output_dir</span> <span class="o">=</span> <span class="n">data_dir</span><span class="o">/</span><span class="n">YY</span><span class="o">-</span><span class="n">MM</span><span class="o">-</span><span class="n">DD_exp_name</span><span class="o">/</span><span class="n">YY</span><span class="o">-</span><span class="n">MM</span><span class="o">-</span><span class="n">DD_HH</span><span class="o">-</span><span class="n">MM</span><span class="o">-</span><span class="n">SS_exp_name_s</span><span class="p">[</span><span class="n">seed</span><span class="p">]</span>
</pre>
                                                        </div>
                                                    </div>
                                                    <p>You can force datestamp=True by setting <code
                                                            class="docutils literal"><span class="pre">FORCE_DATESTAMP=True</span></code>
                                                        in
                                                        <code
                                                            class="docutils literal"><span class="pre">spinup/user_config.py</span></code>.
                                                    </p>
                                                    <table class="docutils field-list" frame="void" rules="none">
                                                        <col class="field-name" />
                                                        <col class="field-body" />
                                                        <tbody valign="top">
                                                            <tr class="field-odd field">
                                                                <th class="field-name">Parameters:</th>
                                                                <td class="field-body">
                                                                    <ul class="first simple">
                                                                        <li><strong>exp_name</strong> (<em>string</em>)
                                                                            &#8211; Name for experiment.</li>
                                                                        <li><strong>seed</strong> (<em>int</em>) &#8211;
                                                                            Seed for random number generators used by
                                                                            experiment.</li>
                                                                        <li><strong>data_dir</strong> (<em>string</em>)
                                                                            &#8211; Path to folder where results should
                                                                            be saved.
                                                                            Default is the <code
                                                                                class="docutils literal"><span class="pre">DEFAULT_DATA_DIR</span></code>
                                                                            in <code
                                                                                class="docutils literal"><span class="pre">spinup/user_config.py</span></code>.
                                                                        </li>
                                                                        <li><strong>datestamp</strong> (<em>bool</em>)
                                                                            &#8211; Whether to include a date and
                                                                            timestamp in the
                                                                            name of the save directory.</li>
                                                                    </ul>
                                                                </td>
                                                            </tr>
                                                            <tr class="field-even field">
                                                                <th class="field-name">Returns:</th>
                                                                <td class="field-body">
                                                                    <p class="first last">logger_kwargs, a dict
                                                                        containing output_dir and exp_name.</p>
                                                                </td>
                                                            </tr>
                                                        </tbody>
                                                    </table>
                                                </dd>
                                            </dl>

                                        </div>
                                    </div>
                                </div>
                                <div class="toctree-wrapper compound">
                                    <span id="document-etc/acknowledgements"></span>
                                    <div class="section" id="acknowledgements">
                                        <h2>Acknowledgements<a class="headerlink" href="#acknowledgements"
                                                title="Permalink to this headline">¶</a></h2>
                                        <p>We gratefully acknowledge the contributions of the many people who helped get
                                            this project off of the ground, including people who beta tested the
                                            software, gave feedback on the material, improved dependencies of Spinning
                                            Up code in service of this release, or otherwise supported the project.
                                            Given the number of people who were involved at various points, this list of
                                            names may not be exhaustive. (If you think you should have been listed here,
                                            please do not hesitate to reach out.)</p>
                                        <p>In no particular order, thank you Alex Ray, Amanda Askell, Ben Garfinkel,
                                            Christy Dennison, Coline Devin, Daniel Zeigler, Dylan Hadfield-Menell, Ge
                                            Yang, Greg Khan, Jack Clark, Jonas Rothfuss, Larissa Schiavo, Leandro
                                            Castelao, Lilian Weng, Maddie Hall, Matthias Plappert, Miles Brundage, Peter
                                            Zokhov, and Pieter Abbeel.</p>
                                        <p>We are also grateful to Pieter Abbeel&#8217;s group at Berkeley, and the
                                            Center for Human-Compatible AI, for giving feedback on presentations about
                                            Spinning Up.</p>
                                    </div>
                                    <span id="document-etc/author"></span>
                                    <div class="section" id="about-the-author">
                                        <h2>About the Author<a class="headerlink" href="#about-the-author"
                                                title="Permalink to this headline">¶</a></h2>
                                        <p>Spinning Up in Deep RL was primarily developed by Josh Achiam, a research
                                            scientist on the OpenAI Safety Team and PhD student at UC Berkeley advised
                                            by Pieter Abbeel. Josh studies topics related to safety in deep
                                            reinforcement learning, and has previously published work on <a
                                                class="reference external"
                                                href="https://arxiv.org/pdf/1705.10528.pdf">safe exploration</a>.</p>
                                    </div>
                                </div>
                            </div>
                            <div class="section" id="indices-and-tables">
                                <h1>Indices and tables<a class="headerlink" href="#indices-and-tables"
                                        title="Permalink to this headline">¶</a></h1>
                                <ul class="simple">
                                    <li><a class="reference internal" href="genindex.html"><span
                                                class="std std-ref">Index</span></a></li>
                                    <li><a class="reference internal" href="py-modindex.html"><span
                                                class="std std-ref">Module Index</span></a></li>
                                    <li><a class="reference internal" href="search.html"><span
                                                class="std std-ref">Search Page</span></a></li>
                                </ul>
                            </div>


                        </div>

                    </div>
                    <footer>


                        <hr />

                        <div role="contentinfo">
                            <p>
                                &copy; Copyright 2018, OpenAI.

                                <span class="commit">
                                    Revision <code>038665d6</code>.
                                </span>


                            </p>
                        </div>
                        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
                            href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a
                            href="https://readthedocs.org">Read the Docs</a>.

                    </footer>

                </div>
            </div>

        </section>

    </div>


    <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
        <span class="rst-current-version" data-toggle="rst-current-version">
            <span class="fa fa-book"> Read the Docs</span>
            v: latest
            <span class="fa fa-caret-down"></span>
        </span>
        <div class="rst-other-versions">
            <dl>
                <dt>Versions</dt>

                <dd><a href="/en/latest/">latest</a></dd>

            </dl>
            <dl>
                <dt>Downloads</dt>

                <dd><a href="//spinningup.openai.com/_/downloads/en/latest/pdf/">pdf</a></dd>

                <dd><a href="//spinningup.openai.com/_/downloads/en/latest/htmlzip/">html</a></dd>

                <dd><a href="//spinningup.openai.com/_/downloads/en/latest/epub/">epub</a></dd>

            </dl>
            <dl>
                <dt>On Read the Docs</dt>
                <dd>
                    <a
                        href="//readthedocs.com/projects/openai-education-spinningup/?fromdocs=openai-education-spinningup">Project
                        Home</a>
                </dd>
                <dd>
                    <a
                        href="//readthedocs.com/builds/openai-education-spinningup/?fromdocs=openai-education-spinningup">Builds</a>
                </dd>
            </dl>
            <hr />
            Free document hosting provided by <a href="http://www.readthedocs.org">Read the Docs</a>.

        </div>
    </div>





    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT: './',
            VERSION: '',
            LANGUAGE: 'en',
            COLLAPSE_INDEX: false,
            FILE_SUFFIX: '.html',
            HAS_SOURCE: true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>



    <script type="text/javascript" src="_static/js/theme.js"></script>

    <script type="text/javascript">
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>

</html>